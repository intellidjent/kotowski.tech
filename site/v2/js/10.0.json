[{"date":"Jan-01-1970","title":".DS_Store","fullPath":".DS_St","content":"\u0000\u0000\u0000\u0001Bud1\u0000\u0000\u0010\u0000\u0000\u0000\b\u0000\u0000\u0000\u0010\u0000\u0000\u0000\u0000�\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\b\u0000\u0000\u0000\b\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0002\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0000\u0001\u0000\u0000\u0010\u0000\u0000a\u0000s\u0000e\u0000sdscl\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0000\b\u0000r\u0000e\u0000l\u0000e\u0000a\u0000s\u0000e\u0000sdsclbool\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0000\u0000\u0000\u0000\b \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0000 \u0000\u0000\u0000\u0001\u0000\u0000\u0000@\u0000\u0000\u0000\u0001\u0000\u0000\u0000�\u0000\u0000\u0000\u0001\u0000\u0000\u0001\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0002\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0004\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0010\u0000\u0000\u0000\u0000\u0001\u0000\u0000 \u0000\u0000\u0000\u0000\u0001\u0000\u0000@\u0000\u0000\u0000\u0000\u0001\u0000\u0000�\u0000\u0000\u0000\u0000\u0001\u0000\u0001\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0002\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0004\u0000\u0000\u0000\u0000\u0000\u0001\u0000\b\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0010\u0000\u0000\u0000\u0000\u0000\u0001\u0000 \u0000\u0000\u0000\u0000\u0000\u0001\u0000@\u0000\u0000\u0000\u0000\u0000\u0001\u0000�\u0000\u0000\u0000\u0000\u0000\u0001\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0002\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0004\u0000\u0000\u0000\u0000\u0000\u0000\u0001\b\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0010\u0000\u0000\u0000\u0000\u0000\u0000\u0001 \u0000\u0000\u0000\u0000\u0000\u0000\u0001@\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0003\u0000\u0000\u0000\u0000\u0000\u0000\u0010 \u0000\u0000\u0000E\u0000\u0000\u0000�\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0004DSDB\u0000\u0000\u0000\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0002\u0000\u0000\u0000 \u0000\u0000\u0000`\u0000\u0000\u0000\u0001\u0000\u0000\u0000�\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0001\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0002\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0004\u0000\u0000\u0000\u0000\u0002\u0000\u0000\b\u0000\u0000\u0000\u0018\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0000 \u0000\u0000\u0000\u0000\u0001\u0000\u0000@\u0000\u0000\u0000\u0000\u0001\u0000\u0000�\u0000\u0000\u0000\u0000\u0001\u0000\u0001\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0002\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0004\u0000\u0000\u0000\u0000\u0000\u0001\u0000\b\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0010\u0000\u0000\u0000\u0000\u0000\u0001\u0000 \u0000\u0000\u0000\u0000\u0000\u0001\u0000@\u0000\u0000\u0000\u0000\u0000\u0001\u0000�\u0000\u0000\u0000\u0000\u0000\u0001\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0002\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0004\u0000\u0000\u0000\u0000\u0000\u0000\u0001\b\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0010\u0000\u0000\u0000\u0000\u0000\u0000\u0001 \u0000\u0000\u0000\u0000\u0000\u0000\u0001@\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000"},{"version":"10.0","date":"Oct-14-2019","title":"iac","name":"Intelligent Automation Cloud introduction","fullPath":"iac","content":" Check out an area of interest: Intelligent Automation Cloud Architecture; User Guide – basic WorkFusion concepts and how to guides; Automation Engineer Manual – complete guide for advanced users describing Bot tasks, automation, and robotics; Release Notes – the latest WorkFusion features with user friendly descriptions grouped by releases; WorkSpace – complete Worker and Requester guide for WorkFusion WorkSpace application that serves as a Worker portal; WorkFusion Infrastructure – WorkFusion installation, operational, and maintenance guides for IT OPS; or Search our Knowledge Base for the answer. "},{"date":"Jan-01-1970","title":".DS_Store","fullPath":"releases/.DS_St","content":"\u0000\u0000\u0000\u0001Bud1\u0000\u0000\u0010\u0000\u0000\u0000\b\u0000\u0000\u0000\u0010\u0000\u0000\u0000\u0000%\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0000\u0003\u0000s\u0000p\u0000adsclbool\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0002\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0000\u0001\u0000\u0000\u0010\u0000clbool\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0000\u0000\u0000\u0000\b \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0000 \u0000\u0000\u0000\u0001\u0000\u0000\u0000@\u0000\u0000\u0000\u0001\u0000\u0000\u0000�\u0000\u0000\u0000\u0001\u0000\u0000\u0001\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0002\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0004\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0010\u0000\u0000\u0000\u0000\u0001\u0000\u0000 \u0000\u0000\u0000\u0000\u0001\u0000\u0000@\u0000\u0000\u0000\u0000\u0001\u0000\u0000�\u0000\u0000\u0000\u0000\u0001\u0000\u0001\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0002\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0004\u0000\u0000\u0000\u0000\u0000\u0001\u0000\b\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0010\u0000\u0000\u0000\u0000\u0000\u0001\u0000 \u0000\u0000\u0000\u0000\u0000\u0001\u0000@\u0000\u0000\u0000\u0000\u0000\u0001\u0000�\u0000\u0000\u0000\u0000\u0000\u0001\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0002\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0004\u0000\u0000\u0000\u0000\u0000\u0000\u0001\b\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0010\u0000\u0000\u0000\u0000\u0000\u0000\u0001 \u0000\u0000\u0000\u0000\u0000\u0000\u0001@\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0003\u0000\u0000\u0000\u0000\u0000\u0000\u0010 \u0000\u0000\u0000E\u0000\u0000\u0000%\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0004DSDB\u0000\u0000\u0000\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0000`\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0000�\u0000\u0000\u0000\u0001\u0000\u0000\u0001\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0002\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0004\u0000\u0000\u0000\u0000\u0002\u0000\u0000\b\u0000\u0000\u0000\u0018\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0000 \u0000\u0000\u0000\u0000\u0001\u0000\u0000@\u0000\u0000\u0000\u0000\u0001\u0000\u0000�\u0000\u0000\u0000\u0000\u0001\u0000\u0001\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0002\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0004\u0000\u0000\u0000\u0000\u0000\u0001\u0000\b\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0010\u0000\u0000\u0000\u0000\u0000\u0001\u0000 \u0000\u0000\u0000\u0000\u0000\u0001\u0000@\u0000\u0000\u0000\u0000\u0000\u0001\u0000�\u0000\u0000\u0000\u0000\u0000\u0001\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0002\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0004\u0000\u0000\u0000\u0000\u0000\u0000\u0001\b\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0010\u0000\u0000\u0000\u0000\u0000\u0000\u0001 \u0000\u0000\u0000\u0000\u0000\u0000\u0001@\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000"},{"version":"10.0","date":"Jul-16-2019","title":"lumen-9-2","name":"SPA Lumen 9.2","fullPath":"releases/lumen/lumen-9-2","content":" WorkFusion SPA 9.2 release concludes 2018 themes of the major Lumen (9.0) release, further enhancing performance, security and experience for business users' AI powered automation journey. The AutoML search engine 2.0 reduces model training time from 2 hours (in 9.1 release) down to only 15 minutes — one more step closer to real time training of models created to address the ongoing changes in data that modern enterprises must keep up with. Security is another big theme of the 9.2 release, with filters being added to the Secrets Vault, as well as more tools available to analyze code and systems for vulnerabilities. Further, Dashboards and Analytics are now optimized for faster updates and loading, providing users with even quicker access to the insights they are seeking. Additionally, more capabilities were added and improved, such as Studio enhancements, OCR for Windows, and more. Control Tower Filters for Secrets Vault Previously, all Secrets Vault entries were visible to all users in Control Tower. For security reasons, Secrets Vault now offers standard SPA filtering, already used for all entities in Control Tower. Users can create their filter rule based on Alias (name of the secret entry) and limit visibility of secret entries that fall under this rule to certain user groups. Filters are very helpful when multiple teams use the same Control Tower environment. Now, secret entries are available only to users authorized to access them for business purposes, and hidden from the others. This helps to protect secure information and control its distribution. In addition to this, users can filter entries to narrow down the search for a required entry in a large list. Learn more about Using Filters for Collaborative Work. Secrets Vault Permissions New user permissions were introduced to allow granular access to secret entries. These permissions are: View Secrets Vault Aliases Manage Secrets Vault Aliases View Secrets Vault Entries > Note: View Secrets Vault Entries permission is not granted to anyone by default and needs to be explicitly assigned by the SPA administrator. Multi level user permissions enable secure access to Secrets Vault and ensure appropriate handling of sensitive information. Learn more about Secrets Vault and Role Management. Activity Log To conduct transparent security audits of user actions, new types of activity events are now collected and displayed in Activity Log. These events include the following actions: Change of user groups in the system Change of Secure Vault entries Change of user password Beginning with this release, IP address is logged for every user action. Learn more about Activity Log. AI AutoML Search Engine 2.0 Sticking to one of the major goals of 9.2—namely, performance—AutoML models were improved by applying new advanced search algorithms. This significantly reduces the time to search the best components combination — from 2 hours to just 15 minutes per field without compromising the quality. Given that frequent changes of input require constant re training, this optimization is a major step toward real time training. New OOTB Post Processors AutoML SDK extended its range of post processing capabilities by adding a number of out of the box normalizers. These are the commonly used normalizers added in 9.2: Normalizers for handling uppercase and lowercase, as well as capitalization: LowerCaseNormalizer, UpperCaseNormalizer, CapitalizeNormalizer. For example, the following transformation: \"caliFornia\" → \"California\" can be performed by applying two normalizers that handle lowercase and capitalization. Normalizers for replacing, trimming and removing characters: PatternReplacerNormalizer, TrimmerNormalizer, HtmlSpecialCharactersRemoverNormalizer. For example, \"100€\" → \"100\" transformation by applying a normalizer that removes special HTML characters. For detailed information, refer to Normalizer Javadocs. Back to top Robotics WorkFusion Studio Studio enhancements aim for more stable and convenient work both for business users and developers. Object recording covers more cases from user feedback, including a bunch of improvements when inspecting recording in Windows (cmd, Start Menu components, Calculator) and a number of applications (Excel, Outlook, SwingSet, Total Commander). To avoid possible confusion, disabled actions with no functionality were removed from the Actions Library. See more in Actions Library for WorkFusion Studio. Excel Automation Excel actions were improved, thus broadening the capabilities of Excel scripts execution. The Get Cell Value action for Excel is enabled to work for calculated values (formulas). Numeric value can be set into an Excel cell in Number format. Fixed number variables for Excel actions are parsed in Big Decimal format (not in Scientific as before). See more about Excel automation in Actions Library for WorkFusion Studio. Inspector Inspector was improved for more effective work by simplifying and accelerating the exploring properties and values of the target objects. Inspector supports collections of selectors corresponding to the search criteria. The feature is especially useful when inspecting similar elements in tables or lists for further automation. The Selector field now has Back \"\" arrows to switch between elements and choose the required selector. Inspector guarantees selector uniqueness to ensure the selected UI control is clicked during the script execution. When selecting the element, the user explicitly sees its unique selector with a \"1 of 1\" match. CSS selector is introduced as the most brief and clearly expressed one. The Type filter enables the user to choose between either an Object or a CSS selector. When selecting the needed type, the selector string is rebuilt to match the given notation. In case an \"Access denied\" error is detected (for example, a process is run under the Admin permissions), the user is notified to either run Inspector as Administrator or disable the user account control to continue inspecting the selected application. See more about Inspector. Robotics API API improvements contribute to easier and faster work with complex use cases. When scripting in the Code perspective, the user can find window handles by their attributes: class, title, etc. Three new attributes have been introduced — parentPID, processName, commandLine. With this improvement, script execution accelerates for cases with multiple windows: 18 times faster for 8 windows and 36 times faster for 16 windows. See code examples in Finding Window Handles by Criteria. Process ID can be identified when starting a program. That makes it handy in case the user needs to know and save the ID of the process created by the open command. For example, to detect all child windows of the opened application and perform further actions with them. See more in Identifying Process ID. Clicking on a UI component with offset facilitates automating complex controls like trees or tables with checkboxes, where a click is done not only in the center of the element, but somewhere else (say, the top left corner or custom offset). The feature is valid for any mouse click (double, triple, wheel) with offset counted from top left and works for desktop automation only. Learn more about Clicking on Element with Offset. ABBYY OCR Checkmarks API for Windows Previously OCR capabilities in SPA were available only on Linux machines. Now, ABBYY OCR on Windows is deployed in SPA with a full set of features available on Linux: REST API endpoints, Barcode Recognition, etc. Besides this, OCR on Windows offers Optical Mark Recognition, specifically designed for checkmarks. OCR on Windows machines has a robust infrastructure with a possibility of horizontal scaling for OCR throughput increase. Learn How To Recognize Checkmarks and more about OCR on Windows and Linux. Other Improvements RPA Installation on 32 bit RPA Server can now be installed on Windows 7 32 bit environment. Only manual installation is supported. See more in the RPA Server Installation on Windows 7 32 bit guide. AutoML SDK Wizard AutoML SDK Wizard enhancements include: Display of model default values for secured repository. Ability to save Server Profile if a user is offline. Improved wizard performance with local archetype catalog. Switching to Java perspective by default. Configurable Bot Manager Timeout A new feature enables to modify Bot Manager timeout. It is used to provide more time for execution, for example, when the number of threads for a business process step is bigger than the number of available bots. See how to configure Bot Manager timeout in Bot Manager. Drag and Drop Automation Automating web applications is made easier with drag and drop actions. For the feature to work as expected, it is recommended to edit code in the bot task. See code workaround in Automating Drag and Drop. Back to top Installation Centralized Log Aggregation Components of the system generate a multitude of logs distributed over different sources and locations. To analyze and solve problems in the most effective and timely way, these logs need to be gathered in one place. Previously, users had to determine the necessary log sources, then manually export selected logs one by one. Now execution of simple command is enough to get relevant logs for any day, component or server. Filebeat is used for automatic log forwarding, and Logstash for log storage. All application logs from all servers of SPA environment (APP, DB, OCR, AutoML, RPA, BI) are seamlessly pushed to the APM server. Users can set log level for all SPA components before installation using a variable (prod or dev). Default log level is prod, unless set otherwise by the user before installation. Only error messages and warnings are displayed for the most heavy production log sources. Learn more about Working with Logs. Effective log aggregation helps to accelerate troubleshooting and reduces system's idle time during incidents from days to hours. Other Improvements Default nginx ports for DB, APP, OCR, and APM servers are changed from 8080 8443 to 80 443. Added a redirect from 80 to SSL port (by default 443). loader.sh script was improved to remove accidental spaces at the end of lines in properties files during the update of Secret Vault entries. Embedded Health Check business process now includes an additional check for AutoML components. See Environment Health Check section. It is now possible to update MySQL admin password and Java TrustStore password during the normal upgrade procedure. Multi user functionality is now available for Object Storage (Minio s3 emulator). Back to top Security WorkFusion offers an enterprise product that uses best in class techniques and frameworks to secure customers' sensitive data. In order to meet high industry standards, we've installed special tools to analyze code and systems for vulnerabilities, performed internal testing, engaged well respected companies in external testing and collected customer feedback. Security issues reports were aggregated and appropriately addressed. The 9.2 release includes the following security improvements: DataStore API is now covered by existing permissions: Manage Data Stores and View Data Stores. CSRF (Cross Site Request Forgery) Protection enabled for all pages in CT and WS. Fixed WS Cross site Scripting (XSS) vulnerability in use case name and reply to messages. Direct PM sign in is disabled. PM authentication now happens via CT login page by default. Learn more about Deprecations. The following new permissions were introduced: For Secrets Vault: View Secrets Vault Aliases Manage Secrets Vault Aliases View Secrets Vault Entries For Manual Task AutoML Settings Configuration: Manage AutoML Settings Analytics Optimized Analytics computing pipeline allows decreased visualization time so that a user's interaction with the data is quicker and more convenient. High load testing shows that the Analytics Dashboards page now loads 20% faster. Learn more about OOTB Dashboards. Back to top Deprecated Components and Functionality CT Master Password feature was removed from Control Tower as a part of Secrets Vault improvements. Replaced with filters and multi level permissions. Learn more about Secrets Vault. REMOVED CT and Analytics Manage Dashboard permission was removed because Analytics Screen introduced in 9.1 does not require such function. REMOVED Analytics Detailed WF Automation and Gold Extracted dashboards were removed as obsolete. REMOVED PM PM login page is removed. Authentication and PM sign in sign out happen only via CT login page. REMOVED RPA API getCompletePageAsZip(), getCompletePagesWithPaginationAsZip() methods were removed because they were never used by customers. enableTypeOnScreen() method was removed as it added instability to bot typing. REMOVED Bug Fixes and Improvements Control Tower and WorkSpace LDAP: Resolved an issue with users and group limit. LDAP: User can now log into the system after LDAP is switched off. Secrets Vault: Fixed objects special symbols to be readable in RPAx 2.0. Secrets Vault: Resolved an issue with master password removed. Secrets Vault: In case the same secret variable is used in several bot tasks, BP execution does not fail anymore. Secrets Vault: Disabled export of Secret type variables for security purposes. Labeling: Resolved an issue with saving date after validation if year is in the two digit format. Labeling: Multi value manual answers are displayed as expected. Labeling: Tagged text is restored correctly in a document with tags with content. Labeling: Nearby tags are displayed correctly. Manual Task Preview: Secured document now renders on preview. Control Tower: Migration works as expected if the bot config name is greater than 128 characters. WorkSpace: Resolved an issue with WS Preview option lost during MT import export. WorkSpace: Fixed an issue with a label in the footer (\"RPA Express\" as opposed to \"Smart Process Automation\" in the previous version). Workspace: Fixed an error on attempt to update worker's profile using a name (with slash). Workflow Designer: Resolved an exception issue when clicking on the Edit in WF Studio menu action. Permissions: Copy results to Datastore is now displayed under Manage Data Store permission only. Permissions: Create New button for Training Set is now displayed under Manage Data Store permission only. PM Commands API: Execution of commands using ID only is deprecated. Replaced with a new version with namespace. AutoML: Fixed 500 Error code on the Configure AutoML tab in case of failed response. AutoML: Configure AutoML settings are not reset after importing a package with a model that does not exist on S3. AutoML: Fixed an issue where incorrect values in Data Store dsautomationstat_ caused AutoML dashboards to crash in case some fields were not extracted by the model. AutoML: Fixed an issue where Workflow tab could sometimes be empty after Cognitive bot activation. AutoML: Fixed an issue when model training could not run if WF username contained a dot (.) character. AutoML: Fixed an issue where PresentationIEModel could not be saved in a manual task. AutoML: Training set wouldn't be displayed after clicking Save in the Configure AutoML tab. AutoML: Saving an extraction Business Process could cause a model execution failure due to a null value in the modelVersion column of the Manual Task. AutoML: Fixed an issue with errors written to logs after making changes to AutoML Training Configuration. AutoML: Fixed incorrect behavior of a Cognitive Bot with fields in case non existing values are passed as an input. AutoML: For Classification Models with AutoML Search Engine, \"0\" is set as numberOfParallelFields. AutoML: No indication was displayed when applying a Training Set in a Business Process. AutoML: Fixed an issue where it's impossible to rename or copy a Training Set with a short name on the Configure AutoML tab. AutoML: Saving a Manual Task from the Configure AutoML tab didn't change the task name. AutoML: Fixed an issue where it was possible to save a Manual Task with a missing workflow, in case the workflow was previously manually disabled. AutoML: Group ID was filled with the previous value after validation. AutoML: Fixed a 500 Error when opening a hidden task with a direct URL. AutoML: Training set Data Completeness always showed 0% for Classification tasks. Robotics Resolved an issue with closing driver processes on session shutdown. Resolved an issue with closeAllAnotherWindows closing all windows. Fixed getDriverLogs failing on all log types. Fixed an error when sending null to the clipboard. Resolved an issue with an incorrect log to pressEnter() method (node logs). Resolved an issue with UiElement.getRect(); not working in Chrome. Resolved an issue with the hardcoded path to the RPA install directory. Fixed an issue with selectAllTextAndCopy() method not working in cmd. RPA API: Resolved an issue with UIConditions methods not working. RPA API: Fixed sendKeys(CharSequence... charSequence) performance. RPA API: Resolved an issue with element.pressSpace() not working for Firefox. RPA API: Resolved an issue with failure on getting the source of a page. RPA API: Resolved an issue with Chrome driver not performing clear() operation when setting value. RPA API: Resolved an issue with element condition check. RPA API: Resolved an issue with matchRegexp always returning true. Studio: Resolved an issue with different library versions in Control Tower, Nexus, WorkFusion Studio. Studio: Imported bot task pushing back to Control Tower performs without failure. Studio: \"Show this window\" checkbox works as expected if the welcome page was closed. Studio: Fixed BadPositionCategoryException while templates formatting. Studio: Fixed an issue with failures when adding templates. Studio: Resolved an issue with exception when saved credentials tab is opened. Studio: Fixed a bot task launch in BCB project. Studio: Resolved an issue with opening config after import an existing project. Studio: Resolved an issue with invalid Groovy code after export to bot task. Studio: Resolved an issue with actual version displayed in UI. Studio: Resolved an issue with syntax highlighting after two closing squared brackets. Studio: Resolved an issue with navigation to error line after exception. Studio: Resolved an issue with Custom action execution at the end of the workflow in exported bot task. Studio: Replaced label for validation message (Bot Relay URL instead of RPA Hub URL). Studio: Resolved an issue with bot task not working with the robotics flow plugin. Inspector: Resolved an issue with connection to SwingSet applet. Inspector: Resolved an issue with SwingSet applet components in dropdowns not displayed. Inspector: Highlight of menu components perform as expected in certain applications (SwingSet, Total Commander). Inspector: Selectors for Outlook trees are defined correctly, i.e. different folders have different unique selectors. Inspector: Resolved an issue with non unique instance generation. Inspector: Fixed table cell coordinates enumeration. Inspector: Resolved a freezing issue when connecting to a not responding process. Recorder: Fixed drag and drop in actions flow. Recorder: Resolved an issue with mouse clicks on web elements for Internet Explorer and Firefox. Recorder: Fixed If Else condition to work correctly with decimal numbers. Recorder: Keystrokes actions are executed as expected on VM through RDP connection. Recorder: Resolved an issue with Get row > last row action being enabled to search for the last row that has values. Recorder: Resolved an issue with actions not recorded after resume. Recorder: Resolved an issue with playback window showing incorrect digits. Recorder: Improved bot execution for clicking actions (Start Menu components, File menu level 2 in Notepad , Calculator). Recorder: Fixed bot typing in Windows. Recorder: Resolved an issue with global timeout not used for Window action. OCR Resolved an issue with activated license not applied. OCR action is executed as expected if the capture region is blank. Platform type is added for generated OCR license request email to understand what type to trigger in Jenkins job. OCR API: GET processDocument is superseded with POST processDocument request method to its length limitations. Installation Fixed installation fail on pre check of SSL certificates when an intermediate certificate is used. Fixed installation fail on uploading secure properties to Secrets Vault when another instance of Java is already installed on the server. Now loader.sh script uses Java explicitly embedded in the installer. Fixed incorrect APP server certificate generation with generate certificates.sh script in case when s3hostname and apphostname are not in the same domain. Previously Marathon bound port for ZooKeper connectivity randomly. This resulted in installation fails in rare cases when the port was intended for other service, so the port is assigned statically now. Security: Fixed incorrect \"Content Security Policy\" HTTP header setting in nginx config file for S3 storage. Security: After installation is completed, the file with all sensitive settings (secrets.yml) is encrypted and saved to installation directory for later usage during subsequent product updates. Decryption password for this file is saved to VAULTPASSWORD.DELETEME file. Security: BI Server now uses read only user for connection to application databases. Security: Authentication to Secrets Vault with username password is impossible now which improves security. Only TLS based authentication is available. Security: wf sec storage directory is now cleaned up on OCR and VDS servers after installation. It is now only available on DB server. Stability: Embedded Ansible version is updated to from 2.6.4 to 2.6.6. Installation Speed: Installation speed of DB server is improved due to more accurate logic of task execution. AI AutoML SDK: Fixed an issue where OcrDateNormalizer worked incorrectly with dates having 2 digit years. AutoML SDK: AutoML SDK now doesn't require administrator rights to launch local model training on Windows. AutoML SDK: Fixed an issue where NbLanguageModelFE produced features with null in the feature name. AutoML SDK: Fixed an issue where a number of Feature Extractors produced features with the same feature name. Back to top Known Issues For a full list, refer to Version 9.2 Known Issues. "},{"version":"10.0","date":"May-28-2019","title":"spa-10-0-bug-fixes","name":"Bug Fixes & Improvements","fullPath":"releases/spa/spa-10-0-bug-fixes","content":" Control Tower TBD WorkSpace TBD RPA TBD WFBI TBD Platform Monitor TBD Back to SPA v10.0 Release Notes "},{"version":"10.0","date":"May-31-2019","title":"spa-10-0-deprecations","name":"Deprecations","fullPath":"releases/spa/spa-10-0-deprecations","content":" SPA v10.0 is a fresh start release with major architectural changes, so a number of legacy items and components that are no longer in focus, or not supported altogether, were removed. These items include WorkSpace Sandbox, all Smartcrowd integrations, WFBI service, and a number of minor leftovers scattered around the system. Here's the list of the most noticeable changes: MySQL, OCR MongoDB, WS PostgreSQL are consolidated in a single MS SQL database. Bot Sources are removed. Platform Monitor is no longer available being replaced by ELK and Bot Manager. Sandbox environment is gone since most of enterprise customers use a 3 stage environment stack: Dev, UAT, and Production. Below is the full list of deprecations grouped by component. Control Tower Control Tower MySQL, OCR MongoDB, WS PostgreSQL consolidated in a single MS SQL database. Bot Sources are removed. WFBI Service on APP server is removed from Control Tower and its logic for statistics is moved to AutoML service. Bot Configurations: Spring beans and Control Tower service beans are removed. Bot Configurations: Classes Constants are removed from packages. Basic authentication option in REST API is removed due to security concerns. Now authentication is form based (ligin and password) only. Learn more: 10.0 WorkFusion REST API WorkFusion REST API). userInternalCredentials object in Bot Tasks context is deprecated and contains only the username of the task author and empty password string. Learn more: Bot Tasks Context. Sandbox feature is removed. Confirmation dialog upon Run of the Business Process Manual Task in sandbox production is removed. Support of custom jars in Tomcat lib is removed. Qualifications Test Tab is removed. 'Support IE' inside Correlated Fields is removed. 'Bot assisted extraction for values close to accuracy threshold' functionality is removed. 'Online Learning' functionality support is removed. The following legacy items are removed from Control Tower, as SmartCrowds are not supported by WorkFusion since v8.5: Communication center Balances figures Mturk integration Bonus notifications Amazon notifications Idle predefined filter and settings. WorkSpace Sandbox feature is removed. google places scanner and google api client are removed. Learn more: google places scanner and google api client deprecation. Captcha during registration is removed. WorkSpace index page is eliminated (as referring to crowd system). Login page is used instead. Log4j logging service is replaced with Logback. The following legacy items are removed from WorkSpace, as SmartCrowds are not supported by WorkFusion since v8.5: Geo API functionality Elance, Facebook, Twitter, Clickworker, Upwork integration points Paypal integration Support of 'Payments' and earning related funionality RPA MongoDB and MySQL DB are removed and replaced with MS SQL. `` plugin is removed in favor of RPA API. nodeId attribute is removed from RoboticsFlowPlugin. Now, workers take jobs from the queue and there is always one node in a bot unit. For surface based automation, the in house solution is used, thus Sikuli Robot Framework library cannot be imported into scripts. Custom capabilities cannot be used anymore. Use ` attribute in ` plugin instead. Selenium RemoteDriver is removed. OCR GET processDocument request is removed in OCR API due to its length limitations. Since 9.2, it is superseded with POST processDocument request method. OCR plugin attribute use get process document method is not available anymore. If previously used, it should be removed. Flushing documents is turned off by default. To enable flushing, add the DflushEnabled=true parameter to the config file. See the link for instructions. Platform Monitor Platform Monitor is removed and replaced with Elasticsearch and Kibana stack (ELK). Commands API: Running commands using ID only is deprecated. Replaced with a new version with namespace. Back to SPA v10.0 Release Notes "},{"version":"10.0","date":"May-28-2019","title":"spa-10-0-known-issues","name":"Known Issues","fullPath":"releases/spa/spa-10-0-known-issues","content":" Control Tower TBD WorkSpace TBD RPA TBD WFBI TBD Platform Monitor TBD Back to SPA v10.0 Release Notes "},{"version":"10.0","date":"Jul-09-2019","title":"spa-10-0","name":"SPA 10.0","fullPath":"releases/spa/spa-10-0","content":" WorkFusion's latest 10.0 release is the first in a series of groundbreaking releases with a number of fundamental technology and UX overhauls. As such, it serves as a bridge to WorkFusion's Intelligent Automation Cloud, the only platform powered by artificial intelligence (AI) and built for the enterprise for simpler automation at scale. One of the biggest architectural changes in SPA 10.0 is the new bot orchestration platform designed to address current scalability limitations, as well as to reduce complexity and improve fault tolerance. Another major improvement is replacing 3 databases (MySQL, PostgreSQL, MongoDB) with a single unified database — MS SQL. This fundamental change allowed us to decrease maintenance complexity and enable consistent data backup, as well as to address security and high availability issues. Bot Execution Platform Imagine your business operations grow and expand and you need to process twice as many tasks as you currently have. How do you scale WorkFusion SPA What if you need to scale but the task load is hardly predictable and depends on multiple factors v9.x State In SPA v9.x, the correct answer is add another Control Tower with its infrastructure, services, etc. And then maybe another one. Now that you have 3 Control Towers, how do you manage, track and distribute the task load between them The answer is custom setup and orchestration. Needless to say, each Control Tower deployment is quite a time and resource consuming operation which requires custom actions for every specific use case. We knew, this could not last too long as such monolithic architecture was obviously too complex for large enterprise clients. v10.0 State In SPA v10.0, the very core concept of architecture has undergone a major re thinking. Introducing the new state of the art bot execution platform for a horizontally scalable, predictable, and fault tolerant system. Here's three immediate benefits you get: First off, now the platform dynamically scales the number of workers depending on the task load. This results in lean utilization of resources, which can be available for other tasks. No need to add more Control Tower to handle task load spikes. Second, tasks are performed by completely isolated workers. This allows to avoid situations when one failed task could lead to unexpected behavior or Control Tower being stuck, thus improving the overall system fault tolerance. Finally, we drew an explicit line between the application, such as Control Tower, and the execution platform itself. Now, the platform exposes API that triggers cluster execution, orchestration, monitoring, and analytics. Consolidated Database 9.x architecture utilized PostgreSQL, MySQL and MongoDB databases. SPA 10.0 is fundamentally rethinked to use a consolidated Microsoft SQL database. Let's put it simply: to install SPA 9.x, you had to have at least three different databases. In SPA 10.0, you only need one, and that is MS SQL — a solid enterprise solution. Microsoft SQL is an enterprise grade solution — comprehensive, well supported, reliable and trusted by customers across all industries. Consolidated Database introduction brings the following benefits: A single set of tools and single expertise. Less maintenance and troubleshooting effort for more effective support. Less potential failure points — easier identification of security issues. Consistent data backup in adherence to High Availability and Disaster Recovery standards. Monitoring SPA v10.0 changes the logs collecting approach by introducing a single point for logs and metrics aggregation — a well known enterprise grade solution ELK stack that replaces o ur good old Platform Monitor. Earlier, when investigating an incident, to collect logs and figure out what's going on in the system, you had to go to several servers for chunks of information and glue together pieces into a single picture. ELK is shipped as an out of the box tool and provides centralized logging. You can collect logs via Logstash and beautifully vizualize them on Kibana dashboards. The environment health metrics will also help you to understand the current state of your infrastructure and the possible bottlenecks that may affect the performance. ELK includes but is not limited to the following dashboards: System Overview to assess the CPU Memory Disk space load on servers. Application Logs to browse applications' logs. Environment Status to display the health status of the environment and list events that provide information about recent changes in health status. Robotics In SPA v10.0, we improved the governance and performance of our bots by applying the Bot Execution Platform architecture to manage RPA Windows workers. This change opened ways to the following improvements: faster RPA execution – the speed has been increased by 36% simplified bot installation and configuration enhanced security and stable performance due to execution of all task instructions on the same machine one unified method of task routing – Bot Fleets Based on results of 175 Bot runs. Tagging over Document One of the main stages of an automation use case development is dataset tagging. Preparing and labeling a high quality training set is time consuming. It equals to human worker teaching bot to mimic human work, that is, to recognize the values such as invoice number and date. Workers are accustomed to the real world documents structure, and therefore can perform swiftly and effectively on what's familiar to them: hard and digital copies, and scans. In SPA v9.x, workers tagged the plain text OCR result. It could look nothing like the original documents, because all information about sizes, fonts, positions and layouts was lost. It took significant effort to precisely identify the required values. This led to user mistakes and increased the labeling time. SPA v9.x: SPA v10.0 restores the natural workflow by introducing the Tagging over Document (TOD) experience. Worker views the original document with text values layered underneath. The scan is rotated and scaled, the image is optimized where needed for convenience. Word boundaries are highlighted and single value can be selected with one click. SPA v10.0: 1,100 experiments with 6 fields per document show the average tagging time decrease of 52%: TOD improves the overall user experience and dataset quality. It allows to double the speed of document tagging and cuts the time needed to teach a bot, thus streamlining automation use case delivery. Learn more about building a TOD use case: OOTB TOD Use Case. Analytics Capacity Dashboard New Capacity dashboard provides a quick and comprehensive overview of SPA components utilization for Business operations in a way that helps to understand how infrastructure resources are utilized. There are 4 dashboards explaining resource utilization data: CPU, Memory and Disk utilization over time answer if there are enough resources to dynamically scale to process peaks in SPA workload. Bot Execution Platform Agent chart visualizes how workers with different profiles (Control Tower and AutoML) share CPU and Memory resources. Link to be added. RPA Dashboard New RPA dashboard provides an overview for bots accuracy, performance and utilization, thus allowing to find ways for improvements at process or bot levels. Here are the key metrics: Volume, RPA Accuracy Rate, Bot Availability, Fleets Utilization, Bot Units Workload. Link to be added. AutoML Python Classifiers Python is widely considered as the preferred language for teaching and learning ML because of its relative simplicity. No wonder so many ML algorithms have powerful implementations in Python making it a go to language for this purpose. We couldn't stand aside, so starting with SPA v10.0, AutoML allows using a set of Python classifiers based on SciKit learn and MXNet that identify to which category an object belongs to. For more details, refer to Python Classifiers. Single Source Installation With each release we make steps to improving the process of installation and making it more simple and user oriented. While previous versions might have required dozens of operations to set up all the components, SPA v10.0 can be installed within several hours. The basic installation of all components is now performed 50% faster, compared to previous versions, and requires about 1.5 hours of your time. Such improvement is the result of switching to single source approach, which means you can install all Linux and Windows components from a single server by running a single script without logging in to each server and uploading packages to them. Additional Resources SPA 10.0 Deprecations SPA 10.0 Bug Fixes & Improvements SPA 10.0 Known Issues "},{"version":"10.0","date":"Oct-14-2019","title":"config-yml-options","name":"config.yml options","fullPath":"iac/admin/advanced/config-yml-options","content":" The following page describes the configuration options available during installation of WorkFusion SPA version 10.0. SPA installation begins with setting parameters in config.yml. In this configuration file you define: Service ports Runtime users Home directory Execution parameters Sensitive data, such as database credentials Other essential options Complete the file on your Integration Server. If you install SPA in the recommended way, config.yml will be automatically delivered to all required servers during installation. If you are using Legacy setup, you must deliver it to all the other servers manually. :::note All further customizations, such as adding new servers or configuring LDAP, and post installation upgrades must be performed using this file. Remember to save it after the SPA installation. ::: File format The configuration file uses the YAML format. This file contains a few common parameters shared by all servers. For example, wf_user (runtime user for all software), install_dir (installation directory), which already have reasonable defaults, while the other must be defined by the end user. The configuration file is thoroughly commented to explain each of the parameters. A detailed description of all parameters is provided further. Example of config.yml At least one space should be present between each 'key: value' It is strongly adviced to enclose values with specisl symbols into single quotes PASSWORDS POLICY must be not shorter that 6 symbols and not longer than 20 symbols; must contain at least one uppercase character A Z ; must contain at least one lowercase character a z ; must contain at least one numeric character 0 9 ; must contain at least one of the following special symbols unless limitations are explicitly described in comments: @ *():,.;} must NOT contain the following symbols: \"! &'{ `% must NOT contain spaces Ansible vault password config.yml will be encrypted with this password after encrypt command Change it! ansiblevaultpassword: 'WorkFusion!' Ansible Linux user(used in inventory) linuxinstallationuser: 'ec2 user' Access and Secret keys for S3 storage s3accesskey: 'JOZXPHL1ZC8WXZAJXNU7' Special symbols are not allowed. 3 20 characters length. s3secretkey: 'qAEYOVLOViPjxk3Lbuj0wrDztf8fGJrqMb4Obw==' Allowed special symbols: '=', ' '. 8 40 characters length. SSL Certificates are used to secure Workfusion SPA services communication Put your Trusted Certificates and key files to certificates directory. Otherwise, Self Signed certificates will be generated automatically Provide password for autogenerated ca.key file. This setting will be used only when you're using self signed certs generated by \"install.sh certs generate\" command cakeypass: 'aHja&a T8c' Set password for built in 'cacerts' java truststore on Linux servers. javacacertstruststore_pass: '@ *():,.;} Atd7' Vault Certificates These certificates are generated using \". install.sh cert_vault generate\" command They are used by Workfusion applications for authentication and access control into Vault (Secure Storage) vaultadmincertpass: 'PQS*jzJa9%' password for vaultadmin.p12 certificate vaultclientcertpass: 'a2J@Kn5llk' password for vaultworkfusion.p12 certificate Linux runtime user and group. Must be the same for all linux servers wf_user: wfuser wf_group: wfuser install_dir: opt workfusion Target installation directory, should not contain dot(s) Windows installation properties Existing user on OCRWin server. It will be used ONLY if ocr_platform is windows. Used for OCRWin installation. Should be member of Administrators group. ocrwin_user: 'ec2 user' ocrwin_pass: 'jEn125us ' ocrwininstalldir: 'C: workfusion' Target installation directory for OCRWin on the server. Existing user on Analytics (BI) server. Used for Analytics (BI) installation. Should be member of Administrators group. bi_user: 'ec2 user' User with administrator privileges bi_pass: 'jEn125us ' biinstalldir: 'C: workfusion' Target installation directory for Analytics (BI) server. Existing user on RPA server(s). Used for RPA installation. Should be member of Administrators group. rpa_user: 'ec2 user' User with administrator privileges rpa_pass: 'jEn125us ' rpainstalldir: 'C: RPA' Target installation directory for RPA server. Logging logleveltype: prod Affects log levels for Control Tower, Workspace, OCR, AutoQC. Possible values 'prod' or 'dev'. Use 'dev' for debugging only. Control Tower default user credentials used for login into Control Tower UI from browser. wf_username: 'workfusion' wf_password: 'ldT !E3vHp' Allowed special symbols: @ *(),.;} Marathon marathonwebpass: 'Pt1mdbw jG' password for buint in user \"marathon\" for Marathon Web UI Mesos mesoswebpass: 'JLUm7xsuS!' password for built in user \"mesos\" for Masos Web UI. Allowed special symbols: !* @ Rabbitmq rabbitmqocrpass: 'Ge48fd!jr74' password for built in rabbitmq user \"ocr\" used by OCR services rabbitmqbeppass: 'CSH5rU9( 5' password for built in rabbitmq user \"bep\" used by BEP services rabbitmqadminpass: 'Fj76dj)pW' password for built in rabbitmq user \"admin\" ELK elkadminpass: 'sc@R@ S 9V' password for elasticsearch user 'admin' used as login for Kibana UI elkelasticpass: 'KL7N! v2@M' password for build in elasticsearch user 'elastic' elkkibanapass: 'PM8Q *p1 Q' password for build in elasticsearch user 'kibana' used for install process elklogstashcertificate_pass: 'j3D qEe4!d' auth certificate 'logstash.p12' password Email SMTP mail_user: 'wfadmin' Credentials for SMTP server mail_pass: 'wfadmin' Credentials for SMTP server mail_host: 'localhost' Used by CT, WS mail_port: '25' Used by CT, WS mail_sender: 'no reply@example.com' Used by CT, WS to send email notifications Nexus nexusadminpass: 'EjuzrQo*6O' password for built in nexus user \"admin\" Tableau (BI) All values should be added on BI server to \" todeploy deploy config importusers.csv\" file tableaudashboarduser: 'tableau_user' tableau dashboard user. Can't be changed after initial installation tableaudashboardpass: 'tableau_pass' tableau dashboard password (special symbols are not allowed). Can't be changed after initial installation OCR ocr_platform: 'linux' Which OS is used for OCR Affects Control Tower configuration. Available options: 'linux', 'windows'. ocrsecpass: 'j N3C88JmN' password for user \"ocr\" used for basic authentication to OCR service api via nginx over HTTPS ocrjwtsecret: 'rzqFIc 1jc' JWT secret for OCR authentication Workspace workspacerootrequester_pass: 'I U koa9Jo' password for user \"workfusion\" used by Workspace service RPA The value of 'botmanagerjwt_secret' should consist of 24 symbols Numbers, capital and lowercase letters are allowed Allowed special symbols: ' ', ' ' The string must end with '==' botmanagerjwtsecret: 'SuQ6fzt5693XRmGueUhSbQ==' JWT secret for botmanager authentication rpabotmanager_pass: 'Kt Ol4luYl' password for built in springboot user \"wfagent\" used by Bot manager Provide existing Bot Master Windows user on RPA server(s) This user must be present before installation on each RPA server from rpa_hostnames list The same password must be used for Bot Master User on each RPA server The user must be a member of 'Administrators' group on each RPA server botmasteruser_name: 'BotMaster' botmasteruser_pass: 'Test111' Provide existing Bot Unit Windows user(s) on RPA server(s) The user(s) must be present before installation on each RPA server from rpa_hostnames list The number of created users should be equal to the value of 'rpabotsper_server' setting All Bot Unit users must have common basename and different index numbers in the end of basename For example, if you set 'rpabotsperserver: 2', and 'botunitbasename: BotUnit' then you should have the following users on each RPA server: BotUnit1, BotUnit2 All Bot Unit users on all RPA servers must have the same password rpabotsper_server: 1 Number of Bots on each RPA server botunitbase_name: 'BotUnit' botunitpass: 'Test111' MSSQL parameters mssqldbname: 'workfusion' MSSQL db name If you want to use Windows Authentication instead of SQL logins, uncomment line below and define your working AD realm there. ad_realm: EXAMPLE.COM mssqldbauser: 'wf_dba' db owner login for MSSQL db mssqldbapass: 'f*6F123MRH7' db owner password mssqlctuser: 'workfusion' db user for Control Tower component mssqlctpass: 'f*6FKvMRH7' db password for Control Tower component mssqlwsuser: 'workspace' db user for Workspace component mssqlwspass: 'R Ie4MBT7 ' db password for Workspace component mssqlsqcuser: 'sqc' db user for SQC component mssqlsqcpass: 'g64twVZO@3' db password for SQC component mssqldsuser: 'ds' db user for CT datasource connection mssqldspass: 'f*6FKvMRH7' db password for CT datasource connection mssqlrpauser: 'rpa' db user for RPA component mssqlrpapass: 'vYatC cU7A' db password for RPA component mssqlpmuser: 'pm' db user for monitoring tools mssqlpmpass: 'XzdiA4b4F' db password for monitoring tools. Allowed special symbols: @ ():,.} mssqldmuser: 'dm' db user for analytics component mssqldmpass: 'IKx66 dx0T' db password for analytics component. Allowed special symbols: @ *():,.;} mssqlrapiuser: 'rapi' db user for analytics remote api mssqlrapipass: 'dqPUhUoT*5' db password for analytics remote api mssqlocruser: 'ocr' db user for OCR component mssqlocrpass: 'CItYS_38!sdf' db password for OCR component ADVANCED CONFIGURATION Uncomment lines below if you have external Elasticsearch and Kibana elasticsearch_external: true kibanaurl: 'http: elasticsearch external1.example.com:5701 externalkibana' elasticsearch_host: 'elasticsearch external1.example.com' elasticsearchurl: 'http: elasticsearch external1.example.com:9210 externalelasticsearch' elasticsearch_port: '9210' uncomment and define if your port for Elasticsearch is different than 9200 Workfusion SPA ports configuration If you want to change the default port values, make sure you don't set duplicated ports for the same server. Ports used for SPA installation ssh_port: '22' SSH port listening on all linux servers. Used by SPA installer to deploy SPA Components into linux based servers. winrm_port: '5986' Windows Remote Management HTTPS port. Used by SPA installer to deploy SPA Components into Windows based servers. MSSQL Server ports mssql_port: '1433' The port where MSSQL server is listening. Used by SPA installer and SPA services to connect to MSSQL database. ALL Servers (INT, BEP Master, BEP Agent(s), APP, RPA, OCR) nginxport: '80' Nginx HTTP port. Affects nginx configuration on all servers. All requests to 'nginxport' are redirected to 'nginxportssl'. nginxportssl: '443' Nginx HTTPS port. The main port for accessing Control Tower and many other SPA services with http clients (e.g. browsers). Affects all servers. INT Server ports taskdispatcherservice_port: '9090' Spring Boot port where task dispatcher is listening. Listens on 127.0.0.1 (HTTP). taskdispatcherservicemtlsport: '9092' Nginx port. Listens on 0.0.0.0 (HTTPS) and proxies requests from to 'taskdispatcherservice_port'. Validates client certs . vault_port: '8200' Hashicorp Vault port. Listens on 0.0.0.0 (HTTPS). nexus_port: '8082' Sonatype Nexus port. Listens on 127.0.0.1 (HTTP). minio_port: '9000' Minio Server Port (S3 emulator). Listens on 127.0.0.1 (HTTP). elasticsearch_port: '9200' Elasticsearch port. Listens on 0.0.0.0 (HTTPS). logstashspaport: '4567' Logstash port. Listens on 0.0.0.0 (HTTPS). logstashautomlport: '4568' Logstash port. Listens on 0.0.0.0 (HTTPS). logstashbepmetrics_port: '15072' Logstash port. Listens on 0.0.0.0 (HTTPS). logstashmetricsport: '4569' Logstash port. Listens on 0.0.0.0 (HTTPS). logstashbeplogs_port: '4570' Logstash port. Listens on 0.0.0.0 (HTTPS). logstashheartbeatport: '4571' Logstash port. Listens on 0.0.0.0 (HTTPS). rabbitmqapiport: '15672' RabbitMQ API port. Listens on 127.0.0.1 (HTTP). rabbitmqapiproxyport: '45672' Nginx port. Listens on 0.0.0.0 (HTTPS) and proxies requests to 'rabbitmqapi_port'. rabbitmqamqpport: '5672' RabbitMQ message Queue port. Listens on 0.0.0.0 (TLS ecrtypted AMQP). zookeeper_port: '2181' Zookeeper port. Listens on 0.0.0.0. APP Server ports workfusion_port: '7080' Tomcat port where Control Tower is listening. Listens on 127.0.0.1 (HTTP). workfusionmtlsport: '7083' Nginx port. Listens on 0.0.0.0 (HTTPS) and proxies requests to 'workfusion_port'. Validates client certs. hazelcast_port: '5702' Hazelcast port for Control Tower. Listens on 0.0.0.0. workspace_port: '5080' Tomcat port where Workspace is listening. Listens on 127.0.0.1 (HTTP). sqc_port: '4080' Tomcat port where AutoQC is listening. Listens on 127.0.0.1 (HTTP). botmanagerport: '5555' Spring Boot port where bot manager service is listening. Listens on 127.0.0.1 (HTTP). kibana_port: '5601' Kibana port. Listens on 127.0.0.1 (HTTP). NOTE: 'workermanagementserviceport' and 'automlgatewayserviceport' are also listening on APP server (Nginx HTTP listeners on 127.0.0.1). So, make sure you don't assign any ports on APP server to the same values as 'workermanagementserviceport' or 'automlgatewayserviceport'. NOTE: Additionally the following ports are used by Tomcat processes on APP server: 'workfusionport 5'; 'workfusionport 8'; 'workspaceport 5'; 'workspaceport 8'; 'sqcport 5'; 'sqcport 8'. Make sure you don't use them for any services on APP server. All these ports are used only on 127.0.0.1. BEP Master ports workermanagementservice_port: '9091' Spring Boot port where worker management service is listening. Listens on 127.0.0.1 (HTTP). workermanagementservicemtlsport: '9093' Nginx port. Listens on 0.0.0.0 (HTTPS) and proxies requests to 'workermanagementservice_port'. Validates client certs. automlgatewayservice_port: '9070' Spring Boot port where automl gateway service is listening. Listens on 127.0.0.1 (HTTP). automlgatewayservicemtlsport: '9073' Nginx port. Listens on 0.0.0.0 (HTTPS) and proxies requests to 'automlgatewayservice_port'. Validates client certs. automlmodelservice_port: '9071' Spring Boot port where automl model service is listgning. Listens on 127.0.0.1 (HTTP). marathon_port: '8080' Marathon Port. Listens on 127.0.0.1 (HTTP). marathonproxyport: '8480' Nginx port. Listens on 0.0.0.0 (HTTPS) and proxies requests to 'marathon_port'. marathoncommunicationport: '15000' The port used by Marathon for libprocess communication. Listens on 0.0.0.0 (HTTPS). mesosmasterport: '5050' Mesos master port. Listens on 0.0.0.0 (HTTPS). vdsgatewayservice_port: '9080' Spring Boot port where VDS Gateway Service is listening. Listens on 127.0.0.1 (HTTP). Only used when SPA9.2 AutoML is installed. Used for backward compatibility with SPA9.2 business processes. vdsscalingservice_port: '9081' Spring Boot port where VDS Scaling Service is listening. Listens on 127.0.0.1 (HTTP). Only used when SPA9.2 AutoML is installed. Used for backward compatibility with SPA9.2 business processes. vdsmesosadapter_port: '9052' Spring Boot port where VDS Mesos Adapter Service is listening. Listens on 127.0.0.1 (HTTP). Only used when SPA9.2 AutoML is installed. Used for backward compatibility with SPA9.2 business processes. NOTE: 'taskdispatcherserviceport' is also listening on BEP Master (Nginx HTTP listener on 127.0.0.1). So, make sure you don't assign any ports on BEP Master server to the same value as 'taskdispatcherserviceport'. BEP Agent ports mesosslaveport: '5051' Mesos Agent port. Listens on 0.0.0.0 (HTTPS). mesosslaveport_range: '40000 44000' Ports range on mesos Agent(s) used for communication with Mesos Master (ports from this range are opened randomly during tasks execution). NOTE: 'workfusionport' and 'automlgatewayserviceport' are also listening on BEP Agent(s) server(s) (Nginx HTTP listeners on 127.0.0.1). So, make sure you don't assign any ports on BEP Agent(s) server(s) to the same values as 'workfusionport' or 'automlgatewayserviceport'. OCR Server ports ocr_port: '9002' Spring Boot port where OCR Rest API is listening. Listens on 127.0.0.1 (HTTP). LDAP configuration ldap_enabled: false if you're performing upgrade and already have ldap configured, set this to true ldapinternalauthorization_enabled: false If true, LDAP with WF roles will be used. No groups from LDAP will be used. ldapserverurl: 'ldaps: ldap.example.com:636' Ldap server connection url ldapbinddn: 'ldap user distinguished name' Ldap server bind username ldapbindpassword: 'ldap user password' Ldap server bind password ldapuserbase: 'ou=People,dc=domain,dc=com' Container of LDAP users used for authentication ldapgroupbase: 'ou=Groups,dc=domain,dc=com' Container of LDAP groups used for authorization ldapuserfilter: '(sAMAccountName={0})' User filter for search, optional, example: '(uid={0})' ldapgroupfilter: '(member={0})' Group filter for search, optional, example: '(memberUid={1})' wsldapworker_groups: 'CC Dev,Galiot,CC Ops' Comma separated groups from LDAP for mapping to worker users wsldaprequester_groups: 'CC Admin' Comma separated groups from LDAP for mapping to WorkSpace Requester user Workspace wsenableworkeremailconfirmation: false If true, Email confirmation for worker users will be enabled. Used by Workspace. SSO SAML wfssosaml_enabled: false Enables or disables SSO (SAML 2.0 protocol) authentification for CT wfssosamlspmetadata: 'workfusion metadata id' A unique string. ID of Service Provider. wfssosamlidpmetadata: 'https: SAMLSERVICEURL idp sso' URL that points to a file with IdP metadata parameters. Should be provided by customer. wfssosamlidpfile_metadata: ' path to idp file metadata.xml' Local path to a file with IdP metadata parameters wfssosamlusernameattribute: 'uid' Name of attribute that will be used for authentication. uid for LDAP, sAMAcountName for Active Directory wsssosaml_enabled: false Enables or disables SSO (SAML 2.0 protocol) authentification for CT (WS) wsssosamlspmetadata: 'ws sp hostname' A unique string. ID of Service Provider, for example, https: workspace saml metadata wsssosamlidpmetadata: 'https: SAMLSERVICEURL idp sso' URL that points to a file with IdP metadata parameters wsssosamlidpfile_metadata: ' path to idp file metadata.xml' Local path to a file with IdP metadata parameters wsssosamlusernameattribute: 'mail' Name of attribute that will be used for authentication. uid for LDAP, mail for Active Directory Configuration parameters Ansible Vault password Ansible vault password config.yml will be encrypted with this password after encrypt command Change it! ansiblevaultpassword: 'WorkFusion!' Parameter Description ansiblevaultpassword The password, which is used for encrypting the config.yml file. The installation cannot be started until this password is changed Ansible users used for a Single point setup Ansible Linux user(used in inventory) linuxinstallationuser: 'ec2 user' Parameter Description linuxinstallationuser The user, which is used during the Single point setup of Linux servers for connection over SSH S3 storage keys Access and Secret keys for S3 storage s3accesskey: 'JOZXPHL1ZC8WXZAJXNU7' Special symbols are not allowed. 3 20 characters length. s3secretkey: 'qAEYOVLOViPjxk3Lbuj0wrDztf8fGJrqMb4Obw==' Allowed special symbols: '=', ' '. 8 40 characters length. Parameter Description s3accesskey The access key for the S3 server s3secretkey The secret key for the S3 server SSL certificates SSL Certificates are used to secure Workfusion SPA services communication Put your Trusted Certificates and key files to certificates directory. Otherwise, Self Signed certificates will be generated automatically Provide password for autogenerated ca.key file. This setting will be used only when you're using self signed certs generated by the \"install.sh certs generate\" command cakeypass: 'aHja&a T8c' Parameter Description cakeypass The password for the auto generated ca.key file. This setting will be used only when you're using self signed certs generated by the install.sh certs generate command Vault certificates Vault Certificates These certificates are generated using \". install.sh cert_vault generate\" command They are used by Workfusion applications for authentication and access control into Vault (Secure Storage) vaultadmincertpass: 'PQS*jzJa9%' password for vaultadmin.p12 certificate vaultclientcertpass: 'a2J@Kn5llk' password for vaultworkfusion.p12 certificate. Allowed special symbols: @ *():,.;} Parameter Description vaultadmincertpass The password for the vault admin.p12 certificate vaultclientcertpass The password for the vault workfusion.p12 certificate Linux runtime user and group Linux runtime user and group. Must be the same for all linux servers wf_user: wfuser wf_group: wfuser install_dir: opt workfusion Target installation directory, should not contain dot(s) Parameter Description wf_user The name of a runtime user for Linux servers. The user is created during the installation wf_group The name of a user group for wf_user. The group is created during the installation install_dir The WorkFusion installation directory for Linux servers. The directory is created on all Linux servers during installation. The directory must not contain the dot character (.) due to limitations of UIMA library. Mandatory. For example, the following path won't be processed correctly: opt worfusion 8.5.0 Windows installation properties Windows installation properties Existing user on OCRWin server. It will be used ONLY if ocr_platform is windows. Used for OCRWin installation. Should be member of Administrators group. ocrwin_user: 'ec2 user' ocrwin_pass: 'jEn125us ' ocrwininstalldir: 'C: workfusion' Target installation directory for OCRWin on the server. Existing user on Analytics (BI) server. Used for Analytics (BI) installation. Should be member of Administrators group. bi_user: 'ec2 user' User with administrator privileges bi_pass: 'jEn125us ' biinstalldir: 'C: workfusion' Target installation directory for Analytics (BI) server. Existing user on RPA server(s). Used for RPA installation. Should be member of Administrators group. rpa_user: 'ec2 user' User with administrator privileges rpa_pass: 'jEn125us ' rpainstalldir: 'C: RPA' Target installation directory for RPA server. Parameter Description ocrwin_user The user, which is used during the Single point setup for connection by WinRM to Windows OCR servers ocrwin_pass The password of the Windows installation user ocrwininstalldir The WorkFusion installation directory for the Windows OCRWin server. The directory is created on all Windows servers during the installation bi_user The user, which is used during the Single point setup for connection by WinRM to Windows BI (Analytics) servers bi_pass The password of the Windows installation user biinstalldir The WorkFusion installation directory for the Windows BI server. The directory is created on all Windows servers during installation rpa_user The user, which is used during the Single point setup for connection by WinRM to Windows RPA servers rpa_pass The password of the Windows installation user rpainstalldir The WorkFusion installation directory for the RPA server. The directory is created on all Windows servers during the installation Logging logleveltype: prod Affects log levels for all components. Possible values 'prod' or 'dev'. Use 'dev' for debugging only. Parameter Description logleveltype The ability to change log level for WorkFusion components logs. Possible values: prod. The logs contain only error messages and warnings dev. The logs contain error messages, warnings, info and debug messages User credentials and passwords Control Tower default user credentials used for login into Control Tower UI from browser. wf_username: 'workfusion' wf_password: 'ldT !E3vHp' \":\" and \"&\" characters are restricted for this passwords Marathon marathonwebpass: 'Pt1mdbw jG' password for buint in user \"marathon\" for Marathon Web UI Mesos mesoswebpass: 'JLUm7xsuS!' password for built in user \"mesos\" for Masos Web UI. Allowed special symbols: !* @ Rabbitmq rabbitmqocrpass: 'Ge48fd!jr74' password for built in rabbitmq user \"ocr\" used by OCR services rabbitmqbeppass: 'CSH5rU9( 5' password for built in rabbitmq user \"bep\" used by BEP services rabbitmqadminpass: 'Fj76dj)pW' password for built in rabbitmq user \"admin\" ELK elkadminpass: 'sc@R@ S 9V' password for elasticsearch user 'admin' used for Kibana UI elkelasticpass: 'KL7N! v2@M' password for build in elasticsearch user 'elastic' elkkibanapass: 'PM8Q *p1 Q' password for build in elasticsearch user 'kibana' used for install process elklogstashcertificate_pass: 'j3D qEe4!d' auth certificate 'logstash.p12' password Parameter Description wf_username A default username for Control Tower wf_password The password for a default user of Control Tower. Never use the '%' symbol in a password marathonwebpass The default password for Marathon. Marathon is protected by the HTTP authentication mechanism. Change this option to replace the default password (recommended) mesoswebpass The default password for Mesos. Mesos is protected by the HTTP authentication mechanism. Change this option to replace the default password (recommended) rabbitmqadminpass The password for the RabbitMQ user \"admin\" used by AutoML services rabbitmqocrpass The password for the RabbitMQ user \"ocr\" used by OCR services rabbitmqbeppass The password for the RabbitMQ user \"bep\" used by BEP services elkadminpass The password for the Elasticsearch user \"admin\" used for the Kibana UI elkelasticpass The password for the Elasticsearch user \"elastic\" elkkibanapass The password for the Elasticsearch user \"kibana\" used for the installation elklogstashcertificate_pass The password for the \"logstash.p12\" authentication password Email SMTP Email SMTP mail_user: 'wfadmin' Credentials for SMTP server mail_pass: 'wfadmin' Credentials for SMTP server mail_host: 'localhost' Used by CT, WS mail_port: '25' Used by CT, WS mail_sender: 'no reply@example.com' Used by CT, WS to send email notifications Parameter Description mail_user Credentials for the SMTP server mail_pass Credentials for the SMTP server mail_host The mail server host. Used by CT, WS mail_port The port of a mail server. Used by CT, WS mail_sender The email address to be used by CT and WS to send emails to users Nexus Nexus nexusadminpass: 'EjuzrQo*6O' password for built in nexus user \"admin\" Parameter Description nexusadminpass The password for user \"admin\" used by Nexus services Tableau (BI) Tableau (BI) All values should be added on BI server to \" todeploy deploy config importusers.csv\" file tableaudashboarduser: 'tableau_user' tableau dashboard user. Can't be changed after initial installation tableaudashboardpass: 'tableau_pass' tableau dashboard password (special symbols are not allowed). Can't be changed after initial installation Parameter Description tableaudashboarduser The Tableau dashboard user. Can't be changed after initial installation tableaudashboardpass The password for user \"tableau _user\" used by Tableau dashboard. Special symbols are not allowed OCR ocr_platform: 'linux' Which OS is used for OCR Affects Control Tower configuration. Available options: 'linux', 'windows'. ocrsecpass: j N3C88JmN ocrjwtsecret: rzqFIc 1jc JWT secret for OCR authentication Parameter Description ocr_platform The option to select the OCR platform. Two options available: 'windows' 'linux' These values are mutually exclusive: you have to define only 'linux' or 'windows' OCR platform value before installation. Simultaneously running platforms are not supported at the moment. The option affects Control Tower configuration. ocrsecpass The password for user \"ocr\" used by the OCR service via NGINX over HTTPS ocrjwtsecret The JWT secret for the OCR authentication Workspace workspacerootrequester_pass: 'I U koa9Jo' password for user \"workfusion\" used by Workspace service Parameter Description workspacerootrequester_pass The password of a default requester user created for Workspace RPA RPA Bot Manager botmanagerjwtsecret: 'SuQ6fzt5693XRmGueUhSbQ==' JWT secret for botmanager authentication rpabotmanager_pass: 'Kt Ol4luYl' password for built in springboot user \"wfagent\" used by Bot manager Provide existing Bot Master Windows user on RPA server(s) This user must be present before installation on each RPA server from rpa_hostnames list The same password must be used for Bot Master User on each RPA server The user must be a member of 'Administrators' group on each RPA server botmasteruser_name: 'BotMaster' botmasteruser_pass: 'Test111' Provide existing Bot Unit Windows user(s) on RPA server(s) The user(s) must be present before installation on each RPA server from rpa_hostnames list The number of created users should be equal to the value of 'rpabotsper_server' setting All Bot Unit users must have common basename and different index numbers in the end of basename For example, if you set 'rpabotsperserver: 2', and 'botunitbasename: BotUnit' then you should have the following users on each RPA server: BotUnit1, BotUnit2 All Bot Unit users on all RPA servers must have the same password rpabotsper_server: 1 Number of Bots on each RPA server botunitbase_name: 'BotUnit' botunitpass: 'Test111' Parameter Description botmanagerjwt_secret The JWT secret for the bot_manager authentication rpabotmanager_pass The password for the built in springboot user \"wfagent\" used by Bot manager botmasteruser_name The name of a Bot Master supervisor Windows user on the RPA servers. This user must be present before installation on each RPA server from the rpa_hostnames list botmasteruser_pass The password of a Bot Master Windows user. The same password must be used for Bot Master User on each RPA server rpabotsper_server A number of bots to be created on each RPA Bot Master. botunitbase_name The template name of the common Bot Unit users. The number of the amount of bots specified in rpabotsper_server is added to this name as a postfix botunitpass The password for botunitbase_name MS SQL Starting from SPA 10.0, a consolidated DB server is used based on MS SQL Server. Thus, it is expected that MS SQL is already installed by the customer and configured according to Prepare MS SQL server. MSSQL parameters mssqldbname: 'workfusion' MSSQL db name mssqldbauser: 'wf_dba' db owner login for MSSQL db mssqldbapass: 'f*6F123MRH7' db owner password mssqlctuser: 'workfusion' db user for Control Tower component mssqlctpass: 'f*6FKvMRH7' db password for Control Tower component mssqlwsuser: 'workspace' db user for Workspace component mssqlwspass: 'R Ie4MBT7 ' db password for Workspace component mssqlsqcuser: 'sqc' db user for SQC component mssqlsqcpass: 'g64twVZO@3' db password for SQC component mssqldsuser: 'ds' db user for CT datasource connection mssqldspass: 'f*6FKvMRH7' db password for CT datasource connection mssqlrpauser: 'rpa' db user for RPA component mssqlrpapass: 'vYatC cU7A' db password for RPA component mssqlpmuser: 'pm' db user for monitoring tools mssqlpmpass: 'XzdiA4b4F' db password for monitoring tools. Allowed special symbols: @ ():,.} mssqldmuser: 'dm' db user for analytics component mssqldmpass: 'IKx66 dx0T' db password for analytics component. Allowed special symbols: @ *():,.;} mssqlrapiuser: 'rapi' db user for analytics remote api mssqlrapipass: 'dqPUhUoT*5' db password for analytics remote api mssqlocruser: 'ocr' db user for OCR component mssqlocrpass: 'CItYS_38!sdf' db password for OCR component Parameter Description mssqldbname The name of the MS SQL database mssqldbauser The MS SQL db owner for the workfusion DB – full access admin of Workfusion DB mssqldbapass The password for mssqldbauser mssqlctuser The MS SQL user used by the Control Tower SPA component mssqlctpass The password for mssqlctuser mssqlwsuser The MS SQL user used by the Workspace SPA component mssqlwspass The password for mssqlwsuser mssqlsqcuser The MS SQL user used by the SQC service mssqlsqcpass The password for mssqlsqcuser mssqldsuser The MS SQL user used by the Data Store module mssqldspass The password for mssqldsuser mssqlrpauser The MS SQL user used by the RPA component mssqlrpapass The password for mssqlrpauser. mssqlpmuser The MS SQL user used by the Logstash service mssqlpmpass The password for mssqlpmuser mssqldmuser The MS SQL user used by the Analytics services mssqldmpass The password for mssqldmuser mssqlrapiuser The MS SQL user used by the Analytics remote API mssqlrapipass The password for mssqlrapiuser mssqlocruser The MS SQL user used by the OCR component mssqlocrpass The password for mssqlocruser Advanced configuration Elasticsearch and Kibana Starting from SPA 10.0, you can use external Elasticsearch and Kibana instead of ones provided during installation. All the saved objects (for example, indices, visualizations, dashboards) that are required for SPA Monitoring will be automatically loaded to your Kibana and Elasticsearch during installation. These options are commented by default, so if there are no any customizations here, Elasticsearch and Kibana are installed to the ELK server. Uncomment lines below if you have external Elasticsearch and Kibana elasticsearch_external: true kibanaurl: 'http: elasticsearch external1.example.com:5701 externalkibana' elasticsearch_host: 'elasticsearch external1.example.com' elasticsearchurl: 'http: elasticsearch external1.example.com:9210 externalelasticsearch' elasticsearch_port: '9210' uncomment and define if your port for Elasticsearch is different than 9200 elasticsearch_external The parameter to enable external Elasticsearch. Set to true to enable kibana_url The link to your installed external Kibana elasticsearch_host The host of your installed Elasticsearch elasticsearch_url The link to your installed Elasticsearch elasticsearch_port The external Elasticsearch host. Optional. Define the port, only if you use custom ELK port other than 9200 Application ports Application ports ALL Servers nginxport: '80' Affects all servers. All requests to nginxport are redirected to nginxsslport. nginxportssl: '443' The main ssl port for accessing Control Tower and other services. Affects all servers. APP Server workfusionmtlsport: '7083' Receives requests and validates client certs from workers (from bep agents and rpa servers) hazelcast_port: '5702' INT Server taskdispatcherservicemtlsport: '9092' Receives requests and validates client certs from worker management service vault_port: '8200' rabbitmqapiproxy_port: '45672' zookeeper_port: '2181' MSSQL Server mssql_port: '1433' BEP Master workermanagementservicemtlsport: '9093' Receives requests and validates client certs from Control Tower. automlgatewayservicemtlsport: '9073' marathonproxyport: '8480' mesosmasterport: '5050' BEP Agent mesosslaveport: '5051' mesosslaveport_range: '40000 44000' Application ports must be configured in accordance with Application Ports. LDAP Note that LDAP must be configured only after the SPA installation, since manual actions are required on the UI before enabling LDAP. For more details, see LDAP and AD Integration. In case you set up LDAP integration after the installation, it is recommended to update the config.yml file with the used properties to simplify the upgrade process in future. LDAP ldap_enabled: false if you're performing upgrade and already have ldap configured, set this to true ldapinternalauthorization_enabled: false If true, LDAP with WF roles will be used. No groups from LDAP will be used. ldapserverurl: 'ldaps: ldap.example.com:636' Ldap server connection url ldapbinddn: 'ldap user distinguished name' Ldap server bind username ldapbindpassword: 'ldap user password' Ldap server bind password ldapuserbase: 'ou=People,dc=domain,dc=com' Container of LDAP users used for authentication ldapgroupbase: 'ou=Groups,dc=domain,dc=com' Container of LDAP groups used for authorization ldapuserfilter: '(sAMAccountName={0})' User filter for search, optional, example: '(uid={0})' ldapgroupfilter: '(member={0})' Group filter for search, optional, example: '(memberUid={1})' wsldapworker_groups: 'CC Dev,Galiot,CC Ops' Comma separated groups from LDAP for mapping to worker users wsldaprequester_groups: 'CC Admin' Comma separated groups from LDAP for mapping to WorkSpace Requester user Parameter Description ldap_enabled The option specifies whether LDAP is enabled. It's important to update this property before the upgrade, if LDAP was enabled. Boolean parameter The parametr must be set to false in case of a clean installation. ldapinternalauthorization_enabled The option must be set to true, to activate LDAP with WF roles. No groups from LDAP will be used and properties ldap.group.base and ldap.group.filter will be ignored. Boolean parameter ldapserverurl The LDAP server connection url ldapbinddn The LDAP server bind username ldapbindpassword The LDAP server bind password ldapuserbase A container of corporate user database used for authentication Use the comma separated values. ldapgroupbase A container of corporate user groups used for authentication Use the comma separated values. ldapuserfilter The user filter for search. Optional. Example: '(uid={0})' ldapgroupfilter The group filter for search. Optional. Example: '(memberUid={1})' wsldapworker_groups Workspace: comma separated group names from LDAP for mapping to the worker users wsldaprequester_groups Workspace: comma separated groups from LDAP for mapping to WorkSpace Requester user Workspace email confirmation Wokfspace wsenableworkeremailconfirmation: false If true, Email confirmation for worker users will be enabled. Used by Workspace. Parameter Description wsenableworkeremailconfirmation The option to enable email confirmation for the worker users during registration. Set to true to enable. Used by Workspace. Note that this property must be set to true only if an email server is configured above SSO SAML Note that SSO must be configured only after SPA installation, since manual actions are required in UI before enabling SSO. For more details, refer to SSO integration. In case you set up SSO integration after installation, it is recommended to update config.yml file with the used properties in order to simplify upgrade process in future. SSO SAML wfssosaml_enabled: false Enables or disables SSO (SAML 2.0 protocol) authentification for CT wfssosamlspmetadata: 'workfusion metadata id' A unique string. ID of Service Provider. wfssosamlidpmetadata: 'https: SAMLSERVICEURL idp sso' URL that points to a file with IdP metadata parameters. Should be provided by customer. wfssosamlidpfile_metadata: ' path to idp file metadata.xml' Local path to a file with IdP metadata parameters wfssosamlusernameattribute: 'uid' Name of attribute that will be used for authentication. uid for LDAP, sAMAcountName for Active Directory wsssosaml_enabled: false Enables or disables SSO (SAML 2.0 protocol) authentification for CT (WS) wsssosamlspmetadata: 'ws sp hostname' A unique string. ID of Service Provider, for example, https: workspace saml metadata wsssosamlidpmetadata: 'https: SAMLSERVICEURL idp sso' URL that points to a file with IdP metadata parameters wsssosamlidpfile_metadata: ' path to idp file metadata.xml' Local path to a file with IdP metadata parameters wsssosamlusernameattribute: 'mail' Name of attribute that will be used for authentication. uid for LDAP, mail for Active Directory Parameter Description wfssosaml_enabled The parameter enables or disables SSO (SAML 2.0 protocol) authentification for CT wfssosamlspmetadata An ID of a Service Provider wfssosamlidpmetadata The URL that links to a file with the IdP metadata parameters. The parameter must be provided by the customer wfssosamlidpfile_metadata The local path to a file with the IdP metadata parameters wfssosamlusernameattribute The name of an attribute that will be used for authentication: uid for LDAP sAMAcountName for Active Directory wsssosaml_enabled The parameter enables or disables SSO (SAML 2.0 protocol) authentification for CT (WS) wsssosamlspmetadata An ID of a Service Provider wsssosamlidpmetadata The URL that links to a file with the IdP metadata parameters. wsssosamlidpfile_metadata The local path to a file with the IdP metadata parameters wsssosamlusernameattribute The name of an attribute that will be used for authentication: uid for LDAP sAMAcountName for Active Directory "},{"version":"10.0","date":"Oct-14-2019","title":"configuration-and-health-check","name":"Configuration and health check","fullPath":"iac/admin/advanced/configuration-and-health-check","content":" Validation via Control Tower Control Tower as a central component provides validation of it's own configuration and gather configuration checks from connected components by default from out of the box configuration. Trigger validation Validation is going during startup tomcat by default. The result of that can be accessible in catalina.out and configuration.log files. To turn it off, in the workfusion.properties, set health.check.startup.enabled=false. Validation status can be fetched with REST v1 health check (no authentication needed). The result is in response and in the configuration.log file. For more informationn, see swagger ui. Validation checks Component Validation Control Tower Database db connection, changelog version Secret Management Service Check required fields for SMS (serverApi, platformId, safe, etc); check client certificate and keypass for it. LDAP Authentication url, bind DN, bind password, users and groups base dirs, user and groups filters Single Sign On Authentication Service Provider identifier, username attribute, SP metadata file, additionally check SignOn and LogOut service locations in metadata file Crowd Licenses Configured licenses are available. S3 Storage Check access with default keys, access with additional keys Nexus Repository url, password, ping, try to get artifact list Mail Server url, port, validate and ping OCR url, jwt issuer and secret. Internal OCR Health Check RPA Bot Manager url, ping, access. AutoML url, ping. SQC url, ping, get project info Tableau url, username, password, site, workbook, new workbook, try sign in ImageMagick version if it is installed Ghostscript version if it is installed Additional options If WorkSpace is deployed to the same tomcat server with Control Tower and it is no active yet during validation, you can skip Crowd Licenses validation on start up with health.check.crowd.license.startup.enabled=true in workfusion.properties file. Connection timeout during check external components is 10000 milliseconds by default. To change this value, in workfusion.properties file, set the health.check.connect.timeout property. Health check status example Health check status in the JSON format... { \"name\": \"Control Tower\", \"status\": \"OK\", \"projectInfo\": { \"version\": \"10.0.0 SNAPSHOT\", \"path\": \"develop\", \"revision\": \"eb21349b8ee31d36fb535dd2d97f4f01a32a3bab\", \"shortRevision\": \"eb21349b\" }, \"errorCount\": 0, \"warningCount\": 0, \"errors\": , \"warnings\": , \"infos\": , \"components\": { \"name\": \"Secret Management Service\", \"status\": \"OK\", \"projectInfo\": { \"version\": \"1.1.0.1\", \"path\": \"release 1.1.0.1\", \"revision\": \"8b2cabc9836ed7bba3b8bc0d4a927b8660a46f77\", \"shortRevision\": \"8b2cabc9\" }, \"errorCount\": 0, \"warningCount\": 0, \"errors\": , \"warnings\": , \"infos\": , \"components\": }, { \"name\": \"Control Tower Database\", \"status\": \"OK\", \"projectInfo\": null, \"errorCount\": 0, \"warningCount\": 0, \"errors\": , \"warnings\": , \"infos\": , \"components\": }, { \"name\": \"OCR\", \"status\": \"OK\", \"projectInfo\": null, \"errorCount\": 0, \"warningCount\": 0, \"errors\": , \"warnings\": , \"infos\": , \"components\": }, { \"name\": \"ImageMagick\", \"status\": \"WARN\", \"projectInfo\": null, \"errorCount\": 0, \"warningCount\": 1, \"errors\": , \"warnings\": \"The ImageMagick image conversion library is not found. The OCR pre processing step cannot be executed correctly. Please, setup the ImageMagick library\" , \"infos\": , \"components\": }, { \"name\": \"Ghostscript\", \"status\": \"OK\", \"projectInfo\": { \"version\": \"GPL Ghostscript 9.07 (2013 02 14)\", \"path\": null, \"revision\": null, \"shortRevision\": null }, \"errorCount\": 0, \"warningCount\": 0, \"errors\": , \"warnings\": , \"infos\": , \"components\": }, { \"name\": \"Tableau\", \"status\": \"ERROR\", \"projectInfo\": null, \"errorCount\": 1, \"warningCount\": 1, \"errors\": \"Host velcom2 bi1.workfusion.com:443 is NOT reachable\" , \"warnings\": \"Tableau:Automation is not configured. You should specify following parameter(s): tableau.automation.host,tableau.automation.username,tableau.automation.password\" , \"infos\": , \"components\": }, { \"name\": \"AUTOML SERVICES\", \"status\": \"OK\", \"projectInfo\": null, \"errorCount\": 0, \"warningCount\": 0, \"errors\": , \"warnings\": , \"infos\": , \"components\": }, { \"name\": \"RPA Bot Manager\", \"status\": \"ERROR\", \"projectInfo\": null, \"errorCount\": 1, \"warningCount\": 0, \"errors\": \"Error during check RPA Bot Manager. java.lang.IllegalStateException: Expected BEGIN_OBJECT but was STRING at line 1 column 1 path \" , \"warnings\": , \"infos\": , \"components\": }, { \"name\": \"SQC\", \"status\": \"OK\", \"projectInfo\": { \"version\": \"10.0.0.2 SNAPSHOT\", \"path\": \"develop\", \"revision\": \"a82efe4065ef9b05fdb22b3304c1503418687c45\", \"shortRevision\": null }, \"errorCount\": 0, \"warningCount\": 0, \"errors\": , \"warnings\": , \"infos\": , \"components\": }, { \"name\": \"Crowd Licenses\", \"status\": \"OK\", \"projectInfo\": null, \"errorCount\": 0, \"warningCount\": 0, \"errors\": , \"warnings\": , \"infos\": , \"components\": }, { \"name\": \"Mail Server\", \"status\": \"OK\", \"projectInfo\": null, \"errorCount\": 0, \"warningCount\": 0, \"errors\": , \"warnings\": , \"infos\": , \"components\": }, { \"name\": \"S3 Storage\", \"status\": \"OK\", \"projectInfo\": null, \"errorCount\": 0, \"warningCount\": 0, \"errors\": , \"warnings\": , \"infos\": , \"components\": }, { \"name\": \"Single Sign On Authentication\", \"status\": \"OK\", \"projectInfo\": null, \"errorCount\": 0, \"warningCount\": 0, \"errors\": , \"warnings\": , \"infos\": \"Single Sign On Authentication is not enabled. Use following parameter for enabling: wf.sso.saml.enable\" , \"components\": }, { \"name\": \"Nexus repository\", \"status\": \"OK\", \"projectInfo\": null, \"errorCount\": 0, \"warningCount\": 0, \"errors\": , \"warnings\": , \"infos\": \"Found 0 artifact(s) in the Nexus repository. https: velcom2 int1.workfusion.com:443 nexus \" , \"components\": }, { \"name\": \"LDAP Authentication\", \"status\": \"OK\", \"projectInfo\": null, \"errorCount\": 0, \"warningCount\": 0, \"errors\": , \"warnings\": , \"infos\": \"LDAP Authentication is not enabled. Use following parameter for enabling: ldap.enabled\" , \"components\": } } Health Check Business Process WorkFusion provides a health (sanity) check BP to run on environment to ensure that environment is functional. For more information, see Environment Health Check. "},{"version":"10.0","date":"Jul-01-2019","title":"crontab-jobs","name":"Crontab jobs","fullPath":"iac/admin/advanced/crontab-jobs","content":" This guide describes jobs that crontab executes on corresponding servers at scheduled time. Here, variable `` is assumed to have default value opt workfusion. INT Server Logrotate. 30 1 * * * usr sbin logrotate s opt workfusion logrotate status opt workfusion logrotate logrotate.conf > dev null 2>&1 Elasticsearch retention. 0 3 * * * opt workfusion python site bin curator config opt workfusion elasticsearch curator config.yml opt workfusion elasticsearch curator actions.yml Zookeper snapshots cleanup 3 * * bash l opt workfusion zookeeper bin cleanupSnapshots.sh >> opt workfusion zookeeper log zkclean.log APP Server Logrotate 30 1 * * * usr sbin logrotate s opt workfusion logrotate status opt workfusion logrotate logrotate.conf > dev null 2>&1 BEP Master Server Logrotate. 30 1 * * * usr sbin logrotate s opt workfusion logrotate status opt workfusion logrotate logrotate.conf > dev null 2>&1 Clear javacpp * temp files. 0 2 * * find tmp mtime 1 name \"javacpp*\" xargs I {} rm rf {} Clear desc *.xml temp files. 0 2 * * find tmp mtime 1 name \"desc*.xml\" delete Mesos clean runtime directory. @reboot usr bin find opt workfusion mesos runtime containers * delete > dev null 2>& Clear VDS services temp files. 0 0 * * * find opt workfusion tmp * mtime 3 delete BEP Agent Server Logrotate. 30 1 * * * usr sbin logrotate s opt workfusion logrotate status opt workfusion logrotate logrotate.conf > dev null 2>&1 Clear javacpp * temp files. 0 2 * * find tmp mtime 1 name \"javacpp*\" xargs I {} rm rf {} Clear desc *.xml temp files. 0 2 * * find tmp mtime 1 name \"desc*.xml\" delete Clear VDS services temp files. 0 0 * * * find opt workfusion tmp * mtime 3 delete Mesos clean runtime directory. @reboot usr bin find opt workfusion mesos runtime containers * delete > dev null 2>&1 OCR Server: Logrotate. 30 1 * * * usr sbin logrotate s opt workfusion logrotate status opt workfusion logrotate logrotate.conf > dev null 2>&1 "},{"version":"10.0","date":"Oct-16-2019","title":"health-сheck-messages","name":"Health check messages","fullPath":"iac/admin/advanced/health-сheck-messages","content":" id: version 10.0 health check messages The document lists health check messages received via API. For more information on health checks, see Configuration and health check Control Tower Component name Message type Message text AUTOML SERVICES Error Base automation URL is empty, define the secure automl.services.endpoint properties. General Error %component_name does not respond. Check if it is accessible via the URL. Database connection Error Database validation failed. Error: %s. Use 'liquibase validate' to see details. Database connection Error Database validation failed. %s change sets have not been applied. Use 'liquibase status' to see details. Database connection Error Impossible to call the stored procedure. Make sure that the \"ct.datasource.username\" ControlTower user has appropriate permissions Database connection Error Failed to connect to database. Reason: %s (an error from the MS SQL driver) Nexus Error The Nexus repository username or and password is empty. Please define the secure nexus.user and nexus.password properties. Nexus Info Found %number artifact(s) in the Nexus repository. \"nexus.url\" OCR Error The OCR URL is empty. Please define the ocr.api.base_url property. OCR Warning The JWT issuer or and secret are not set. Please define the secure jwt.issuer, jwt.secret properties to enable correct operation of the ocr plugin. S3 Warning The secure s3.access key or and s3.secret key properties are not set. The S3 file storage might be not accessible. Please define the secure s3.access key or and s3.secret key property. Tableau Warning Tableau secure properties for Dashboard and Automation do not match. Dashboard reports might not be displayed. Please check that secure properties match as follows:tableau.dashboard.host equals tableau.automation.hosttableau.dashboard.username equals tableau.automation.usernametableau.dashboard.password equals tableau.automation.password. Tableau Warning Tableau:Automation is not configured. You should specify following parameter(s): tableau.automation.host,tableau.automation.username,tableau.automation.password\" SSO Error Error parsing the identity provider’s (IdP) metadata file %wf.sso.SSO.idp.file.metadata for Single Sign On Authentication. %Error message% SSO Error The Service Provider ID property is not set for Single Sign On. Please define the secure wf.sso.SSO.sp.metadata property. SSO Error The secure username attribute property is not defined for Single Sign On. Please define the secure wf.sso.SSO.username.attribute=username property. If you do not have the username attribute, contact your Security Administrator. SSO Error Identity Provider (IdP) metadata file does not exist for Single Sign On. Please request the IdP metadata file from your Security Administrator and copy the file to configured path: %wf.sso.SSO.idp.file.metadata. LDAP Error Users and groups base directories for LDAP server are not set. Please, define the ldap.group.base and ldap.user.base properties for searching the users and groups. LDAP Error User or and groups filters for LDAP server are not set. Please, define the ldap.user.filter, ldap.group.filter filters for searching the users and groups. LDAP Error User filter for LDAP server is not set. Please, define the \"ldap.user.filter\" property for searching the users. LDAP Warning User groups are not found at LDAP server. Users can not log in. Please, setup the ldap.group.base or and ldap.group.filter properties correctly. LDAP Info Credentials for the LDAP server system user are not set. Please define the secure ldap.bind.password or and ldap.bind.dn properties. LDAP Error Error loading the user groups from LDAP server. Please check the ldap.server.url, ldap.bind.dn, ldap.bind.password secure connection properties. LDAP Error The URL property for LDAP server is not set. Please define the secure ldap.server.url property. LDAP Error LDAP Server URL %URL is invalid. Please define the secure ldap.server.url property correctly. LDAP Info LDAP Integration without groups roles is enabled. LDAP Info LDAP Integration with groups is enabled ImageMagick Warning The ImageMagick image conversion library is not found. The OCR pre processing step cannot be executed correctly. Please, setup the ImageMagick library. GhostScript Warning The GhostScript image conversion library is not found. The OCR pre processing step cannot be executed correctly. Please, setup the GhostScript library.GhostScript is a secondary tool used for OCR. This warning can be ignored in case either OCR or GhostScript capabilites are not used by the designed Business Process. License Warning The %s license is inactive. Manual tasks are not possible to publish with this license. License Error The problem with the '%s' license (%s) occurred. Error: %s RPA Warning RPA Bot Manager host is empty, please define the property rpa.bot.manager.base.url. WorkSpace Component name Message type Message text LDAP authentication Error Users and groups base directories for LDAP server are not set. Please define ldap.group.base and ldap.user.base for searching the users and groups. SSO Error Identity Provider (IdP) metadata file does not exist for Single Sign On. Please request the IdP metadata file from your Security Administrator and copy the file to configured path: %ws.sso.SSO.idp.file.metadata. SSO Error Service Provider ID property not defined for Single Sign On. Please define the secure ws.sso.SSO.sp.metadata property. SSO Error Secure username attribute property is not defined for Single Sign On. Please define the secure ws.sso.SSO.username=mail attribute property. If you do not have the username attribute, contact your Security Administrator. SSO Error Identity Provider (IdP) metadata file does not exist for Single Sign On. Please request IdP metadata file from your Security Administrator and copy file to configured path: %ws.sso.SSO.idp.file.metadata. SSO Error Error during parsing identity provider's metadata file. LDAP Error Users and groups base directories for LDAP server are not set. Please define ldap.group.base and ldap.user.base properties for searching users and groups. LDAP Error Error loading the user groups from LDAP server. Please check the ws.secure.ldap.server.url, ws.secure.ldap.bind.dn, ws.secure.ldap.bind.password secure connection properties. LDAP Error Users and groups base directories for LDAP server are not set. Please define ldap.group.base and ldap.user.base properties for searching users and groups. LDAP TBD Info Credentials for the LDAP server system user are not set. Please define the ws.secure.ldap.bind.password or and ws.secure.ldap.bind.dn secure properties. LDAP Warn One or more groups are not found at LDAP server. Please, setup the ldap.group.base or and ldap.group.filter properties correctly. LDAP Error User groups defined in the application properties are not found at LDAP server. Users cannot log in. Please setup the ldap.group.base or and ldap.group.filter properties correctly. LDAP Error User or and groups filters for LDAP server are not set. Please, define the ldap.user.filter, ldap.group.filter filters for searching the users and groups properties. LDAP Error User filter for LDAP server are not set. Please, define the ldap.user.filter filter for searching the users. Database connection Error Database validation failed. Error: %s. Use 'liquibase validate' to see details. Database connection Error Database validation failed. %s change sets have not been applied. Use 'liquibase status' to see details. Database connection Error Database connection error occurred. Please, check the database endpoint, security settings and connections number limits. Database connection Error Database access error occurred. Please, check that the ws.datasource.username and or ws.datasource.password secure database credentials are correct. Database connection Error Database access error occurred. Connection to database does not exist. OCR Component name Type of issue Message Text Microsoft SQL Server Error MS SQL check failed: e.getMessage() Possible causes: OCR could not connect to Microsoft SQL Server. Microsoft SQL Server is down. The Microsoft SQL Server node is not reachable from ocr rest. OCR Service is not operational. Remedy: Make sure: Microsoft SQL Server is up. The network between Microsoft SQL Server node and ocr rest node is operational. S3 Error Failed loading a file: e.getMessage() Possible causes: S3 is not reachable from ocr rest. OCR Service is not operative. Remedy: Make sure S3 is up. S3 Error Save operation failed during a storage test: e.getMessage() Possible causes: S3 MongoDB (depending on the active profile in configuration) is not reachable from ocr rest. The connection is not configured correctly. MongoDB is down. OCR Service is not operational. Remedy: Make sure: S3 is up. The connection is configured properly. RabbitMQ Error Receive operation failed with an error during MQ test: \", e Possible causes: RabbitMQ or MongoDB (depending on the active profile in configuration) is not reachable from ocr rest or ocr worker. RabbitMQ or MongoDB is down. ocr worker is down. OCR Service is not operational. Remedy: Make sure: RabbitMQ or MongoDB is up. The network between RabbitMQ MongoDB node and ocr rest node is operational. RabbitMQ Error A bad message payload was received, expected='\" expectedPayload \"' actual='\" actual \"'\" Possible causes: An incompatible version of ocr worker is deployed. Message Queue for Health Check has incorrect custom configurations. OCR Service is not operational. Remedy: Make sure: Compatible version if ocr worker is deployed. ocr worker is up and has the correct connection settings to RabbitMQ. Message Queue for Health Check is configured properly and consistent between ocr rest and ocr worker. RabbitMQ Error A message was not received during MQ testing, expected='\" expectedPayload \"', workers supposedly are not handling messages in MQ\". Possible causes: RabbitMQ MongoDB (depending on the active profile in configuration) is not reachable from OCR worker OCR worker is not operational. Wrong version of ocr worker is deployed. Message Queue for Health Check has incorrect custom configurations. OCR Service is not operational. Remedy: Make sure: RabbitMQ or MongoDB is up. ocr worker is up and has the correct connection settings to RabbitMQ MongoDB. Message Queue for Health Check is configured properly and consistent between ocr rest and ocr worker. RabbitMQ Error Failed sending a message to RabbitMQ. Please, check that RabbitMQ is up and running. Refer to Installation Guide. ocr worker Error No reports from OCR workers are found or the reports have expired. Please check number of the OCR workers operating on the server and of the OCR workers number is less then required start required OCR worker number. Refer to WorkFusion OCR Guide. ocr worker Error Number of workers without errors is below the error threshold: \" counts 3 ) Possible causes: MongoDB is not reachable from OCR workers. OCR workers are not operational. OCR Service might handle the requests slowly or be non operational. ocr worker Warning Number of well operational OCR workers is below warning threshold: \" okCount ocr worker Info Number of well operational OCR workers: \" okCount ocr worker Error OCR worker where reports exceeded expiration threshold (\" errorTtl \" seconds): \" Make sure these ocr workers are up and can access MongoDB. License Info Remaining number of pages to be processed under the current OCR license: volumeRemaining License Warning Remaining number of pages to be processed under the current license is below warning treshold: volumeRemaining. Contact Workfusion support team to obtain a new license. License Warning Your OCR license is about to expire. Less than 10% of pages left. You will need to purchase a new license in order to continue using OCR. License Error Your OCR license has expired. You have to purchase a new license in order to continue using OCR. License Error OCR license is not found. Please, check that the OCR license is activated. License Error Failed to load the OCR license. Please, check the access permissions for it. "},{"version":"10.0","date":"Jul-01-2019","title":"hosts-yml-options","name":"hosts.yml options","fullPath":"iac/admin/advanced/hosts-yml-options","content":" The hosts.yml config file is located in the installation package and contains parameters of servers' hostnames. The file is to be completed during the preparation for the setup of the INT Server. Note that some of the services, which are installed to the INT Server by default, can be installed to other servers. However, this should be done wisely, taking into account system requirements and expected servers load. :::note If a single server has several host roles, the server must include all domain names that match the host roles it performs. All domain names must be specified in the config.yml configuration file. ::: Workfusion servers hostnames int_hostname: app_hostname: s3_hostname: ocr_hostname: ocrwin_hostname: bi_hostname: mssql_hostname: bepmasterhostname: bepagenthostname: rpa_hostnames: Uncomment in case of ELK installation to a separate dedicated apm server apm_hostname: elasticsearchhost: '{{ apmhostname }}' kibanahost: '{{ apmhostname }}' The variables below are only for internal usage, they should not be edited by end user. miniohost: '{{ inthostname }}' vaulthost: '{{ inthostname }}' nexushost: '{{ inthostname }}' rabbitmqhost: '{{ inthostname }}' zookeeperhost: '{{ inthostname }}' logstashhost: '{{ inthostname }}' tdshost: '{{ inthostname }}' Parameter Description int_hostname A DNS name of the INT Server app_hostname A DNS name of the APP Server s3hostname An additional DNS name for the APP Server. Specify the name only if Minio S3 backend is used. If an external S3 is used, refer to External S3 description. The name must be a separate existing DNS name assigned to the APP Server. The APP server's certificate must be valid for both apphostname and s3_hostname (either wildcard, or SAN). ocr_hostname A DNS name of the OCR Linux server. Leave the parameter empty, if OCR is installed to a Windows server ocrwin_hostname A DNS name of the OCR Windows server. Leave the parameter empty, if OCR is installed to a Linux server bi_hostname A DNS name of the Analytics Server mssql_hostname A DNS name of the MS SQL Server bepmasterhostname A DNS name of the BEP Master Server (former VDS or AutoML) bepagenthostname A DNS name of the BEP Agent Server (former VDS or AutoML). If you have several agents, they must be installed one by one with a name of each agent specified rpa_hostnames A DNS name of the RPA Server. RPA team to add information about possibility to have several hostnames apm_hostname A DNS name of the APM Server. Add it if you want to install ElasticSearch and Kibana to a separate dedicated APM server elasticsearchhost By default ElasticSearch is installed to INT server. Uncomment this option and specify 'apmhostname' above if you want to install it to a separate server. kibanahost By default Kibana is installed to APP server. Uncomment this option and specify 'apmhostname' above if you want to install it to a separate server. :::important Do not change the following service variables since they already have reasonable defaults: minio_host vault_host nexus_host rabbitmq_host zookeeper_host logstash_host tds_host ::: "},{"version":"10.0","date":"Jul-01-2019","title":"notifications-via-email","name":"Notifications via email","fullPath":"iac/admin/advanced/notifications-via-email","content":" Here, opt workfusion wf _installer is the default directory, where the installation package is extracted to. To setup the email notifications during the installation of the WorkFusion platform: In the configuration file config.yml, specify the following parameters: Name Description Default value mail_host Mail server host. Used by CT, WS localhost mail_port Port of mail server. Used by CT, WS 25 mail_sender Email address which will be used by CT and WS to send emails to users. noreply@example.com wsenableworkeremailconfirmation Whether to enable Email confirmation for worker users. Used by Workspace. false If the SMTP server requires authentication, in the configuration file config.yml, specify the following parameters: Name Description Default value mail_user Credentials used for authentication to SMTP server. Used by CT, WS, Monitoring. wfadmin mail_password Credentials used for authentication to SMTP server. Used by CT, WS, Monitoring. wfadmin "},{"version":"10.0","date":"Jul-01-2019","title":"prepare-rpa-ocrwin-and-bi-configurations","name":"Prepare RPA, OCRWIN and BI configurations","fullPath":"iac/admin/advanced/prepare-rpa-ocrwin-and-bi-configurations","content":" Windows configs can be generated on the Integration Server. In this guide, you can find the steps how to do it manually. Configuration files must be placed on the Integration Server, to the distr win _confs directory according to the server role: RPA – distr win confs rpa configs OCRWIN – distr win confs ocrwin configs BI – distr win confs bi configs To prepare configurations: Log in to the Integration Server via SSH. Change user to default user wfuser. su wfuser cd opt workfusion wf_installer Make sure that hostnames for desired server roles are filled with actual hostnames in hosts.yml: rpa _hostnames ocrwin _hostname bi _hostname For RPA, if there is more than one hostname, divide them with commas. Make sure that certificates are generated and located at certificates. For the BI server config generation: Make sure that tableau dashboard user and tableau dashboard pass are filled with correct values. . install.sh edit_config int For the RPA server config generation: In case of multiple bots, specify the rpabotsper_server parameter and define the number of bots. . install.sh edit_config int Run a configuration generation step for each server to generate config files: RPA server: . install.sh generate_conf rpa OCRWIN server: . install.sh generate_conf ocrwin BI server: . install.sh generate_conf bi These commands will ask for the Secrets Vault password that is used for encrypting the config.yml file. Copy the appeared directories to the destination Windows hosts. "},{"version":"10.0","date":"Oct-14-2019","title":"secret-vault-properties","name":"Secrets vault properties","fullPath":"iac/admin/advanced/secret-vault-properties","content":" Secrets Vault use cases Secret Vault addresses the following major use cases. Retrieve credentials from Password Manager Scope: Passwords used by Automation are managed by established Password Management software. Examples: Corporate applications which are integrated with Password Management software (CyberArk AIM, HashiCorp Vault Enterprise, Dell TPAM, etc.). Benefits: Ability to leverage already established password management solution. Integration: In such cases, the passwords are retrieved in the automation logic through API. See also: Automation Technology Classification Store passwords of legacy applications Scope: passwords that are used by Automation and are configured in Secrets Vault. Applications: legacy applications without integrated password management. Benefits: an ability to securely store sensitive third party and external passwords. Out of the box: by default, WorkFusion uses bundled HashiCorp Vault as a storage backend for such passwords. Integration: Alternatively OOB Hashicorp Vault can be replaced with customer's CyberArk AIM instance. Default safe name: _WFApp. Store internal SPA passwords Scope: passwords that are used by an SPA component or a module to connect to another component. Benefits: elimination of plain text passwords from configuration files. Out of the box: by default, WorkFusion stores such passwords in bundled HashiCorp Vault. Integration: Alternatively OOB HashiCorp Vault can be replaced with customer's CyberArk AIM instance. Default safe name: _WFInternal Examples: Database password used by Control Tower. OCR password used by Control Tower. Platform configuration properties Secure: The properties which contain sensitive information and stored in Password Vault. Non secure: The set of properties is not secure and can be defined or changed on the INT server, in the wf sec storage directory in the following files (if you have the OCR server installed on the same server): ocr rest secure.properties ocr worker secure.properties Control Tower secure properties (workfusion) The following properties must be configured by a separate method. If you apply the properties with an empty value, they are erased from the system. s3.access key s3.secret key s3.context.key.map s3.context.key.map.presign jwt.secret bot.manager.jwt.secret jwt.issuer ldap.server.url ldap.bind.dn ldap.bind.password wf.sso.saml.idp.metadata wf.sso.saml.idp.file.metadata wf.sso.saml.sp.metadata wf.sso.saml.username.attribute tableau.automation.host tableau.automation.username tableau.automation.password tableau.dashboard.host tableau.dashboard.username tableau.dashboard.password automl.services.endpoint automl.services.username automl.services.password automation.application.url automation.username automation.password nexus.url nexus.user nexus.password mail.username mail.password workfusion.accessKey workfusion.secretKey hazelcast.group.name hazelcast.group.password twilio.account.sid twilio.auth.token twilio.application.sid ct.datasource.url ct.datasource.username ct.datasource.password ds.datasource.url ds.datasource.username ds.datasource.password spring.rabbitmq.host spring.rabbitmq.port spring.rabbitmq.virtual host spring.rabbitmq.username spring.rabbitmq.password WorkSpace Secure Properties (workspace) The following properties are stored separately and required to configure WorkSpace: ws.datasource.username ws.datasource.password ws.secure.jwt.secret ws.secure.ldap.server.url ws.secure.ldap.bind.dn ws.secure.ldap.bind.password ws.sso.saml.idp.metadata ws.sso.saml.idp.file.metadata ws.sso.saml.username.attribute ws.sso.saml.sp.metadata ws.mail.username ws.mail.password OCR Secure Properties The following properties are stored separately in ocr rest secure.properties and ocr worker secure.properties, and are required for OCR configuration. OCR Linux ocr rest secure.properties ocr.jwt.secret ocr.task.sn ocr.task.engine.license.password ocr.spring.security.username ocr.spring.security.password ocr.spring.rabbitmq.username ocr.spring.rabbitmq.password ocr.spring.datasource.username ocr.spring.datasource.password ocr worker secure.properties ocr.spring.rabbitmq.username ocr.spring.rabbitmq.password ocr.spring.datasource.username ocr.spring.datasource.password OCR Windows ocr rest secure.properties ocr.jwt.secret ocr.task.sn.win ocr.task.engine.license.password.win ocr.spring.security.username ocr.spring.security.password ocr.spring.rabbitmq.username ocr.spring.rabbitmq.password ocr.spring.datasource.username ocr.spring.datasource.password ocr worker secure.properties ocr.spring.rabbitmq.username ocr.spring.rabbitmq.password ocr.spring.datasource.username ocr.spring.datasource.password View SPA secrets Follow this instruction to retrieve all secrets (passwords) that are used in SPA. To view SPA secrets, do the following: Log in to the INT server via SSH. Switch to . su wfuser Go to the directory. cd opt workfusion wf sec storage Run the following command: . loader.sh ansible review Output example INFO You are working with safe : workfusionansiblesecrets INFO Properties from secure storage INFO Key : Value INFO ansiblevaultpassword : secret_value INFO botmanagerjwtsecret : secretvalue INFO botrelayusername : secretvalue INFO botrelayuserpass : secretvalue INFO botuserbasename : secretvalue INFO botuserpass : secret_value INFO cakeypass : secret_value INFO rpabotsperserver : secretvalue INFO elkadminpassword : secret_value INFO elkelasticpassword : secret_value INFO elkkibanacertificatepass : secretvalue INFO elkkibanapassword : secret_value INFO elklogstashcertificatepass : secretvalue INFO elklogstashpassword : secret_value INFO hazelcastport : secretvalue INFO installdir : secretvalue INFO internetaccess : secretvalue INFO ldapbinddn : secret_value INFO ldapbindpassword : secret_value INFO ldapenabled : secretvalue INFO ldapgroupbase : secret_value INFO ldapgroupfilter : secret_value INFO ldapinternalauthorizationenabled : secretvalue INFO ldapserverurl : secret_value INFO ldapuserbase : secret_value INFO ldapuserfilter : secret_value INFO linuxinstallationuser : secret_value INFO logleveltype : secret_value INFO mailhost : secretvalue INFO mailpass : secretvalue INFO mailport : secretvalue INFO mailsender : secretvalue INFO mailuser : secretvalue INFO marathonproxyport : secret_value INFO marathonwebpassword : secret_value INFO mesosmasterport : secret_value INFO mesosslaveport : secret_value INFO mesosslaveportrange : secretvalue INFO mesoswebpassword : secret_value INFO mssqlctpass : secret_value INFO mssqlctuser : secret_value INFO mssqldbapass : secret_value INFO mssqldbauser : secret_value INFO mssqldmapipass : secretvalue INFO mssqldmapiuser : secretvalue INFO mssqldmpass : secret_value INFO mssqldmuser : secret_value INFO mssqldspass : secret_value INFO mssqldsuser : secret_value INFO mssqlexternal : secretvalue INFO mssqlocrpass : secret_value INFO mssqlocruser : secret_value INFO mssqlpmpass : secret_value INFO mssqlpmuser : secret_value INFO mssqlport : secretvalue INFO mssqlrapipass : secret_value INFO mssqlrapiuser : secret_value INFO mssqlrpapass : secret_value INFO mssqlrpauser : secret_value INFO mssqlsapass : secret_value INFO mssqlsqcpass : secret_value INFO mssqlsqcuser : secret_value INFO mssqlwspass : secret_value INFO mssqlwsuser : secret_value INFO nexusadminpass : secret_value INFO nexusapipass : secret_value INFO nexusdeploymentpass : secret_value INFO nginxport : secretvalue INFO nginxportssl : secret_value INFO ocrjwtsecret : secret_value INFO ocrplatform : secretvalue INFO ocrsecpass : secret_value INFO rabbitmqapiproxyport : secretvalue INFO rabbitmqbeppass : secret_value INFO rabbitmqocrpass : secret_value INFO rabbitmqvdspass : secret_value INFO rpabotagentpass : secretvalue INFO rpabotmanagerpass : secretvalue INFO s3accesskey : secret_value INFO s3secretkey : secret_value INFO ssosamlenabled : secret_value INFO ssosamlidpfilemetadata : secret_value INFO ssosamlidpmetadata : secretvalue INFO ssosamlspmetadata : secretvalue INFO ssosamlusernameattribute : secretvalue INFO tableauautomationpass : secret_value INFO tableaudashboardpass : secret_value INFO tableaulicensekey : secret_value INFO vaultadmincertpass : secretvalue INFO vaultport : secretvalue INFO vaultworkfusioncertpass : secretvalue INFO wfgroup : secretvalue INFO wfpassword : secretvalue INFO wfuser : secretvalue INFO wfusername : secretvalue INFO wininstalldir : secret_value INFO windowsinstallationpass : secret_value INFO windowsinstallationuser : secret_value INFO workspacelicenseaccesskey : secretvalue INFO workspacelicensesecretkey : secretvalue INFO workspacerootrequesterpass : secretvalue INFO wsenableworkeremailconfirmation : secret_value INFO wsldaprequestergroups : secretvalue INFO wsldapworkergroups : secretvalue INFO zookeeperport : secretvalue INFO It's all "},{"version":"10.0","date":"Sep-02-2019","title":"test","name":"test","fullPath":"iac/admin/advanced/test","content":" Test test 2"},{"version":"10.0","date":"Oct-04-2019","title":"using-tableau","name":"Setup end-to-end SSI audit chart","fullPath":"iac/admin/analytics/using-tableau","content":" This guide describes the usage of the WorkFusion Analytics Desktop, WorkFusion Analytics Server, and WorkFusion application integration. You can explore the end to end usage process based on simple Area Charts setup for one PostgreSQL table. The guide shows how the quantity of processed human tasks versus automation tasks changes in the course of time. WorkFusion Analytics Desktop Prepare Install WorkFusion Analytics Desktop on your local machine. Obtain access to the database on the environment where you will deploy your chart. In this example, the credentials to the 'wf _datastore' PostgreSQL database are required. Alternatively you can work with your local file (text, Excel, etc.). Obtain access to the WorkFusion Analytics Server integrated with the environment where you will deploy your chart. Connect to the DB Launch the WorkFusion Analytics Desktop. In the top left corner, click Connect to Data > PostgreSQL, and then enter the DB credentials. If you cannot connect to your DB, try to install the appropriate DB drivers from https: www.tableau.com support drivers. Now you can see the tables of your database. Drag the ds SSI AUDIT _TRAIL table to the white area. Click Update Now. The actual data from your table is displayed. Update now In this example, it is important to change the type in the Ssi Upload Timestamp column from String to Date & time to be able to take advantage of WorkFusion Analytics Date features. After you setup the data source, proceed to the chart visual editor. For that, in the center of the window, click Go to Worksheet. Setup Worksheet The Worksheet contains the following working areas: a . Dimensions Measures; b . Pages Filters Marks; c . Chart Preview Area; d . Columns Rows; e . Chart Type (Show Me). tableau workbook.png To setup a worksheet: Drag the following elements: Dimensions > Ssi Upload Timestamp to Columns Measures > Number of Records to Rows. In the Columns (d) area, right click the Ssi Upload Timestamp element and select the Day option. Upload timestamp In the Marks (b) toolbox, select the Area type. The area under the chart will be filled with color. To add chart differentiation by the Processed By column, drag the Processed By column from Dimensions (a) to the bottom of the Marks (b) toolbox. Select Color from the left context menu left. tableau marks.png Setup Dashboard As the Chart is ready, you can polish it with some filters. To setup Dashboard: Create a Dashboard for the chart. For that, in the main menu, select Dashboard > New Dashboard. On the Dashboard view, drag your chart from the Dashboard section to the design area. The Chart opens. tableau dashboard From the Chart context menu, add filter by Ssi Upload Timestamp and change the position of filter and the legend. tableau filter It's important to meet markup guides for charts in WF application. Change the Size as shown below. tableau width Change the locale by clicking File > Workbook Locale > English (United States). Edit chart colors, fonts, labels, etc., if needed. Deploy Dashboard After the above operations, you are ready to deploy the Dashboard on the WorkFusion Analytics Server. To deploy Dashboard: Connect to the server. For that, in the main meny, select Server > Sign In, and then enter the server name. To start the deployment, select Server > Publish Workbook, and then select your site from the drop down menu. Give a name to your Workbook for the WorkFusion Analytics Server. It's important to use ' _custom' postfix in name to meet WF Dashboard configuration requirements. The name must not start with a digit. The name of project must be the same as the name of the instance. In the Views to Share panel, check only the SSI Dashboard. Click Authentication button, and then select the Embedded password to let users see the chart in WF without entering the WorkFusion Analytics Server password. tableau publish.png Click Publish. The confirmation dialog with the preview appears. WorkFusion Analytics Server To manage the deployed chart, sign in to the WorkFusion Analytics Server web Admin with the same credentials as for WorkFusion Analytics Desktop. Here you can delete and change permissions to the Workbook. WF Application Dashboard The published chart appears on the WorkFusion Analytics > Custom page. "},{"version":"10.0","date":"Oct-04-2019","title":"workfusion-analytics-custom-dashboards-backup-and-restore","name":"Backup and restore custom Analytics dashboards","fullPath":"iac/admin/analytics/workfusion-analytics-custom-dashboards-backup-and-restore","content":" id: version 10.0 backup restore custom dashboards original_id: backup restore custom dashboards General Migration Flow Workbook migration flow includes the following steps: Download a workbook. Change data source connections if needed. Rename a workbook according to the naming convention. Publish a workbook to a new instance. Backup To backup dashboards means to download them from WorkFusion Analytics Server. WorkFusion Analytics Server 9 To backup WorkFusion Analytics Server 9: Open the WorkFusion Analytics Server page and select appropriate Site. Click Projects. Select the required project. Select project Find a workbook for downloading and expand the workbook menu by clicking the menu button (...). Click Download. The workbook file in the .twbx format will be downloaded. Download workbook WorkFusion Analytics Server 10 Open the WorkFusion Analytics Server page and select appropriate Site. Click Projects. Select the required project: Select project Find a workbook for downloading and expand the workbook menu by clicking the menu button (...). Click Download. The workbook file in the .twbx format will be downloaded. Select project Restore WorkFusion Analytics Desktop Step 1: Change connections (optional) :::important If all connections to database server remain the same, skip the step and proceed further. ::: Open the recently downloaded workbook file in WorkFusion Analytics Desktop. In the Dashboard section, click Go to sheet. Go to sheet On the Data tab, right click the data source and select Edit data source. Edit data source Edit data base connection settings and click OK. Db connection Move to dashboard sheet. See step 2. Step 2: Sign in to server On the menu bar, click Server > Sign in. server sign in Enter the URL of the BI server, and then click Connect. connect tableau server Enter the WorkFusion Analytics Server credentials, and then click Sign in. The user must have the Publisher, Site or Server Administrator role. sign in tableau Select the required Site for publishing. Selectt site Step 3: Publish workbook On the menu bar,click Server > Publish Workbook. publish workbook Check Project and Name fields. We strongly recommend using the ' _custom' postfix in the name to meet the WF Dashboard configuration requirements. The name must not start with a digit. Once the workbook is published, the dashboard appears on the Analytics > Custom page of CT. Select Show sheets as tabs option. Optionally, you can add the Refresh extract schedule. refresh extract schedule in the Sheets section, click Edit, and select sheets to publish. edit sheets in the Data Sources section, click Edit and change the publish type to Embedded in workbook and Authentication mode to Allow refresh access. sign in tableau Click Publish. Check the published workbook on the BI server. "},{"version":"10.0","date":"Oct-04-2019","title":"bi-windows-server-installation","name":"Install BI Windows Server","fullPath":"iac/admin/legacy/bi-windows-server-installation","content":" Prerequisites Installation packages are uploaded to one or several servers and unarchived. WorkFusionAnalyticsInstaller.zip contains the following directories with installers: WorkfusionAnalyticsServer WorkfusionAnalyticsDesktop License keys for WorkFusion Analytics Server and Desktop (optional) are provided by Workfusion Team. The predefined common variables are the following: . WorkFusion group. Default is wfuser. . WorkFusion user. Default is wfuser. . WorkFusion home directory C: workfusion. Defaults directories are the following: WorkfusionAnalyticsServer – C: workfusion WorkFusionServer WorkfusionAnalyticsDesktop – C: workfusion WorkFusionDesktop . WorkFusion Installer Directory. Default is C: workfusion WorkfusionAnalyticsServer. **. DNS of Application server (APP), for example, app.example.com. FileBeat installation directory. Default is C: workfusion filebeat. Analytics server hostname, for example, bi.example.com. Integration server hostname, for example,int.example.com. port of INT Logstash service. Default port is 4567. Install WorkFusion Analytics Server Always run installation of WorkFusion Analytics Server as a Windows Administrator user. The Installation process takes about 20 25 minutes. Before the installation, the Analytics databases must be migrated. To migrate the databases, run: . install.sh migrate bi The databases can also be migrated after the installation. In this case, run the earlier command and restart the Analytics server to apply the changes. To install the Analytics Server: On your prepared server, start the Command Prompt (not PowerShell) and, in the unpacked installation package, go to the WorkfusionAnalyticsServer directory. Set the following environment variables: SET win_user=ec2 user SET win_pass=1q2w3e! SET TABLEAUUSER=tableauuser SET TABLEAU_PASS=tab1q2w! Here, win _user is a user with Administrator rights under which installation is performed. win pass is a password of a win user. TABLEAU USER is a tableau dashboard _user of Workfsion Analitycs Server. TABLEAU PASS is a tableaudashboardpass of a TABLEAU USER. Note that special symbols are not allowed in the password. Copy the bi.crt and bi.key certificate files to the directory. Note: SSL configuration is mandatory starting from SPA version 10.0. Run the installation Batch file install _server.bat with the specified installation directory and APP host name. install_server.bat Example: install_server.bat C: workfusion WorkFusionServer app.exapmle.com Log in to the server and activate it with the license key. Optional. tsm login u tsm licenses activate k By default, the install _server.bat script activates the trial Tableau license for two weeks. Thus the step is optional. On the BI server, in a browser, open . CReate Tableau acc Install WorkFusion Analytics Desktop To install the Analytics Desktop: On your server, start the Command Prompt (not PowerShell) and go to the WorkfusionAnalyticsDesktop directory in the unpacked installation package. Run the installation Batch file install _desktop.bat with the specified installation directory: install_desktop.bat Example: install_desktop.bat C: workfusion WorkFusionDesktop Click the WorkFusion Analytics Desktop shortcut to start the application, and then enter the license key. Enter key For more information on Activation options, see Manage the WorkFusion Analytics license. Install WorkFusion Analytics Filebeat To install Filebeat: Start the Command Prompt (not PowerShell) and go to the Filebeat directory in the unpacked installation package. To enable log gathering, run the installation file install _filebeat.bat with the specified parameters. Specify as a directory on WorkfusionAnalyticsServer. install_filebeat.bat : Example: install_filebeat.bat C: workfusion filebeat C: workfusion WorkFusionServer bi.example.com int.example.com:4567 Copy the root certificate ca.crt file, which has been generated earlier, to the BI server's directory , and in the filebeat.yml config file, add the path to the certificate. Filebeat Start the Filebeat service: sc start filebeat Install WorkFusion Analytics Metricbeat To install Metricbeat: Start the Command Prompt (not PowerShell) and go to the metricbeat directory in the unpacked installation package. To enable metrics gathering, run the installation file install _metricbeat.bat with the specified parameters. Specify as a directory on WorkfusionAnalyticsServer. install_metricbeat.bat : Example: install_metricbeat.bat C: workfusion metricbeat C: workfusion WorkFusionServer bi.example.com INT.example.com:4569 Copy the root certificate ca.crt file, which was generated earlier, to BI server's directory and in the metricbeat.yml config file, add the path to the certificate: Metricbeat Start the etricbeat service: sc start metricbeat Deploy workbooks to WorkFusion Analytics Server This section describes deployment of Tableau workbooks to Tableau 2018.3 Server OEM on WorkFusion Analytics Server. The following workbooks must be uploaded to WorkFusion Analytics Server: Overview _WB Capacity _WB AutoML _WB RPA _WB Process _WB Speed _WB Manual _WB AA ETL Scheduler Prerequisites Pre installed software: Microsoft Server 2008 R2 or newer WorkFusion Analytics Server 2018.3.2 or newer Workbooks packages are uploaded to the servers and unarchived. The WorkFusionAnalitycsWorkbooks.zip archive contains the following directories: deploy _scripts dashboards Deploy workbooks To deploy workbooks, log in to the BI server with the installed WorkFusion Analytics Server and perform the following steps: Create the c: workfusion tdeploy home install directory and copy files from the deploy scripts directory to it. Pre installation directories after unpacking .zip c: workfusion tdeploy_home install (file: install.cmd) lib (installation file pack) Open the Command Prompt (not PowerShell), and run the following installation command: cd c: workfusion tdeploy_home install install.cmd c: workfusion tdeploy_home Created files and directories after install.cmd c: workfusion tdeploy_home deploy (files: createsitefolder.cmd, publish.cmd, manageserver.cmd) config (files: config.sample.cmd, deployconfig.cmd, importusers.csv) lib (files: deployworkbook.cmd, deploydatasource.cmd, getdatetime.vbs, getlogfile.cmd, logecho.cmd, replacestr.vbs) to_deploy (empty) deployed (empty) log (empty) The installation log file in the install YYYY MMDD HHMI.log format is created in c: workfusion tdeploy home install . Create directories for a New website: cd c: workfusion tdeploy_home deploy createsitefolder.cmd sitename Here, site_name is a part of the APP instance host. For example, if APP is hosted at https: wfapp.company.com , the command parameter is wfapp. A website and a project with the same name will be created on the WorkFusion Analytics Server. To create several site folders, specify parameters for the create sitefolder.cmd command. The command creates the new configuration file and new directories. The configuration file is copied from deploy config config. sample _.cmd. New files and directories after create _sitefloder.cmd c: workfusion tdeploy_home deploy config config.sitename.cmd (settings for the site \"sitename\") to_deploy site_name (dashboards for deploying) dashboard (generic dashboard) datasource (data sources for generic dashboard) deployed sitename (already deployed dashboards: are being moved from todeploy site_name during deploy process) dashboard datasource Final directory structure c: workfusion tdeploy_home install lib deploy config lib to_deploy site_name dashboard datasource deployed site_name dashboard datasource log Prepare configuration files. You can download the configuration files from the APP Server. For more information, see Prepare RPA, OCRWIN and BI configurations. If needed, change the files manually. To edit the configuration files manually, in c: workfusion tdeploy _home deploy config , open config.site _name.cmd, and specify the appropriate values. Common config parameters REM MSSQL DB options SET DATABASESERVERNAME=servername (MSSQL_HOSTNAME) SET DATABASE_NAME=workfusion SET DATABASE_PORT=1433 SET DATABASE_USERNAME=dbuser SET DATABASE_PASSWORD=dbpass REM WorkFusion Analytics Dashboard option SET REQUIRE_SSL=1 REM WorkFusion Analytics (Tableau) Server options SET TABLEAUSERVERURL=\"https: tableau.com\" SET TABLEAUSITE=usersite (sitename) SET TABLEAU_USERNAME=tabuser (user role must be Server Administrator) SET TABLEAU_PASSWORD=tabpass Config parameter Values for Control Tower config.site_name.cmd {{ x }} Ansible variable defined earlier Description DATABASESERVERNAME MSSQL_HOSTNAME {{ mssql_hostname }} A DNS name of the MS SQL database server DATABASE_NAME workfusion An MS SQL database name DATABASE_PORT 1433 {{ mssql_port }} The default value is 1433 for MS SQL DATABASE_USERNAME {{ mssqldmuser }} mssqldmuser from secrets.yml DATABASE_PASSWORD {{ mssqldmpass }} mssqldmpass from secrets.yml REQUIRE_SSL 1 The option for dashboard connection to require SSL. Possible values are 0 or 1. Default value is 1 TABLEAUSERVERURL &quot;https: {{ bi_hostname }}&quot; The URL of the Tableau Server TABLEAU_SITE site_name {{ wftableausite }} A part of the APP_HOSTNAME if APP is hosted at https: wfapp.company.com the value will be wfapp TABLEAU_USERNAME {{ bi_user }} A Tableau Server user name with \"Server Administrator\" rights TABLEAU_PASSWORD {{ bi_pass }} The \"Server Administrator\" user's password In c: workfusion tdeploy home deploy config , open import users.csv, and specify the appropriate values. A new user is created on WorkFusion Analytics Server and must be used for Control Tower – Analytics integration. The new user is added with the 'Viewer' role. Also, replace the existing values with already configured credentials. Name and Password must be equal to {{wf tableau dashboard username}} and {{wf tableau dashboard password}} properties generated in config.yml file during preparation. import _users.csv tableauuser,tableaupass,,,,,email@example.com Do not use '%', '&', ' ' or '=' characters and other special symbols in the password. The ' ' symbol is allowed.Do not add or delete any commas in import _users.csv. Create a WorkFusion Analytics website, a project and a new user by running the following command in the command line tool: cd c: workfusion tdeploy_home deploy manageserver.cmd sitename A website and a project are created on WorkFusion Analytics Server as sitename and WF Automation respectively. The new user is added to a newly created website. To create several websites on the Server, specify the required parameters for the manage server.cmd command. Publish workbooks To publish workbooks: Download the Workbook packages, which are provided by the WorkFusion team, and copy files from folder dashboards workbook to the c: workfusion tdeploy home deploy to deploy site _name dashboard directory. The datasource directory must remain unchanged for backward compatibility. Run publish.cmd to consequently publish (add or replace) each file on WorkFusion Analytics Server. cd c: workfusion tdeploy_home deploy publish.cmd site_name To publish workbooks on several websites on WorkFusion Analytics Server, specify the corresponding parameters for publish.cmd command. This command also moves the published file to the c: workfusion deploy deployed site _name dashboard directory. As a result, the updated workbooks appear at WorkFusion Analytics page.The log file is created in deploy log. Do not use TWB TWBX files located in the deploy to _deploy or deploy deployed directories repeatedly. Reload them from the delivery package each time. Schedule Configuration A user with the administrator privileges configures a Schedule on WorkFusion Analytics Server. The new Schedule must be added to the Server along with other already existing pre configured Schedules. To create a schedule: In Tableau, in the main menu, click Schedules, and then click New Schedule. Schedules Specify the following options: Name. Execution: serial. Select this option for correct data update. Frequency: minimum 15 minutes. New schedule Assign a Schedule to a Workbook: Go to Content → Workbooks, select a required Workbook, click More (...) to expand the options, and then click Refresh Extracts. The Schedule must be assigned to the AA ETL Scheduler and all * _WB Workbooks. Refresh Go to the Schedule a Refresh tab, select the required refresh time and click Schedule Refreshes. Schedule A Schedule is now created for the selected Workbook. Specify the Workbooks refresh order. The priority for ETL workbook mustbe changed to prevent data gap. For that: Go to Tasks → Extract Refreshes, select the AA ETL Scheduler workbook, and then click More (...) → Change Priority. Change priority Set priority to 10 and click Change Priority. Priority The ETL workbook will be refreshed first. Enable WorkFusion Analytics Dashboard in Control Tower To enable BI Analytics Server on WorkFusion platform: Install WorkFusion Analytics server, as described in Install WorkFusion Analytics Server. Deploy workbooks to WorkFusion Analytics server, as described in Deploying Workbooks to WorkFusion Analytics Server. Enable WorkFusion Analytics server on WorkFusion platform: On the INT Server, check and configure the Vault secure properties file. The file must already be configured before the installation. tableau.dashboard.host=https: bi.example.com tableau.automation.host=https: bi.example.com tableau.dashboard.username=tableau_user tableau.dashboard.password=tableau_pass tableau.automation.username=tableau_user tableau.automation.password=tableau_pass Property Name Example Value Description Required for tableau.dashboard.host https: bi.example.com WorkFusion Analytics Server website url for Dashboard WorkFusion Analytics Dashboard tableau.automation.host https: bi.example.com WorkFusion Analytics Server website url WorkFusion Analytics UI tableau.dashboard.username tableauuser WorkFusion Analytics Server website username for Dashboard (was used in importusers.csv) WorkFusion Analytics Dashboard tableau.dashboard.password tableaupass WorkFusion Analytics Server website password for Dashboard (was used in importusers.csv) WorkFusion Analytics Dashboard tableau.automation.username tableau_user WorkFusion Analytics Server website username for WF Automation. Automation is published on the same server so the same credentials as for tableau.dashboard should be used. WorkFusion Analytics UI tableau.automation.password tableau_pass WorkFusion Analytics Server website password for WF Automation. Automation is published on the same server so the same credentials as for tableau.dashboard should be used. WorkFusion Analytics UI On the APP Server, restart the Workfusion service: sudo su wfuser; wfmanager restart workfusion Wait for about one minute and check that there are no any errors in the Control Tower's application logs: sudo su wfuser; tailf n 50 opt workfusion supervisord log workfusion.out.log Check the installation To check the installation: In a browser, open URL https: > to go to the APP instance. The following menu appears: ct menu In the main menu, go to the Analytics group to check that the the dashboard is displayed. check dashboard "},{"version":"10.0","date":"Jul-08-2019","title":"install-linux-servers","name":"Install Linux servers","fullPath":"iac/admin/legacy/install-linux-servers","content":" Overview :::note Before you proceed, make sure, you have prepared the servers according to Prepare for Installation and executed Precheck. ::: The SPA component must be installed in the following order: MS SQL server INT server BEP master server BEP agent(s) server(s) APP server OCR server (Linux) All installation steps must only be run from the directory on your Integration server. cd Install MS SQL Server On the INT server, check that MS SQL meets the requirements: . install.sh precheck mssql If all checks have passed successfully, run the following command to grant necessary permissions for database schemes. . install.sh configure mssql INT Server To install the Integration Server: As a root user, run the following command: . install.sh preinstall int As a , run the following commands: . install.sh install int . install.sh check int BEP Master Server To install the BEP Master Server: As a root user, run the following command: . install.sh preinstall bep master As a , run the following commands: . install.sh install bep master . install.sh check bep master BEP Agent Server To install the Agent Server: As a root user, run the following command: . install.sh preinstall bep agent As a , run the following commands: . install.sh install bep agent . install.sh check bep agent APP Server To install the APP Server: As a root user, run the following command: . install.sh preinstall app As , run the following commands: . install.sh install app . install.sh check app OCR Server (Linux) To install the OCR Linux Server: As a root user, run the following command: . install.sh preinstall ocr As a , run the following commands: . install.sh install ocr . install.sh check ocr "},{"version":"10.0","date":"Oct-04-2019","title":"install-ocr-windows-server","name":"Install OCR Windows server","fullPath":"iac/admin/legacy/install-ocr-windows-server","content":" Prepare for installation Before manual OCR Windows Server installation, you need to Initialize secure storage (Secrets Vault) Run DB migrations Generate configuration files for copying to OCR Windows Server. All the commands are run on Integration Server. To prepare for installation: Run DB migrations for OCR Windows Server. To populate MSSQL database with required tables in OCR schema, run the following command: . install.sh migrate ocrwin Initialize Secrets Vault for OCR Windows Server. . install.sh configure ocrwin Generate OCR Windows Server config files and OCR certificates to be copied to OCR Windows Server while installation. . install.sh generate_conf ocrwin As soon as generation is complete, go to distr win configs ocrwin configs to view the generated files. See the output example below. ls la distr winconfs ocrwinconfigs total 24 drwxr xr x. 2 wfuser wfuser 100 Jun 13 07:44 . drwxr xr x. 3 wfuser wfuser 28 Jun 13 07:44 .. rw r r . 1 wfuser wfuser 2984 Jun 13 07:44 ca.crt rw r r . 1 wfuser wfuser 3085 Jun 13 07:44 ocr_config.ini rw r r . 1 wfuser wfuser 4835 Jun 13 07:44 ocrwin.crt rw r r . 1 wfuser wfuser 1674 Jun 13 07:44 ocrwin.key rw r r . 1 wfuser wfuser 2517 Jun 13 07:44 workfusion.p12 Install OCR Windows Server To install OCR Windows Server: Extract all files from the OCRInstaller {version}.zip to any folder on your Windows PC. Go to distr win configs ocrwin configs and copy all the generated config files ( prepare for installation) to the folder with the extracted OCR installer on the Windows machine. Extracted If you use the self signed certificates, ca.cert must be imported into Local Machine's Certificate TrustStore > the Trusted Root Certificates Authorities section. For more information, see Import Root or Intermediate CA to Store. Launch OCR Installer. Click Options to specify the installation location. Install location Wait until the installation process is finished. Restart all services. Activate OCR License. Configure Nginx on Windows machines The section describes using a secure connection between OCR and Workfusion Services. Skip the step, if the setup does not use TLS encryption. Configure Windows Firewall In Windows Firewall, create a new rule. Allow inbound SSL secure connection to applications on the OCR server. New rule ocr allow connection Define the local port 443 to allow the connection. port 443 "},{"version":"10.0","date":"Oct-14-2019","title":"install-rpa-windows-server-using-generated-configs","name":"Install RPA Windows servers with generated configs","fullPath":"iac/admin/legacy/install-rpa-windows-server-using-generated-configs","content":" Install RPA package Install RPA as the Administrator user on a Windows machine. To install the RPA package: Download the installer file, and then extract all files from the RPAInstaller {version}.zip to any folder on your PC. Launch RPAInstaller.exe and follow the RPA Setup Wizard steps. Allow the installer to make changes to your computer. Accept the License Agreement and proceed. Set the Destination Folder for the RPA installation and wait until the installation process is finished. By default, the destination directory for RPA installation is C: RPA that will be further referred to as . Destination Configure Users Before RPA Windows Server installation, users must already be created and have local profiles on the RPA machine. In case you need to create users, see the instructions. Run the following commands on a Windows machine, in PowerShell, as Bot Master. Bot Master is the user that triggers Bot Units. in Control Tower, register Bot Master in Secrets Vault. To do that, add two secret entries with Alias for Bot Master user and Alias for Bot Master password. The Key box must be the same as Alias, while Value stands for either username or password. Use the following format to proceed: Bot Master User secret entry secretsentry user Bot Master Password secret entry secretsentry user Grant RPA with full folder permissions to all created users – Bot Master, Bot Unit X. The RPA folder is the folder where your RPA is installed. Make sure that Bot Master is a member of the Administrator group. For more information on configuring groups and permissions in Windows, see Configuring permissions and groups. Enable RDS to use multiple bots on one server Multiple RDP Sessions are configured on RPA Windows Server. To enable RDS to use multiple bots: Go to Server Manager > Manage > Add Roles and Features. Dashboard The Add Roles and Features Wizard opens. Dashboard Go to Server Selection > Server Roles > Remote Desktop Services > Next. Dashboard Select Remote Desktop Session Host > Next > Add Features. Dashboard Restart your machine. Dashboard Verify Internet Explorer settings (optional) Verification is required in case Internet Explorer is used. The settings are verified for each added user. For more details, see Configuring Internet Explorer for RPA. Configure firewall for Nginx Nginx configuration is required to provide secure communication among RPA machines. To provide access to Nginx, configure Windows Firewall to define a port for connection: Go to Control Panel > Windows Defender Firewall. Select Advanced settings > Inbound Rules, and then click New rule to allow inbound secure communication for applications to use the SSL connection. Select TCP and define the local port 443 to allow the connection. Specify the connection name and click Finish. Generate configuration files for RPA Windows Server RPA Windows Server configuration files are generated on the Integration Server. Log in to the Integration Server via SSH. Change to default user wfuser. su wfuser cd opt workfusion wf_installer Mind that your default username may differ. When installing SPA, you can set up the username in the config.yml file. The default value is wfuser. In the hosts.yml file, specify the actual hostnames for rpa _hostnames. If there is more than one hostname, use a comma to separate them. Make sure that certificates are generated and located in certificates. In case of multiple bots, specify the rpa bots per _server parameter and define the number of bots. . install.sh edit_config int Run a configuration generation command. . install.sh generate_conf rpa Configuration files are placed to distr win confs rpa configs. For more information, see prepare RPA, OCRWIN and BI configurations. Apply generated configuration files All the generated configuration files are formed in the corresponding RPA folder structure for each RPA server separately. To apply configuration files: Archive all the configuration files for each RPA server: zip r rpa testserver rpa2.workfusion.com.zip . rpa testserver rpa2.workfusion.com Copy all the generated files from the Integration server to a required RPA server. Use the following path to the files: distr win confs rpa configs. Unarchive and put all the generated configuration files to the RPA package home directory(or ), while preserving the directory structure. Schedule Bot Agent startup on logon Scheduling of tasks can be performed using the task scheduler or advanced group policies. Bot Agent startup is scheduled to autostart an RDP session (Bot Master, Bot Unit X) by Bot Manager. To schedule Bot Agent's startup: For one Bot Unit per server: Open the Windows console and enter the following commands: SchTasks Create SC onlogon TN \"bot agent BotMaster\" TR \" bot agent bin bot agent nordp.bat\" RU BotMaster IT F For several Bot Units per server: Configure tasks – one for Bot Master user and one for each Bot Unit. To do that, enter the following commands: SchTasks Create SC onlogon TN \"bot agent BotMaster\" TR \" bot agent bin bot agent master.bat\" RU BotMaster IT F SchTasks Create SC onlogon TN \"bot agent BotUnit1\" TR \" bot agent bin bot agent unit1.bat\" RU BotUnit1 IT F SchTasks Create SC onlogon TN \"bot agent BotUnitX\" TR \" bot agent bin bot agent unitX.bat\" RU BotUnitX IT F Configure Bot Master autostart Bot Master autostart is set up to run RDP sessions. To allow autostart, provide correct RPA Windows Server hosts. To configure Bot Master autostart: On the Application Server, go to the bot manager config directory. In machines.yml, provide correct RPA Windows Server hosts. machines.hosts: test1.workfusion.com test2.workfusion.com test3.workfusion.com Restart Bot Manager. wfmanager restart bot manager Start Bot Units Before starting Bot Units, make sure you made necessary changes. There are several ways to run Bot Units – either with Bot Manager (automatic start from Application linux server call), or by using Bot Agent. We recommend avoiding manual start. Use automatic start from the Applicationlinux server call instead. To start Bot Units manually... Before starting the services, select Deployment Architecture. For one Bot Unit per server (VDI setup): Run bot agent bin bot agent nordp.bat to start Nginx, Bot Master, and Bot Unit. Your user session must be active to guarantee correct work. For multiple Bot Units per server (RDP in RDP): Run bot agent bin bot agent master.bat to start Nginx, Unit RDP sessions in a separate RDP session. Make sure to change The Master Agent configuration that depends on your sessions Unit count and create additional configs for Units. For correct work, Unit sessions must be active internally in the master session, but the master session can have the disconnected status. Run Unit agents in each Unit RDP session: bot agent bin bot agent unit{Unit_ID}.bat. Check RPA Installation There are several ways to check RPA Windows Server installation. To check installation: Make sure that Bot Units are available, and WebDrivers exist on the connected Bots. To do that, open the following link as any user on the RPA machine. http: localhost:1500{unit_id} grid console On evoking the command, view the list of available Bot Units in your browser. To check RPA Worker, open the following link in your browser under as any user on the RPA machine: http: localhost:1520{unit_id} health. You receive the following response: { \"status\": \"UP\", \"details\": { \"rabbit\": { \"status\": \"UP\", \"details\": { \"version\": \"3.7.14\" } }, \"diskSpace\": { \"status\": \"UP\", \"details\": { \"total\": 53317988352, \"free\": 25653489664, \"threshold\": 10485760 } }, \"refreshScope\": { \"status\": \"UP\" }, \"zookeeper\": { \"status\": \"UP\", \"details\": { \"connectionString\": \"hostname.workfusion.com:port\", \"state\": \"STARTED\" } } } } Open Bot Manager and make sure that your RPA Windows Server hostname is available, and that you are able to send commands to Bot Unit components. Run any RPA task and view logs in Kibana. "},{"version":"10.0","date":"Oct-14-2019","title":"install-rpa-windows-server","name":"Install RPA Windows server","fullPath":"iac/admin/legacy/install-rpa-windows-server","content":" Install RPA Windows Server 1. Install RPA Package Run the following steps as the Administrator user on a Windows machine. Download the installer file, and then extract all files from the RPAInstaller {version}.zip to any directory on your PC. Launch RPAInstaller.exe, follow the RPA Setup Wizard steps, and allow the program to make changes to your computer. Allow changes Accept the License Agreement and proceed. Specify the Destination Folder for RPA installation and wait until the installation process is finished. By default, the destination directory for RPA installation is C: RPA that will be referred to as below. 2. Import and configure certificates The list of your certificates needed at this step is as follows. Certificate Usage Description ca.crt Filebeat Metricbeat Nginx Java Root CA certificate in the .pem format (the file may have .crt, .cer, .pem, or .cert extension). All the other certificates in the table must be signed by this Root CA that is imported into Java Truststore on all servers. The certificate file must include only Root cert, without any intermediate certs. rpa.crt &gt; Rename to server.pem while configuration. rpa.key &gt; Rename to server.key while configuration. Nginx Certificate and its private key files in the .pem format. Certificate's CommonName or SANs must match all servers in the list of rpa_hostnames. The certificate file must include end certificate plus intermediate cert(s) if exist(s). Private key should not be password protected. mtls client.crt mtls client.key Nginx Certificates used to implement Mutual TLS authentication between services. vault_workfusion.p12 &gt; Rename to workfusion.p12 while configuration. Secrets Vault Specific Secret Storage certificate used for authentication with Secrets Vault. For more information on certificates, see Prepare TLS certificates for installation. To import certificates: To locate the trusted certificates, follow the path opt workfusion wf _installer certificates . The certificates are placed in the certificates directory on the Integration Server. Import your root certificate file ca.crt to Local Machine's Certificate TrustStore > Trusted Root Certificates Authorities. Configure certificates for: Filebeat Metricbeat Secrets Vault Nginx Java Filebeat The Filebeat service sends content of specific log files. To get logs, Filebeat must be imported. For that, in your filebeat directory, create the ssl directory, and copy ca.crt to filebeat ssl. Metricbeat The Metricbeat service sends metrics data to the Logstash service. To import the Metricbeat certificates, in the metricbeat directory, create the ssl directory, and copy ca.crt to metricbeat ssl. Secrets Vault To import the Secrets Vault certificates: In the bot agent directory, create the ssl directory, and upload the vault _workfusion.p12 file to bot agent ssl. Rename the file to workfusion.p12. In the worker directory, create the ssl directory, and upload vault _workfusion.p12 to the worker ssl. Rename the file to workfusion.p12. Nginx To import the Nginx certificates: In your nginx directory, create the ssl directory, and copy ca.crt to nginx ssl. Upload SSL Certificate and SSL key files to the nginx ssl directory. Rename rpa.crt to server.pem, and rpa.key to server.key. Upload the TLS certificate mtls client.crt and key file mtls client.key to the same directory. Update Bot Agent keystore RPA Bot Agent does not support SSL while Application Server does. For Server, RPA Bot Agent keystore needs to be updated. To update the keystore: Go to the Java bin directory java bin. Import the downloaded ca.crt to the RPA Java keystore. Run the following command with your arguments from the java bin directory. keytool import alias localCA file \"path to ca.crt\" keystore \"path to cacerts\" storepass changeit where Arguments are as follows: Argument Description Example path to ca.crt the absolute path to downloaded ca.crt nginx ssl ca.crt path to cacerts absolute path to default RPA Java keystore java jre lib security cacerts changeit default password for RPA Java keystore 3. Configure users Before RPA Windows Server installation, users must be created and have local profiles on RPA machine. For more information on user creation, see Create RPA users. Run the following commands on a Windows machine, in PowerShell as the Bot Master user. Bot Master is the user that triggers Bot Units. To configure users: in Control Tower, register Bot Master in Secrets Vault. To do that, add two secret entries with Alias for Bot Master user and Alias for Bot Master password. The Key field must be the same as Alias, while Value stands for either username or password. Bot Master User secret entry: Bot Master Password secret entry: Grant RPA full directory permissions to all created users: Bot Master Bot Unit X The RPA full directory is the directory where your RPA is installed. Make sure that Bot Master is a member of the Administrator group. Follow the link to learn more on configuring groups and permissions in Windows. 4. Enable RDS to use multiple bots on one server The number of RDS sessions must be the same as the number of bots. To configure multiple RDP sessions on the RPA Windows server: Navigate to Server Manager > Manage > Add Roles and Features. The Add Roles and Features Wizard opens. Go to Server Selection > Server Roles > Remote Desktop Services > Next. Select Remote Desktop Session Host > Next > Add Features. Restart your machine. 5. Verify Internet Explorer settings (optional) Verification is required in case Internet Explorer is used. The settings are verified for each added user. For more details, see Configuring Internet Explorer for RPA. 6. Configure Nginx Nginx configuration is required to provide secure communication among RPA machines. To provide access to Nginx, configure Windows Firewall to define a port for connection: Go to Control Panel > Windows Defender Firewall. Select Advanced settings > Inbound Rules, and then click New rule to allow inbound secure communication for applications to use the SSL connection. Select TCP and define the local port 443 to allow the connection. Specify the connection name and click Finish. To configure Nginx: Go to nginx conf nginx.conf. Replace all {RPA _HOSTNAME} with a valid DNS name of your RPA Windows Server. Replace {APP HOST IP} with the IP address of Application Server. Replace {RPA HOST IP} with the IP address of your RPA Windows Server. To set up communication with Control Tower, add the following settings to nginx.conf. Replace APP DNS NAME} with Application Server's FQDN. mTLS client config for communication with Control Tower. Used by worker. server { listen 127.0.0.1:7080; location workfusion internal api { proxysslcertificate .. ssl mtls client.crt; proxysslcertificate_key .. ssl mtls client.key; proxysslciphers HIGH:!aNULL:!MD5; proxysslprotocols TLSv1.1 TLSv1.2; proxysslsession_reuse on; proxysslverify on; proxyssltrusted_certificate .. ssl ca.crt; proxysslverify_depth 2; proxypass https: {APPDNS_NAME}:7083 workfusion internal api; } location workfusion { deny all; return 403; } } To set up communication with BEP, add the following settings to nginx.conf. Replace BEP MASTER DNS _NAME} with BEP Master Server's FQDN. mTLS client config for communication with automl gateway service server { listen 127.0.0.1:9070; location automl gateway service { proxysslcertificate .. ssl mtls client.crt; proxysslcertificate_key .. ssl mtls client.key; proxysslciphers HIGH:!aNULL:!MD5; proxysslprotocols TLSv1.1 TLSv1.2; proxysslsession_reuse on; proxysslverify on; proxyssltrusted_certificate .. ssl ca.crt; proxysslverify_depth 2; proxypass https: {BEPMASTERDNSNAME}:9073 automl gateway service; } } 7. Configure Bot Agent For correct performance of Bot Units, you should configure Bot Agent environment parameters as well as to set up Bot Master autostart and Bot Agent startup. Configure Bot Agent environment parameters For correct RPA environment work, set the required environment parameters. In bot agent conf environment.yml, replace parameters with {} brackets. Do not replace parameters in {} with the symbol, for example, {rpa.rabbitmq.worker.queue.template} . Specify the following parameters: {ZOOKEEPER _HOSTNAME} – hostname of the Integration Server where the Zookeeper service in installed. {ZOOKEEPER _PORT} – by default, 2181. {BOT MANAGER HOSTNAME} – hostname of Application Server where Bot Manager is installed. {RPA _HOSTNAME} – RPA Windows Server hostname. {LOGSTASH _HOSTNAME} – hostname of Integration Server where the Logstash service is installed. Default environment parameters file: spring: cloud: zookeeper: connect string: {ZOOKEEPERHOSTNAME}:{ZOOKEEPERPORT} environment: rpanginxport: 443 botmanagerhostname: {BOTMANAGERHOSTNAME} botmanagerport: 443 botmanagerscheme: https botmanagerpath_context: \"bot manager\" botmanageruser: ' {bot.manager.security.user.name}' botmanagerpassword: ' {bot.manager.security.user.password}' agent.hostname: {RPA_HOSTNAME} agent.port: 443 agent.scheme: https logstashhost: {LOGSTASHHOSTNAME} logstash_port: 4567 metricbeat_port: 4569 rabbitmqqueuefull_name: ' {rpa.rabbitmq.worker.queue.template}. {fleet}' worker_gavp: ' {rpa.default.worker.gavp}' workerclientid: ' {rpa.default.worker.client.id}' agentzookeeperconnectstring: {ZOOKEEPERHOSTNAME}:{ZOOKEEPER_PORT} server_address: 127.0.0.1 :::important Bot Manager credentials are not visible in bot agent conf environment.yml. The credentials are set up and stored in Secrets Vault. ::: Schedule Bot Agent startup on logon Scheduling of tasks can be performed using the task scheduler or advanced group policies. Bot Agent startup is scheduled to autostart an RDP session (Bot Master, Bot Unit X) by Bot Manager. To schedule Bot Agent's startup: For one Bot Unit per server: Open the Windows console and enter the following commands: SchTasks Create SC onlogon TN \"bot agent BotMaster\" TR \" bot agent bin bot agent nordp.bat\" RU BotMaster IT F For several Bot Units per server: Configure tasks – one for Bot Master user and one for each Bot Unit. To do that, enter the following commands: SchTasks Create SC onlogon TN \"bot agent BotMaster\" TR \" bot agent bin bot agent master.bat\" RU BotMaster IT F SchTasks Create SC onlogon TN \"bot agent BotUnit1\" TR \" bot agent bin bot agent unit1.bat\" RU BotUnit1 IT F SchTasks Create SC onlogon TN \"bot agent BotUnitX\" TR \" bot agent bin bot agent unitX.bat\" RU BotUnitX IT F Configure Bot Master autostart Bot Master autostart is set up to run RDP sessions. To allow autostart, provide correct RPA Windows Server hosts. To configure Bot Master autostart: On the Application Server, go to the bot manager config directory. In machines.yml, provide correct RPA Windows Server hosts. machines.hosts: test1.workfusion.com test2.workfusion.com test3.workfusion.com Restart Bot Manager. wfmanager restart bot manager 8. Configure Bot Units The configuration step includes configuring multiple Bot Units as well as Master Bot Agent. Configure multiple Bot Units To configure Nginx for multiple Bot Units: In the nginx conf bot units directory, create as many templates as the number of Bot Units you have to set up, for example, five templates for five Bot Units. Copy unit.template to unit{unit id}.conf where unit id is a digit between 1...99 (if your unit id more than **9, remove last \"0\" in the proxy pass port). Open the template and replace all the unit id placeholders with the same unit id number that you used to name the file. Modify or copy file bot agent conf units unit{Unit _id}.yml. (Unit1.yml base example). Change unit_id and fleet. Here, shared is a default fleet. If your unit_id is more than 9, remove the last \"0\" in each port – maximum 99 units for one machine. unit_id: 2 fleet: shared server.port: 1000 {2} logging.directory: logs unit {2} node_port: 1510 {2} hub_port: 1500 {2} worker_port: 1520 {2} bot.agent.context.path: unit {2} environment.agent.ns: \" {environment.agent.hostname}: {server.port}\" If you need to have custom fleets to submit tasks to, specify fleet for each Bot Unit. Fleets are used to distribute tasks between RPA Workers. To learn more, see Task Distribution. unit_id: 3 fleet: SAP Create the start .bat file for each Unit: bot agent bin bot agent unit{Unit _ID}.bat, and then change unitid. @ECHO OFF set unitid=2 cd %~dp0 set config=conf bootstrap.yml,conf environment.yml,conf units unit%unitid%.yml,conf bot agent unit.yml call bot agent.cmd run Unit%unitid% \"%config%\" Configure Master Bot Agent To configure Master Bot Agent: Edit or copy bot agent conf bot agent master.yml. Add additional Unit processes for start. Default config part: processes: id: rdp1 expression: \"cmd c start wait mstsc unit.rdp v:127.0.0.1\" directory: \"rdp\" id: rpa.nginx expression: \"cmd c start wait start_nginx.bat\" directory: \".. nginx\" tag: \"nginx\" id: rpa.filebeat expression: \"cmd c start wait filebeat pipeline.bat {environment.agent.hostname} {environment.logstashhostname} {environment.logstashport}\" directory: \".. filebeat\" tag: \"filebeat\" The code example for two Bot Units is the following. processes: id: rdp1 expression: \"cmd c start wait mstsc unit.rdp v:127.0.0.1\" directory: \"rdp\" id: rdp2 expression: \"cmd c start wait mstsc unit.rdp v:127.0.0.2\" directory: \"rdp\" id: rpa.nginx expression: \"cmd c start wait start_nginx.bat\" directory: \".. nginx\" tag: \"nginx\" id: rpa.filebeat expression: \"cmd c start wait filebeat pipeline.bat {environment.agent.hostname} {environment.logstashhostname} {environment.logstashport}\" directory: \".. filebeat\" tag: \"filebeat\" 9. Start Bot Units Before starting Bot Units, make sure you made necessary changes. There are several ways to run Bot Units – either with Bot Manager (automatic start from Application linux server call), or by using Bot Agent. We recommend avoiding manual start. Use automatic start from the Applicationlinux server call instead. To start Bot Units manually... Before starting the services, select Deployment Architecture. For one Bot Unit per server (VDI setup): Run bot agent bin bot agent nordp.bat to start Nginx, Bot Master, and Bot Unit. Your user session must be active to guarantee correct work. For multiple Bot Units per server (RDP in RDP): Run bot agent bin bot agent master.bat to start Nginx, Unit RDP sessions in a separate RDP session. Make sure to change The Master Agent configuration that depends on your sessions Unit count and create additional configs for Units. For correct work, Unit sessions must be active internally in the master session, but the master session can have the disconnected status. Run Unit agents in each Unit RDP session: bot agent bin bot agent unit{Unit_ID}.bat. 10. Check RPA Windows Server installation There are several ways to check RPA Windows Server installation. To check installation: Make sure that Bot Units are available, and WebDrivers exist on the connected Bots. To do that, open the following link as any user on the RPA machine. http: localhost:1500{unit_id} grid console On evoking the command, view the list of available Bot Units in your browser. To check RPA Worker, open the following link in your browser under as any user on the RPA machine: http: localhost:1520{unit_id} health. You receive the following response: { \"status\": \"UP\", \"details\": { \"rabbit\": { \"status\": \"UP\", \"details\": { \"version\": \"3.7.14\" } }, \"diskSpace\": { \"status\": \"UP\", \"details\": { \"total\": 53317988352, \"free\": 25653489664, \"threshold\": 10485760 } }, \"refreshScope\": { \"status\": \"UP\" }, \"zookeeper\": { \"status\": \"UP\", \"details\": { \"connectionString\": \"hostname.workfusion.com:port\", \"state\": \"STARTED\" } } } } Open Bot Manager and make sure that your RPA Windows Server hostname is available, and that you are able to send commands to Bot Unit components. Run any RPA task and view logs in Kibana. "},{"version":"10.0","date":"Jul-08-2019","title":"legacy-overview","name":"Overview","fullPath":"iac/admin/legacy/legacy-overview","content":" This chapter describes installation and configuration process of WorkFusion SPA on customer provided servers. Components Currently, the full Workfusion SPA product installation requires a minimum of 8 servers: Integration server (Linux) MS SQL server (Windows) APP server (Linux) OCR Server (Linux or Windows) BEP Master (Linux) BEP Agent(s) Linux RPA Server(s) (Windows) BI Server (Windows) Prerequisites The servers are up and running and all system requirements are met as described in System Requirements. SSH access is allowed to the Linux servers with root privileges. Users with the administrator privileges are available on all Windows servers. DNS names for all servers (Linux and Windows) are configured according to DNS Names Configuration. Application Ports are open among servers. Installation packages are uploaded to servers. The default Workfusion installation directory (for example, opt workfusion) is created on the Integration server. The default Workfusion directory for extracted packages (for example, opt workfusion wf _installer) is created on the Integration server. The default Workfusion installation directory (for example, C: workfusion) is created on the server. The time synchronization service must be set up on all servers. All servers must have the same time zone. Installation steps Download packages to servers and fill config files: Prepare for Installation. Install Workfusion SPA components to Linux Servers. Install Workfusion SPA components to Windows servers by following the guides: RPA Windows Server. OCR Windows Server. Optional. OCR can also be a part of Linux Servers Installation. BI Windows Server Installation. Perform post installation steps. "},{"version":"10.0","date":"Oct-14-2019","title":"prepare-for-installation","name":"Prepare for installation","fullPath":"iac/admin/legacy/prepare-for-installation","content":" Create RPA users Before installation, the Bot Master and Bot Unit users must be created on each server that you plan to deploy and specified later in hosts.yml, in the rpa_hostnames list (see the Configure installation step). The users must meet the following requirements: A Bot Master user must have the same username and password on all RPA servers. A Bot Master user must be a member of the Administrators and Remote Desktop Users groups on all RPA servers. All Bot Unit users must have the same username and password on all RPA servers. A Bot Unit user must be a member of the Remote Desktop Users group on all RPA servers. The number of created Bot Unit users must be specified in config.yml in the rpabotsper_server parameter, on the Configure installation) step. All Bot Unit users must have common basename and different index numbers in the end of the basename. For more information, see config.yml options, section RPA. For example, if, in config.yml*, you set rpa bots per server: 2, and bot unit base name: BotUnit, then the following users must be present on each RPA server: BotUnit1, BotUnit2,... To create an RPA user on the Windows server: Connect to the remote Windows server via Remote Desktop, or any other software, as a user with the Administrator's privileges. Run PowerShell as Administrator. In PowerShell, run the following commands to create a Bot Master user and add it to groups: net user BotMaster add net localgroup \"Administrators\" BotMaster add net localgroup \"Remote Desktop Users\" BotMaster add Here, is the password for the Bot Master user. In PowerShell, run the following commands to create the Bot Unit users and add them to groups: net user BotUnit1 add net localgroup \"Remote Desktop Users\" BotUnit1 add Here, is the password for the Windows Bot Unit user. Prepare the MS SQL Server For Workfusion SPA installation you need to have an existing activated instance of the MS SQL server. The server must be up and running during the Workfusion SPA installation. MS SQL Server must run in SQL Server and Windows Authentication mode: SQL Server and Windows Authentication mode To prepare a MS SQL server: Create the database, and specify the database name. For other options you may use the default values. Create DB Create the owner with the full access permissions for the database. This user must be later specified in config.yml. Note: for all logins, the following password policies are applied: Allowed special symbols for db password for monitoring tools: @ *():,.} Allowed special symbols for db password for the Analytics component: @ *():,.;} To create : On a MS SQL server, create the mssql login, specify the login name, and then make sure that the Enforce password policy check box is not selected. In the Default database box, specify the database, and then choose English as Default language. Create DB owner In the Select a page group, click User mapping, and then select the check box to map the user to the database. Leave all other options unchanged. Create DB user In the database Users, select the db owner check box to assign to the db owner role. Assign to group Create the following MS SQL login names and passwords for the Workfusion components: mssql dba user mssql ct user mssql ws user mssql sqc user mssql ds user mssql rpa user mssql pm user mssql dm user mssql rapi user mssql ocr user See the password policy earlier. For the description of these parameters, see config.yml options. For each login, specify the following parameters: If you use the SQL Server authentication, Specify the login name. Make sure that the Enforce password policy check box is not selected. In the Default database box, specify the database. Choose English as Default language. SQL auth If you use Windows authentication, Select a corresponding login name. In the Default database box, specify the database. Choose English as Default language. For other options leave the default values. Win auth When configuring the Integration server, on the INT server, in the config.yml file, in the MSSQL section, remember to specify the corresponding login names and passwords, that you have created on the previous step. Prepare the INT Server All further steps must be performed on the Integration server. Extract installation packages To extract installation packages: Log in to the INT server as a user with the root privileges, by using Linux Terminal, Windows PuTTY, or any other SSH client: Example: Linux Terminal ssh i @ sudo su root In the directory, create a directory for the WorkFusion installation package, for example, opt workfusion wf _installer. Grant the read access for this directory to all users: mkdir p opt workfusion wf_installer chmod 0755 opt workfusion wf_installer Extract the downloaded package to the directory: tar xzvf opt workfusion workfusion full package .tar.gz strip 1 C opt workfusion wf_installer Configure installation To configure installation: On your INT Server, in the directory, edit the hosts.yml configuration file. The vi text editor is used in the example below: vi hosts.yml Refer to the hosts.yml options section for more details on each option. On your INT Server, in the directory, open config.yml, and in the ansiblevaultpassword parameter, change the password. vi config.yml The password is used for encrypting the config.yml at the following step and will be asked several times during the installation of the components. We strongly recommend to secure and memorize it. When creating passwords, make sure they are strong enough and meet the following requirements: Ansible pass If needed, change the other secrets and parameters as well. After that, save the changes and close the file. Refer to the config.yml options section for more details on each configuration option. Optionally, on the INT server, run the following command to automatically generate passwords for the WorkFusion internal services according to product policies. We strongly recommend to change them manually for security reasons. Note that the command doesn't generate end user password. . install.sh passwords generate As a result, in config.yml, the passwords for most services, except for the following end user ones, are created: \"wf_password\" \"ldapbindpassword\" \"mail_pass\" \"tableaudashboardpass\" \"tableauautomationpass\" \"windowsinstallationpass\" \"elkadminpass\" \"ansiblevaultpassword” \"botrelayuser_pass\" \"botuserpass\" If you prefer to set the passwords manually, remember to use only Latin letters and allowed symbols. If needed, change other secrets and parameters, as well. After that, save and close the file. Remember to specify the corresponding MS SQL login names and passwords, that you have created earlier, in the MSSQL section. Encrypt config.yml. . install.sh encrypt config Now config.yml is encrypted via ansible vault with the password that you have provided earlier. If needed, you can edit the encrypted file with the following command: . install.sh edit_config int Enter ansiblevaultpassword, when asked. Prepare the TLS certificates, as described in Prepare TLS certificates for installation. Distribute the certificates folder, along with config.yml files across all other Linux servers: Create the wf _configs.tar.gz archive with these files: tar czvf wf_configs.tar.gz certificates config.yml hosts.yml Copy wf _configs.tar.gz to all other servers. For that a python SimpleHTTPServer can be used. Start the HTTP server on any INT Server's port, which is accessible by other servers: Example: SimpleHTTPServer is on port 9999 python m SimpleHTTPServer 9999 After running this command in a specific directory, the content of the directory can be downloaded via HTTP. Prepare other Linux servers The following steps must be performed on each other Linux server. To prepare another Linux server: Log in to a targer server, as a root user, and download wf _configs.tar.gz from the INT server: cd opt workfusion wf_installer install wget if not installed yum y install wget wget http: inthostname:9999 wfconfigs.tar.gz O wf_configs.tar.gz Unpack configuration files and certificates: tar xzvf wfconfigs.tar.gz C opt workfusion wfinstaller Install the WorkFusion license The following step must be performed on APP Server. To install the license, copy the provided license.properties file to the directory, for example, opt workfusion wf_installer license.properties. "},{"version":"10.0","date":"Oct-14-2019","title":"rpa-server-overview","name":"RPA server overview","fullPath":"iac/admin/legacy/rpa-server-overview","content":" RPA Deployment Diagram rpa bep components To learn more about deployment, see RPA Deployment. For general SPA deployment architecture, go here. Installation options There are two ways to install RPA Windows Server: RPA Windows Server installation based on generated config files. Recommended. Fully manual installation and configuration of RPA Windows Server and related components. RPA Folder Structure As soon as you have downloaded the installer file, extract all the files from the RPAInstaller {version}.zip to any directory on your PC. The packages inside the directory are as follows... Package name Package usage autoit executes autoit scripts bot agent contains Bot Agent together with its configuration and logs filebeat contains Filebeat configuration and logs java used to execute Java processes (for 64 bit Windows) java _x86 used to execute Java processes (for 32 bit Windows) libs contains dll files used for native work of the system logs contains logs of Bot Relays, Bots, and Workers metricbeat contains Metricbeat together with its configuration and logs nginx contains Nginx configuration and logs registry contains registry files used while installation installer runs machine settings and user settings rpa grid performs configuration and runs Bot Relay and Bot files scripts collects logs from the package, contains scripts for both logs and configuration worker contains executable code Prerequisites Internet Explorer 11. The CA certificate must be imported into Trusted Root Certificates Authorities of Local Machine's Certificate TrustStore. For more information, see Import Root or Intermediate CA to Store. Before configuring users, make sure your Internet Explorer browser is turned off and no Java processes are running. The MS SQL, Integration, and Application Servers must be installed. "},{"version":"10.0","date":"Jul-08-2019","title":"run-post-installation-procedures","name":"Run post-installation procedures","fullPath":"iac/admin/legacy/run-post-installation-procedures","content":" Check the installation Once the installation is completed, check WorkFusion Platform with Business Process (BP) and Manual Task: To check WorkFusion S3, OCR, RPA, AutoML and WorkSpace run Environment Health Check. All the checks must produce the \"OK\" status. Activate the OCR license Activate the license on the OCR Server as described in the OCR License topic. Activate the WF ELK license Activate the WF ELK license as described in the WorkFusion ELK License topic. Activate the Tableau license After WorkFusion Analytics Server or WorkFusion Analytics Desktop are installed, you must activate the Tableau component. To complete the activation process, receive the WorkFusion Analytics Server and Desktop product keys from the WorkFusion team. There are two ways to activate or deactivate product keys depending whether the computer has internet access or not. Follow the official Tableau guide that best suits your scenario. Activate: Tableau Desktop Tableau Server For more information on licenses, see Manage the WorkFusion Analytics license. Import SSL certificates The SSL certificates are required for correct rendering of web pages over HTTPS. If you have installed Workfusion SPA version 10.0 using the self signed certificates, remeber to add the ca.crt to your browser. To acquire the certificate: In your browser, type the address of installed Control Tower or any other server, for example, BEP or INT, and add \" ca.crt\" to it: http: ct1.workfusion.com ca.crt. Press Enter. The certificate is downloaded to your computer. Setup backups Setup the cold backup procedure, which should be aligned with your company backup policy, by following the Backup and Restore guide. Configure BEP (optional) The following commands must be run on the Integration server. To update models, run the following commands: cd Will update the models, merging the already uploaded with the new ones . bepmodelsuploader.sh Use o flag to overwrite everything with the new models instead of merging . bepmodelsuploader.sh o "},{"version":"10.0","date":"Jul-04-2019","title":"run-preinstall-manually","name":"Prepare for installation manually","fullPath":"iac/admin/legacy/run-preinstall-manually","content":" If the policy in your company does not allow to run anything from root automatically, you will have to do the preinstall steps manually. Common Steps The following steps must be performed on each server as a root user. To prepare for install: Create the wfuser group: groupadd wfuser Create the runtime user and add it to the group: useradd m g wfuser wfuser Create the installation directory: mkdir opt workfusion chmod 0750 opt workfusion chown wfuser:wfuser opt workfusion Install the common required packages: yum y install unzip openssl rsync python setuptools coreutils To check the version, run the following command: rpm q openssl Configure the limits for the maximum number of open files and the number of processing units available: cat > etc security limits.conf nofile 200000 nproc 10240 EOF Create the systemd service for supervisord: cat etc systemd system supervisord.service Unit Description=supervisord Requires=local fs.target After=local fs.target Service Type=simple Restart=on failure RestartSec=10 StartLimitInterval=300 StartLimitBurst=5 User=wfuser Group=wfuser LimitNOFILE=200000 LimitNPROC=10240 ExecStartPre= bin bash c 'while ! f opt workfusion python site bin supervisord ; do sleep 1; done; exit 0' ExecStart= bin bash c 'source opt workfusion environment.sh && opt workfusion python site bin supervisord c opt workfusion supervisord conf supervisord.conf' TimeoutSec=300 Install WantedBy=multi user.target EOF Enable autostart for the supervisord service: systemctl daemon reload systemctl enable supervisord Configure cron. Follow the instructions below, only if etc cron.allow, or etc cron.deny, or the PAM (Pluggable Authentication Modules) configurations are changed. cat > etc cron.allow wfuser EOF Check that wfuser isn't restricted by the PAM policies, otherwise the similar error appears: wfuser@rhel server crontab e You (wfuser) are not allowed to access to (crontab) because of pam configuration. To enable the user to use cron, in the login access control table file, use the following entry, which allows wfuser to run Cron jobs: vi etc security access.conf Allow the coins user to run cron jobs : wfuser : cron crond :0 Ensure it is above the last entry: Deny all other users access by any means. : ALL : ALL Unpack the nginx binary and enable the capability to bind it to a port below 1021. Run this step, if your config.yml file contains the following key value: nginx port: below 1025. For example, if you set nginx db port: 80 or nginx db port _ssl: 443 (by default these settings are 80 443), run this step on the INT server. The same applies to all other servers. The step is not required on BEP Agent servers, because nginx is not used on them. mkdir p opt workfusion nginx bin cd tar xvf sources nginx rhel7 1.15.8.tar.gz strip=1 C opt workfusion nginx bin chown R wfuser:wfuser opt workfusion usr sbin setcap CAPNETBIND_SERVICE= eip opt workfusion nginx bin nginx BEP Master Server To configure BEP Master: Configure sysctl parameters for BEP Master: Create file etc sysctl.d workfusion.conf with the required sysctl parameters. Do not edit the standard one, as it will be overwritten by the system. Run as root cat etc sysctl.d workfusion.conf net.ipv4.tcpkeepalivetime = 120 net.ipv4.tcpkeepaliveintvl = 15 net.ipv4.tcpkeepaliveprobes = 5 net.ipv4.tcptwrecycle = 1 net.ipv4.tcptwreuse = 1 net.core.somaxconn = 65535 net.ipv4.iplocalport_range = 9000 61000 fs.file max = 2097152 net.core.rmem_max = 16777216 net.core.wmem_max = 16777216 net.ipv4.tcp_rmem = 4096 87380 16777216 net.ipv4.tcp_wmem = 4096 16384 16777216 net.core.netdevmaxbacklog = 16384 net.ipv4.tcpmaxsyn_backlog = 8192 net.ipv4.tcp_syncookies = 1 EOF Apply the configuration: Run as root sysctl p etc sysctl.d workfusion.conf Install the required packaged dependencies for BEP Master: Run as root yum y install libevent nfs utils boost redhat lsb core Setup the nfs server: Enable and start rpcbind: Run as root systemctl enable rpcbind systemctl daemon reload systemctl start rpcbind Create directories to be exported: Run as root mkdir opt workfusion vds data chmod 0750 opt workfusion vds data chown wfuser:wfuser opt workfusion vds data Hereinafter, VDS (Virtual Data Since) is the former name for the BEP component. In the exports file etc exports, add the following line: Run as root cat > etc exports opt workfusion vds data *(fsid=0,rw,async,norootsquash,nosubtreecheck,insecure) EOF Start the nfs server: systemctl enable nfs server systemctl start nfs server BEP Agent Server To configure BEP Agent, run the following commands as a root user: Install the required packages for BEP Agent Server(s) from the RHEL repository: yum y install libtiff libtiff tools jbigkit libs libjpeg turbo Configure sysctl parameters for BEP Agents: Create the etc sysctl.d workfusion.conf file with the required sysctl parameters. Do not edit the standard one, as it will be overwritten by system. cat etc sysctl.d workfusion.conf net.ipv4.tcpkeepalivetime = 120 net.ipv4.tcpkeepaliveintvl = 15 net.ipv4.tcpkeepaliveprobes = 5 net.ipv4.tcptwrecycle = 1 net.ipv4.tcptwreuse = 1 net.core.somaxconn = 65535 net.ipv4.iplocalport_range = 9000 61000 fs.file max = 2097152 net.core.rmem_max = 16777216 net.core.wmem_max = 16777216 net.ipv4.tcp_rmem = 4096 87380 16777216 net.ipv4.tcp_wmem = 4096 16384 16777216 net.core.netdevmaxbacklog = 16384 net.ipv4.tcpmaxsyn_backlog = 8192 net.ipv4.tcp_syncookies = 1 EOF Apply the configuration: sysctl p etc sysctl.d workfusion.conf Install the required packaged dependencies for BEP Agents: yum y install libevent nfs utils boost redhat lsb core Create a directory for NFS share: mkdir opt workfusion vds data chmod 0750 opt workfusion vds data chown wfuser:wfuser opt workfusion vds data In etc fstab, add the following line: cat > etc fstab : opt workfusion vds data opt workfusion vds data nfs rw,bg,retrans=5000,rsize=8192,vers=3,wsize=8192,timeo=14,intr 0 0 EOF Mount NFS share mount opt workfusion vds data "},{"version":"10.0","date":"Oct-04-2019","title":"import-root-or-intermediate-ca-to-store","name":"Import Root or Intermediate CA to Store","fullPath":"iac/admin/info-sec/import-root-or-intermediate-ca-to-store","content":" Microsoft Windows uses a global certificate storage to keep certificates. The certificates must be imported to each workspace, to enable the browser to pass the certificate chain check successfully. To import certificates: Download the CA certificate file in the PEM CRT format and save it to your desktop. Right click the root certificate and click Install Certificate. Install certificates In the Store location group, select Local Machine and click Next. Install certificates Select Place all certificates in the following store and click Browse. Install certificates Select Trusted Root Certification Authorities and click OK. Install certificates Click Next, and then click Finish. Install certificates Double check that your connection is secured now. Install certificates "},{"version":"10.0","date":"Oct-14-2019","title":"ldap-and-ad-integration","name":"LDAP and AD integration","fullPath":"iac/admin/info-sec/ldap-and-ad-integration","content":" Overview Lightweight Directory Access Protocol (LDAP) is an open, vendor neutral, industry standard application protocol for accessing and maintaining distributed directory information services over an IP network. Directory services play an important role in developing intranet and Internet applications by allowing the sharing of information about users, systems, networks, services, and applications throughout the network. As examples, directory services may provide any organized set of records, often with a hierarchical structure, such as a corporate email directory. Similarly, a telephone directory is a list of subscribers with an address and a phone number. Active Directory (AD) is a directory service that Microsoft developed for Windows domain networks. It is included in most Windows Server operating systems as a set of processes and services. Initially, Active Directory was only in charge of centralized domain management. Starting with Windows Server 2008, however, Active Directory became an umbrella title for a broad range of directory based identity related services. WorkFusion Control Tower and WorkSpace can provide security (authentication and authorization) through LDAP AD integration. Authentication is based on Spring Security standard provider. Control Tower Configuration By default, Control Tower provides local user groups role management. LDAP AD integration can be enabled by setting the following parameters in: workfusion conf workfusion.properties: ldap properties ldap.enabled=true to activate LDAP with WF roles switch this property to true. No groups from LDAP will be used. Properties ldap.group.base and ldap.group.filter will be ignored ldap.internal.authorization.enabled=false container of corporate user database used for authentication support several base directories pipe separated ... ldap.user.base=ou=People,dc=somecompanydomain,dc=com container of corporate user groups used for authorization support several base directories pipe separated ... ldap.group.base=ou=Groups,dc=somecompanydomain,dc=com user filter for search, optional, alternative = (uid={0}) ldap.user.filter=(sAMAccountName={0}) group filter for search, optional, alternative = (memberUid={1}) ldap.group.filter=(member={0}) page size to load big amount of records to avoid size limit error ldap.search.page.size=500 group filter for list of all group names ldap.search.all.groups.filter=( (objectClass=group)(objectClass=groupOfNames)(objectClass=groupOfUniqueNames)(objectclass=posixGroup)) ldap object attribute name that contains group name ldap.search.groups.name.attribute=cn Default configuration: ldap properties ldap.enabled=false ldap.internal.authorization.enabled=false ldap.user.filter=(sAMAccountName={0}) ldap.group.filter=(member={0}) ldap.search.page.size=500 ldap.search.all.groups.filter=( (objectClass=group)(objectClass=groupOfNames)(objectClass=groupOfUniqueNames)(objectclass=posixGroup)) ldap.search.groups.name.attribute=cn Secure properties: ldap server url ldap.server.url=ldaps: {host}:{port} system user cridentials ldap.bind.dn= ldap.bind.password= External authorization Before enabling LDAP, at least one group must be mapped. User groups are mapped in Control Tower during create edit a User Group (default user roles you can find in Role Management): After successful login, a user will be created updated with data fetched from LDAP AD. The user properties cannot be changed in Control Tower, if LDAP configuration is enabled, only available filters can be added. The direct role selection is unavailable too. Edit LDAP user If LDAP is enabled, A user cannot be created, deleted, or disabled in Control Tower. The Create, Delete, and Disable buttons are unavailable. Sections System Preferences → User Settings: Account Details, and Change Password are disabled too. Internal authorization To enable internal authorization, set property ldap.internal.authorization.enabled to true. Common assumptions: If LDAP authentication is passed successfully, a user is checked for existence in Control Tower, and Control Tower Roles are applied to the user, if such user exists. Otherwise, unable to login. User Properties cannot be changed in Control Tower, if LDAP Internal configuration is enabled. You can only edit roles and enable disable the user. User can be created, deleted, and disabled in Control Tower. The Create, Delete, and Disable buttons on the User Management page are available, if LDAP with internal authorization is enabled. The System Preferences → User Settings: Account Details, and Change Password sections are disabled too. The following properties are not used, if LDAP with internal authorization is enabled: ldap.group.base ldap.group.filt Integrate LDAP AD This is an example to give an overview of LDAP AD integration. You must have distinqueshedName and password of the \"system user\" to initialize the connection properties. It can be any user with privileges to read the directory tree and the object attributes. ldap.bind.dn=CN=System User,OU=demo,DC=somecompany,DC=local ldap.bind.password=** System user Configure the base directories for users and groups. ldap.group.base=OU=Groups,OU=demo,DC=somecompany,DC=local ldap.user.base=OU=Minsk,OU=Users,OU=demo,DC=somecompany,DC=local It supports multi values with pipe separator for users and groups. ldap.user.base=OU=Minsk,OU=Users,OU=demo,DC=somecompany,DC=local OU=New York,OU=Users,OU=demo,DC=somecompany,DC=local When the base directories are configured properly, it can help reducing time for searching users and or groups. ldap.group.base is a base directory for loading user's groups during login and a list of groups for \"External Group Name\" mapping on the Edit User Group page. User groups User groups Configure user filter. User filter Usually, a username attribute for Microsoft Active Directory is sAMAccountName. In this case ldap.user.filter=(sAMAccountName={0}). For example, if we are going to use the email for login, so the filter looks as ldap.user.filter=(mail={0}). Keep in mind, that a user filter must return a unique result, otherwise it is configured incorrectly. Configure a user groups filter. User groups filter ldap.group.filter=(member={0}). In this case the {0} parameter is distinqueshedName of the logging in user. For example, an alternative filter ldap.group.filter=(memberUid={1}), In this case the {1} parameter is username of the logging in user. Configure the filter to fetch all group names. For that in Control Tower → Create Edit User Group page, fill External Group Name. ldap.search.all.groups.filter=(objectClass=groupOfNames) ldap.search.groups.name.attribute=cn Fetch group names WorkSpace configuration By default, WorkSpace provides local user role management. LDAP AD integration can be enabled by setting the following parameters: In workspace conf workspace.properties or workspace sandbox.properties: ldap properties ldap.enabled=true activate internal authorization if swithed to true with ldap.enabled ldap.internal.authorization.enabled=false Comma separated group names from LDAP for mapping to worker users ldap.worker.groups=CC Dev,Galiot,CC Ops Comma separated group of roles from LDAP which are mapped to the WorkSpace Requester role ldap.requester.groups=CC Admin Name of license that is set to requester if new requester is created ldap.requester.license=license_name container of corporate user database used for authentication support several base directories pipe separated ... ldap.user.base=ou=People,dc=crowdcomputingsystems,dc=com container of corporate user groups used for authorization support several base directories pipe separated ... ldap.group.base=ou=Groups,dc=crowdcomputingsystems,dc=com user filter for search, optional, alternative= (uid={0}) ldap.user.filter=(sAMAccountName={0}) group filter for search, optional, alternative = (memberUid={1}) ldap.group.filter=(member={0}) optional block ldap.user.attribute.email=mail ldap.user.attribute.firstName=givenname ldap.user.attribute.lastName=sn ldap.user.attribute.country=country User group mapping is configured through ldap.worker.groups Default configuration: ldap properties ldap.enabled=false ldap.internal.authorization.enabled=false ldap.user.filter=(sAMAccountName={0}) ldap.group.filter=(member={0}) ldap.user.attribute.email=mail ldap.user.attribute.firstName=givenname ldap.user.attribute.lastName=sn ldap.user.attribute.country=country Secure properties: ws.secure.ldap.server.url=ldaps: {host}:{port} system user cridentials ws.secure.ldap.bind.dn= ws.secure.ldap.bind.password= External authorization After successful login, a user will be created updated with data fetched from LDAP AD. Register and Forgot password functionality is unavailable, if LDAP AD is integrated. workspace login Internal authorization To enable internal authorization, set property ldap.internal.authorization.enabled to true. Common assumptions: If LDAP authentication is passed successfully, a user is checked for existence in WorkSpace, and WorkSpace Roles are applied to the user, if such user exists. Otherwise, unable to login. User Properties cannot be changed in WorkSpace, if LDAP Internal configuration is enabled. You can only enable and disable the user. A user can be created, deleted, or disabled in WorkSpace. The Create, Delete, and Disable buttons on the Workers and Requesters pages are available for requesters. The following properties are not used, if LDAP with internal authorization is enabled: ldap.group.base ldap.group.filter "},{"version":"10.0","date":"Jun-28-2019","title":"manage-secure-properties","name":"Manage secure properties","fullPath":"iac/admin/info-sec/manage-secure-properties","content":" To manage the secure properties, use the designated utility, which you can find on the INT server: {INSTALLDIR} wfsec_storage loader.sh Here, component name – defines the component to process: workfusion workspace wfagent action – defines, whether the properties are reviewed or updated: review – read the content of the secured storage properties_file – the path to the properties file containing the secure properties to add, update or delete. Read properties To read the application's configuration properties, run the utility with the action set to review. Example: sh loader.sh workspace review As a result, the utility returns a list of all secured properties with values, which are available for the defined component. Edit properties Use the utility to modify the secure properties. For that, in the action parameter, specify the name of the file that contains properties to be modified. Add a property To add a new secure property, in the properties file, specify the properties and their values. Update a property To edit the secure properties, in the properties file, specify the existing properties with new values. Example: sh loader.sh workfusion workfusion secure.properties File example: s3.access key={{ s3accesskey }} s3.secret key={{ s3secretkey }} s3.context.key.map={{ s3contextkey_map }} ... automation.password= Here, specify new automation.password value, and save the file. Delete a property To remove a secure property, in the property file, add the property with an empty value. Example: mturkds.database.password= ldap.server.url= If the properties file is created in the same folder with the utility (for example, the file is named as new_values.properties), the full path is not necessary, so you can call the utility, as shown below: sh loader.sh workfusion new_values.properties "},{"version":"10.0","date":"Oct-04-2019","title":"sso-integration","name":"SSO integration","fullPath":"iac/admin/info-sec/sso-integration","content":" :::important Before starting the SSO integration, request the IdP metadata (file or URL) from the security team. ::: SAML 2.0 Security Assertion Markup Language (SAML) is an XML based, open standard data format for exchanging authentication and authorization data between parties, in particular, between an identity provider (IdP) and a service provider (SP). The SAML specification defines three roles: the principal (typically a user) the identity provider (IdP) the service provider (SP) In the use case addressed by SAML, the principal requests a service from the service provider. The service provider requests and obtains an identity assertion from the identity provider. On the basis of this assertion, the service provider can make an access control decision, that means it can decide, whether or not to perform some service for the connected principal. Before delivering the identity assertion to the SP, the IdP can request some information from the principal – such as a user name and password – in order to authenticate the principal. SAML specifies the assertions between the three parties: in particular, the messages that assert identity that are passed from the IdP to the SP. In SAML, one identity provider may provide SAML assertions to many service providers. Similarly, one SP may rely on and trust assertions from many independent IdPs. SAML does not specify the method of authentication at the identity provider; it may use a username and password, or other form of authentication, including multi factor authentication. A directory service such as LDAP, RADIUS, or Active Directory that allows users to log in with a user name and password is a typical source of authentication tokens at an identity provider. 3 The popular Internet social networking services also provide identity services that in theory could be used to support SAML exchanges. Use case SAML Web Browser SSO (Service Provider initiated authentication): Request the target resource at the SP. The principal (via an HTTP user agent) requests a target resource at the service provider: https: sp.example.com myresource The service provider performs a security check on behalf of the target resource. If a valid security context at the service provider already exists, skip steps 2–7. Redirect to the SSO Service at the IdP. The service provider determines the user's preferred identity provider (by unspecified means) and redirects the user agent to the SSO Service at the identity provider: https: idp.example.org SAML2 SSO Redirect SAMLRequest=request The value of the SAMLRequest parameter is the Base64 encoding of a deflated `` element. Request the SSO Service at the IdP. The user agent issues a GET request to the SSO service at the identity provider where the value of the SAMLRequest parameter is taken from the URL query string at step 2. The SSO service processes the AuthnRequest and performs a security check. If the user does not have a valid security context, the identity provider identifies the user (details omitted). Respond with an XHTML form. The SSO service validates the request and responds with a document containing an XHTML form: ... The value of the SAMLResponse parameter is the base64 encoding of a `` element. Request the Assertion Consumer Service at the SP. The user agent issues a POST request to the assertion consumer service at the service provider. The value of the SAMLResponse parameter is taken from the XHTML form at step 4. Redirect to the target resource. The assertion consumer service processes the response, creates a security context at the service provider and redirects the user agent to the target resource. Request the target resource at the SP again The user agent requests the target resource at the service provider (again): https: sp.example.com myresource Respond with requested resource. Since a security context exists, the service provider returns the resource to the user agent. Control Tower SAML Integration Security profiles The following security profiles for SAML are supported: PKIX. The java.security.cert package is utilized for assertion and validation of the certificates. It implements RFC 5280. MetaIOP. Trusted certificates are not mandatory. Other mechanisms are used for validation (e.g. explicitly supplying the keys to trust). The desired profile is specified in the SAML metadata file. Related documentation: Package java.security.cert (https: docs.oracle.com javase 8 docs api java security cert package summary.html) Configuring SAML Extension. Security profiles (http: docs.spring.io spring security saml docs 1.0.x reference html security.html configuration security profiles) Application Properties conf workfusion.properties Property Name Value Description wf.sso.saml.enable true Enables or disables SSO (SAML 2.0 protocol) authentification for Control Tower. Accepted values: true false wf.sso.saml.entity.base.url https: HOSTNAME workfusion Base URL of Control Tower (Service Provider). Used in SP metadata. (see Service Provider metadata example) wf.sso.saml.response.skew 60 Maximum difference between local time and time of the assertion creation which still allows message to be processed.Basically determines maximum difference between clocks of the IDP and SP machines. Defaults to 60 (seconds) workfusion.properties sample option to enable disable SSO authentication wf.sso.saml.enable=true base url used during generation SP metadata wf.sso.saml.entity.base.url=https: HOSTNAME workfusion response skew in seconds wf.sso.saml.response.skew=60 Secure Properties These properties are placed to Secure Storage. Property Name Value Description wf.sso.saml.idp.file.metadata path to local meta Local path to a file with IdP metadata parameters. wf.sso.saml.idp.metadata https: SAMLSERVICEURL idp sso URL that points to a file with IdP metadata parameters. Must be provided by a customer.This parameter is an alternative to wf.sso.saml.idp.file.metadata, which is a recommended option. wf.sso.saml.sp.metadata workfusion metadata id A unique string. ID of Service Provider. wf.sso.saml.username.attribute uid Name of attribute that will be used for authentication.uid for LDAPsAMAcountName for Active Directory Must be provided by customer. Secure Storage sample properties url to IdP metadata, use url or file wf.sso.saml.idp.metadata=none path to IdP metadata file wf.sso.saml.idp.file.metadata= path to local metadata some unique name wf.sso.saml.sp.metadata=wf sp hostname the name of an attribute taken from IdP response that contains username wf.sso.saml.username.attribute=uid Service Provider metadata example For example Control Tower is installed and available by URL Metadata for this Service Provider can be generated and downloaded by URL SP metadata example Meta ` has attributes with Service Provider ID and entityID. These attributes are configured through property wf.sso.saml.sp.metadata`. Identity Provider metadata example Usually IdP metadata is provided by a security team. It can be a file or URL. The file should be placed on the application server (SP) and be accessible for Service Provider (Control Tower). It is configured via the property wf.sso.saml.idp.file.metadata. If URL provided, it can be configured via wf.sso.saml.idp.metadata. Both properties wf.sso.saml.idp.file.metadata and wf.sso.saml.idp.metadata cannot be configured at the same time. Show IdP metada example ``** must contain information about login logout endpoints: SingleSignOnService, SingleLogoutService, and attributes' description – saml:Attribute. `` is the username attribute name and must be mapped via wf.sso.saml.username.attribute ws.sso.saml.username.attribute in Secure Storage. (for example, wf.sso.saml.username.attribute=ACCOUNT). The value of this mapped attribute in IdP response will be used for authorization. Identity Provider Authentication Response After a user logs in on the IdP side, the server sends a response to SP (CT WS) and this response must include username attributes configured (if not encrypted) in wf.sso.saml.username.attribute or ws.sso.saml.username.attribute. How to decrypt SAML Response To catch SamlResponse from IdP, use a debuger (press F12 to open it) in your browser and then try to login . Then select the SSO name among all names of the Network tab and there will be present decoded SamlResponse. Copy this value and decode it using https: www.samltool.com decode.php tool. Example of the IdP response with not encrypted data: The Destination attribute in Response must contain an URL that leads to the `` tag into SP metadata. The ` tag from ` must contain a value of wf.sso.saml.sp.metadata ws.sso.saml.sp.metatdata from vault (depends on component you are configuring SSO for). ` is \"someusername\" and will be used for authentication on the SP side (if configuration is wf.sso.saml.username.attribute=uid`). Example wf.sso.saml.sp.metadata = test.workfsuin.com_workspace sandbox In this case, Audience will look like: test.workfusion.com_workspace sandbox Example of the IdP response with the encrypted data: IdP and SP metadata description Tag Source Description EntityDescriptor IdP SP metadata main metadata's tag with ID EntityID attributes IDPSSODescriptor IdP metadata IdP configuration description SPSSODescriptor SP metadata SP configuration description SingleSignOnService IdP metadata Endpoint to sign on SingleLogoutService IdP SP metadata Endpoint to sign out AssertionConsumerService SP metadata SP endpoint to assert response from IdP side saml:Atribute IdP metadata response Attribute description goes from IdP side saml:AtributeValue IdP response Attribute value saml:AudienceRestriction IdP response Container for saml:Audience tag saml:Audience IdP response SP EntityID for which the response is sent (used in response Assertion) samlp:StatusCode IdP response Response status, success example value (urn:oasis:names:tc:SAML:2.0:status:Success) Configure SSO Before enabling SSO, in Control Tower, create a user with the Administrator role and a name that is suitable for your SSO account. Create user Configure application and secure properties to enable SSO: workfusion.properties file wf.sso.saml.enable=true wf.sso.saml.entity.base.url=https: control tower host workfusion Secure Storage wf.sso.saml.idp.file.metadata= path to ipd metadata provided by security team wf.sso.saml.sp.metadata=unique service provider name wf.sso.saml.username.attribute=ACCOUNT the name of the attribute into SSO response with username, usually described in IdP metadata file. Restart the Application server. Go to https: control tower host workfusion saml metadata. The metadata file will be downloaded. Send it to security team to be added to Identity Provider (SSO server). Open any CT page. From now you'll be redirected to your Identity Provider Login Page. Enter your account. User authentication and authorization This implementation assumes that the user, who is going to log in via the identity provider (IdP)* already exists in Control Tower* The Admin user who is responsible for creation new users and assigning privileges must be primarily created directly via database. The Admin user must create new users with usernames that already exists in IdP. If a deleted, disabled, or non existing user tries to log in, they will be redirected to the WorkFusion login failed screen with a validation message. Login fail Users are authorized on the WorkFusion side. The admin user can manage user roles. IdP configuration Assertion Consumer Service, EntityID and Public Key can be found in the metadata XML file. Service Provider metadata can be downloaded by the url: https: workfusion saml metadata, for example, https: SOMESITE.WORKFUSION_HOSTNAME workfusion saml metadata. LDAP AD authorization Click here to expand LDAP authrization use case diagram sso ldap integration.png For information on enabling LDAP and AD auhotization, and mappping user groups, see LDAP and AD integration WorkSpace SAML Integration Application Properties Property Name Value Description ws.sso.saml.enable true Enables or disables SAML 2.0 protocol for Control Tower. Accepted values: true and false ws.sso.saml.entity.base.url https: HOSTNAME workspaceorhttps: HOSTNAME workspace sandbox Base url of Workfusion SAML provider ws.sso.saml.response.skew 60 Maximum difference between the local time and the time of the assertion creation, which still allows a message to be processed.Basically determines maximum difference between clocks of the IDP and SP machines. By default, 60 (seconds) Sample of workspace.properties (workspace sandbox.properties) option to enable disable SSO authentication ws.sso.saml.enable=true base url used during generation SP metadata ws.sso.saml.entity.base.url=https: WORKSPACE_HOSTNAME workfusion response skew in seconds ws.sso.saml.response.skew=60 Secure Properties Property Name Value Description ws.sso.saml.idp.file.metadata path to local meta Path to file with IdP metadata parameters. ws.sso.saml.idp.metadata none URL that points to a file with IdP metadata parameters. Must be provided by a customer.This parameter is an alternative to wf.sso.saml.idp.file.metadata, which is a recommended option. ws.sso.saml.sp.metadata ws sp hostname A unique string. ID of Service Provider, for example, https: workspace saml metadata ws.sso.saml.username.attribute mail A name of the attribute that will be used for authentication Secure Storage sample properties url to IdP metadata, use url or file ws.sso.saml.idp.metadata=none path to IdP metadata file ws.sso.saml.idp.file.metadata= path to local metadata some unique name ws.sso.saml.sp.metadata=ws sp hostname the name of an attribute taken from IdP response that contains user's email address ws.sso.saml.username.attribute=mail :::note At least one of the \"idp\" properties must be set. Otherwise, you'll get the \"No IdP was configured\" page. ::: User Authentication and Authorization This implementation assumes that the user that is going to log in via the identity provider (IdP)** already exists in WorkSpace. A requester must be primarily created directly via the database.The user's email in IdP must be the same as the username in WorkSpace. A Worker can get registered by himself. The email in IdP must be the same as the username in WorkSpace. If a deleted, disabled, or non existing user tries to log in to Workspace, they will be redirected to the the index page with a validation message. A disabled (non active) user can be activated by email or by a Requester. IdP Configuration Assertion Consumer Service, EntityID and Public Key can be found in metadata xml file Service Provider metadata can be downloaded by the url: https: workspace saml metadata. "},{"version":"10.0","date":"Oct-04-2019","title":"activate-the-ocr-license","name":"Activate OCR license","fullPath":"iac/admin/licenses/activate-the-ocr-license","content":" WorkFusion provides the following license categories: Base category: includes supported languages, except for Arabic, Farsi, Chinese, Japanese, Korean, Thai, Hebrew, Yiddish, and Vietnamese. CJK category: includes the Base category languages plus Chinese Traditional, Chinese Simplified, Japanese, Korean, and Korean Hangul. All category: all supported languages are included (see the language list). Activate license FRE SDK is located in the following directory: Linux: ABBYY _FRE11 Windows: FineReaderEngine After installation of the OCR components, on the OCR server, the license _request.txt file appears. To activate the license: Send an email to your account manager in WorkFusion, or create a Service Desk ticket according to the following template. Attach license _request.txt to the email. > Dear Support Team, > > Please provide OCR license according to the following requirements: > > Company name: _ Number of pages to include: _ *) Platform type: _(Linux Windows) > >Status of the server (Production, POC, UAT, DEV) Category of languages: Base, or CJK (supports Chinese, Japanese, Korean), or All Token returned by prepareLicense API: _ (content of the file license _request.txt on the OCR server) *) The number of OCR pages must be estimated during POC on basis of the customer's workload. For example, if 1000 pages is processed per one day, then per month the amount will be 30.000 pages. As stated further, the non prod licenses have some predefined values 10K, 50K, 100K pages depending on complexity of the customer workload. Wherever practical, request the minimum possible license. The account manager is always aware of the number of pages included into the contract – it’s a part of negotiation with the customer. Note that the license cost depends on the number of pages as well as on the category. Make sure your contract with WorkFusion covers it. If you have multiple OCR servers to balance the workload, request multiple OCR licenses: a separate license for each server. In response to your email, WorkFusion sends you file license response.txt. Copy the file to license response.txt on the OCR server. On your OCR server, go to the directory, and then run the following script according to the OS of the OCR server: activate _wf.sh on Linux activate _ocr.bat on Windows in SPA Example: cd . activate_wf.sh If everything is OK, the command reports a success result: {\"message\":{\"content\":\"License Activation Succeeded\"}} Note that, if the category of the newly activated license differs from your previous one, the change takes effect immediately after activation, even if you have a lot of pages left from your previous license. If you migrate from SPA versions 8.5 and earlier, follow the steps below to reuse the available number of pages in your new environment. Get the number of unused pages from pre 8.5 versions... Perform the following operations on all OCR servers for all activated licenses. To re use the available number of pages: Run License Manager as any user. export LDLIBRARYPATH= LDLIBRARYPATH: apps ABBYY FREngine11 Bin apps ABBYY FREngine11 Bin LicenseManager.Console Select the activated license. Activated license The license serial number and other license parameters can differ from the examples. Open license parameters. Activated license Make a screenshot of the first page. Make sure that line Volume → Regular Text → Remains is visible. Activated license Request and activate the license by sending an email to your account manager in WorkFusion, or create a Service Desk ticket according to the template (see earlier). Remember to attach all license screenshots, if Remains is greater than 0. Verify license After activation, you can check the state of your active license. For that, connect via SSH to the OCR server, and then use the activeLicense API. If authentication is disabled (the auth disabled profile is configured), no authentication header is needed and the command looks as follows: curl s http: localhost:9002 api v1 cloud activeLicense python m json.tool If authentication is enabled, the command is as follows: curl s H \"Authorization: \" http: localhost:9002 api v1 cloud activeLicense python m json.tool Replace `` with your credentials according to the OCR Service configuration. The header may look like: \"Authorization: Basic your username pwd _base64\" for basic authentication \"Authorization: Bearer your jwt token\" for JWT authentication "},{"version":"10.0","date":"Oct-04-2019","title":"manage-workfusion-analytics-license","name":"Manage WorkFusion Analytics license","fullPath":"iac/admin/licenses/manage-workfusion-analytics-license","content":" License type Current Tableau version is 2018.3.2. The license for Tableau Server is requested by an Account manager and provided by WorkFusion. For SPA 10.0 you can also use the licenses that have been requested for earlier SPA versions. The license includes three activations, for example, for DEV, UAT, and PROD. Each license is issued for one year. If you have to change the default dashboards, your Account manager may also request the Tableau Desktop license from Workfusion. BI Scheme View product keys and license details The current number of licensed seats and product key information for WorkFusion Analytics Server administrators can be viewed on the server web interface. Single Site License To view product keys: on the Analytics server, on the main menu, click Settings, and then go to the Licenses tab. Licenses Multi Site License To view product keys: On the main menu, click All Sites > Manage All Sites. Manage all sites The Manage All Sites item is displayed only when you are signed in as a server administrator. On the main menu, click Settings, and then go to the Licenses tab. Licenses tab To view license details, on the main menu, click Users. License details The License Details page provides summarized information for all product keys installed on the server, including: Seat licenses total: the total number of seats – unique WorkFusion Analytics Server users – covered by all product keys Seat licenses in use: the total number of seats in use Seat licenses available: the total number of seats available Unlicensed users: the number of users on WorkFusion Analytics Server without a license assigned For each Product key you can see the following information: Product Key: a key to activate the license. Seats: a number of seats covered by the license. Maintenance Ends: the date when the Maintenance contract expires. Expires: the date when the license expires. Valid: the license status. Activate and deactivate products After WorkFusion Analytics Server or WorkFusion Analytics Desktop are installed, you must activate these products. To complete the activation process, receive the WorkFusion Analytics Server and Desktop product keys from the WorkFusion team. If WorkFusion Analytics Server and Desktop are to be upgraded, deactivated keys will be used to activate the new versions. There are two ways to activate or deactivate product keys depending whether the computer has internet access or not. Follow the official Tableau guide that best suits your scenario. :::note The account that you use to sign in to TSM must have administrative access to the local computer where Tableau Server is installed. ::: Activate: Tableau Desktop Tableau Server Deactivate: Tableau Desktop Tableau Server "},{"version":"10.0","date":"Oct-14-2019","title":"update-the-elk-license","name":"Update ELK license","fullPath":"iac/admin/licenses/update-the-elk-license","content":" Initial installation of WorkFusion SPA version 10.0 enables the 30 day trial license for ELK. For production purposes, it is required to update the license in Kibana. :::important The WorkFusion ELK License file must be provided by the installation team or your account manager. ::: To update the ELK license: Go to Kibana, for example, at . Log in Authenticate to Kubana, using the ELK admin credentials that you specify in the config.yml file during the installation. Pass On the left menu, click Management. Pass Go to the license management section. Pass Click Update license. Pass Select or drag your license file to the window. The license is uploaded to Kibana and updated automatically. "},{"version":"10.0","date":"Jul-10-2019","title":"workfusion-libraries-and-tools","name":"WorkFusion libraries and tools","fullPath":"iac/admin/licenses/workfusion-libraries-and-tools","content":" WorkFusion IA 2019 WorkFusion libraries and tools "},{"version":"10.0","date":"Oct-04-2019","title":"aggregate-and-view-logs","name":"Aggregate and view logs","fullPath":"iac/admin/maintenance/aggregate-and-view-logs","content":" This article describes basic concepts of log aggregation. Overview The following log level options are available: prod — logs contain only error messages and warnings dev — logs contain error messages, warnings, info and debug messages The default log level is prod, unless user specifies dev option before installation. User can also change log level for specific server or component, except for RPA and BI servers. Learn more in Change a component's log level. The solution for logs aggregation consists of the following components: Kibana with logs filtering dashboards Elasticsearch for storing logs data Logstash for aggregation and forwarding log records The log aggregation process is the following: An application writes logs to the filesystem. The Filebeat component reads the application log and forwards it to the log aggregation server. On the log aggregation server, Logstash receives log events from Filebeat and stores lines on the filesystem in a predefined structure. By default, WorkFusion SPA stores all component logs on the Integration server in the logs directory. All application logs remain both on source and Integration servers but with different retention policies. Logs are stored with the following convention: logs filename.log. Example: opt workfusion int int.workfusion.com nginx 2018 10 25 nginx.err.log. Element Description Example install dir A directory to place user files, logs, and configs opt workfusion server group An alias for group of components app, int, ocr, bi, rpa, bepmaster, bepagent hostname A real hostname of the server that sends logs int.workfusion.com component A component's name wfagent, workfusion, nginx date A date when a log event was created on the source server. Pattern: YYYY MM DD The date is created based on UTC time and thus may differ from the timezone configured on the server. 2018 11 30 filename A log file name metrics.log, status agent error.log View logs on UI To view logs on UI: Go to Kibana, for example, at https: app.example.com kibana. Alternatively, in the Control Tower dropdown menu, select Platform monitor. Platform Monitor Authenticate to Kibana using the ELK admin credentials, which you specify during the installation in the config.yml file. ELK elkadminpass: 'sc@R@ S 9V' password for elasticsearch user 'admin' used as login for Kibana UI In Kibana, go to the Dashboard tab to open the preconfigured dashboards, including the designated one for logs. Dashboards With this dashboard you can search and browse logs from all the servers and components. To find logs from a particular component, in the **Log source selector** section, select one or several following parameters: Hostname – hostname of a server where the component is running. Service name – the name of a service. Log name – the name of a log to find. Click Apply changes. Dashboards You can also refine the results by using the advanced filter with Kibana queries syntax (https: www.elastic.co guide en beats packetbeat current kibana queries filters.html) in the Search field. :::tip For the services' description, see the General services Information section. "},{"version":"10.0","date":"Jun-26-2019","title":"app","name":"APP","fullPath":"iac/admin/maintenance/app","content":" Php fpm (TinyMCE) Management: wfmanager {start stop restart status} php fpm Log files: supervisord log php fpm.log Startup scripts: supervisord apps php fpm.ini php etc php.ini Configuration files: Service directory: php General configuration: php etc php fpm.conf Default ports: unix socket: {{ php_home }} php5 fpm.sock Bot Manager Management: wfmanager {start stop restart status} bot manager Log files: supervisord log bot manager.log Startup scripts: supervisord apps bot manager.ini bot manager run bot manager.sh Configuration files: Service directory: bot manager General bot manager configuration: bot manager config Default ports: tcp 5555 SQC Software owner: The Apache Software Foundation Management: wfmanager {start stop restart status} sqc Log files: supervisord log sqc.out.log Startup scripts: supervisord apps sqc.ini sqc bin catalina.sh Configuration files: General configuration: sqc conf Startup parameters: sqc bin setenv.sh Default ports: tcp 4080 Workfusion Software owner: The Apache Software Foundation Management: wfmanager {start stop restart status} workfusion Log files: supervisord log workfusion.log Startup scripts: supervisord apps workfusion.ini workfusion bin catalina.sh Configuration files: General configuration: workfusion conf Startup parameters: workfusion bin setenv.sh Default ports: tcp 7080 Workspace Software owner: The Apache Software Foundation Management: wfmanager {start stop restart status} workspace Log files: supervisord log workspace.log Startup scripts: supervisord apps workspace.ini workspace bin catalina.sh Configuration files: General configuration: workspace conf Startup parameters: workspace bin setenv.sh Default ports: tcp 5080 Kibana Software owner: ElasticSeach BV Management: wfmanager {start stop restart status} kibana Log files: supervisord log kibana.log Startup scripts: supervisord apps kibana.ini kibana bin Configuration files: General configuration: kibana config Default ports: tcp 5601 "},{"version":"10.0","date":"Jun-26-2019","title":"archive-logs","name":"Archive logs","fullPath":"iac/admin/maintenance/archive-logs","content":" Archive logs This article describes necessary steps to create an archive with selected logs, which can be requested in a support ticket for investigation. Make sure to run these steps as the runtime user, as defined in config.yml, in the wf_user variable. To archive logs: Log in to the INT server via SSH. Change the user to default wfuser, su wfuser Go to the logs directory logs. cd opt workfusion logs In the logs directory, run the opt workfusion tools log _archive.sh script to archive specific logs. For more details on available options, see Specification of log _archive.sh. opt workfusion tools log_archive.sh Usage: log_archive.sh c d g h Options c, d, g, h can be used multiple time Examples: log_archive.sh c nginx g int g app collect logs for nginx component from int and app groups log_archive.sh c workfusion c nginx g app collect logs for nginx, workfusion components from app group log_archive.sh c nginx g app g int d 2018 10 * collect logs for nginx from int and app groups on October 2018 While creating a logs archive, keep in mind that the date pattern is UTC based. In some cases, required logs may have earlier date or date in future. We recommend to create an archive for additional dates before and after an incident to be sure that all logs are gathered correctly. Example: the following command with parameters collects logs for the nginx component in the INT and APP groups for date 2018 11 01 and packs them into the current directory in file wf logs .tar.gz. opt workfusion tools log_archive.sh c nginx g int g app d 2018 11 01 collect logs for nginx component from int and app groups tar: Removing leading ` ' from member names opt workfusion logs int int example.workfusion.com nginx 2018 11 01 nginx.err.log opt workfusion logs int int example.workfusion.com nginx 2018 11 01 nginx.out.log opt workfusion logs app app example.workfusion.com nginx 2018 11 01 nginx.out.log opt workfusion logs app app example.workfusion.com nginx 2018 11 01 nginx.err.log Logs were packed into archive wf logs 1541062518.tar.gz in current directory Validate that archive is not empty and contains all required files. ls la wf logs 1541062518.tar.gz rw rw r . 1 wfuser wfuser 71562 Nov 1 08:55 wf logs 1541062518.tar.gz Specification of log _archive.sh This section explains advanced options of the log _archive.sh script, which is used to create an archive of aggregated logs. The archiving script runs with the following parameters: opt workfusion tools log_archive.sh c nginx g int g app d 2018 11 01 Here, c . The name of a component to aggregate logs from. d . The date of logs in format YYYY mm dd, for example, 2018 10 25. You can use regular expressions. Example: to view all logs for October 2018, run the script with the following parameter: opt workfusion tools log_archiver.sh d 2018 10 * g. The server role of logs. Available server groups are the following: int app ocr bi bep _master bep _agent rpa h . A hostname to receive logs from a specific host. You can use regular expressions. Example: to receive logs from multiple servers: bep agent 20.example.com, bep agent 21.example.com, run the script with the following parameter: opt workfusion tools log_archiver.sh h bep agent 2 0 1 .example.com "},{"version":"10.0","date":"Jun-26-2019","title":"bep","name":"BEP","fullPath":"iac/admin/maintenance/bep","content":" Marathon Software owner: Mesosphere, Inc. System requirements: https: mesosphere.github.io marathon Management: wfmanager {start stop restart status} marathon Log files: Access logs: supervisord log marathon_access.log Error logs: supervisord log marathon_error.log Startup scripts: supervisord apps marathon.ini marathon bin start Configuration files: Service directory: marathon Default ports: tcp 8480 and 8080 Mesos Master Software owner: Apache Software Foundation System requirements: http: mesos.apache.org Management: wfmanager {start stop restart status} mesos master Log files: supervisord log mesos master.log Startup scripts: supervisord apps mesos master.ini mesos usr sbin mesos master Configuration files: Service directory: mesos General mesos master configuration: supervisord apps mesos master.ini Default ports: tcp 5050 Mesos Agent Software owner: Apache Software Foundation System requirements: http: mesos.apache.org Management: wfmanager {start stop restart status} mesos slave Log files: supervisord log mesos slave.log Startup scripts: supervisord apps mesos slave.ini mesos usr sbin mesos slave Configuration files: Service directory: mesos General mesos slave configuration: supervisord apps mesos slave.ini Default ports: tcp 5051 AutoML gateway service Software owner: Workfusion Service description:Gateway service receives requests from WorkFusion app and sends responses (list of available models, extracted data). It provides REST API for all available ML operations. It communicates with other ML services via RabbitMQ. Log files: Access logs: supervisord log automl gateway service.log Management: wfmanager {start stop restart status} automl gateway service Installation directories: automl gateway service main directory: automl gateway service Startup scripts: supervisord apps automl gateway service.ini java Configuration files: automl gateway service automl gateway service.yml Default ports: tcp 9080 VDS mesos adapter Software owner: Workfusion Log files: supervisord log vds mesos adapter.log Management: wfmanager {start stop restart status} vds mesos adapter Installation directories: vds mesos adapter main directory: vds mesos adapter Startup scripts: supervisord apps vds mesos adapter.ini java Configuration files: vds mesos adapter vds mesos adapter.properties Default ports: tcp 9052 VDS scaling service Software owner: Workfusion Log files: supervisord log vds scaling service.log Management: wfmanager {start stop restart status} vds scaling service Installation directories: vds scaling service main directory: vds scaling service Startup scripts: supervisord apps vds scaling service.ini java Configuration files: vds scaling service vds scaling service.properties Default ports: tcp 9081 VDS whitelister Software owner: Workfusion Log files: supervisord log vds whitelister.log Installation directories: vds whitelister main directory: vds whitelister Startup scripts: supervisord apps vds whitelister.ini vds whitelister vds_whitelister.py Configuration files: nginx auth whitelist Default ports: no ports "},{"version":"10.0","date":"Oct-04-2019","title":"backup-and-restore-data","name":"Backup and restore data","fullPath":"iac/admin/maintenance/backup-and-restore-data","content":" Currently, if DNS names are changed, the following backup and restoration process cannot be used as a part of migration process, due to the fact that the VAULT DB component will contain records with old DNS names from the previous installation. The described backup script can backup and restore only SPA data components. General Information The Ansible installation features the script that implements the following cold backup solution: Backup Backup script Syntax wf backup.sh { OPERATION } { COMPONENT } { ROLE } Usage: . wf backup.sh {prepare,backup,restore} {all,minio,vault,nexus,distr} {int} EXTRA_VARS... Examples: . wf backup.sh prepare all int . wf backup.sh backup all int . wf backup.sh restore all int Available procedures Backup script supports the following operations: Prepare (optional) Hot sync data. It is part of the Backup operation, but can be launched separately before the main backup process to achieve minimal downtime. Backup All SPA services are turned off, components are archived. Once completed, the services are restarted. Restore The script searches for backup archives in the backup directory and performs the restoration process. Components for backup The script supports backup and restoration of the following components: minio vault nexus distr Script variables cat config.yml install_dir: opt workfusion backupdirectory: '{{ installdir }} backup' Backup data To backup data: On the INT Server, run the script with the prepare parameter (hot sync step): . wf backup.sh prepare all int The script copies (hot sync) all INT components to {{ backup_directory }}. Result ls 1 opt workfusion backup minio nexus vault distr On the APP Server, manually terminate all APP services: wfmanager stop all Run the main backup operation. . wf backup.sh backup all int The script checks if there are no external connections to WF databases, shuts WF services down, then performs cold sync of database components files. After that, the files are archived. As result in your {{ backup directory }}, you find the archives with backups of components' content: ls 1 opt workfusion backup ... minio 08 47 21 02 05 2019.tar.gz nexus 08 47 21 02 05 2019.tar.gz vault 08 47 21 02 05 2019.tar.gz distr 08 47 21 02 05 2019.tar.gz Move the archives to a backup storage. Restore data from archives On the INT Server, run installation as a root. . install.sh preinstall int su wfuser . install.sh install int && . install.sh check int Place the backup archives with the INT components to a backup directory. Before proceeding, make sure that all services on the APP Server are stopped. Run the restore command: . wf backup.sh restore all int Verify that the services have started successfully. wfmanager status After restoration, the old data is located in {{ backup directory }} state before _restore . "},{"version":"10.0","date":"Jul-08-2019","title":"change-a-component-s-log-level","name":"Change a component's log level","fullPath":"iac/admin/maintenance/change-a-component-s-log-level","content":" This article describes the steps to change a log level type between default prod (WARN ERROR) and dev (DEBUG INFO) for a specific server or a component. The log level can be updated for the following components: ocr sqc workspace workfusion Make sure to run the following steps as the runtime user, which is defined in config.yml, in the wf_user variable. To change a component's log level: Login via SSH to the server, where the required component is installed. For the list of components and their locations, see Product Architecture (.. .. .. spa). Supported servers include: APP, INT, OCR. The log level type cannot be modified for the RPA and BI servers. Change a user to default wfuser. su wfuser Change the directory to . cd opt workfusion wf_installer If this directory is missing, the installer has been either deleted or unpacked to a different location. In this case unpack the installer to the right location once again. In , in config.yml, update the logleveltype variable with the prod or dev value. . install.sh edit_config int logleveltype: prod possible values 'prod' or 'dev' With the The logleveltype variable you can switch a log level for specific SPA components, prod is less verbose than dev. For more information, in group vars all main.yml, see the logconfig variable. There are two valid scenarios to follow: On the current server, update the log level for all components. In this case, specify a server role, for example, app, as the server role in the example below. For more information, see Server roles description. . install.sh updateloglevel app Update log level for a specific component. In this case, specify a component's name (workspace in the example below): . install.sh updateloglevel workspace "},{"version":"10.0","date":"Jun-26-2019","title":"common-services","name":"Common services","fullPath":"iac/admin/maintenance/common-services","content":" Filebeat This service is responsible for sending content of specific logs files to the Logstash service. See the inputs.d configuration directory. Software owner: Elasticsearch BV Management: wfmanager {start stop restart status} filebeat Log files: Logs: supervisord log filebeat.out.log Service logs: filebeat logs filebeat.log Startup scripts: supervisord apps filebeat.ini Configuration files: filebeat filebeat.yml filebeat inputs.d *.yml Default ports: doesn't listen to any port Metricbeat This service is responsible for sending metrics data to the Logstash service. See the modules.d configuration directory. Software owner: Elasticsearch BV Management: wfmanager {start stop restart status} metricbeat Log files: supervisord log metricbeat.log metricbeat logs metricbeat Startup scripts: supervisord apps metricbeat.ini Configuration files: metricbeat metricbeat.yml metricbeat inputs.d *.yml Default ports: doesn't listen to any port Heartbeat This service is responsible for checking status of monitored services to the Logstash service. See the monitors.d configuration directory. Software owner: Elasticsearch BV Management: wfmanager {start stop restart status} heartbeat Log files: supervisord log heartbeat.log Startup scripts: supervisord apps heartbeat.ini Configuration files: heartbeat heartbeat.yml heartbeat monitors.d *.yml Default ports: doesn't listen to any port Nginx Software owner: NGINX Inc System requirements: https: www.nginx.com products technical specs Management: wfmanager start stop restart status nginx Log files Access logs: supervisord log nginx_access.log Error logs: supervisord log nginx_error.log Startup scripts: supervisord apps nginx.ini nginx run_nginx.sh Configuration files: Service directory: nginx General nginx configuration: nginx nginx.conf nginx sites.d * Default ports: tcp 80, 443 "},{"version":"10.0","date":"Oct-14-2019","title":"filebeat-configuration","name":"Filebeat configuration","fullPath":"iac/admin/maintenance/filebeat-configuration","content":" General Configuration Configuraiton file – filebeat.yml Directory config general settings path: home: '{{ install_dir }} filebeat' config: '{{ install_dir }} filebeat' data: '{{ install_dir }} filebeat data' logs: '{{ install_dir }} filebeat logs' modules: '{{ install_dir }} filebeat modules.d' Input config filebeat: registryfile: registryin config: inputs: enabled: true path: inputs.d *.yml reload: enabled: true period: 10s Name config The name of the shipper that publishes the network data. It can be used to group all the transactions sent by a single shipper in the web interface. name: \"{{ hostname }}\" Output config output settings output.logstash: enabled: true hosts: \"{{ apmhostname }}:{{ logstashspa_port }}\" ssl.certificate_authorities: \"{{ filebeat.path.home ' ssl ca.crt' }}\" Logging config logging.level: info logging.to_syslog: False logging.to_files: True logging.files: path: {{ filebeat.path.logs }} name: filebeat.log rotateeverybytes: 104857600 keepfiles: 30 Input Configuration Configuration files – inputs.d *.yml Example of the input config: type: log enabled: true paths: tmp service.log fields: servicegroupname: 'app' service_name: 'service' fieldsunderroot: true close_inactive: 2m "},{"version":"10.0","date":"Jun-26-2019","title":"general-services-information","name":"General services Information","fullPath":"iac/admin/maintenance/general-services-information","content":" Wfmanager The wfmanager utility is designed to facilitate management of WorkFusion applications. To use the utility, login as ` and run the wfmanager` command: Will show status of all applications wfmanager status Start, stop, restart individual services wfmanager start nginx wfmanager restart nginx wfmanager stop nginx Show log tail of an application wfmanager tail nginx Start, stop, restart all at once wfmanager start all wfmanager restart all wfmanager stop all Show status and start an interactive console. The console enables the user to run all the same commands without typing wfmanager every time. wfmanager :::tip A special \"fake service\" start all is used to autostart applications after boot and reboot. You don't need to control it manually. ::: Ansible installer logs Installation logs are stored in logs in the following format: install{ operation }%year%month%day%hour%minute For example: ls 1 logs installprecheck190424140332 installencrypt190424133049 installpreinstall190424140250 installinstall190425083232 installcheck190425083928 Location of logs The logs for each individual application are stored in supervisord log .log. "},{"version":"10.0","date":"Jun-26-2019","title":"inorder-and-deps-checker","name":"Inorder and deps_checker","fullPath":"iac/admin/maintenance/inorder-and-deps-checker","content":" Inorder and deps _checker are the part of the wfmanager utility, which is based on supervisord. Supervisord‘s primary purpose is to create and manage processes, based on the data in its configuration file, by creating subprocesses. Each subprocess that is spawned by supervisor is managed during its lifetime by supervisord, which is parent for each process it creates. When a child dies, supervisord is notified about its termination via the SIGCHLD signal, and it performs the appropriate operation. Inorder service Why: The priority order in supervisor determines the startup order, but when autostart=true, supervisor doesn't wait for the previous process to be RUNNING in order to continue. Moreover, initialization scripts that need to exit before continuing can cause additional difficulties. The problem can be seen in supervisor bug 122. Solution: We use the Inorder service to start an additional library that helps to control the sequential startup. Known behavior: \"inorder FATAL Exited too quickly (process log may have details)\", if other services are in the RUNNING state, it means the Inorder service has performed correctly. Deps _checker service Why: Before starting main applications, we recommend to ensure that databases and some other services are available. Solution: use supervisord bash wrapper to wait for connectivity. Current implementation waits for the TCP connectivity to host and port for deps_apps. Process States A process controlled by supervisord will have one of the following states at any given time: STOPPED – The process has been stopped due to a stop request or has never been started. STARTING – The process is starting due to a start request. RUNNING – The process is running. BACKOFF – The process entered the STARTING state but subsequently exited too quickly to move to the RUNNING state. STOPPING – The process is stopping due to a stop request. EXITED – The process exited from the RUNNING state, expectedly or unexpectedly. FATAL – The process could not be started successfully. UNKNOWN – The process is in an unknown state (supervisord programming error). "},{"version":"10.0","date":"Jun-26-2019","title":"int","name":"INT","fullPath":"iac/admin/maintenance/int","content":" Logstash The Logstash service is running on the Integration server and contains six pipelines: logstash spa receives log events from all Filebeat services, places them into the install dir logs directory and sends to the ElasticSearch filebeat * index. logstash automl extract receives log events from running AutoML extract models and places them into the logs bep master BEP MASTER HOSTNAME automl extract directory. logstash bep logs receives log events from BEP services and workers and places them into the logs bep directory. logstash bep metrics receives BEP services metrics and sends them into a DB for further processing by Analytics. logstash metrics receives system metrics from all hosts with the installed Metricbeat service and sends them to a DB for further processing by Analytics. logstash heartbeat receives Heartbeat status logs from all the hosts and components with installed Heartbeat and sends them to the heartbeat * index in ElasticSearch and to logs SERVICE NAME SERVICE HOST COMPONENT _NAME DATE heartbeat status.log Software owner: Elasticsearch BV Management: wfmanager {start stop restart status} logstash Log files: supervisord log logstash.log Startup scripts: supervisord apps logstash.ini logstash bin logstash Configuration files: logstash logstash.conf Default ports: tcp 4567 (logstash spa) 4568 (logstash automl extract) 4569 (logstash metrics) 4570 (logstash bep logs) 15072 (logstash bep metrics) 4571 (logstash heartbeat) ElasticSearch Software owner: ElasticSearch BV Management: wfmanager {start stop restart status} elasticsearch Log files: supervisord log elastisearch.log Startup scripts: supervisord apps elasticsearch.ini elasticsearch bin Configuration files: elasticsearch config elasticsearch.yml Default ports: tcp 9200 Vault Software owner: Hashicorp Vault Management: wfmanager {start stop restart status} vault:* Log files: Access logs: supervisord log vault server.out.log Error logs: supervisord log vault server.err.log Startup scripts: supervisord apps vault.ini vault vault Configuration files: vault config server.hcl Default ports: tcp 8200 Zookeeper Software owner: Apache Software Foundation System requirements: https: zookeeper.apache.org doc r3.1.2 zookeeperAdmin.html sc_systemReq Management: wfmanager {start stop restart status} zookeeper Log files: supervisord log zookeeper.log Startup scripts: supervisord apps zookeeper.ini zookeeper bin zkServer.sh Configuration files: Service directory: zookeeper General zookeeper configuration: zookeeper zoo.cfg Default ports: tcp 2181 Rabbitmq Software owner: Pivotal Software, Inc. System requirements: 4 CPUs, 4 GB RAM, 2 GB free space Management: wfmanager {start stop restart status} rabbitmq Log files: supervisord log rabbitmq.log Startup scripts: supervisord apps rabbitmq.ini rabbitmq sbin rabbitmq server Configuration files: Service directory: rabbitmq General rabbitmq configuration: rabbitmq etc rabbitmq rabbitmq.config Default ports: tcp 5672 Minio Software owner: Minio, Inc Management: wfmanager (start stop restart status) minio Log files: supervisord log minio.log Startup scripts: supervisord apps minio.ini Configuration files: minio etc config.json Default Ports: tcp 9000 Nexus Software owner: Sonatype Management: wfmanger (start stop restart status) nexus Log files: supervisord log nexus. *.log Data Files: nexus sonatype work nexus Installation directories: nexus Configuration files: nexus conf nexus.properties Default Ports: 80 443 Task Dispatcher Service Software owner: The Apache Software Foundation Management: wfmanager {start stop restart status} task dispatcher service Log files: supervisord log task dispatcher service.log Startup scripts: supervisord apps task dispatcher service.ini Configuration files: task dispatcher service task dispatcher service.yml "},{"version":"10.0","date":"Jun-26-2019","title":"ocr","name":"OCR","fullPath":"iac/admin/maintenance/ocr","content":" OCR ABBYY Finereader engine Service overview Software owner: ABBYY System requirements: https: www.abbyy.com en us ocr sdk technical specifications Log files: no log files Installation directories: Abbyy engine main directory: ABBYY _FRE11 Configuration files: no configuration files Default ports: no ports Default parameters Configuration file Name Default Value Description config.yml install dir opt workfusion The directory to place user files, logs and configs config.yml ocr abbyy pool _size 2 A number of OCR concurrent executions. Must match license productivity config.yml ocr spring profiles _active disabled auth, mongoDbQueue, gridfs storage, mongo task Default value ocr2rest & ocr2worker Service overview Management: wfmanager {start stop restart status} ocr2rest wfmanager {start stop restart status} ocr2worker Log files: Access logs: supervisord log ocr2rest.out.log Error logs: supervisord log ocr2rest.err.log Access logs: supervisord log ocr2worker.out.log Error logs: supervisord log ocr2worker.err.log Startup scripts: supervisord apps ocr2 rest.ini supervisord apps ocr2 worker.ini Configuration files: ocr etc ocr rest.yml ocr etc ocr worker.yml Default ports: tcp 9002 Default parameters Configuration file Name Default Value Description config.yml install dir opt workfusion The directory to place user files, logs and configs config.yml ocr_hostname ocr.example.com FQDN for OCR DNS name ocr worker.yml ocrabbyypool_size CPU Cores count – 1 A number of OCR concurrent executions; musr match license productivity ocr rest.yml and ocr worker.yml ocrspringprofiles_active disabled auth,rabbitmqQueue,s3 storage,mssql default value Windows OCR Service overview Log files: Access logs: wfagent log ocr.api out.log Error logs: wfagent log ocr.api err.log Access logs: wfagent log ocr.worker out.log Error logs: wfagent log ocr.worker err.log Startup scripts: ocr ocr api start.bat ocr ocr worker start.bat Configuration files: ocr etc ocr rest.yml ocr etc ocr worker.yml Default parameters Name Default value c: Workfusion OCR API Port 9002 OCR API active profiles disabled auth,rabbitmqQueue,s3 storage,mssql OCR Worker active profiles rabbitmqQueue,s3 storage,mssql "},{"version":"10.0","date":"Jun-26-2019","title":"reference-log-configuration-of-components","name":"Reference: log configuration of components","fullPath":"iac/admin/maintenance/reference-log-configuration-of-components","content":" This article explains advanced configuration of most important WorkFusion components. Control Tower Configuration file: workfusion conf logback.xml Hot reload: Yes The primary log configuration file for the Control Tower application. Allows to switch log level for specific packages and libraries :::note If this config is modified manually, it is required to update both appender and logger to an appropriate log level. If appender has the treshold value warn, setting logger to \"info\" will not have any effect. ::: Configuration file: workfusion conf logging.properties Hot reload: Yes A custom configuration file for controlling log level for specific Control Tower classes. The reload interval is specified in workfusion conf workfusion.properties (by default, 60 seconds) WorkSpace Configuration files: workspace _production conf workspace logback.xml workspace production conf workspace api logback.xml Hot reload: Yes workspace and workspace api have separate logging configuration. SQC Configuration file: sqc conf log4j.xml Hot reload: No OCR Configuration file: ocr etc logback rest.xml ocr etc logback task.xml ocr etc logback worker.xml Hot reload: No logback rest.xml – OCR Rest service logback worker.xml – OCR Worker service logback task.xml – OCR Task process "},{"version":"10.0","date":"Jun-26-2019","title":"reference-log-rotation","name":"Reference: log rotation","fullPath":"iac/admin/maintenance/reference-log-rotation","content":" This article explains advanced configuration of installer to control log rotation and retention. Logs rotation policies The following policies control the rotation and expiration of logs on the PM server in logs directory: Size based log rotation for current date. When an original log file reaches its maximum, it is rotated and the suffix is added. Number of rotated files: 7. Parameter logs rotate count in group _vars all vars.yml). Maximum size of a file: 200MB. Parameters logs rotate maxsize and logs rotate size in group _vars all vars.yml). If the log file size exceeds the Maximum size * Number of rotated files (by default 1.4GB), log data is removed from the central server, but remains on the source server. Log compression. All files older than today will be automatically compressed with gunzip. Log cleanup. All files older than 14 days will be automatically removed from the aggregation server. This behaviour is configured in installer's group vars all vars.yml file, in parameter logs retention, before installation of the INT server. If not specified, the default value is 13, which means that files older than 14 days are removed. Log rotation configuration These parameters are optional during the configuration process, but they may be used to control log retention in case of specific data storage policies. Configuration Default Description Manual reconfiguration logs_retention 13 The number of days to keep old logs (files older than 14 days will be deleted Edit crontab crontab e under wfuser Modify line below ... Ansible: logstash cleanup logs ... mtime 13 ... ... logsrotatecount 7 The number of rotated files of the same log file during same date. The parameter is used to control piece size of logs Edit &lt;install dir&gt; logrotate logrotate.d wfaggregatedlogs.conf vi opt workfusion logrotate logrotate.d wfaggregatedlogs.conf under wfuser Modify line below ... rotate 7 ... logsrotatemaxsize logsrotatesize 200M Should be same for both options. Size of each rotated file Edit &lt;install dir&gt; logrotate logrotate.d wfaggregatedlogs.conf vi opt workfusion logrotate logrotate.d wfaggregatedlogs.conf under wfuser Modify both lines below ... size 200M maxsize 200M ... "},{"version":"10.0","date":"Sep-02-2019","title":"restart-servers","name":"Restart servers","fullPath":"iac/admin/maintenance/restart-servers","content":" The following Linux Servers can be restarted for maintenance activities: INT, APP, OCR, BEP Cluster . Prerequisites Windows Servers will be recovered automatically. RPA services use Task Scheduler task(s) to start automatically after the restart. The BI service is added to autostart. The following steps must be performed as , for example, wfuser. Available server roles: app – APP server int – INT server ocr – OCR server bep master – BEP Master server bep agent – BEP Agent server Restart Linux servers To restart Linux servers: Log into the servers in the following order: OCR BEP Agent server(s) BEP Master server APP INT Run the following command on each server: wfmanager stop all Restart the servers in the following order: INT APP BEP Master server BEP Agent server(s) OCR The servers restore their state automatically. Log into each server and check the services: wfmanager status all Run the check command: cd . install.sh check "},{"version":"10.0","date":"Oct-04-2019","title":"search-a-specific-message-in-logs","name":"Search a specific message in logs","fullPath":"iac/admin/maintenance/search-a-specific-message-in-logs","content":" This article describes necessary steps to search and filter for specific messages in selected logs. To search a mesage in logs: Open the Kibana Dashboard page. Open Workfusion SPA Application Logs dashboard. app logs dashboard Select Hostname, Service name and Log name, if needed, and then click Apply changes. app dash settings In the Search field, enter a message to search, for example, message:\"automl gateway service\", and then click Refresh. search kibana For more details on search queries, see Kibana queries and filters (https: www.elastic.co guide en beats packetbeat current kibana queries filters.html). To change filtering time range, open Time range menu and choose the time interval for logs filter. time range "},{"version":"10.0","date":"Oct-14-2019","title":"update-passwords","name":"Update passwords","fullPath":"iac/admin/maintenance/update-passwords","content":" This guide describes the process of passwords update for the installed Workfusion SPA product. We assume that \"passwords\" are any settings specified in config.yml file before installation. This guide may be useful in the following cases: Workfusion SPA was initially installed with weak passwords, and you want to update some or all of them. You have a password rotation policy in your organization, which requires regular passwords update. Predefined common variables: – the WorkFusion user. By default, wfuser – a user that was created for SPA installation. By default, ec2 user – the WorkFusion home directory. By default, opt workfusion – the WorkFusion installer directory. By default, opt workfusion wf _installer Once you have begun to update passwords, make sure to finish it on all servers. Otherwise, your product will not be operational. If you changed database usernames, it is up to you to remove previous users manually, if needed, for example, any INT users. :::important Remeber to run the following steps as . ::: To update passwords: On the INT server, in config.yml, change the required passwords: cd . install.sh edit_config int open for editing Note: You can use the following steps both to update your passwords and the vault certificates. If you have changed passwords for any certificate, remove the appropriate certificates and re generate them, as described in article Prepare TLS certificates for installation. Example... If you have changed the following lines in config.yml: vaultadmincert_pass vaultclientcert_pass re generate all vault certificates: rw rw r . 1 ec2 user ec2 user 1298 Jun 3 12:15 vault_admin.crt rw rw r . 1 ec2 user ec2 user 2517 Jun 3 12:15 vault_admin.p12 rw rw r . 1 ec2 user ec2 user 1298 Jun 3 12:15 vault_workfusion.crt rw rw r . 1 ec2 user ec2 user 2517 Jun 3 12:15 vault_workfusion.p12 If you have changed the following lines in config.yml, cakeypass re generate the following certificates: rw rw r . 1 ec2 user ec2 user 1424 Jun 3 12:15 elk ca.crt rw rw r . 1 ec2 user ec2 user 1766 Jun 3 12:15 elk ca.key rw rw r . 1 ec2 user ec2 user 1757 Jun 3 12:15 kibana.crt rw rw r . 1 ec2 user ec2 user 3272 Jun 3 12:15 kibana.key rw rw r . 1 ec2 user ec2 user 1761 Jun 3 12:15 logstash.crt rw rw r . 1 ec2 user ec2 user 3272 Jun 3 12:15 logstash.key rw rw r . 1 ec2 user ec2 user 1428 Jun 3 12:49 mtls ca.crt rw rw r . 1 ec2 user ec2 user 1766 Jun 3 12:49 mtls ca.key rw rw r . 1 ec2 user ec2 user 1769 Jun 3 12:49 mtls client.crt rw rw r . 1 ec2 user ec2 user 3272 Jun 3 12:49 mtls client.key If you have changed the following lines in config.yml elklogstashcertificate_pass re generate the logstash.p12 package: rw rw r . 1 ec2 user ec2 user 4013 Jun 3 12:15 logstash.p12 Update the passwords depending on your installation approach: Single point setup: Update passwords on all servers in the following order: INT > BEP Master > BEP Agents > APP > OCR (Linux). For that, run the following commands: export ANSIBLEPRIVATEKEY_FILE=\"\" export ANSIBLEREMOTEUSER=\"\" cd . install.sh configure Replace with the respective server role. Supported values: int, bep master, bep agent, app, ocr. Legacy setup: Copy the newly generated certificates (see step 2) and the config.yml file from the initial installation directory on the INT server to all other Linux servers (BEP Master, BEP Agents, APP, OCR (Linux).) to the same locations, as on the INT server. Update passwords locally on all other servers in the following order: INT > BEP Master > BEP Agents > APP > OCR (Linux). cd . install.sh configure Replace with the respective server role. Supported values: int, bep master, bep agent, app, ocr As a result, the new passwords and Vault certs (if they have been re generated) are applied. :::note If you have re generated any other certificates, except Vault, you need to complete updating them, as described in Update TLS certificates (step 3). ::: Note that the following parameters can only be specified before the installation and can't be changed later. wftableauautomation_username wftableauautomation_password wftableaudashboard_username wftableaudashboard_password For more information, see the config.yml options. "},{"version":"10.0","date":"Oct-14-2019","title":"update-tls-certificates","name":"Update TLS certificates","fullPath":"iac/admin/maintenance/update-tls-certificates","content":" After the installation, you can update the SSL certificates on all Linux servers with the single install.sh script. Prerequisites New SSL certificates must be located in the certificates directory ( certificates). Be sure to run these steps as the installation user. Available : int INT server bep master BEP Master server bep agent BEP Agent server app APP server ocr OCR server apm if you use APM server separately Update certificates Before updating the existing certificates, they must be already generated. For more infromation, see Prepare TLS certificates for installation. To update certificates: From the certificates directory on the INT server, remove the certificates that you are going to update. On the INT server, generate the missing certificates once again. See Prepare TLS certificates for installation. Update the certificates, depending on your installation approach: Single point setup: Make sure that the following variables are exported: export ANSIBLEPRIVATEKEY_FILE=\"\" export ANSIBLEREMOTEUSER=\"\" Go to the directory, and then run the script to update the certificates for a chosen server role: cd . install.sh update_certs Update certificates for all server roles in the following order: INT > BEP Master > BEP Agents > APP > OCR (Linux) > APM (if you use APM separately). Legacy setup: Copy the newly generated certificates from the initial directory on the INT server to the same directory on other Linux servers (BEP master, BEP agents, APP, OCR, APM (if you use APM separately)). On each Linux server, run the following command with the specified server role to update certificates and distribute them among required services: cd . install.sh update_certs Replace with the respective server role. See the supported values earlier. For example: ~installation user . install.sh update_certs app or ~installation user . install.sh update_certs ocr :::note The script won't update the *.p12 Vault certificates. To update the Vault certificates see Update Passwords. ::: Run Post installation Checks To check that the certificates are updated correctly, run the following command: . install.sh check "},{"version":"10.0","date":"Jun-21-2019","title":"connect-external-elk","name":"Integrate external ELK","fullPath":"iac/admin/monitoring/connect-external-elk","content":" You can use your own ELK, which is deployed on an external server, instead of the integrated solution provided by WorkFusion during the installation. To connect your own Elsaticsearch to the WorkFusion system: In your directory, open the config.yml configuration file for editing. The vi text editor is used in the example below: vi config.yml In config.yml, in the Advanced configuration section, uncomment the following lines, and specify the parameters: elasticsearch_external: true kibanaurl: 'http: elasticsearch external1.example.com:5701 externalkibana' elasticsearch_host: 'elasticsearch external1.example.com' elasticsearchurl: 'http: elasticsearch external1.example.com:9210 externalelasticsearch' elasticsearch_port: '9210' uncomment and define if your port for Elasticsearch is different than 9200 elasticsearch_external. Set to true to enable external Elasticsearch and Kibana. kibana_url. Set the external Kibana URL. elasticsearch_host. Set the Elasticsearch host on an external server. elasticsearch_url. Set the external Elasticsearch URL. elasticsearch_port. Set the external Elasticsearch host. Optional. Define the port, only if you use custom ELK port other than 9200. Make sure that the following users are added to the external ELK components and their passwords are the same as specified in config.yml > section ELK. ELK elkadminpass: 'sc@R@ S 9V' password for elasticsearch user 'admin' used as login for Kibana UI elkelasticpass: 'KL7N! v2@M' password for build in elasticsearch user 'elastic' elkkibanapass: 'PM8Q *p1 Q' password for build in elasticsearch user 'kibana' used for install process elklogstashcertificate_pass: 'j3D qEe4!d' auth certificate 'logstash.p12' password Kibana UI user: 'admin' Kibana installation user: 'kibana' Elasticsearch user:'elastic' Save and exit config.yml. "},{"version":"10.0","date":"Oct-14-2019","title":"heartbeat-configuration-guide","name":"Heartbeat configuration guide","fullPath":"iac/admin/monitoring/heartbeat-configuration-guide","content":" General Configuration Configuration file heartbeat.yml Input config heartbeat.config.monitors: Directory glob pattern to search for configuration files path: {{ heartbeat.path.monitor ' *.yml' }} reload.enabled: true How often to check for changes reload.period: 60 Output config output.logstash: The Logstash hosts hosts: \"{{ logstashhost }}:{{ logstashheartbeat_port }}\" Optional SSL. By default is off. List of root certificates for HTTPS server verifications ssl.certificate_authorities: \"{{ heartbeat.path.home ' ssl ca.crt' }}\" Input Configuration Configuration files monitors.d and *.yml HTTP monitors Workfusion Configuration file heartbeat workfusion.yml. This monitor checks status of the workfusion service. type: http urls: \"https: {{ apphostname }}:{{ nginxport_ssl }} workfusion\" schedule: '@every {{ heartbeatmonitorshttp_schedule }}' timeout: 30s name: 'workfusion' ssl: certificateauthorities: '{{ heartbeathome }} ssl ca.crt' supported_protocols: \"TLSv1.2\" check.request: method: GET check.response: status: 200 Marathon Configuration file heartbeat marathon.yml. This monitor checks status of marathon service. type: http urls: \"https: {{ vdsmasterhostname }}:{{ marathonproxyport }} ui\" schedule: '@every {{ heartbeatmonitorshttp_schedule }}' timeout: 30s name: 'marathon' ssl: certificateauthorities: '{{ heartbeathome }} ssl ca.crt' supported_protocols: \"TLSv1.2\" check.request: method: GET check.response: status: 200 TCP monitors Zookeeper Configuration fileheartbeat zookeeper.yml. This monitor checks status of zookeeper service. type: tcp hosts: \"{{ zookeeper_host }}\" ports: {{ zookeeper_port }} schedule: '@every {{ heartbeatmonitorshttp_schedule }}' timeout: 30s name: 'zookeeper' "},{"version":"10.0","date":"Oct-04-2019","title":"investigate-performance-hot-spot","name":"Investigate performance hot spot in environment","fullPath":"iac/admin/monitoring/investigate-performance-hot-spot","content":" To find performance hot spots: In Kibana, click Dashboard > System Overview. System overview In the Top hosts by CPU group, click a host with the highest CPU load. Top hosts Find a process with the highest CPU load. Highest load To return to the first dashboard, click System Overview. Return "},{"version":"10.0","date":"Oct-14-2019","title":"metricbeat-configuration-guide","name":"Metricbeat configuration guide","fullPath":"iac/admin/monitoring/metricbeat-configuration-guide","content":" General Configuration Configuraiton file metricbeat.yml. Directory config general settings path.home: {{ install_dir }} metricbeat path.config: {{ install_dir }} metricbeat path.data: {{ install_dir }} metricbeat data Input config metricbeat.config.modules: path: {{ install_dir }} metricbeat modules.d *.yml reload.enabled: false Fields config fields: ns: {{ server_group }} rpa, bi, app, ocr, bep master, bep agent Name config The name of the shipper that publishes the network data. It can be used to group all the transactions sent by a single shipper in the web interface. name: \"{{ hostname }}\" Output config output settings output.logstash: hosts: \"{{ inthostname }}:{{ logstashmetricsport }}\" logstashmetric_port=4569 by default ssl.certificateauthorities: \"{{ installdir 'metricbeat ssl ca.crt' }}\" Input Configuration Configuration files modules.d *.yml. System metrics Configuration file system.yml. The module collects such system metrics as CPU Memory Disk Processes. module: system period: 30s metricsets: cpu load memory network process process_summary diskio processes: '.*' process.includetopn: bycpu: 10 include top {{ metricbeattopprocessescpu_count }} processes by CPU bymemory: 10 include top 10 {{ metricbeattopprocessesmemory_count }} processes by memory module: system period: 30s metricsets: filesystem fsstat filesystem.ignore_types: nfs, smbfs, autofs, tmpfs, devtmpfs processors: drop_event.when.regexp: system.filesystem.mount_point: ' (sys cgroup proc dev etc host lib)( )' module: system period: 15m metricsets: uptime Workfusion custom metrics Configuration file workfusion.yml. The module collects directory metrics where WorkFusion is installed. module: workfusion metricsets: \"directory\" enabled: true period: 60s hosts: \"localhost\" path: {{ install_dir }} module: workfusion metricsets: \"volume\" enabled: true period: 60s hosts: \"localhost\" path: {{ install_dir }} alias: install "},{"version":"10.0","date":"Oct-14-2019","title":"monitoring-overview","name":"Monitoring overview","fullPath":"iac/admin/monitoring/monitoring-overview","content":" The monitoring solution consists of the following components: Kibana with system performance dashboards Elasticsearch for storing metrics data Logstash for aggregation and forwarding metrics data Open monitoring dashboards To browse monitoring dashboards on Kibana UI: Go to Kibana, for example, at https: apm.example.com kibana. Log in to Kibana by using the predefined user admin, and the password, specified in config.yml during installation. Kibana On the main menu, click Dashboard to open the list of preconfigured dashboards. Kibana Browse dashboards System overview This dashboard contains summary of all configured hosts: Kibana Click a host to open page Host overview dashboard with detailed information on the respective instance. Host overview This dashboard contains detailed time series information of the host: Kibana Environment status This Dashboard displays services' status of SPA environment: Kibana The TCP monitors status and HTTP monitors status diagrams display the overall status: Kibana See diagrams Heartbeat Services, HTTP up status, and HTTP ping times for detailed information: Kibana Kibana "},{"version":"10.0","date":"Oct-16-2019","title":"сhange-elk-admin-password","name":"Change ELK admin password in Kibana","fullPath":"iac/admin/monitoring/сhange-elk-admin-password","content":" id: version 10.0 change elk admin password original_id: change elk admin password To change a password: Go to Kibana, for example, , and log into it with your current password. On the main menu, click Management. Management In the Security section, click Users. The Users window appears. Security In the Users window, select user admin. Admin Click Change password. Change password In the corresponding fields, enter your Current password, and New password with confirmation. Edit user Click Update user. "},{"version":"10.0","date":"Jul-08-2019","title":"create-users","name":"Create users","fullPath":"iac/admin/install-spa/create-users","content":" Create installation users Temporary installation users must be created on all Linux and Windows servers. For successful installation, the user must meet the following requirements: The name of the installation user must be the same on all servers. The user must have the sudo permission without passwords. The SSH public key for the installation user must be placed in the authorized _keys file. On the INT server To create an installation user on the Integration server: Connect to the Integration server as a user with the sudo privileges: ssh i @ Create a Linux installation user: sudo useradd m Here, `` is the login name of the Linux installation user. The user's name must be written in lowercase letters. Provide the no password sudo privileges for the Linux installation user. For that: Make sure that the line includedir etc sudoers.d exists in the sudoers file. sudo cat etc sudoers grep includedir includedir etc sudoers.d Run the command: sudo visudo f etc sudoers.d In the end of the sudoers file, add the following line: ALL=(ALL) NOPASSWD: usr bin sh, usr bin su, usr bin mkdir, usr bin chmod, usr bin chown Save and close the file. Switch to the Linux installation user: sudo su Generate the SSH pair key for the installation: ssh keygen t rsa C 'inventory@workfusion.com' f ~ .ssh ansiblesshkey N '' In case of using a private SSH key that is protected by a passphrase, enter the passphrase, when prompted during the installation. Open the generated public key and save it to clipboard: cat ~ .ssh ansiblesshkey.pub After the above operation, proceed to other Linux and Windows servers to create the same user there. For the instructions, see the further sections. On other Linux servers To create an installation user on other Linux servers: Connect to a Linux server as a user with the sudo privileges: ssh i @ Create a Linux installation user: sudo useradd m The user's name must be the same as on the Integration server and written in lowercase letters. Provide the no password sudo privileges for the Linux installation user. For that: Make sure that the line includedir etc sudoers.d exists in the sudoers file. sudo cat etc sudoers grep includedir includedir etc sudoers.d Run the command: sudo visudo f etc sudoers.d In the end of the sudoers file, add the following line: ALL=(ALL) NOPASSWD: usr bin sh, usr bin su, usr bin mkdir, usr bin chmod, usr bin chown Save and close the file. Create the folder for the SSH keys and change its permissions: sudo mkdir home .ssh sudo chmod 700 home .ssh In the new directory, create the SSH key file authorized _keys and paste the copied public key from the Integration server to it: sudo vi home .ssh authorized_keys Change permissions and the owner of the directory: sudo chown R : home .ssh sudo chmod 600 home .ssh authorized_keys Make sure that SSH key based authentication is enabled on all Linux servers. In this case, file etc ssh sshd _config on these servers must contain the following line: PubkeyAuthentication yes If you use SSH password based authentication instead, file etc ssh sshd _config on all Linux servers must contain the following line: PasswordAuthentication yes To check that you have done everything right, make the SSH connection from the Integration server to the current server as a Linux installation user: @ ~ ssh i ~ .ssh ansiblesshkey @ On Windows servers For successful installation, the user must meet the following requirements: User's credentials must be the same on all Windows servers. The administrator permissions must be granted to the installation user. To create an installation user on the Windows server: Download Remote Desktop or any other software for connecting to Windows servers. Open the software and specify: server name user's name user's password Connect to the remote Windows server as a user with the administrator's privileges. Run PowerShell as Administrator. In PowerShell, run the following commands: net user add net localgroup administrators add Here, is the name of the Windows installation user, and `` is the password for the Windows installation user. For future maintenance make sure that the Windows user is active, has all required administrator permissions, and the password is not expired. Create RPA users Before installation, the Bot Master and Bot Unit users must be created on each server that you plan to deploy and specified later in hosts.yml, in the rpa_hostnames list (see the Configure installation step). The users must meet the following requirements: A Bot Master user must have the same username and password on all RPA servers. A Bot Master user must be a member of the Administrators and Remote Desktop Users groups on all RPA servers. All Bot Unit users must have the same username and password on all RPA servers. A Bot Unit user must be a member of the Remote Desktop Users group on all RPA servers. The number of created Bot Unit users must be specified in config.yml in the rpabotsper_server parameter, on the Configure installation step. All Bot Unit users must have common basename as well as different and sequential index numbers in the end of the basename, if you plan to use several Bot Units. For more information, see config.yml options, section RPA. For example, if, in config.yml*, you set rpa bots per server: 2, and bot unit base name: BotUnit, then the following users must be present on each RPA server: BotUnit1, BotUnit2,*. To create an RPA user on the Windows server: Connect to the remote Windows server via Remote Desktop, or any other software, as a user with the Administrator's privileges. Run PowerShell as Administrator. In PowerShell, run the following commands to create a Bot Master user and add it to groups: net user BotMaster add net localgroup \"Administrators\" BotMaster add net localgroup \"Remote Desktop Users\" BotMaster add Here, `` is the password for the Bot Master user. In PowerShell, run the following commands to create the Bot Unit users and add them to the group: net user BotUnit1 add net localgroup \"Remote Desktop Users\" BotUnit1 add Here, `` is the password for the Windows Bot Unit user. "},{"version":"10.0","date":"Oct-15-2019","title":"overview","name":"Overview","fullPath":"iac/admin/install-spa/overview","content":" install install The entire WorkFusion SPA product includes a number of components. The components are logically grouped into 8 roles that are distributed across 8 servers as the default recommended architecture scheme. Each server role, like APP, INT, etc., represents a predefined set of software – WorkFusion produced binaries and integrated third party programs. Each server role is assumed to be installed on a separate server for best performance. WorkFusion SPA product consists of the following servers (listed in the order of installation): MS SQL server – Windows. If you already have an existing MS SQL, provide access to it before WorkFusion SPA installation Integration server (INT) – Linux BEP Master – Linux BEP Agent(s) – Linux APP server – Linux OCR Server – Linux or Windows. Depends on the Operating System that is chosen for the OCR component before WorkFusion SPA installation RPA Server(s) – Windows BI Server – Windows For more information on the components and how they are distributed across the servers, see Product Architecture. You can install SPA in two different ways: the simple one that is we have introduced in SPA version 10.0 and, which we indeed recommend to follow, and the former complex one. The difference between them is clearly seen on the diagrams. Single point Legacy way of installation Legacy "},{"version":"10.0","date":"Oct-14-2019","title":"install-spa-сomponents","name":"Install SPA","fullPath":"iac/admin/install-spa/install-spa-сomponents","content":" id: version 10.0 install spa original_id: install spa Overview The SPA components must be installed in the following order: MS SQL server Integration server BEP Master server BEP Agent server APP server RPA server OCR server (either Linux or Windows) BI server All installation steps must only be run from the directory on your Integration server. :::note In case of using password sudo privileges, add the ask become pass option to the installation script, when installing components on Linux servers. For example, . install.sh preinstall int ask become pass In case of using the password based SSH authentication, add the ask pass option to the installation script, when installing all Linux components, except the INT server. For example, . install.sh preinstall bep master ask pass ::: Prepare MS SQL Server Сheck that MS SQL meets the requirements: . install.sh precheck mssql If all checks have passed successfully, run the following command to grant necessary permissions for database schemes. . install.sh configure mssql Install Run all following steps as . Before the installation, prepare the environment: Login to your Integration server, and then go to the WorkFusion installer directory: ssh i @ cd Export the following variables: export ANSIBLEPRIVATEKEY_FILE=\"\" export ANSIBLEREMOTEUSER=\"\" export ANSIBLEVAULTPASS=\"\" Use the key that you generated, when creating an installation user. During the installation of the components, you will be asked to type your vault password several times. All components The components of SPA version 10 can be installed all at once with a single command. Note that before the setup, the corresponding servers must be defined in hosts.yml, otherwise the installation of a non specified server will be skipped. To install all components: Prepare the setup of all componenets: . install.sh preinstall full Install the components: . install.sh install full Check that the installation is performed correctly: . install.sh check full If needed, you can skip the installation of any server. For that, add a corresponding argument with a server role to the script: skip_int skipbepmaster skipbepagent skip_app skip_rpa skip_ocr skip_ocrwin skip_bi For example, . install.sh preinstall full e skiprpa=true e skipocr=true e skip_bi=true . install.sh install full e skiprpa=true e skipocr=true e skip_bi=true . install.sh check full e skiprpa=true e skipocr=true e skip_bi=true If you want to have more control over the installation of separate components, proceed to the following steps. Integration server To install the Integration server's components: Prepare the Integration server for installation: . install.sh preinstall int Install the Integration server's components: . install.sh install int Check that the installation is performed correctly: . install.sh check int BEP Master server To install the BEP Master server: Prepare the Master server for installation: . install.sh preinstall bep master Install the Master server: . install.sh install bep master Check that the installation is performed correctly: . install.sh check bep master BEP Agent server To install the BEP Agent server: Prepare the Agent server for installation: . install.sh preinstall bep agent Install the Agent server: . install.sh install bep agent Check that the installation is performed correctly: . install.sh check bep agent APP server To install the APP server's components: Prepare the APP server for installation: . install.sh preinstall app Install the APP server's components: . install.sh install app Check that the installation is performed correctly: . install.sh check app RPA server To install the RPA server: Download the RPA installer package: curl 'https: sourcewithRPAInstaller {version}.zip' output sources RPAInstaller {version}.zip Note: If internet access is not available on the target server, download RPA Installer using the download link and copy it to the INT server as sources RPAInstaller {version}.zip. Prepare the RPA server for installation: . install.sh preinstall rpa Install the RPA server: . install.sh install rpa OCR server Follow the instruction for the corresponding Operating System. Linux To install the OCR server: Prepare the OCR server for installation: . install.sh preinstall ocr Install the OCR server: . install.sh install ocr Check that the installation is performed correctly: . install.sh check ocr Windows Download the OCR Installer package: curl 'https: sourcewithOCRInstaller {version}.zip' output sources OCRInstaller {version}.zip Note: If the Internet access is not available on the target server, download the OCRWin installer using the https: source with OCRInstaller {version}.zip link and copy it to the Integration server as sources OCRInstaller {version}.zip The destination file must mandatorily be named as OCRInstaller {version}.zip (for example, OCRInstaller 10.0.0.12 1.zip). In config.yml, ensure that the ocr _platform parameter is set to windows: . install.sh edit_config int Prepare the OCR server for installation: . install.sh preinstall ocrwin Install the OCR server: . install.sh install ocrwin Check that the installation is performed correctly: . install.sh check ocrwin BI server To install the BI server: Download BI installer: curl 'https: linktobi_installer' output sources WorkFusionAnalyticsInstaller .zip Note: If the Internet access is not available on the target server, download the Analytics Server installer using the link and copy it to the Integration server as sources WorkFusionAnalyticsInstaller .zip. Download BI Workbooks: curl 'https: linktobiworkbooks' output sources WorkFusionAnalyticsWorkbooks .zip 'https: linktobiworkbooks' Note: If the Internet access is not available on the target server, download the Analytics Workbooks installer using the link and copy it to the Integration server as sources WorkFusionAnalyticsWorkbooks .zip. Prepare the BI server for installation: . install.sh preinstall bi Install the BI server: . install.sh install bi Install the analytics desktop. Optional. . install.sh install analytics desktop :::note In some cases the BI server installation may fail on task \"Ensure Analitics Server installed\" with a following error message: \"Trial activation for Tableau server was not applied correctly. Refer to documentation for further steps.\" \"There is not enough license to promote user 3 to system admin. (errorCode=10009)\" This issue relates to the Tableau software and appears when Tableau trial activation finishes with errors. In this case, follow the instruction: Complete BI server installation if it was failed on task \"Ensure Analytics Server Installed\". ::: Check the installation Once the installation is completed, check WorkFusion Platform with Business Process (BP) and Manual Task. The OCR must be activated. To check the installation, on the Integration server, go to the directory, and then run the following command: . install.sh check app e testml=true e testrpa=true e test_ocr=true Optionally, you can test IE, Chrome, Firefox and desktop on RPA, by passing additional parameters test ie, test chrome, test firefox, or test desktop. Full check example: . install.sh check app e testml=true e testrpa=true e testocr=true e testie=true e testchrome=true e testfirefox=true e test_desktop=true All the checks must produce the OK status. "},{"version":"10.0","date":"Oct-14-2019","title":"prepare-servers","name":"Prepare servers","fullPath":"iac/admin/install-spa/prepare-servers","content":" Prepare MS SQL server For Workfusion SPA installation you need to have an existing activated instance of the MS SQL server. The server must be up and running during the Workfusion SPA installation. To prepare a MS SQL server: Create the database, and specify the database name. For other options you may use the default values. Create DB Create the owner with the full access permissions for the database. This user must be later specified in config.yml. Note: for all logins, the following password policies are applied: Allowed special symbols for db password for monitoring tools: @ *():,.} Allowed special symbols for db password for the Analytics component: @ *():,.;} To create the owner: On a MS SQL server, create the mssql login, specify the login name, and then make sure that the Enforce password policy check box is not selected. In the Default database box, specify the database, and then choose English as Default language. In the Select a page group, click User mapping, and then select the check box to map the user to the database. Leave all other options unchanged. Create DB User In the database Users, select the db owner check box to assign to the db owner role. Assign to group Create the following MS SQL login names and passwords for the Workfusion components: mssql dba user mssql ct user mssql ws user mssql sqc user mssql ds user mssql rpa user mssql pm user mssql dm user mssql rapi user mssql ocr user See the password policy earlier. For the description of these parameters, see config.yml options. For each login, specify the following parameters: If you use the SQL Server authentication, Specify the login name. Make sure that the Enforce password policy check box is not selected. In the Default database box, specify the database. Choose English as Default language. SQL auth If you use Windows authentication, Select a corresponding login name. In the Default database box, specify the database. Choose English as Default language. For other options leave the default values. Win auth When configuring the Integration server, on the INT server, in the config.yml file, in the MSSQL section, remember to specify the corresponding login names and passwords, that you have created on the previous step. Prepare Integration server Extract installation packages To extract installation packages: Log in to the Integration server as a Linux installation user, by using Linux Terminal, Windows PuTTY, or any other SSH client: ssh i @ In , create a directory for the WorkFusion installation package, for example, opt workfusion wf _installer. Grant the read access for this directory to all users: sudo mkdir p opt workfusion wf_installer sudo chmod 0755 opt workfusion wf_installer Make sure that the Linux installation user is an owner of the installation directory: sudo chown R : opt workfusion Download the provided installation package with any file retrieving tool, for example, wget or curl: If the Internet access is available on the server: curl 'https: linktoinstaller' output opt workfusion workfusion full package .tar.gz If the Internet access is not available on the Integration server, download the SPA installer and copy it as opt workfusion workfusion full package .tar.gz on the Integration server. Extract the downloaded package to the directory: tar xzvf opt workfusion workfusion full package .tar.gz strip 1 C opt workfusion wf_installer Configure installation To configure installation: In your directory, edit the hosts.yml configuration file. The vi text editor is used in the example below: vi hosts.yml Refer to the hosts.yml options section for more details on each option. In the same directory, change the permissions for the config.yml file: sudo chmod 644 config.yml Open the config.yml file, and in the parameter ansiblevaultpassword, change the ansible vault password: vi config.yml The password is used for encrypting the config.yml at the following step and will be asked several times during the installation of the components. We strongly recommend to secure and memorize it. When creating passwords, make sure they are strong enough and meet the following requirements: Ansible pass Optionally, on the INT server, run the following command to automatically generate passwords for the WorkFusion internal services according to product policies. We strongly recommend to change them manually for security reasons. Note that the command doesn't generate end user password. . install.sh passwords generate As a result, in config.yml, the passwords for most services, except for the following end user ones, are created: \"wf_password\" \"ldapbindpassword\" \"mail_pass\" \"tableaudashboardpass\" \"tableauautomationpass\" \"windowsinstallationpass\" \"elkadminpass\" \"ansiblevaultpassword” \"botrelayuser_pass\" \"botuserpass\" If needed, change other secrets and parameters, as well. After that, save and close the file. Remember, in the MSSQL section, specify the corresponding MS SQL login names and passwords, that you have created earlier. In your , encrypt the config.yml file: . install.sh encrypt config Note that WorkFusion SPA 10 installer will not let you start the installation until config.yml is encrypted. If needed, you can later edit the encrypted file with the following commands: cd opt workfusion wf_installer . install.sh edit_config int Refer to the config.yml options section for more details on each configuration option. Prepare the TLS certificates, as described in Prepare TLS certificates for installation. Copy the provided license.properties file to the installation directory to install the WorkFusion license, for example, opt workfusion wf _installer license.properties. If you perform installation on your own, request the license file from your WorkFusion Account Manager in the same way, as you receive the Tableau license. Prepare Windows servers The following operations must be performed on all Windows servers. To configure access for SPA installation: Connect to the remote Windows server via Remote Desktop or any other client as a Windows installation user (see Create installation users). Run PowerShell as Administrator. In PowerShell, run the following commands: url = \"https: s3.amazonaws.com workfusion installer blobs artifacts winrm config ConfigureRemotingForAnsible.ps1\" file = \" env:c ConfigureRemotingForAnsible.ps1\" (New Object TypeName System.Net.WebClient).DownloadFile( url, file) powershell.exe ExecutionPolicy ByPass File file Verbose The script checks the current WinRM (PS Remoting), enables CredSSP authentication, creates SSL listener and configures firewall for WinRM HTTPS connections to allow Ansible to connect, authenticate, and execute PowerShell commands. If the Internet access is not available on the target server: Download the script from https: s3.amazonaws.com workfusion installer blobs artifacts winrm config ConfigureRemotingForAnsible.ps1 and save it to the target windows server manually, for example, to C: tmp ConfigureRemotingForAnsible.ps1. Run PowerShell as Administrator. In PowerShell, run the following command: powershell.exe ExecutionPolicy ByPass C: tmp ConfigureRemotingForAnsible.ps1 "},{"version":"10.0","date":"Jul-08-2019","title":"post-installation-steps","name":"Post-installation tasks","fullPath":"iac/admin/install-spa/post-installation-steps","content":" id: version 10.0 post install original_id: post install After the setup of the components, do not delete the installation directory for future on demand password changes and common upgrade procedures. Activate the OCR license Activate the license on the OCR Server as described in the OCR License topic. Activate the WF ELK license Activate the WF ELK license as described in the WorkFusion ELK License topic. Activate the Tableau license After WorkFusion Analytics Server or WorkFusion Analytics Desktop are installed, you must activate the Tableau component. To complete the activation process, receive the WorkFusion Analytics Server and Desktop product keys from the WorkFusion team. There are two ways to activate or deactivate product keys depending whether the computer has internet access or not. Follow the official Tableau guide that best suits your scenario. Activate: Tableau Desktop Tableau Server For more information on licenses, see Manage the WorkFusion Analytics license. Import SSL certificates The SSL certificates are required for correct rendering of web pages over HTTPS. If you have installed Workfusion SPA version 10.0 using the self signed certificates, remeber to add the ca.crt to your browser. To acquire the certificate: In your browser, type the address of installed Control Tower or any other server, for example, BEP or INT, and add \" ca.crt\" to it: http: ct1.workfusion.com ca.crt. Press Enter. The certificate is downloaded to your computer. Setup backups Setup the cold backup procedure, which should be aligned with your company backup policy, by following Cold Backup and Restore guide. "},{"version":"10.0","date":"Jun-21-2019","title":"prepare-tls-certificates","name":"Prepare TLS certificates","fullPath":"iac/admin/install-spa/prepare-tls-certificates","content":" Before installation of Workfusion SPA, the TLS certificates must be placed to the INT server. See the detailed description of certificates... The following table describes all certificates that are required to install Workfusion SPA: Nginx Certificates File names Description ca.crt The Root CA certificate in the pem format. All following certificates in this table must be signed by this Root CA. It is imported into Java Truststore on all servers. Certificate file must include only Root cert, without any intermediate certs. app.crtapp.key The certificate and its private key files in the pem format for the APP server. Used by nginx on the APP server.Certificate's CommonName or SANs must match app _hostname.The certificate file must include end certificate plus intermediate certs, if exist.The private key must not be password protected. s3.crts3.key The certificate and its private key files in the pem format for S3 proxy located on the APP server. Used by nginx on the APP server.Certificate's CommonName or SANs must match s3 _hostname.The certificate file must include the end certificate plusintermediate certs, if exist.The private key must not be password protected. int.crtint.key The certificate and its private key files in the pem format for the INT server. Used by nginx on the INT server. Certificate's CommonName or SANs must matchint _hostname.The certificate file must include the end certificate plus intermediate certs, if exist.The private key must not be password protected. ocr.crtocr.key The certificate and its private key files in the pem format for the OCR server. Used by nginx on OCR server. Certificate's CommonName or SANs must match ocr _hostname. The certificate file must include the end certificate plus intermediate certs, if exist. The private key must not be password protected. bi.crtbi.key The certificate and its private key files in the pem format for the BI server. Used by Tableau on BI server.Certificate's CommonName or SANs must match bi _hostname.The certificate file must include the end certificate plus intermediate certs, if exist. The private key must not be password protected. rpa.crtrpa.key The certificate and its private key files in the pem format for the RPA server. Used by nginx on RPA server(s).Certificate's CommonName or SANs must match all servers in the list of rpa _hostnames. The certificate file must include end certificate plus intermediate certs, if exist. The private key must not be password protected. ocrwin.crtocrwin.key Optional, required if OCR platform is Windows.The certificate and its private key files in the pem format for the OCR Windows server. Used by nginx on the OCR Windows server.Certificate's CommonName or SANs must match all servers in the list of ocrwin _hostname.The certificate file must include end certificate plus intermediate certs, if exist. The private key must not be password protected. bep master.crtbep master.key The certificate and its private key files in the pem format for the AutoML Master server. Used by nginx and mesos on the AutoML Master server.Certificate's CommonName or SANs must match vds master hostname.The certificate file must include the end certificate plus intermediate certs, if exist. The private key must not be password protected. bep agent.crtbep agent.key The certificate and its private key files in the pem format for the AutoML Agent server(s). Used by mesos on the AutoML Agent server(s).Certificate's CommonName or SANs must match ALL existing and planned hostnames of AutoML Agent(s).The certificate file must include the end certificate plus intermediate certs, if exist.The private key must not be password protected. Prerequisites Hostnames of all your servers are in a single domain. For example, in hosts.yml, you have the following hostnames: int_hostname: int.example.com app_hostname: app.example.com ocr_hostname: ocr.example.com bi_hostname: bi.example.com bepmasterhostname: automlmaster.example.com bepagenthostname: automlagent.example.com s3_hostname: app s3.example.com You have received the following certificates from your SSL provider: ca.crt contains ROOT CA certificate in pem format (the file may have .crt, .cer, .pem, or .cert extension) intermediate.crt contains intermediate CA certificate in pem format signed by ROOT CA certificate (the file may have .crt, .cer, .pem, or .cert extension) domain.crt contains wildcard server certificate in pem format signed by intermediate CA certificate issued for wildcard CommonName *.example.com (the file may have .crt, .cer, .pem, or .cert extension) domain.key contains private key file for domain.crt certificate in pem format (the file may have .key, or .pem extension). This file should have been generated initially with CSR. Install certificates Append intermediate.crt to domain.crt certificate: cat intermediate.crt >> domain.crt Order does matter. File domain.crt must be the first. If you have several intermediate CA certs, you need to bundle all of them. Each subsequent intermediate cert that is added to the file must be signed by the previous one. If you don't have any intermediate CA certs, you don't need the earlier step. Place the following trusted certificates (if you have them) to the certificates directory on the INT server: ca.crt app.crt app.key s3.crt s3.key int.crt int.key ocr.crt ocr.key bi.crt bi.key rpa.crt rpa.key bep_master.crt bep_master.key bep_agent.crt bep_agent.key The certificates must mandatory have the names specified earlier. If in certificates, you have: All required certificates – make sure they are named correctly, and then go to step 3. Only some certificates, including ca.crt. The missing certificates can be generated on the basis of the existing one. To generate other required certificates, go to step 3. No certificates at all – you can generate all the required certificates automatically. For that: In hosts.yml, specify hostnames for all servers. Note: a сertificate won't be created, if a respective hostname is not set in hosts.yml. Go to step 3. Generate the certificates by running the following command: . install.sh certs generate The script generates all certificates that are required for product operation and places them to the certificates directory. rw rw r . 1 ec2 user ec2 user 1424 Jun 3 12:15 elk ca.crt rw rw r . 1 ec2 user ec2 user 1766 Jun 3 12:15 elk ca.key rw rw r . 1 ec2 user ec2 user 1757 Jun 3 12:15 kibana.crt rw rw r . 1 ec2 user ec2 user 3272 Jun 3 12:15 kibana.key rw rw r . 1 ec2 user ec2 user 1761 Jun 3 12:15 logstash.crt rw rw r . 1 ec2 user ec2 user 3272 Jun 3 12:15 logstash.key rw rw r . 1 ec2 user ec2 user 4013 Jun 3 12:15 logstash.p12 rw rw r . 1 ec2 user ec2 user 1428 Jun 3 12:49 mtls ca.crt rw rw r . 1 ec2 user ec2 user 1766 Jun 3 12:49 mtls ca.key rw rw r . 1 ec2 user ec2 user 1769 Jun 3 12:49 mtls client.crt rw rw r . 1 ec2 user ec2 user 3272 Jun 3 12:49 mtls client.key rw rw r . 1 ec2 user ec2 user 1298 Jun 3 12:15 vault_admin.crt rw rw r . 1 ec2 user ec2 user 2517 Jun 3 12:15 vault_admin.p12 rw rw r . 1 ec2 user ec2 user 1298 Jun 3 12:15 vault_workfusion.crt rw rw r . 1 ec2 user ec2 user 2517 Jun 3 12:15 vault_workfusion.p12 These certificates are used by internal services. :::note The script will not override the existing certificates, if a file with the same name exists in the certificates directory. ::: "},{"version":"10.0","date":"Jul-08-2019","title":"prerequisites","name":"Prerequisites","fullPath":"iac/admin/install-spa/prerequisites","content":" id: version 10.0 overview original_id: overview Servers must be up and running and all system requirements are met as described in System Requirements. An Installation user exists on each server and is provided with the SSH access via the private key authentication and no password sudo privileges. If a user doesn't exist, see Create installation users below. DNS names for all servers are configured according to DNS Names Configuration. Application Ports are open between servers. The default Workfusion installation directory (for example, opt workfusion) is created on the Integration server. The default Workfusion installation directory (for example, C: workfusion is created on the Windows servers. TLS certificates are placed to the directory. The time synchronization service must be set up on all servers. All servers must have the same time zone. "},{"version":"10.0","date":"Oct-14-2019","title":"uninstall-win","name":"Uninstall WF components from Windows servers","fullPath":"iac/admin/install-spa/uninstall-win","content":" Uninstall RPA To uninstall the RPA server: Log in to the INT server via SSH. Run the command: . install.sh uninstall rpa Enter the vault password, when the prompt appears. Reboot the RPA server, for example, via RDP. To reinstall the RPA component, see the Install SPA guide. Uninstall BI To uninstall the BI server: Log in to the INT server via SSH. Run the command: . install.sh uninstall bi Enter the vault password, when the prompt appears. Reboot the BI server, for example via RDP. To reinstall the BI component, see the Install SPA guide. Uninstall OCRWIN (Windows) To uninstall the OCRWIN server: Log in to the INT server via SSH. Run the command: . install.sh uninstall ocrwin Enter the vault password, when the prompt appears. Reboot the OCR server, for example via RDP. To reinstall the OCR component, see the Install SPA guide. "},{"version":"10.0","date":"Jun-21-2019","title":"application-ports","name":"Applications' ports","fullPath":"iac/admin/system-requirements/application-ports","content":" Before installation, in the config.yml file, specify ports for required application components. The following table lists the default ports for each application component. Access types: external for end users internal for internal application communication We highly recommend restricting the internal ports using the IPtables or Firewalld Linux firewall or Windows Firewall to protect data according to the company's standard security policy, so that outside of workfusion application should not access internal API thought ports. Applying this change will restrict the malicious user to get into the application to perform any activity. table tr:nth child(2n) { background color: white; } Destination Server Name Destination Port Source Server Name Service name Access Type APP (WF Control Tower) 80, 443 tcp all nginx (proxy for Control Tower, Workspace, bot manager, AutoQC, Nexus, Minio, Tinymce, Kibana) external 5702 tcp CT Worker hazelcast internal 7083 tcp BEP Agents, RPA nginx (mTLS proxy for Control Tower) internal 22 tcp INT ssh internal Integration Server 80, 443 tcp BEP Master, APP, OCR nginx (proxy for Nexus, Minio) internal 8200 tcp APP, BEP Master, BEP Agents, OCR, OCRWIN, RPA Vault (secure store) internal 45672 tcp Integration server (TDS) nginx (proxy for rabbitmq api) internal 5672 tcp APP, BEP Master,&nbsp;BEP Agents, OCR, RPA rabbitmq queue internal 2181 tcp APP, BEP&nbsp;Master,&nbsp;BEP Agents zookeeper internal 4567 4571 tcp, 15072 udp APP,&nbsp;BEP Master, BEP Agents, OCR, OCRWIN, RPA, OCR logstash internal 9200 tcp APP, INT ElasticSearch internal 9092 tcp BEP Master nginx (mTLS proxy for TDS) internal OCR 80, 443 tcp BEP agent, APP nginx (proxy for OCR rest API) internal 22 tcp INT ssh internal OCRWIN 80, 443 tcp BEP agent, APP nginx (proxy for OCR rest API) internal 5986 tcp INT WinRM internal BEP Master 80, 443 tcp APP nginx (proxy for AutoML services APIs) internal 8480 tcp BEP&nbsp;Agent nginx (proxy for marathon) internal 5050 tcp BEP&nbsp;Agent mesos master internal 9000 61000 tcp udp BEP&nbsp;Agent, INT mesos master,&nbsp;filebeat, heartbeat, metricbeat internal 111 tcp udp BEP&nbsp;Agent NFS internal 1039 tcp udp BEP&nbsp;Agent NFS internal 1047 tcp udp BEP&nbsp;Agent NFS internal 1048 tcp udp BEP&nbsp;Agent NFS internal 2049 tcp udp BEP&nbsp;Agent NFS internal 20048 tcp udp BEP&nbsp;Agent NFS internal 22 tcp INT ssh internal 9093 tcp APP nginx (mTLS proxy for worker management service ) internal 9073 tcp APP, RPA, BEP Agent nginx (mTLS for AutoML gateway service) internal BEP Agent(s)&nbsp; 5051 tcp BEP&nbsp;Master mesos slave internal 9000 61000 tcp udp BEP&nbsp;Master, INT mesos slave, filebeat, heartbeat, metricbeat internal 22 tcp INT ssh internal RPA 80, 443 tcp APP nginx (proxy for RPA hub) external 5986 tcp INT WinRM internal 3389 tcp APP RDP internal BI 80, 443 tcp all apache (proxy for Tableau Web Interface) external 5986 tcp INT WinRM internal MSSQL 1433 INT, APP, OCR, OCRWIN, BEP Agent(s), BI MS SQL internal Workfusion URLS Server URLS Description APP (WF Control Tower) https: app_hostname workfusion Control Tower (external) https: app_hostname workfusion resources CT static resources (external) https: app_hostname workfusion public HTMLRenderer CT HTML renderer (external) https: app_hostname workspace Workspace (external) https: app_hostname nexus Nexus (external) https: app_hostname sqc rest AutoQC (external) https: app_hostname tinymce Tinymce (external) https: app_hostname kibana Kibana (external) https: s3_hostname minio Minio (S3 emulator) (external) Integration server https: int_hostname:45672 RabbitMQ API https: int_hostname minio Minio https: int_hostname nexus Nexus https: int_hostname:9200 Elasticsearch BEP Master https: bepmasterhostname automl model service AutoML Model Service https: bepmasterhostname:8480 Marathon https: bepmasterhostname:5050 Mesos OCR Server https: ocr_hostname OCR Rest API BI Server https: bi_hostname Tableau web interface (external) "},{"version":"10.0","date":"Jun-26-2019","title":"dns-names-configuration","name":"DNS names configuration","fullPath":"iac/admin/system-requirements/dns-names-configuration","content":" :::warning The following DNS names are for example purpose only. Note that the maximum length of a full domain name must not exceed 253 characters, including dots. For example, www.example.com consists of 15 characters. Maximum length of a \"label\" – a part of domain name that is separated by a dot – must not exceed 63 characters. Thus, labels for www.example.com are \"com\", \"example\", and \"www\". ::: In the following table you will find the examples of domain names to be assigned to the servers. Example Domain Name Record Type Description int.example.com A or CNAME Integration Server mssql.example.com A or CNAME MS SQL Server app.example.com A or CNAME APP Server s3.example.com additional A or CNAME record for APP Server The domain name is required to access S3 emulator's URLs, like s3.example.com bucket ocr.example.com A or CNAME OCR Server bep master.example.com A or CNAME BEP Master Server bep agent.example.com A or CNAME Multiple BEP agent servers may be present. DNS records must be created for all of them. rpa.example.com A or CNAME RPA server bi.example.com A or CNAME BI server (Analytics) Alternative way If the DNS service is not available, on each server and on PCs of end users' that will use the product, do the following: Open the hosts file: On Linux machines: file etc hosts. On Windows machines: file C: Windows system32 drivers etc hosts. In the hosts file, add the following records: 192.168.1.1 int.example.com 192.168.1.2 mssql.example.com 192.168.1.3 app.example.com 192.168.1.3 s3.example.com 192.168.1.4 ocr.example.com 192.168.1.5 bep master.example.com 192.168.1.6 bep agent.example.com 192.168.1.7 rpa.example.com 192.168.1.8 bi.example.com :::note The above IP addresses are for the example purpose only. Remember to use your real IP addresses. ::: "},{"version":"10.0","date":"Jun-21-2019","title":"env-readiness-checklist","name":"Environment readiness checklist","fullPath":"iac/admin/system-requirements/env-readiness-checklist","content":" Hardware See the checklist in the Excel format Category Value Reporter: WorkFusion version: 10 Date updated: Deployment type Dev Prod Purpose Name (DNS entries) SSL certificate Static IP Additional information Integration server yes no int.example.com MS SQL (Database) yes no mssql.example.com Application server yes no app.example.com s3 hostname yes no s3.example.com OCR server yes no ocr.example.com BEP Master yes no bep master.example.com BEP Agent 1 yes no bep agent1.example.com BEP Agent 2 yes no bep agent2.example.com BEP Agent 3 yes no bep agent3.example.com RPA Server yes no rpa.example.com BI Server yes no bi.example.com CATEGORY Default value (with Description Details) Actual Value Additional information MS SQL (Database) POC Development Production CPU 4 cores 4 cores 8 cores nproc RAM 16 GB 16 GB 32 GB free m Disk space 500 GB 500 GB 500 GB df Th OS Windows Windows Windows cat etc redhat release Application server POC Development Production CPU 4 cores 4 cores 8 cores nproc RAM 16 GB 16 GB 32 GB free m Disk space 250 GB 250 GB 250 GB df Th OS RHEL CentOS 7.6 RHEL7.6 CentOS7.6 RHEL7.6 CentOS7.6 cat etc redhat release Integration server POC Development Production CPU 4 cores 4 cores 8 cores nproc RAM 16 GB 16 GB 32 GB free m Disk space 250 GB 250 GB 250 GB df Th OS RHEL CentOS 7.6 RHEL7.6 CentOS7.6 RHEL7.6 CentOS7.6 cat etc redhat release OCR engine POC Development Production CPU 2 cores 4 cores 8 cores nproc RAM 4 GB 8 GB 16 GB free m Disk space 100 GB 100 GB 250 GB df Th OS RHEL CentOS 7.6 Windows Server 2016 RHEL7.6 CentOS7.6 RHEL7.6 CentOS7.6 cat etc redhat release BEP Master (Bot Execution Platform) POC Master Development Production CPU 8 cores 8 cores 8 cores nproc RAM 32 GB 32 GB 32 GB free m Disk space 2 TB 2 TB 250 GB df Th OS RHEL CentOS 7.6 RHEL7.6 CentOS7.6 RHEL7.6 CentOS7.6 cat etc redhat release BEP Agent 1 POC Development Production CPU 8 cores 8 cores 8 cores nproc RAM 32 GB 32 GB 32 GB free m Disk space 250 GB 250 GB 250 GB df Th OS RHEL CentOS 7.6 RHEL7.6 CentOS7.6 RHEL7.6 CentOS7.6 cat etc redhat release BEP Agent 2 POC Development Production CPU ÔøΩ 8 cores 8 cores nproc RAM ÔøΩ 32 GB 32 GB free m Disk space ÔøΩ 250 GB 250 GB df Th OS ÔøΩ RHEL7.6 CentOS7.6 RHEL7.6 CentOS7.6 cat etc redhat release BEP Agent 3 POC Development Production CPU ÔøΩ 8 cores 8 cores nproc RAM ÔøΩ 32 GB 32 GB free m Disk space ÔøΩ 250 GB 250 GB df Th OS ÔøΩ RHEL7.6 CentOS7.6 RHEL7.6 CentOS7.6 cat etc redhat release RPA Server POC Development Production CPU 8 cores 8 cores 8 cores RAM 16 GB 16 GB 16 GB Disk space 250 GB 250 GB 250 GB OS Windows server 2012 Microsoft Windows 10 64 bit Windows server 2012 Microsoft Windows 10 64 bit Windows server 2012 Microsoft Windows 10 64 bit Business Intelligence POC Development Production CPU 4 cores 4 cores 8 cores RAM 16 GB 16 GB 32 GB Disk space 100 GB 100 GB 250 GB OS Windows server 2012 Microsoft Windows 10 64 bit Windows server 2012 Microsoft Windows 10 64 bit Windows server 2012 Microsoft Windows 10 64 bit MS SQL An instance of the MS SQL server is activated. The workfusion database is created. The mssql db owner with full access permissions for the workfusion database is created. The following MS SQL login names and passwords for the Workfusion components are created: mssql dba user mssql ct user mssql ws user mssql sqc user mssql ds user mssql rpa user mssql pm user mssql dm user mssql rapi user mssql ocr user There are no special requirements for the MS SQL Server Instance name. "},{"version":"10.0","date":"Oct-14-2019","title":"infrastructure-capacity-planning","name":"Infrastructure capacity planning","fullPath":"iac/admin/system-requirements/infrastructure-capacity-planning","content":" A common concern of an enterprise is whether it has enough allocated resources to handle an increase in users or a number of transactions. The capacity is defined as the maximum load that a service, system, or a device can handle. Infrastructure capacity planning is the process of determining the hardware and software resources required to meet processing demand as for the current moment and in future. Capacity Planning is a part of a wider Capacity Management domain. Capacity Management is concerned with ensuring that cost effective capacity, which meets or exceeds the needs of the business as established in Service Level Agreements, exists at any given time. Standard build The System Requirements page summarizes the recommended standard specification for the environment to run WorkFusion SPA. A number of factors can impact performance and scalability of the software. Some of the important system variables include business processes design, server configuration, infrastructure tuning, data environment, compute capacity, and networking. The following numbers were established in a closed network lab on automated load tests simulating realistic production workload. The recommended production configuration is set up to provide the following capacity: 7 simultaneous OCR jobs or a total of 10 000 000 OCR pages a year 1 14 simultaneous ML prediction jobs 4 simultaneous Search Engine ML model training 5 bots 50 simultaneous Control Tower jobs 50 simultaneous (unlimited of named or registered) WorkSpace workers 8 simultaneous (unlimited of named or registered) Control Tower users 50 simultaneous Analytics users 2 For typical automation cases such set up is capable of handling of 4 000 000 transactions a year. 1 – 100 000 pages is typical bundled volume. Additional pages require an extra license to be purchased. 2 – 5 named Analytics users is typical bundled volume. Additional users require an extra license to be purchased. RPA Bot number estimation When planning the RPA capacity, first of all you need to estimate the number of bots required to handle the work. For a possible estimation approach, refer to Number of bots estimation. Horizontal scaling The RPA scaling is performed by increasing horizontally the hardware resources (VDIs or RPA servers). Scaling to 500 bots is possible purely by adding the RPA resources. For higher number of bots multiple deployments is recommended. Production deployment on VDI In case of bots deployments on the VDI technology, each bot has a dedicated VDI. Typical VDI capacity is the following: 2 cores 8 GB RAM (4 GB is minimum) 50 GB HDD (10 GB is minimum) Production deployment on server In case of bots deployments on Windows Server, multiple bots can be deployed to a single server. Typical hardware requirements for bots are the following: Number of bots CPU RAM HDD 5 8 cores 16 GB 250 GB 10 16 cores 32 GB 500 GB For this type of deployment the Terminal Services license is required. OCR OCR engine allows processing of incoming requests in parallel using multiple CPU cores. Requirements for 1 processor are as follows: 1 core 2 GB RAM 10 GB HDD In the OCR cluster, one CPU is reserved for orchestration (master) activities. On all the remaining CPU Cores, OCR requests can be paralleled. workers = CPU Cores 1 Calculate transaction throughput You can calculate the minimal number of required OCR workers knowing the following details: TT: transaction time – average time for processing a single item N: a number of transactions per time interval T: time interval in seconds min(workers) = TT * N T Example: *OCR processing takes 20 seconds (TT = 20) for a one page document. To process 30 documents (N = 30) within one minute (T = 60), you need 20 * 30 60 = 10 workers minimum* Scenario A 10 seconds per page 5 pages per document Servers CPU Pages hour Pages day Pages month Documents hour Documents day Documents month 1 4 1080 25920 777600 216 5184 155520 1 8 2520 60480 1814400 504 12096 362880 1 16 5400 129600 3888000 1080 25920 777600 1 32 11160 267840 8035200 2232 53568 1607040 2 4 2160 51840 1555200 432 10368 311040 2 8 5040 120960 3628800 1008 24192 725760 2 16 10800 259200 7776000 2160 51840 1555200 2 32 22320 535680 16070400 4464 107136 3214080 4 4 4320 103680 3110400 864 20736 622080 4 8 10080 241920 7257600 2016 48384 1451520 4 16 21600 518400 15552000 4320 103680 3110400 4 32 44640 1071360 32140800 8928 214272 6428160 8 4 8640 207360 6220800 1728 41472 1244160 8 8 20160 483840 14515200 4032 96768 2903040 8 16 43200 1036800 31104000 8640 207360 6220800 8 32 89280 2142720 64281600 17856 428544 12856320 Scenario B 10 seconds per page 10 pages per document Servers CPU Pages hour Pages day Pages month Documents hour Documents day Documents month 1 4 1080 25920 777600 108 2592 77760 1 8 2520 60480 1814400 252 6048 181440 1 16 5400 129600 3888000 540 12960 388800 1 32 11160 267840 8035200 1116 26784 803520 2 4 2160 51840 1555200 216 5184 155520 2 8 5040 120960 3628800 504 12096 362880 2 16 10800 259200 7776000 1080 25920 777600 2 32 22320 535680 16070400 2232 53568 1607040 4 4 4320 103680 3110400 432 10368 311040 4 8 10080 241920 7257600 1008 24192 725760 4 16 21600 518400 15552000 2160 51840 1555200 4 32 44640 1071360 32140800 4464 107136 3214080 8 4 8640 207360 6220800 864 20736 622080 8 8 20160 483840 14515200 2016 48384 1451520 8 16 43200 1036800 31104000 4320 103680 3110400 8 32 89280 2142720 64281600 8928 214272 6428160 The following factors can affect the processing speed: OCR parameters, for example, the number of output document types. CPU clock speed. Document complexity. Efficiency of Business Process in loading the OCR queue. Ideally, the number of documents in the queue must be 30% more than the number of workers. For example, there can be 32 workers and 40 documents in the queue. To estimate the exact hardware requirements, we recommend to run several documents through OCR on a test server and check the OCR logs for timing details. Machine Learning Additional system resources may be required for the following cases: Parallel extraction with different models Scheduled parallel training Customized system requirements for model training and prediction You can setup the Mesos cluster on several ML servers and use all system resources of the cluster. The exact CPU RAM requirements can be estimated during runtime. We recommend you to start with the recommended production setup. Besides that, consider the following: Compute intensive training or extract process is performed on demand or with intervals. Training can be run in a cloud using WorkFusion infrastructure, if input data can be supplied. ML capacity planning components Search Engine training Running Search Engine to produce a model requires the following resources for each field: 1 core 14 GB RAM 30 minutes of processing time per every field 500 GB HDD By adding additional resources, the Search Engine can be trained for multiple fields in parallel. Training is executed on BEP Agent machines. On a single BEP Agent machine up to 2 parallel training jobs can be executed. Model prediction Use of the trained model (multi field) on an unseen document to generate ML predictions requires the following resources: 1 core 4 GB RAM 5 GB HDD a few seconds of the processing time (ranges from milliseconds to a minute depending on the document size) per field (prediction for a document is done sequentially, field by field) If server resources are available, the clustering engine automatically spins up additional model instances to process multiple incoming requests from the queue concurrently. Prediction is executed on BEP Agent machines. On a single BEP Agent one CPU and 4 GB of RAM are reserved for orchestration and Operating System activities. Remaining 7 CPU cores and 28 GB of RAM are available for jobs execution. It allow executing up to 7 parallel jobs per BEP agent. Control Tower Tasks Starting with SPA 10.0 release Control Tower tasks are executed on BEP Agent machines. It's the same machines where Machine Learning jobs are being executed. Each worker requires the following resources: 0.25 core 1 GB RAM Single BEP Agent machine can run up to 28 parallel workers. Calculator The studies from this page has been collected in the below calculator. The best use of the calculator and this page is to produce a high level estimate of the capacity required and hardware cost, especially for low and mid scale deployments. For optimal resources utilization and processing speed it's beneficial to build an independent capacity planning framework based on the infrastructure features, previous studies, load testing results, and performance optimizations applied. For high volume large scale deployments it’s recommended to perform additional analysis, have dedicated stages for high scale design, implementation, testing and load testing, configuration and implementation optimizations. "},{"version":"10.0","date":"Oct-14-2019","title":"system-requirements","name":"System requirements","fullPath":"iac/admin/system-requirements/system-requirements","content":" The system requirements described in this article are suitable for most typical automation projects. For detailed capacity and scaling planning, see the Infrastructure Capacity Planning guide. Operating Systems Server type Recommended Supported INT, BEP Master, BEP Agent, APP, OCR (if the OCR platform is Linux) RHEL 7.6 CentOS 7.6 RHEL 7.3, 7.4, 7.5, 7.6 CentOS 7.3, 7.4, 7.5, 7.6 RPA, Analytics (BI) Microsoft Windows Server 2012 R2 Microsoft Windows 10 64 bit only Microsoft Windows Server 2012, 2012 R2, 2016 Windows 8, and 10 64 bit only OCR (if the OCR platform is Windows) Microsoft Windows Server 2012 R2 Microsoft Windows Server 2012, 2012 R2, 2016 File System Recommended Supported XFS XFS, EXT4 NFS only for BEP NFS share NFS only for BEP NFS share Hardware Unless stated otherwise, \"Disk space\" denotes the addressable space left on a single partition, where you plan to install the software, after the disk has been formatted with a filesystem. For the installation, it is advised to use Disk volumes mounted separately from the root volume of the operating system. The values in Disk column describe required disk space in of SPA components. SPA components are allocated on the file system only within , and do not use space allocated for operating system. There are no special requirements for amount of disk space dedicated to Operating System. Example: INT server requires 250 GB*. This means that 250 GB must be available on a disk mounted to (typically = opt workfusion). The root disk with the operating system must have reasonably enough space for Red Hat family 7.x OS functioning (for example, 20 Gb is enough for root drive).* The recommended amount of BEP (Bot Execution Platform) Agents may vary, with up to 10 Agents maximum. Note: specifications are subject to change. .tg {border collapse:collapse;border spacing:0;} .tg td{font family:Arial, sans serif;font size:14px;padding:10px 5px;border style:solid;border width:1px;overflow:hidden;word break:normal;border color:black;} .tg th{font family:Arial, sans serif;font size:14px;font weight:normal;padding:10px 5px;border style:solid;border width:1px;overflow:hidden;word break:normal;border color:black;} .tg .tg myn2{background color: d8f9d8;border color:inherit;text align:center;vertical align:top} .tg .tg jfyn{font weight:bold;background color: d8f9d8;border color:inherit;text align:center;vertical align:top} .tg .tg xldj{border color:inherit;text align:left} .tg .tg f79y{font weight:bold;background color: ffccc9;border color:inherit;text align:center;vertical align:top} .tg .tg py60{font weight:bold;background color: ffffc7;border color:inherit;text align:center;vertical align:top} .tg .tg fymr{font weight:bold;border color:inherit;text align:left;vertical align:top} .tg .tg bolj{background color: ffccc9;border color:inherit;text align:center;vertical align:top} .tg .tg mfhl{background color: ffffc7;border color:inherit;text align:center;vertical align:top} Minimum for Pilot environment0 Recommended for Development environment Recommended for Production environment Server Components OS CPU (cores) RAM (GB) Disk Components OS CPU (cores) RAM (GB) Disk Components OS CPU (cores) RAM (GB) Disk MS SQL4 Database Windows 4 16 500 GB Database Windows 4 16 500 GB Database Windows 8 32 500 GB APP Application RHEL CentOS 4 16 250 GB Application RHEL CentOS 4 16 250 GB Application RHEL CentOS 8 32 250 GB INT Integration RHEL CentOS 4 16 250 GB Integration RHEL CentOS 4 16 250 GB Integration RHEL CentOS 8 32 250 GB OCR1 OCR engine RHEL CentOS Windows Server 2016 2 4 100 GB OCR engine RHEL CentOS 4 8 100 GB OCR engine RHEL CentOS 8 16 250 GB BEP2 Master RHEL CentOS 8 32 2 TB Master RHEL CentOS 8 32 2 TB Master RHEL CentOS 8 32 250 GB 1 Agent (on a separate server) RHEL CentOS 8 32 250 GB 3 Agents (each on a separate server) RHEL CentOS 8 32 250 GB 3 Agents (each on separate server) RHEL CentOS 8 32 250 GB RPA1, 3 5 bots Windows 8 16 250 GB 5 bots Windows 8 16 250 GB 5 bots Windows 8 16 250 GB Analytics Business Intelligence Windows 4 16 100 GB Business Intelligence Windows 4 16 100 GB Business Intelligence Windows 8 32 250 GB 0 the Pilot environment with the minimal requirements is fully operational, though its performance is 50% less, compared to the recommended configuration. 1 – the server is required only if you are planning to use the functionality. 2 – the BEP (Bot Execution Platform) Search Engine (.. .. automl automl sdk search engine.md) training is performed in the Development environment. 3 – the RPA component is installed in the environment mimicking business operations environment with all the applications required to complete the task(s). 4 – the activated instance of MS SQL server should be provided. MS SQL server Recommended version: MS SQL Server 2016 (The RTM version is 13.0.1601.5) Supported versions: MS SQL Server 2012 R2 MS SQL Server 2014 MS SQL Server 2017 Workfusion SPA was not thoroughly tested with the mentioned MS SQL Server versions. Supported MS SQL Server editions: Developer edition for non production environments Standard edition Enterprise edition Required MS SQL Server features: DB Engine Configuration The installation user is created on each server: Linux: a user has the SSH access via a private key authentication and no password sudo privileges. Windows: a user has the administrator privileges. SSH is available for Linux installation user from INT server to all other Linux servers. WinRM is available for Windows installation user from INT server to all other Windows serves. DNS names for all servers are configured according to DNS Names Configuration. Application Ports are open between the servers. Time is synchronized on all servers. Browser For optimal work we recommend using the Chrome V 67. browser. "},{"version":"10.0","date":"Jun-27-2019","title":"advanced-configuration","name":"Advanced Configuration","fullPath":"iac/admin/ocr/advanced-configuration","content":" This guide describes advanced configuration of OCR. In most cases, the default configuration is enough. OCR profiles If you need to change the OCR authentication, consider the following set of profiles, as a basis for your changes: spring.profiles.active: basic and jwt auth,rabbitmqQueue,s3 storage,mssql There are five groups of profiles for OCR Service that must be specified exactly one profile per group. Specify the same type of profiles in the ocr rest.yml and ocr worker.yml configuration files. Profiles in ocr worker and ocr rest must be set up consistently: that means to have the same file storage and task storage profiles. ocr worker does not require authentication type and task storage, and processing strategy profiles to be configured. Authentication Setup The following profiles define an authentication type to apply: disabled auth – the authorization is not done for the OCR Service, all resources are accessed anonymously without authentication. basic auth – the basic authentication is done, some resources are accessible anonymously, the others require authentication. Basic authentication additionally requires a username and a password to be configured in application.properties of ocr rest: spring.security.username: your_username spring.security.password: your_password basic and jwt auth – both basic and JWT authentication are available, some resources are accessible anonymously, the others require authentication. jwt auth – the JWT authentication is done, all resources require authentication, except for the root path: . JWT authentication additionally requires a JWT secret, an issuer, and the token expiration time to be configured in application.properties of ocr rest. jwt.secret:yourjwtsecret jwt.expiration.minutes:60 jwt.issuer:workfusion S3 Endpoint Configuration The following profile specifies a file storage back end to use – a technology to store OCR input and output payload: s3 storage – files will be stored in Amazon S3. The ocr.bucket property must be additionally specified. S3 for ocr rest spring.profiles.active: basic and jwt auth,rabbitmqQueue,s3 storage,mssql Root folder to keep task input and output files (either in S3, depending on the active file storage profile). ocr.tasks.abbyy.storage.folder=abbyy_tasks spring.profiles: s3 storage aws.endpoint: Endpoint to S3 or emulator. E.g. http: 127.0.0.1:9000 ocr.bucket: doc upload name of bucket to use for tasks aws.accessKeyId: Access KeyID for specified endpoint aws.secretKey: Secret Keyfor specified endpoint "},{"version":"10.0","date":"Jul-08-2019","title":"install-second-ocr-server-in-same-spa-instance","name":"Install second OCR server","fullPath":"iac/admin/ocr/install-second-ocr-server-in-same-spa-instance","content":" The guide describes the process of installation of another Linux or Windows OCR instance on existing SPA. Note that this is not OCR scaling procedure. Prepare :::important Run all commands as installation user. ::: To prepare for remore installation of an additional OCR server: Connect to the INT server via SSH. To prepare the environment, run the following commands one by one: cd opt workfusion wf_installer export ANSIBLEREMOTEUSER=\"ec2 user\" export ANSIBLEPRIVATEKEYFILE=\"idrsa\" Here, opt workfusion wf_installer: the path to the Workfusion Installer that contains the main installation script install.sh. export ANSIBLEREMOTEUSER=\"ec2 user\": the ec2 user user is used for the remote access to the OCR Linux machine. Depending on the selected OCR platform: OCR Linux: the user must be specified in file sudoers without password OCR Windows: the user must have permissions to connect remotely, and be in the Administrators group. export ANSIBLEPRIVATEKEYFILE=\"idrsa\": the id _rsa private key is applied for ec2 user. The public key must be located in the .ssh authorized _keys file in the home directory of the user on the remote OCR machine. Verify the user: ls la id_rsa When installing an additional instance of OCR Windows, make sure that the winOCR installer is located in opt workfusion wf _installer sources . curl \"linktoocrWinpackage\" o opt workfusion wfinstaller sources OCRInstaller 10.0.0.0 X.zip Install :::tip vi vim commands: To open a file in the edit mode, in the vi vim editor, press 'i'. To exit the edit mode in vi vim, click Escape, and then type without quotes ':wq' Enter. To exit without changes, click Escape, then type ':q!' Enter ::: To install the second OCR server: In the hosts.yml file, change the ocrhostname ocrwinhostname parameter with OCR Linux or Windows hostname, according to the selected OCR platform. vi hosts.yml In the config.yml file, change the MS SQL credentials and RabbitMQ password for the new OCR environment. vi config.yml The following parameters are to be changed: rabbitmqocrpass: NewPWDveryStrong ocr_platform: 'windows' (according to the type of the additional OCR platform) mssqlocruser: ocr2 mssqlocrpass: Str0nGp@sswd If config.yml is encrypted, run the following command. . install.sh edit_config int When the prompt appears, enter the password for Secrets Vault. Note that the entered password is not displayed. Use the same commands to update windows installation user and windows installation pass. Modify the MS SQL OCR schema name. vi group_vars all vars.yml The following parameters are to be changed: rabbitmqocruser: 'ocr2' mssqlocrschema_name: ocr2 Change the OCR Linux templates. vi roles ocr2 templates properties ocr worker.yml.j2 vi roles ocr2 templates properties ocr rest.yml.j2 The following parameters are be changed: spring.rabbitmq.username: {sms.ocr.spring.rabbitmq.username2} spring.rabbitmq.password: {sms.ocr.spring.rabbitmq.password2} spring.rabbitmq.virtualHost: ocr2 spring.datasource.username: {sms.ocr.spring.datasource.username2} spring.datasource.password: {sms.ocr.spring.datasource.password2} Change the RabbitMQ OCR username and the vhost name: vi roles rabbitmq defaults main.yml The following parameters are to be changed: rabbitmqocruser: ocr2 rabbitmqocrpass: NewPWDveryStrong ocr: vhost: ocr2 Note: skip this step, if the user in DB was not created in advance. Temporary disable all other MS SQL user credentials (except for mssql ocr user) with the \" \" symbol to prevent users and tables recreation. vi roles mssql win tasks create users.yml For example: with_items: {user: '{{ mssqlctuser }}', pass: '{{ mssqlctpass }}', schema: '{{ mssqlctschema_name }}' } {user: '{{ mssqlwsuser }}', pass: '{{ mssqlwspass }}', schema: '{{ mssqlwsschema_name }}' } {user: '{{ mssqlsqcuser }}', pass: '{{ mssqlsqcpass }}', schema: '{{ mssqlsqcschema_name }}' } {user: '{{ mssqlrpauser }}', pass: '{{ mssqlrpapass }}', schema: '{{ mssqlrpaschema_name }}' } {user: '{{ mssqlpmuser }}', pass: '{{ mssqlpmpass }}', schema: '{{ mssqlpmschema_name }}' } {user: '{{ mssqldmuser }}', pass: '{{ mssqldmpass }}', schema: '{{ mssqldmschema_name }}' } {user: '{{ mssqlocruser }}', pass: '{{ mssqlocrpass }}', schema: '{{ mssqlocrschema_name }}' } {user: '{{ mssqldsuser }}', pass: '{{ mssqldspass }}', schema: '{{ mssqldsschema_name }}' } {user: '{{ mssqlrapiuser }}', pass: '{{ mssqlrapipass }}', schema: '{{ mssqlrapiv1schemaname }}'} Add new credentials to Secrets Vault. For WinOCR vi roles ocr config win templates properties ocr rest secure.properties.j2 vi roles ocr config win templates properties ocr worker secure.properties.j2 For OCR Linux vi roles ocr2 templates properties ocr rest secure.properties.j2 vi roles ocr2 templates properties ocr worker secure.properties.j2 The following parameters are to be changed: ocr.spring.datasource.username2={{ mssqlocruser }} ocr.spring.datasource.password2={{ mssqlocrpass }} ocr.spring.rabbitmq.username2={{ rabbitmqocruser }} ocr.spring.rabbitmq.password2={{ rabbitmqocrpass }} Change the following parameters (for Windows OCR only): vi roles ocr win templates ocr_conf.ini.j2 The following parameters are to be changed: spring.datasource.username= {sms.ocr.spring.datasource.username2} spring.datasource.password= {sms.ocr.spring.datasource.password2} spring.rabbitmq.password= {sms.ocr.spring.rabbitmq.password2} spring.rabbitmq.username= {sms.ocr.spring.rabbitmq.username2} spring.rabbitmq.virtualHost=ocr2 Run the following commands. Note: the second command in the section must be run, only if step 6 has been done. . install.sh configure rabbitmq e @config_overrides.yml . install.sh install mssql e @config_overrides.yml Perform the following operation, according to the chosen OCR platfrom: For additional Windows OCR: Run the following commands: . install.sh preinstall ocrwin e @config_overrides.yml . install.sh configure ocrwin e @config_overrides.yml . install.sh install ocrwin e @config_overrides.yml If the installation is successful, WinOCR starts automatically. Still, you need to add parameters to the ocr rest.yml and ocr worker.yml files. For that: Log in to WinOCR VM, and edit the following files: C: workfusion OCR etc ocr rest.yml and C: workfusion OCR etc ocr worker.yml. The following lines should be added: spring.datasource.username: {sms.ocr.spring.datasource.username2} spring.datasource.password: {sms.ocr.spring.datasource.password2} spring.jpa.properties.hibernate.default_schema: ocr2 Server restart is required. On Windows Server, in cmd, run the following commands to prepare a license for correct license generation: cd d \"path to ocr folder\" curl X POST http: 127.0.0.1:9002 api v1 cloud prepareLicense > license_request.txt For additional Linux OCR: Verify that SSH connection to Linux server is successful. . install.sh preinstall ocr e @config_overrides.yml . install.sh install ocr e @config_overrides.yml . install.sh check ocr e @config_overrides.yml Check that everything (except for the license) is fine, wait for one minute after the service restart, and then in your browser, open the https: your.newocr.hostname api v1 health check. "},{"version":"10.0","date":"Jun-27-2019","title":"migration-notes","name":"OCR Migration Notes","fullPath":"iac/admin/ocr/migration-notes","content":" OCR processing Flushing documents is disabled by default. To enable flushing, add the DflushEnabled=true parameter to the config file: For SPA, edit ocr worker.yml: ocr.task.command: ' \"java\",\" Xmx3072m\", \" DflushEnabled=true\", \" cp\",\"ocr task .jar:lib *\",\"com.wf.ocr.OcrTaskApplication\",\" CONFIG\" ' For RPA Express, edit application.properties: ocr.task.command= \"java\",\" Xmx1024m\", \" DflushEnabled=true\", \" cp\",\"ocr task.jar;lib *\",\"com.wf.ocr.OcrTaskApplication\",\" CONFIG\" Removed API The GET api v1 cloud processDocument endpoint has been removed due to URL length limitations of GET method. POST processDocument is used instead of GET processDocument. Removed GET method must be replaced by the POST method in business processes. For example, `` will be replaced by ``. OCR API Methods Replace patternName parameter with the pattern parameter that is passed by content, not file name. Affected endpoints: processImage submitPattern added trainPattern renamed to mergePattern that is combined with new one processDocument uses pattern submitted with submitPattern Use useDefaultPattern=true instead of patternName=default. Affected endpoints: processImage processDocument "},{"version":"10.0","date":"Oct-14-2019","title":"scale-ocr","name":"Scale OCR","fullPath":"iac/admin/ocr/scale-ocr","content":" A typical deployment features only one OCR instance. You can add more instances and combine them into a single cluster without additional configuration of Control Tower. The guide assumes that the user is already familiar with the WorkFusion installation procedure. Primary server The primary server is installed, as described in the installation guide. Concurrent executions are based on available CPU or can be configured. Secondary server The main idea is to add the OCR Worker component by installing the full OCR component pointing to the existing the INT server. Prerequisites The INT Server is installed. The primary OCR Server is installed. Install and configure See Linux or Windows servers Installation according to the selected OCR platform. Workers point to the same MSSQL instance. Ensure that ocr.health.worker.name is unique. See OCR Health Check Mandatory Configurations for details. To install secondary OCR server: Copy the installer, certificates, and configuration file from the primary server to the designrated one. In the ocr _config.ini file, specify correct OCR Server hostnames and ocr.health.worker.name. On the designated secondary server, launch OCR Installer. Validate Check that the Worker process is started without errors (see ocr worker.log). Upload documents to the OCR Server: the ocr task process must start and ocr task.log is created on both primary and secondary servers. "},{"version":"10.0","date":"Sep-20-2019","title":"10x-hardware-requirements","name":"Hardware Requirements","fullPath":"iac/automl/10x-hardware-requirements","content":" :::note Due to significant architecture changes in v10.0 AutoML requirements are now part of the Infrastructure documentation. ::: "},{"version":"10.0","date":"Sep-20-2019","title":"9x-hardware-requirements","name":"9.x Hardware Requirements","fullPath":"iac/automl/9x-hardware-requirements","content":" :::note These infrastructure requirements are also valid for v9.2 and v9.3. ::: AutoML minimal recommended infrastructure requirements is 64 GB RAM, 16 CPU, 500 GB HDD and additional 2TB NFS in case training. Required AutoML cluster capacity is calculated using the following formula: Master capacity Nnodes Agent Node* capacity**. See the following table for details. Components RAM CPU HDD * Master capacity 32 GB 8 250 GB Agent node capacity 32 GB 8 250 GB * Addressable space after formatting. Example: Using the above mentioned formula, an infrastructure with 64 GB RAM, 16 CPU, 500 GB HDD will be able to train 1 field and execute 4 models in parallel. NFS Share Requirements Condition HDD * AutoML cluster is used for model training. 2 TB AutoML cluster is used only for model execution. Not required. * Addressable space after formatting. Cluster Configuration and AutoML Performance The following estimates are correct in case AutoML cluster is used either only for training, or only for execution. If training and execution runs simultaneously, then AutoML performance should be re calculated in each individual case. Agent Nodes Fields trained in parallel Models executed in parallel 1 1 4 3 3 12 7 7 28 Requirements per AutoML Task Requirements for Training During the training process multiple experiments run to define the best feature combination. Memory: 32 GB ParallelFields* CPU: 8 ParallelFields* Requirements for Execution During the execution process there is an already trained model, therefore execution requires less resources than training. Memory: 6 GB ParallelModels* CPU: 1 ParallelModels* "},{"version":"10.0","date":"Jul-09-2019","title":"zookeeper-settings","name":"ZooKeeper Settings","fullPath":"iac/automl/zookeeper-settings","content":" AutoML settings can be configured via ZooKeeper client or any other client capable of connecting to ZooKeeper, for example, ZooNavigator. Follow the official ZooKeeper documentation for instructions on how to install and use the client. Then, refer to WorkFusion Infrastructure documentation for ZooKeeper connection details. You will need INT server address and ZooKeeper port. Parameters for AutoML Gateway Service AutoML Gateway Service routes all requests from client systems to internal systems. automl.service.gateway.external host External host and port when application is used under proxy. Example: some host.workfusion.com:8000. Default value: localhost:8080. automl.service.gateway.base path Base path is used to define version of services. Default value: v1. automl.service.model.url URL to internal AutoML Model Service. Example: http: localhost:7052 automl model service Default value: http: localhost:8081 Parameters for AutoML Model Service AutoML Model Service manages all AutoML operations, such as Training (start, stop, check status) and Execution. Service also provides functionality for uploading, downloading, getting information of model artifacts and already trained models. automl.service.model.external host External host and port when application is used under proxy. Example: some host.workfusion.com:8000. automl.service.model.base path Base path is used to define version of services. Default value: v1. automl.service.model.base dir Base directory for persisting service data. Default value: . data. spring.profiles.include Active spring profiles for application. Default value: execution client. automl.service.model.execution.operation.total timeout Timeout for execution operations starting from sending task till receiving the result. Supports expression for java.util.Duration. Default value: 2m. Required value: 4m. automl.service.model.execution.prepare.shared dir Path to shared NFS folder to prepare data for execution. automl.service.model.execution.worker.profile Worker profile name for execution. (error) execution execution.app.id Application name that starts the execution operation, Execution Framework parameter. Default value: execution default app. automl.service.model.env Environment name. Used to look up model artifacts on S3. Default value: local. Required value: flat prod. automl.service.model.cluster.zookeeper.endpoint Zookeeper endpoint. automl.service.model.cluster.username Username used for running training workers on cluster. automl.service.model.cluster.work dir Working folder path on Mesos. Example: opt workfusion some folder . automl.service.model.s3.bucket.models Bucket name with model artifacts on S3. automl.service.model.s3.bucket.assets Bucket name with trained models and other resources on S3. automl.service.model.s3.bucket.worker.execution.models Bucket name with trained models for BEP execution operations (may be the same as assets bucket name). s3.endpointUrl URL for S3 connection. Example: https: some s3.workfusion.com:443 s3.accessKey Authentication access key for S3. s3.secretKey Authentication secret key for S3. automl.service.model.training.timeout Timeout for model training starting from sending task till receiving the result. Supports expression for java.util.Duration. Default value: 12h. automl.service.model.training.limit.total Limit for simultaneously running model trainings. Default value: 8. automl.service.model.schedule.update artifact fixed delay Artifacts update interval. Supports java.util.Duration. Default value: 5m. automl.service.model.schedule.update timeout training fixed delay Interval for model training timeout check. Supports java.util.Duration. Default value: 15m. nexus.endpoint Endpoint to Nexus instance. Default value: http: localhost:8081 nexus. nexus.username Nexus username. Default value: admin. nexus.password Nexus password. Default value: admin123. nexus.repository Nexus repository for workers deployment. Default value: Workers. "},{"version":"10.0","date":"Sep-20-2019","title":"annotators","name":"Annotators","fullPath":"iac/automl/automl-sdk/annotators","content":" Annotators help to extract structured information from unstructured data. As documents pass through the processing pipeline Annotators analyze words, phrases, named entities in unstructured content, and then create Elements. Elements are represented as Tokens, Sentences, Named Entities, or Entity Boundary Elements and are used for further content analysis. Annotator Implementation To create a custom Annotator and use it in AutoML SDK configuration, follow these steps: Implement an Annotator interface. Add Annotator to configuration. Implementing Annotator Interface All custom Annotator classes should implement an Annotator interface, as in the following example. import com.workfusion.vds.sdk.api.nlp.annotator.Annotator; import com.workfusion.vds.sdk.api.nlp.model.IeDocument; public class MyAnnotator implements Annotator { @Override public void process(IeDocument document) { TODO put your code here } } In a typical use case, Annotator should analyze a text in a Document and add elements based on this text. The following example creates Sentence elements between dots. import java.util.regex.Matcher; import java.util.regex.Pattern; import com.workfusion.vds.sdk.api.nlp.annotator.Annotator; import com.workfusion.vds.sdk.api.nlp.model.IeDocument; import com.workfusion.vds.sdk.api.nlp.model.Sentence; public class SentenceAnnotator implements Annotator { @Override public void process(IeDocument document) { Pattern pattern = Pattern.compile(\" .\"); Matcher matcher = pattern.matcher(document.getText()); int index = 0; while (matcher.find()) { add sentence into document document.add(Sentence.descriptor() .setBegin(index) .setEnd(matcher.start())); index = matcher.end(); } } } Adding Annotator to Configuration To embed your Annotator into the model pipeline, add it to the AutoML SDK configuration. import java.util.ArrayList; import java.util.List; import com.workfusion.vds.sdk.api.hypermodel.annotation.ModelConfiguration; import com.workfusion.vds.sdk.api.hypermodel.annotation.Named; import com.workfusion.vds.sdk.api.nlp.annotator.Annotator; @ModelConfiguration public class AnnotatorConfiguration { @Named(\"annotators\") public List annotators() { List annotators = new ArrayList(); annotators.add(new SentenceAnnotator()); return annotators; } } For detailed information about the configuration, refer to the TBD AutoML SDK Configuration () section. :::warning All fields of an Annotator must be serializable*. Although, the Annotator does not have to implement the serializable interface. Keep in mind that lambdas are not serializable by default. ::: If you cannot avoid using non serializable fields for some reason, then you can do the following trick: Define a field and keep it null. Initialize the field in the @OnInit method. Do not initialize the field in a constructor. An Annotator that uses non serializable date formatter Your Annotator converts dates from one format to another with help of DateTimeFormatter which is not serializable. The correct initialization of the Annotator would be as follows. public class DateNerAnnotator implements Annotator { private DateTimeFormatter dateTimeFormatter; @OnInit public void init() { dateTimeFormatter = DateTimeFormatter.ofPattern(\"dd MM yyyy\"); } * The Annotator implementation goes here * } An Annotator that uses a lambda Let's say, you want to create an Annotator that has a function as a class member. In your Annotator, you would want to refer to one of the functions using a lambda. Then, in order to avoid issues with serialization the code should look the following way. public class MyAnnotator implements Annotator { private Function function; @OnInit public void init() { function = e > e.getBegin() } * The Annotator implementation goes here * } OOTB Annotators Implementation The majority of all use case implementations apply a number of common Annotators most of which are reusable. AutoML SDK contains most used Annotators optimized and ready out of the box with TBD AutoML SDK API. These Annotators are intended to cover general use cases without any need to write your own from scratch. To use OOTB Annotators, add them to your AutoML SDK configuration. You can configure the following types of Annotators for the document processing pipeline: Tokenizer Entity Boundary Annotators NER Annotators Tokenizer There's one Annotator used in all cases — a Tokenizer which splits a Document into separate Tokens. Tokens are the smallest elementary units of a Document, usually words separated by spaces or other special characters. The following example shows how a Sentence is split by Tokens. The following Tokenizer splits text by spaces (dot, comma, etc.). import com.workfusion.vds.sdk.api.hypermodel.annotation.ModelConfiguration; import com.workfusion.vds.sdk.api.hypermodel.annotation.Named; import com.workfusion.vds.sdk.api.nlp.annotator.Annotator; import com.workfusion.vds.sdk.nlp.component.annotator.tokenizer.SplitterTokenAnnotator; @ModelConfiguration public class MyModelConfiguration { @Named(\"annotators\") public Annotator annotators() { return new SplitterTokenAnnotator(\" W \"); } } For more information about Tokenizers, refer to TokenAnnotator documentation. The following code sample is an example of a Tokenizer which creates Sentence elements. import com.workfusion.vds.sdk.api.hypermodel.annotation.ModelConfiguration; import com.workfusion.vds.sdk.api.hypermodel.annotation.Named; import com.workfusion.vds.sdk.api.nlp.annotator.Annotator; import com.workfusion.vds.sdk.api.nlp.model.Content; import com.workfusion.vds.sdk.api.nlp.model.Sentence; import com.workfusion.vds.sdk.nlp.component.annotator.tokenizer.SplitterTokenAnnotator; @ModelConfiguration public class MyModelConfiguration { @Named(\"annotators\") public Annotator annotators() { create a Sentence elements inside Content element(Content element equal full document) return new SplitterTokenAnnotator(Sentence.class, Content.class, \" .! s \"); } } Entity Boundary Annotator Entity Boundary Annotator is an optional Annotator which creates Elements of type Entity Boundary based on text chunks between the specific boundary HTML tags. Entity Boundary Annotator supports the following Elements: Boundary HTML tags: body, code, h1, h2, h3, h4, h5, h6, head, li, p, pre, table, th, title, tr. Document elements: Cell, Table, Sentence, Row. :::note Entity Boundary Annotator is added before NER Annotators in a model configuration. NER Annotators will not analyze the whole document, but only the text inside each Entity Boundary Element. This improves the training performance and helps to avoid creating NER Elements for independent Tokens that adjoin in an AutoML Document, but placed in different HTML blocks in the original document, and thus most likely refer to different logical parts. ::: The following figure shows boundary HTML tags that split the document, and are used to create Entity Boundary elements. NER Annotators Named entity recognition (NER) is a subtask of information extraction that searches for words or patterns in the input text, and then classifies Named Entities in text into pre defined types such as names of persons, organizations, locations, etc. When a NER Annotator finds a proper NER mention in a text Annotator labels its type. The following example contains a text with Named Entities: Persons, Dates and Organizations. The NER Annotator finds each of them in the text, and then creates a corresponding Element type. Dictionary NER Annotators Dictionary NER Annotator creates Named Entity Elements based on word lists from dictionaries. AutoML SDK allows to re use out of the box dictionary matching algorithm — Aho Corasick to add and configure a dictionary for each Named Entity mention type that should be annotated. To configure a dictionary NER Annotator, follow these steps: Create a dictionary file, and then put it to the project dir main resources project folder. Define a dictionary reader. Add a NER Annotator. Here's an example of Country Named Entity Annotator which uses the Aho Corasick alghorithm. In this case, a dictionary source file countries.csv in folder project dir main resource dictionary . Afghanistan Aland Islands Albania Algeria ... etc Note that in the following example we also apply a Tokenizer and an Entity Boundary Annotator prior to a NER Annotator which creates Named Entity Elements with attribute type country. import java.util.ArrayList; import java.util.List; import com.workfusion.vds.sdk.api.hypermodel.annotation.ModelConfiguration; import com.workfusion.vds.sdk.api.hypermodel.annotation.Named; import com.workfusion.vds.sdk.api.nlp.annotator.Annotator; import com.workfusion.vds.sdk.api.nlp.configuration.IeConfigurationContext; import com.workfusion.vds.sdk.nlp.component.annotator.EntityBoundaryAnnotator; import com.workfusion.vds.sdk.nlp.component.annotator.ner.AhoCorasickDictionaryNerAnnotator; import com.workfusion.vds.sdk.nlp.component.annotator.tokenizer.SplitterTokenAnnotator; import com.workfusion.vds.sdk.nlp.component.dictionary.CsvDictionaryKeywordProvider; @ModelConfiguration public class MyModelConfiguration { @Named(\"annotators\") public List annotators(IeConfigurationContext context) { List annotators = new ArrayList(); Adding a Tokenizer annotators.add(new SplitterTokenAnnotator(\" W \")); Adding an Entity Boundary Annotator annotators.add(new EntityBoundaryAnnotator()); annotators.add(new AhoCorasickDictionaryNerAnnotator(\"country\", Provider is used to read the CSV dictionary file from classpath new CsvDictionaryKeywordProvider(context.getResource(\"classpath:dictionary countries.csv\")))); return annotators; } } Here is a list of dictionaries with Named Entities that can be created for your use cases: Countries, cities, states. Companies, agencies, institutions, etc. Nationalities or religious groups. Persons, including fictional. Airports, buildings, highways, bridges, etc. Titles of books, songs, etc. Named events like sports events, battles, wars, etc. Any named language. Absolute or relative dates or periods. Times smaller than a day. Ordinal numbers. Monetary values, including units. Measurements, like weight, distance, etc. Regex Annotators Regex Annotator detects email addresses, URLs, phone numbers, zip codes, IBANs, CUSIP numbers, or any other entity that can be identified using a regular expression. The following example of a NER Annotator is used to find bank codes that consist of 5 digits. It uses the default implementation of BaseRegexNerAnnotator which analyzes the text from Entity Boundary Elements according to the provided pattern, and then creates Named Entity Elements with type bankCode. :::note Entity Boundary Annotator is added before NER Annotators. ::: import java.util.ArrayList; import java.util.List; import com.workfusion.vds.sdk.api.hypermodel.annotation.ModelConfiguration; import com.workfusion.vds.sdk.api.hypermodel.annotation.Named; import com.workfusion.vds.sdk.api.nlp.annotator.Annotator; import com.workfusion.vds.sdk.nlp.component.annotator.EntityBoundaryAnnotator; import com.workfusion.vds.sdk.nlp.component.annotator.ner.BaseRegexNerAnnotator; import com.workfusion.vds.sdk.nlp.component.annotator.tokenizer.SplitterTokenAnnotator; @ModelConfiguration public class MyModelConfiguration { @Named(\"annotators\") public List annotators() { List annotators = new ArrayList(); annotators.add(new SplitterTokenAnnotator(\" W \")); Adding an Entity Boundary Annotator before a NER Annotator to create Entity Boundary elements annotators.add(new EntityBoundaryAnnotator()); annotators.add(BaseRegexNerAnnotator.getJavaPatternRegexNerAnnotator(\"bankCode\", \" b( d{5}) b\")); return annotators; } } "},{"version":"10.0","date":"Sep-20-2019","title":"automl-sdk-api","name":"AutoML SDK API","fullPath":"iac/automl/automl-sdk/automl-sdk-api","content":" Your browser does not support iframes. > Method Description getId() gets unique identifier of the document getText() returns plain text of the document getContent() gives original document text remove(Element e) removes element from document removeAll() removes all elements from document add(ElementDescriptor ed) creates element in document from its description findAll() finds all elements in the document findAll(Class&lt;T&gt; type) finds all elements of given type in the document findCovered(Class&lt;T&gt; type, Element element) gets list of elements of the given element type overlapped by a 'covering' element findCovered(Class&lt;T&gt; type, int begin, int end) gets list of elements of the given element type which are completely located within the given interval findCovering(Class&lt;T&gt; type, Element element) gets list of elements of the given element type that 'cover' a certain element findCovering(Class&lt;T&gt; type, int begin, int end) gets list of elements of the given element type that a completely overlap the given interval findNext(Class&lt;T&gt; type, Element anchor, int count) returns the n elements next to the given element findPrevious(Class&lt;T&gt; type, Element anchor, int count) returns the n elements previous to the given element findAllCovered(Class&lt;T&gt; type, Class&lt;S&gt; covered type) returns covering elements of type T and collection with covered elements of type S from the document. findAllCovering( Class&lt; T &gt; type, Class &gt; covering type ) returns map with covered elements of type T and covering elements of type S from the document. IeDocument specific class for IE use case, used in conjunction with IeProcessor. Provides convenient methods specific for this use case: Method Description findField(String name) finds field in Document by its name findFields(String name) finds fields in Document by name findFields() finds all fields in Document. getFieldsInfo() returns filedInfo DTOs which represents Human Task structure for this model. getFieldInfo(String code) return fieldInfo by answer code. ClassificationDocument document in Classification use case. Has methods to work with document labels: Method Description findLabels() find all labels in the document findLabel(String name) find label by its name Main Elements Element basic class for all document elements. Exact elements representations will be described below. Element provides basic methods like: Method Description getId() unique identifier of the element getDocument() gets document associated with the document getText() returns original text of the element getBegin() gets begin index of element text in document getEnd() gets end index of element text in document Elements: Cell Field Label Line Page Row Table Tag Token Content EntityBoundary LayotBlock NamedEntity Sentence Field class which represents extracted fields in Document for IE use case. Method Description getName() field name, corresponds to answer code in Control Tower getValue() corrected value from text. If value is not corrected original text is returned. setValue(String value) sets corrected value getScore() score of field setScore() sets score of field getAttributes() custom field attributes Label class represents Document label for Classification use case. Method Description getName() gets label name. getScore() returns Label score. Read Only Elements Following elements represent document layout, so they are read only and can not be added or removed from document. Page represents document page. Method Description getPageIndex() returns the page index Table represents table in document, do not provides any additional methods, uses in getCovered getCovering methods. Row row in the table, corresponds to &lt;tr&gt; html tag. Method Description getRowIndex() returns the number of rows directly preceding the current row in the same table Cell table cell Method Description getRowIndex() gets row index of the cell getColumnIndex() returns column index of the cell Line represents OCR &lt;line&gt; element in xml. Method Description getLineIndex() the number of lines directly preceding the current line in the same block Tag class representing xml tag. (eg. &lt;a&gt;, &lt;br&gt;, etc.) Method Description getName() tag name getAttributes() return tag attributes Token represents token element in the Document. Content represents a full content of the document, do not provides any additional methods. EntityBoundary represents some portions of XML HTML text, do not provides any additional methods. LayotBlock represents a block of text based on OCR pixel positions. Method Description getRightPosition() returns right position getLeftPosition() returns left position getTopPosition() returns top position getBottomPosition() returns bottom position NamedEntity represents some real object inside Document, like person, location, bank code, etc. Method Description getType() returns type of mentioned entity getScore() returns score value Sentence represents a sentence element in the document, do not provides any additional methods. 1 : https: workfusion docs.s3.amazonaws.com vds ml sdk latest com workfusion vds sdk api nlp processing Processor.html process T 2 : https: workfusion docs.s3.amazonaws.com vds ml sdk latest com workfusion vds sdk api nlp processing Processor.html 3 : https: workfusion docs.s3.amazonaws.com vds ml sdk latest com workfusion vds sdk api nlp annotator Annotator.html > "},{"version":"10.0","date":"Sep-20-2019","title":"automl-sdk-configuration","name":"SDK Configuration","fullPath":"iac/automl/automl-sdk/automl-sdk-configuration","content":" AutoML SDK configuration is a Java Spring like set of definitions for AutoML SDK components — Annotators, Feature Extractors, Post Processors, etc. AutoML SDK configuration is created declaratively using Java Annotations. There are two main annotations: @ModelConfiguration and @Named (similar to Spring's @Configuration and @Bean annotations). Annotating a class with @ModelConfiguration indicates that the class can be used as a source of component definitions. The @Named annotation declares a component and tells that a method will return an actual component which should be registered in ComponentRegistry. Your @ModelConfiguration class can have a declaration for more than one @Named. Once your classes are defined, you can use the configuration for model training. Configuration Let's start with a basic configuration: First, use @ModelConfiguration to declare a class as a configuration instance. Next, add the main configuration class @HypermodelConfiguration to the hyper model declaration. @ModelConfiguration public class ExampleConfiguration { } @HypermodelConfiguration(ExampleConfiguration.class) public class ExampleHypermodel extends GenericIeHypermodel { public ExampleHypermodel() throws Exception { super(); } } Note, that the previous configuration example extends the default GenericIeHypermodel configuration. This can be changed during the project configuration by specifying the importConfigurationFromGenericModel parameter. Components Configuration To declare one or multiple AutoML SDK components, use @Named annotation with a string value to attach a name identifier to a component. Note, that @Named can return a single component or a list of components. Components are recognized based on the *response type*. The following example declares 3 named components: basePostProcessors, featureExtractors, and a list of annotators. import java.util.ArrayList; import java.util.List; import com.workfusion.vds.sdk.api.hypermodel.annotation.ModelConfiguration; import com.workfusion.vds.sdk.api.hypermodel.annotation.Named; import com.workfusion.vds.sdk.api.nlp.annotator.Annotator; import com.workfusion.vds.sdk.api.nlp.fe.FeatureExtractor; import com.workfusion.vds.sdk.api.nlp.model.Cell; import com.workfusion.vds.sdk.api.nlp.model.Element; import com.workfusion.vds.sdk.api.nlp.model.IeDocument; import com.workfusion.vds.sdk.api.nlp.processing.Processor; import com.workfusion.vds.sdk.nlp.component.annotator.EntityBoundaryAnnotator; import com.workfusion.vds.sdk.nlp.component.annotator.tokenizer.SplitterTokenAnnotator; import com.workfusion.vds.sdk.nlp.component.fe.ner.IsFirstNerInFocusElementFE; import com.workfusion.vds.sdk.nlp.component.processing.grouping.RowBasedGroupingProcessor; @ModelConfiguration public class ExampleConfiguration { @Named(\"basePostProcessors\") public Processor postProcessors() { return new RowBasedGroupingProcessor(); } @Named(\"featureExtractors\") public FeatureExtractor featureExtractors() { return new IsFirstNerInFocusElementFE(Cell.class); } @Named(\"annotators\") public List annotators() { List annotators = new ArrayList(); annotators.add(new SplitterTokenAnnotator(\" s \")); annotators.add(new EntityBoundaryAnnotator()); return annotators; } } The result of this configuration will be: Post Processor from basePostProcessors, Feature Extractor from featureExtractors and two Annotators from annotators. Advanced components are described in the following chapters of the document. Configuration Context ConfigurationContext is an interface that provides a map of initial configuration parameters and a reference to the field object. Field value is used to specify settings for a concrete sub model configuration. For example, apply specific annotators only for field with type — ADDRESS. @Named methods can accept a ConfigurationContext object or an IeConfigurationContext object in case of information extraction models. ConfigurationContext and IeConfigurationContext contain methods to access different properties. The most commonly used property is FieldInfo — a class for information extraction, which contains information about the current sub model configuration. Using this property one @Named method can produce different sets of AutoML SDK components for different sub models using a switch case or an if else statement. The following ConfigurationContext methods are considered the most useful. For a full list of methods, refer to ConfigurationContext documentation. getField Method getField gets FieldInfo associated with the current sub model configuration. For ConfigurationContext that isn't related to the particular field, getField returns empty FieldInfo with code — default. FieldInfo getField() The following example contains an IeConfigurationContext object. In this case, each AutoML SDK configuration produces a different set of Annotators depending on the code type: invoice_number or address. import java.util.ArrayList; import java.util.List; import com.workfusion.vds.sdk.api.hypermodel.annotation.ModelConfiguration; import com.workfusion.vds.sdk.api.hypermodel.annotation.Named; import com.workfusion.vds.sdk.api.nlp.annotator.Annotator; import com.workfusion.vds.sdk.api.nlp.configuration.IeConfigurationContext; import com.workfusion.vds.sdk.nlp.component.annotator.EntityBoundaryAnnotator; import com.workfusion.vds.sdk.nlp.component.annotator.ner.BaseRegexNerAnnotator; import com.workfusion.vds.sdk.nlp.component.annotator.tokenizer.SplitterTokenAnnotator; @ModelConfiguration public class TrainingIeModelConfiguration { public static final String REGEXSPLITTERTOKEN = \" s \"; public static final String REGEX_ZIP = \" 0 9 {5}( 0 9 {4,5}) \"; public static final String REGEX_CITY = \"A Z{3,18}\"; public static final String REGEXINVOICENUMBER = \"( d {10,11})\"; @Named(\"annotators\") public List annotators(IeConfigurationContext context) { List annotators = new ArrayList(); annotators.add(new SplitterTokenAnnotator(REGEXSPLITTERTOKEN)); annotators.add(new EntityBoundaryAnnotator()); String codeType = context.getField().getCode(); switch (codeType) { case \"invoice_number\": { annotators.add(BaseRegexNerAnnotator.getJavaPatternRegexNerAnnotator(\"invoicenumber\", REGEXINVOICE_NUMBER)); break; } case \"address\": { annotators.add(BaseRegexNerAnnotator.getJavaPatternRegexNerAnnotator(\"city\", REGEX_CITY)); annotators.add(BaseRegexNerAnnotator.getJavaPatternRegexNerAnnotator(\"zip\", REGEX_ZIP)); break; } } return annotators; } } For more information, refer to FieldInfo documentation. getResource Method Resource getResource(String path) getResource provides a Resource instance for a specified path: Classpath resource, for example classpath:test.csv. Model resource, for example ref data test.csv, dictionaries test.csv, corrections test.csv. Here's an example of getResource usage with a dictionary. @Named(\"annotators\") public List annotators(IeConfigurationContext context) { List annotators = new ArrayList(); annotators.add(new AhoCorasickDictionaryNerAnnotator(\"company_name\", new CsvDictionaryKeywordProvider(context.getResource(\" dictionaries CompanyNames.csv\")))); return annotators; } InputStream InputStream openInputStream() In case, the Resource instance is provided by an external WorkFusion source, for example attached to human task, use an input stream to read data. The try with resources construct should be used to ensure that the opened stream is closed at the end of the statement. getParameter Method getParameter gets a configuration parameter (for example, parameter name) acquired externally from a WorkFusion service. This parameter can be used for further advanced model configuration. Object getParameter(String name) For more information, refer to getParameter() documentation. Component Registry ComponentRegistry is an interface to provide access to @Named components as Java Objects. An instance of such object can be obtained in a @Named method. Using this interface, you can obtain a Component instance by ID or internal return type (for example, FeatureExtactor.class ). In some cases, this can simplify the configuration code. ComponentRegistry default implementation contains the following methods: getComponent getComponents For usage information, refer to ComponentRegistry documentation. getComponent getComponent gets a single component based on a unique component ID. Component getComponent(String componentName) Parameter Description @componentName @Named component annotation value. @return Single component. getComponents getComponents gets a collection of @Named components based on the return type. Collection getComponents(Class type) Parameter Description @type Components type. @return Collection of components. import com.workfusion.vds.nlp.model.configuration.DefaultComponentRegistry; import com.workfusion.vds.sdk.api.hpo.Dimensions; import com.workfusion.vds.sdk.api.hpo.ParameterSpace; import com.workfusion.vds.sdk.api.hypermodel.annotation.ModelConfiguration; import com.workfusion.vds.sdk.api.hypermodel.annotation.Named; import com.workfusion.vds.sdk.api.nlp.annotator.Annotator; import com.workfusion.vds.sdk.api.nlp.configuration.IeConfigurationContext; import com.workfusion.vds.sdk.nlp.component.annotator.ner.BaseRegexNerAnnotator; @ModelConfiguration public class Config { public static final String REGEX_ZIP = \" 0 9 {5}( 0 9 {4,5}) \"; public static final String REGEX_CITY = \"A Z{3,18}\"; public static final String REGEXINVOICENUMBER = \"( d {10,11})\"; @Named(\"a1\") public Annotator a1() { return BaseRegexNerAnnotator.getJavaPatternRegexNerAnnotator(\"invoicenumber\", REGEXINVOICE_NUMBER); } @Named(\"a2\") public Annotator a2() { return BaseRegexNerAnnotator.getJavaPatternRegexNerAnnotator(\"zip\", REGEX_ZIP); } @Named(\"a3\") public Annotator a3() { return BaseRegexNerAnnotator.getJavaPatternRegexNerAnnotator(\"city\", REGEX_CITY); } @Named(\"featureExtractorsParameterSpace\") public ParameterSpace configureHPO(DefaultComponentRegistry registry, IeConfigurationContext configurationContext) { return new ParameterSpace.Builder() .add(Dimensions.selectOne(registry.getComponent(\"a1\"), registry.getComponent(\"a3\"))) .build(); } } Importing a Configuration One or multiple configurations can be imported into another configuration to avoid writing it from scratch. In this case, the imported configuration is considered a child and the target configuration is considered a parent. The resulting configuration contains all components from child configurations and a parent configuration. To import a configuration( s) to another configuration, use @Import class level annotation together with @ModelConfiguration. The latter contains a reference to the child configuration class. import com.workfusion.vds.sdk.api.hypermodel.annotation.Import; import com.workfusion.vds.sdk.api.hypermodel.annotation.ModelConfiguration; import com.workfusion.vds.sdk.api.hypermodel.annotation.Named; import com.workfusion.vds.sdk.api.nlp.processing.Processor; @ModelConfiguration class PostProcessorConfig { @Named(\"presentationPostProcessor\") public Processor getPostProcessors() { return new ExamplePostProcessing1(); } @Named(\"basePostProcessor\") public Processor postProcessors() { return new ExamplePostProcessing2(); } } @ModelConfiguration @Import(configurations = { @Import.Configuration(PostProcessorConfig.class) }) class GenericPostProcessorConfig { @Named(\"basePostProcessor\") public Processor postProcessors() { return new ExamplePostProcessing3(); } } The result of GenericPostProcessorConfig processing will consist of all named components from the child PostProcessorConfig configuration and all components from the parent GenericPostProcessorConfig configuration. :::note Both configurations contain a Post Processor basePostProcessor. In this case, the component declared in the parent GenericPostProcessorConfig configuration will override the component with the same name declared in the child PostProcessorConfig configuration. ::: The resulting configuration will contain the following Components: presentationPostProcessor from PostProcessorConfig basePostProcessor from GenericPostProcessorConfig Hyper Parameter Optimization Configuration described in previous sections is an example of fixed configuration with a strictly pre defined set of components. Starting with v9.0 AutoML SDK enables configuring Hyper Parameter Optimization which significantly extends the capabilities of a configuration allowing smart selection of components. For example, a configuration can have a number of declared Annotators, but only a particular combination of these Annotators performs the best result. To configure HPO within your model configuration, proceed with the following steps: Declare a class as a configuration instance using the @ModelConfiguration annotation, just like in a fixed configuration. Declare your named components. For example, a couple of Annotators and a list of Feature Extractors. Alternatively, import a child configuration( s) with a ready to go set of components. Declare a @Named component with some unique ID (for example, annotatorParameterSpace), and then add a method producing ParameterSpace. Use ParameterSpace.Builder() to define the dimensions—component selection strategy—according to the following criteria: Parameter Description Example required Component is required at all times. required(\"annotator1\") selectOne Only one component from a subset is required. selectOne(\"annotator2\", \"annotator3\") optionalOne None or only one component from a subset is required. optionalOne(\"annotator2\", \"annotator3\") optional Component is optional. optional(\"annotator3\") :::warning Configuring HPO requires explicit selection strategy setup. This means that components not mentioned in HPO configuration will not be processed. ::: The following configuration with HPO will result in finding the best combination of Annotators. import java.util.ArrayList; import java.util.List; import com.workfusion.vds.sdk.api.hpo.Dimensions; import com.workfusion.vds.sdk.api.hpo.ParameterSpace; import com.workfusion.vds.sdk.api.hypermodel.annotation.ModelConfiguration; import com.workfusion.vds.sdk.api.hypermodel.annotation.Named; import com.workfusion.vds.sdk.api.nlp.annotator.Annotator; import com.workfusion.vds.sdk.api.nlp.fe.FeatureExtractor; @ModelConfiguration public class ExampleConfiguration { @Named(\"annotator1\") public Annotator getAnnotator1() { return new AnnotatorExample1(); } @Named(\"annotator2\") public Annotator getAnnotator2() { return new AnnotatorExample2(); } @Named(\"annotator3\") public Annotator getAnnotator3() { return new AnnotatorExample3(); } @Named(\"fes\") public List getPostProcessors() { List fes = new ArrayList(); fes.add(new FeatureExtractorExample1()); fes.add(new FeatureExtractorExample2()); return fes; } @Named(\"annotatorParameterSpace\") public ParameterSpace configureParameterSpace() { return new ParameterSpace.Builder() .add(Dimensions.required(\"annotator1\")) .add(Dimensions.selectOne(\"annotator2\", \"annotator3\")) .add(Dimensions.required(\"fes\")) .build(); } } The above mentioned configuration can be further extended by adding ConfigurationContext to choose the best configuration for each field by adding dimensions as in the following example. For invoice_amount the configuration will always use fes and annotator1, and then choose the best from annotator2 or annotator3. For client_address the configuration will always use fes and annotator1, and then optionally choose annotator4. import java.util.ArrayList; import java.util.List; import com.workfusion.vds.sdk.api.hpo.Dimensions; import com.workfusion.vds.sdk.api.hpo.ParameterSpace; import com.workfusion.vds.sdk.api.hypermodel.annotation.ModelConfiguration; import com.workfusion.vds.sdk.api.hypermodel.annotation.Named; import com.workfusion.vds.sdk.api.nlp.annotator.Annotator; import com.workfusion.vds.sdk.api.nlp.configuration.IeConfigurationContext; import com.workfusion.vds.sdk.api.nlp.fe.FeatureExtractor; @ModelConfiguration public class ExampleConfiguration { @Named(\"annotator1\") public Annotator getAnnotator1() { return new Annotator1(); } @Named(\"annotator2\") public Annotator getAnnotator2() { return new Annotator2(); } @Named(\"annotator3\") public Annotator getAnnotator3() { return new Annotator3(); } @Named(\"annotator4\") public Annotator getAnnotator4() { return new Annotator4(); } @Named(\"fes\") public List getPostProcessors() { List fes = new ArrayList(); fes.add(new FeatureExtractorExample1()); fes.add(new FeatureExtractorExample2()); return fes; } @Named(\"annotatorParameterSpace\") public ParameterSpace configureParameterSpace(IeConfigurationContext context) { ParameterSpace.Builder builder = new ParameterSpace.Builder(); builder.add(Dimensions.required(\"annotator1\")).add(Dimensions.required(\"fes\")); switch (context.getField().getCode()) { case \"invoice_amount\": builder.add(Dimensions.selectOne(\"annotator2\", \"annotator3\")); break; case \"client_address\": builder.add(Dimensions.optional(\"annotator4\")); break; } return builder.build(); } } Fixed Serialized Configuration By default AutoML SDK in built GenericIeHypermodel configuration produces a set of HPO experiments. Each HPO experiment results in a list of JSON files with serialized configurations, each related to a particular field in a document. A configuration may have serialized configurations imported for specific fields to avoid repeated HPO selection and thus reduce training time. To create a fixed serialized configuration, proceed with the following steps: Put all serialized JSON files into CLASSPATH. Create a new @ModelConfiguration class. This class should import the default main generic configuration class where serialized configurations were created from. Add the resources property to the @Import annotation. Provide a list of inner @Import.Resource annotations. :::note Each @Import.Resource annotation produces a configuration for one field and contains the following properties: value — a path to a file with fixed serialized configuration. condition — @Filter annotation which contains a SpEL expression that produces a boolean value. This expression checks the field value to indicate whether a fixed serialized configuration needs to be applied. The condition is optional. If it is not specified then fixed serialized configuration will be applied every time. This can be used for classification models as there is no field check. ::: Here's an example: import com.workfusion.vds.nlp.hypermodel.ie.generic.config.GenericIeHypermodelConfiguration; import com.workfusion.vds.sdk.api.hypermodel.annotation.Filter; import com.workfusion.vds.sdk.api.hypermodel.annotation.Import; import com.workfusion.vds.sdk.api.hypermodel.annotation.ModelConfiguration; @ModelConfiguration @Import( configurations = { @Import.Configuration(GenericIeHypermodelConfiguration.class) }, resources = { @Import.Resource(value=\" path to file for date field parameters.json\", condition = @Filter(expression = \"field.code eq 'invoice_date'\")), @Import.Resource(value=\" path to file for amount field parameters.json\", condition = @Filter(expression = \"field.code eq 'invoice_amount'\")) } ) public class FixedConfiguration { } Depending on the condition expression result various configurations can be built: If each condition expression returns false, the resulting configuration is built using the default GenericIeHypermodelConfiguration class. If one condition expression returns true, the configuration is built using the serialized configuration from the corresponding value parameter. If multiple condition expressions return true, the configuration is built using the FIRST imported serialized configuration, all consequent serialized configurations are ignored. :::note The described configuration can have additional FixedConfiguration components. The resulting configuration will aggregate both the @Import.Resource configuration (if any) and FixedConfiguration. ::: Filtering Configuration Imports Importing a configuration allows filtering out certain components, HPO in child configuration( s), or even sub child configurations, if any. The following two examples display two cases: Example 1 with @Filter disabled, Example 2 with @Filter enabled. Example 1 There are 2 configurations each containing some named components and HPO. ChildAnnotatorConfig is imported to ParentAnnotatorConfig with @Filter disabled. What is the result of the configured HPO in that case import com.workfusion.vds.sdk.api.hpo.Dimensions; import com.workfusion.vds.sdk.api.hpo.ParameterSpace; import com.workfusion.vds.sdk.api.hypermodel.annotation.Import; import com.workfusion.vds.sdk.api.hypermodel.annotation.ModelConfiguration; import com.workfusion.vds.sdk.api.hypermodel.annotation.Named; import com.workfusion.vds.sdk.api.nlp.annotator.Annotator; @ModelConfiguration class ChildAnnotatorConfig { @Named(\"annotatorA\") public Annotator testAnnotatorA() { return new AnnotatorA(); } @Named(\"annotatorB\") public Annotator testAnnotatorB() { return new AnnotatorB(); } @Named(\"annotatorParameterSpace\") public ParameterSpace configureParameterSpace() { return new ParameterSpace.Builder() .add(Dimensions.optional(\"annotatorA\")) .add(Dimensions.required(\"annotatorB\")) .build(); } } @ModelConfiguration @Import(configurations = {@Import.Configuration(ChildAnnotatorConfig.class)}) class ParentAnnotatorConfig { @Named(\"annotatorC\") public Annotator testAnnotatorC() { return new AnnotatorC(); } @Named(\"additionalAnnotatorParameterSpace\") public ParameterSpace configureParameterSpace() { return new ParameterSpace.Builder() .add(Dimensions.required(\"annotatorC\")) .build(); } } HPO in ChildAnnotatorConfig is processed first, HPO in ParentAnnotatorConfig is processed next. The result is a merged configuration from both the child and parent configurations which contains the following components: Optional annotatorA as a result of ChildAnnotatorConfig HPO. Fixed annotatorB as a result of ChildAnnotatorConfig HPO. Fixed annotatorC as a result of ParentAnnotatorConfig HPO. Example 2 There are 2 configurations each containing some named components and HPO. ChildAnnotatorConfig is imported to ParentAnnotatorConfig but this time with enabled @Filter so the child's HPO is ignored. What is the result of the configured HPO in that case To enable the filter, use @Filter in the @Import annotation. import com.workfusion.vds.sdk.api.hpo.Dimensions; import com.workfusion.vds.sdk.api.hpo.ParameterSpace; import com.workfusion.vds.sdk.api.hypermodel.annotation.Filter; import com.workfusion.vds.sdk.api.hypermodel.annotation.Import; import com.workfusion.vds.sdk.api.hypermodel.annotation.ModelConfiguration; import com.workfusion.vds.sdk.api.hypermodel.annotation.Named; import com.workfusion.vds.sdk.api.nlp.annotator.Annotator; @ModelConfiguration class ChildAnnotatorConfig { @Named(\"annotatorA\") public Annotator testAnnotatorA() { return new AnnotatorA(); } @Named(\"annotatorB\") public Annotator testAnnotatorB() { return new AnnotatorB(); } @Named(\"annotatorParameterSpace\") public ParameterSpace configureParameterSpace() { return new ParameterSpace.Builder() .add(Dimensions.optional(\"annotatorA\")) .add(Dimensions.required(\"annotatorB\")) .build(); } } @ModelConfiguration @Import(configurations = { @Import.Configuration(value = ChildAnnotatorConfig.class, exclude = {@Filter(type = ParameterSpace.class)}), }) class ParentAnnotatorConfig { @Named(\"annotatorC\") public Annotator testAnnotatorC() { return new AnnotatorC(); } @Named(\"parentAnnotatorParameterSpace\") public ParameterSpace configureParameterSpace() { return new ParameterSpace.Builder() .add(Dimensions.selectOne(\"annotatorA\", \"annotatorB\")) .add(Dimensions.required(\"annotatorC\")) .build(); } } The resulting configuration contains the following components: Fixed annotatorC from ParentAnnotatorConfig HPO. Either annotatorA or annotatorB from ParentAnnotatorConfig HPO. Child's HPO is ignored. Advanced Configuration Components Apart from regular components like Annotators, Feature Extractors and Post Processors a model may contain fundamental components for advanced configuration. These components may require a deeper understanding of AutoML SDK. Furthermore, these components are not subject to HPO selection. :::caution Any of the following components can be declared only once, otherwise, errors may occur. ::: Advanced components include: HpoConfiguration Liblinear Classifier, Vowpal Wabbit Classifier PipelineConfiguration HpoConfiguration HpoConfiguration is a set of configurable global parameters for HPO. An instance is created via HpoConfiguration.Builder. Here's an example of HpoConfiguration. @Named(\"hpoConfig\") public HpoConfiguration hpoConfig(ConfigurationContext context) { return new HpoConfiguration.Builder() .timeLimit(1, TimeUnit.HOURS) .maxExperimentsWithSameScore(10) .targetScore(0.7) .build(); } For more information, refer to HpoConfiguration documentation. Liblinear Classifier and Vowpal Wabbit Classifier LiblinearClassifier and VWClassifier are utility classes to store possible training arguments of Liblinear and Vowpal Wabbit. @Named(\"mlConfig\") public LiblinearClassifier mlConfig(){ return new LiblinearClassifier.Builder() .solver(Solver.L2RL2LOSSSVC) .quietMode() .build(); } :::warning Only one classifier at a time can be used in a configuration. Otherwise, errors may occur. ::: For more information, refer to LIBSVM and Vowpal Wabbit documentation. PipelineConfiguration PipelineConfiguration is a set of parameters that are used to build a pipeline. The defined parameters remain unchanged across all configurations created for experiments. The following pipelineConfiguration example sets the NORMALIZESCORE and CASESENSITIVE parameters to true. @Named(\"pipelineConfiguration\") public PipelineConfiguration pipelineConfiguration() { return new PipelineConfiguration.Builder() .parameter(GenericPipelineConfiguration.NORMALIZE_SCORE, true) .parameter(GenericPipelineConfiguration.CASE_SENSITIVE, true) .build(); } For more information, refer to PipelineConfiguration and GenericPipelineConfiguration documentation. Additional Resources Python Classifiers "},{"version":"10.0","date":"Jul-08-2019","title":"code-examples","name":"Code Examples","fullPath":"iac/automl/automl-sdk/code-examples","content":" Creating NER Annotators import java.util.ArrayList; import java.util.List; import com.google.common.collect.Lists; import com.workfusion.vds.sdk.api.hypermodel.annotation.ModelConfiguration; import com.workfusion.vds.sdk.api.hypermodel.annotation.Named; import com.workfusion.vds.sdk.api.nlp.annotator.Annotator; import com.workfusion.vds.sdk.api.nlp.configuration.IeConfigurationContext; import com.workfusion.vds.sdk.api.nlp.model.Document; import com.workfusion.vds.sdk.nlp.component.annotator.EntityBoundaryAnnotator; import com.workfusion.vds.sdk.nlp.component.annotator.ner.AhoCorasickDictionaryNerAnnotator; import com.workfusion.vds.sdk.nlp.component.annotator.ner.BaseRegexNerAnnotator; import com.workfusion.vds.sdk.nlp.component.dictionary.CsvDictionaryKeywordProvider; import com.workfusion.vds.sdk.nlp.component.dictionary.MultiDictionaryDataProvider; import com.workfusion.vds.sdk.nlp.component.dictionary.StringDictionaryProvider; @ModelConfiguration public class AnnotatorModelConfiguration { @Named(\"annotators\") public List> getAnnotators(IeConfigurationContext context) { final List> annotators = new ArrayList(); annotators.add(new EntityBoundaryAnnotator()) annotators.add(BaseRegexNerAnnotator.getJavaPatternRegexNerAnnotator(\"email\", \" b w._% @ w. . a zA Z {2,} b\")); annotators.add(BaseRegexNerAnnotator.getRe2jPatternRegexNerAnnotator(\"email\", \" b w._% @ w. . a zA Z {2,} b\")); annotators.add(new AhoCorasickDictionaryNerAnnotator(\"company\", new StringDictionaryProvider(Lists.newArrayList(\"Company A\", \"Company B\", \"Company C\")))); annotators.add(new AhoCorasickDictionaryNerAnnotator(\"company\", new CsvDictionaryKeywordProvider(context.getResource(\"classpath:dictionary company.txt\")))); annotators.add(new AhoCorasickDictionaryNerAnnotator(\"company\", new MultiDictionaryDataProvider(new CsvDictionaryKeywordProvider(context.getResource(\"classpath:dictionary company1.txt\")), new CsvDictionaryKeywordProvider(context.getResource(\"classpath:dictionary company2.txt\"))))); return annotators; } } Overriding Generic Tokenizer import com.workfusion.vds.nlp.hypermodel.ie.generic.config.GenericIeAnnotatorConfiguration; import com.workfusion.vds.nlp.hypermodel.ie.generic.config.GenericIeHypermodelConfiguration; import com.workfusion.vds.sdk.api.hypermodel.annotation.Import; import com.workfusion.vds.sdk.api.hypermodel.annotation.ModelConfiguration; import com.workfusion.vds.sdk.api.hypermodel.annotation.Named; import com.workfusion.vds.sdk.api.nlp.annotator.Annotator; import com.workfusion.vds.sdk.api.nlp.model.Document; import com.workfusion.vds.sdk.nlp.component.annotator.tokenizer.SplitterTokenAnnotator; @ModelConfiguration @Import(configurations = { @Import.Configuration(value = GenericIeHypermodelConfiguration.class) }) public class CustomModelConfiguration { @Named(GenericIeAnnotatorConfiguration.TOKENIZER_ANNOTATOR) public Annotator getTokenizer() { return new SplitterTokenAnnotator(\"( s )\"); } } Describing How Token Looks Like import com.workfusion.vds.nlp.hypermodel.ie.generic.config.GenericIeAnnotatorConfiguration; import com.workfusion.vds.nlp.hypermodel.ie.generic.config.GenericIeHypermodelConfiguration; import com.workfusion.vds.sdk.api.hypermodel.annotation.Import; import com.workfusion.vds.sdk.api.hypermodel.annotation.ModelConfiguration; import com.workfusion.vds.sdk.api.hypermodel.annotation.Named; import com.workfusion.vds.sdk.api.nlp.annotator.Annotator; import com.workfusion.vds.sdk.api.nlp.model.Document; import com.workfusion.vds.sdk.nlp.component.annotator.tokenizer.MatcherTokenAnnotator; @ModelConfiguration @Import(configurations = { @Import.Configuration(value = GenericIeHypermodelConfiguration.class) }) public class CustomModelConfiguration { @Named(GenericIeAnnotatorConfiguration.TOKENIZER_ANNOTATOR) public Annotator getTokenizer() { return new MatcherTokenAnnotator(\"( w )\"); } } Simple Annotator import java.util.regex.Matcher; import java.util.regex.Pattern; import com.workfusion.vds.sdk.api.nlp.annotation.OnDestroy; import com.workfusion.vds.sdk.api.nlp.annotation.OnInit; import com.workfusion.vds.sdk.api.nlp.annotator.Annotator; import com.workfusion.vds.sdk.api.nlp.model.Document; import com.workfusion.vds.sdk.api.nlp.model.Token; public class MyTokenAnnotator implements Annotator { private static final Pattern TOKEN_PATTERN = Pattern.compile(\" w \"); @OnInit public void init() { initialization method, executed once after constructor } @Override public void process(Document document) { final Matcher matcher = TOKEN_PATTERN.matcher(document.getText()); while (matcher.find()) { int start = matcher.start(); int end = matcher.end(); document.add(Token.descriptor() .setBegin(start) .setEnd(end) ); } } @OnDestroy public void destroy() { destroy method, executed once after documents processing is finished } } Simple FE import java.util.Collection; import java.util.Collections; import java.util.Map; import com.workfusion.vds.sdk.api.nlp.annotation.OnDestroy; import com.workfusion.vds.sdk.api.nlp.annotation.OnDocumentComplete; import com.workfusion.vds.sdk.api.nlp.annotation.OnDocumentStart; import com.workfusion.vds.sdk.api.nlp.annotation.OnInit; import com.workfusion.vds.sdk.api.nlp.fe.Feature; import com.workfusion.vds.sdk.api.nlp.fe.FeatureExtractor; import com.workfusion.vds.sdk.api.nlp.model.Document; import com.workfusion.vds.sdk.api.nlp.model.Element; public class MyCoveredTextFE implements FeatureExtractor { @OnInit public void init(Map parameters) { } @OnDocumentStart public void documentStart(Document document, Class focusElementClass) { } @OnDocumentComplete public void documentComplete() { } @OnDestroy public void destroy() { } @Override public Collection extract(Document document, T element) { return Collections.singleton(new Feature(\"CvrdTxt_\" element.getText(), 1.0)); } } Post Processor to Change the Extracted Value import java.util.Collection; import com.workfusion.vds.sdk.api.nlp.model.Field; import com.workfusion.vds.sdk.api.nlp.model.IeDocument; import com.workfusion.vds.sdk.api.nlp.normalization.NumberNormalizer; import com.workfusion.vds.sdk.api.nlp.processing.Processor; import com.workfusion.vds.sdk.nlp.component.processing.normalization.OcrAmountNormalizer; public class ChangeValuePostProcessor implements Processor { private final NumberNormalizer normalizer = new OcrAmountNormalizer(); @Override public void process(IeDocument document) { Collection amounts = document.findFields(\"total_amount\"); amounts.forEach(a > { String normalizedValue = normalizer.normalize(a.getValue(), \" .00\"); a.setValue(normalizedValue); }); } } Post Processor to Change Remove the Extracted Value import java.math.BigDecimal; import java.util.Collection; import com.workfusion.vds.sdk.api.nlp.model.Field; import com.workfusion.vds.sdk.api.nlp.model.IeDocument; import com.workfusion.vds.sdk.api.nlp.processing.Processor; public class RemoveFieldPostProcessor implements Processor { @Override public void process(IeDocument document) { Collection amounts = document.findFields(\"total_amount\"); amounts.stream() .filter(a > a.getScore().compareTo(BigDecimal.valueOf(0.5)) { @Override public void process(IeDocument document) { document.add( Field.descriptor() .setName(\"person_id\") .setValue(\"XSF12312\") .setScore(1.0)); } } Importing Fixed Configuration import com.workfusion.vds.nlp.hypermodel.ie.generic.config.GenericIeHypermodelConfiguration; import com.workfusion.vds.sdk.api.hypermodel.annotation.Filter; import com.workfusion.vds.sdk.api.hypermodel.annotation.Import; import com.workfusion.vds.sdk.api.hypermodel.annotation.ModelConfiguration; @ModelConfiguration @Import(configurations = { @Import.Configuration(value = GenericIeHypermodelConfiguration.class), }, resources = { @Import.Resource(value = \" configuration totalamount config parameters.json\", condition = @Filter(expression = \"field.code == 'totalamount'\")) } ) public class MyModelConfiguration { } "},{"version":"10.0","date":"Jul-08-2019","title":"components","name":"Overview","fullPath":"iac/automl/automl-sdk/components","content":" AutoML SDK Java library is a complete toolset for any information extraction or classification use case. Proceeding through the components pipeline this toolset delivers a ready to use model. AutoML SDK consists of the following components, each responsible for a specific task in a pipeline. Parser removes HTML tags from documents, creates an AutoML SDK Document. Annotator splits text into tokens, adds boundary elements, adds NERs. Feature Extractors analyze tokens in documents and create features. Algorithms aggregate all field documents and feature extractors, provides a model and its statistics. Post Processing applies normalization logic and field grouping. "},{"version":"10.0","date":"Sep-20-2019","title":"environment-configuration","name":"Environment Configuration","fullPath":"iac/automl/automl-sdk/environment-configuration","content":" :::important Make sure you have installed all the following prerequisites: Java 1.8 ⚠️ Maven 3.0—3.5 Access to Maven releases repository or on premise instance with AutoML SDK installation WorkFusion Studio, or any Java IDE (Eclipse, IntelliJ Idea) ::: Java installation Download the appropriate Java Dev Kit (JDK). Make sure you install JDK, not JRE. Follow the steps in the installation wizard. Do not customize any settings, use the default ones. Create a system variable JAVA_HOME with full path to the directory with installed Java. Run java version in the command line to verify the installation. Archetype installation AutoML SDK provides a Maven Archetype to create a project required for model customization. To install Maven, follow these steps: Download Maven v3.0–3.5 (not higher) from the official Maven website. Unzip the archive to C: Program Files apache maven 3.6.0. Create system variable MAVEN_HOME with full a path to the directory with unzipped Maven. Add %MAVEN_HOME% bin to PATH system variable. Run mvn version in new command line to verify the installation. Using a WorkFusion repository If you have access to WorkFusion Maven releases repository, add it to your Maven installation by following these steps: Locate the settings.xml file in .m2 folder. Create the settings.xml file if the .m2 folder is empty. Modify the settings.xml file as in the following example: wf nexus releases wf repo wf nexus releases nexus content repositories ml sdk wf repo Replace the `` placeholder with your Nexus instance containing AutoML SDK. Training AutoML SDK dependencies URL: Using a Mirror Repository In case, there's no Internet connection when setting up the environment, add the following code to the settings.xml file. central central nexus content repositories ml sdk * Maven to Nexus SSL To use a secure connection when setting up the environment, follow these steps: Create a .mavenrc file in the ** directory. Add the following code snippet. MAVEN_OPTS=\" Dmaven.wagon.http.ssl.insecure=true Dmaven.wagon.http.ssl.allowall=true Dmaven.wagon.http.ssl.ignore.validity.dates=true\" "},{"version":"10.0","date":"Sep-20-2019","title":"e2e-automl-sdk-walkthrough","name":"End-to-End AutoML SDK Walkthrough","fullPath":"iac/automl/automl-sdk/e2e-automl-sdk-walkthrough","content":" This tutorial walks you through the implementation of 2 typical custom models — Information Extraction and Classification. By installing the following 2 samples on your local WorkFusion Studio and going through code you will learn by example how to build AutoML SDK based solutions. Information Extraction Information Extraction sample is an example of possible model customization for a particular real use case related to information extraction. It contains the following components: Annotators Feature Extractors Post Processors It also contains an example of model configuration with ready to use ModelTrainingRunner and ModelExecutionRunner. Classification Classification sample is an example of possible model customization for a real use case related to classification. It contains almost the same components as IE sample but has additional Feature Extractors. Sample Project Setup Both model samples are packed into separate archetypes. To generate a sample project: Create a project using AutoML SDK Wizard, or any of the available ways mentioned in the Project Configuration section. Use the following parameters: Archetype URL: https: watt db1.workfusion.com nexus content repositories ml sdk Use the following credentials: bcbdeploy Workfusion!5. You may change Maven's settings.xml to access the archetype located in this repository. Archetype ID: quickstart archetype (for IE) or quickstart classification archetype (for Classification). AutoML SDK Version: 1.0 Alternatively, use one of the following commands in Terminal: Information Extraction mvn archetype:generate DarchetypeGroupId=com.workfusion.ml DarchetypeArtifactId=quickstart archetype DarchetypeVersion=1.0 Classification mvn archetype:generate DarchetypeGroupId=com.workfusion.ml DarchetypeArtifactId=quickstart classification archetype DarchetypeVersion=1.0 The resulting project looks as follows. quickstart ├── input ├── src └── main └── java ├── quickstart ├── annotator └── config ├── Fields.java └── QuickstartModelConfigurationjava └── fe ├── classification └── general ├── IsCoveredWithParticularNerFeatureExtractor.java ├── IsFirstInSentenceFeatureExtractor.java ├── IsFirstWordInDocumentFeatureExtractor.java ├── IsFitCustomPatternFeatureExtractor.java ├── IsPrecededWithCurrencySignFeatureExtractor.java ├── IsPrecededWithKeywordsInLineFeatureExtractor.java ├── IsUniqueWordInLineFeatureExtractor.java └── IsUpperLowerCaseFeatureExtractor.java └── table ├── IsCoveredWithCellFeatureExtractor.java ├── IsLastCelllInTableFeatureExtractor.java ├── MatchFullColumnOrRowWithSpecifiedHeaderFeature..java ├── TableColumnindexFeatureExtractor.java └── TableRowIndexFeatureExtractor.java └── model └── QuickstartModel.java └── processing ├── AmountNormalizationPostProcessor.java ├── AppendCurrencyPostProcessor.java ├── DatePostProcessing..java ├── TableColumnindexFeatureExtractor.java ├── EmailPostProcessor.java ├── ProductPostProcessor..java ├── RemoveCommasFromPrice.java └── RemoveSInTheBeginningFromPrice.java Solution Review Information Extraction Note, the current Information Extraction model uses GenericIeHypermodelConfiguration with a number of pre defined basic components and add some specific related for a particular case. The presented solution is configured for a typical case of Information Extraction — Invoice Processing. Invoice samples are located in project's input folder. Solution is designed to extract data related to these single values: Supplier name Email Invoice date Invoice number Total amount And group values: Price Quantity Product Field configuration is added to the ModelTrainingRunner class. This class is configured for model training and points to input training directory. The ModelExecutionRunner class is configured to use the already trained model and points to test and trainedModel directories. In addition, there is some extra customization: Class Fields provides a sample how to keep all model fields and model specifics in one place to avoid hard code. Every class contains a brief description of its functionality. Input and test folders contain sample documents for training and execution respectively. Classification Classification custom model is based on MultiClassClassificationGenericSe20Hypermodel with some additional case related Feature Extractors. This model is created for sentiment analysis based on Twitter data related to the American airplane industry. There are 2 classes: positive and negative sentiments. Input data is located in the in train input.csv file and has a \"text, class\" format. After that, it is processed to a separate folder out train in a number of .txt files, each of them contains 1 row from the§ initial file. The trained model is stored in out train output. The same structure is used for the test part with substitution of train to test in name respectively. Components Configuration :::note Components related to classification custom model are marked with an asterisk ( *). ::: Annotators See annotators folder of the project and QuickModelConfiguration.getAnnotators(). CeoNerAnnotator is an example of custom NER annotator used for specific goal of CEO lookup in the documents. AhoCorasickDictionaryNerAnnotator creates Named Entity Elements based on word lists from dictionaries. BaseRegexNerAnnotator analyzes the text from Entity Boundary Elements according to the provided pattern, and then creates Named Entity Elements. GenericIeAnnotatorConfiguration contains base annotators configuration used in GenericIeHypermodelConfiguration. Feature Extractors For Feature Extractors of the Information Extraction model, see the fe folder of the project and QuickModelConfiguration.getFeatureExtractors(). For Feature Extractors of the Classification model, see the fe.classification folder of the project and ClassificationModelConfiguration.getFeatureExtractors(). Classification IsInFirstOrLastNLinesInDocumentFeatureExtractor creates a feature if specific data was found in first last N rows of the document. LinePositionOfKeywordFeatureExtractor creates a feature if specific data was found in a line of the document. Value of the feature depends on the line number of the document. PositionOfKeywordsFeatureExtractor creates a feature if specific data was found the the document. Value of the feature depends on the absolute position of data in the document text. * IsSpecificPunctuationOrSmilesPresentFeatureExtractor finds specific punctuations in text of the document. * IsPositiveNegativeKeywordPresentFeatureExtractor finds specific keywords in text of the document. General IsCoveredWithParticularNerFeatureExtractor creates a feature if data is covered with a particular Named Entity. IsFirstInSentenceFeatureExtractor creates a feature if data is located in the beginning of the sentence. IsFirstWordInDocumentFeatureExtractor creates a feature if data is located in the beginning of the document. IsFitCustomPatternFeatureExtractor creates a feature if data fits provided pattern. IsPrecededWithCurrencySignFeatureExtractor creates a feature if data preceded with currency sign. IsPrecededWithKeywordsInLineFeatureExtractor creates a feature if data is preceded in line with any keyword from provided list. IsUniqueWordInLineFeatureExtractor creates a feature if data isequal to whole line. IsUpperLowerCaseFeatureExtractor creates a feature if is all upper lowercase. Table IsCoveredWithCellFeatureExtractor creates a feature if data is in a cell of a table. IsLastCellInTableFeatureExtractor creates a feature if data is located in lower right cell of the table. MatchFullColumnOrRowWithSpecifiedHeaderFeatureExtractror creates a feature for every value in a column row where appropriate column row header contains any item of the keyword list provided. TableColumnIndexFeatureExtractor creates a feature for every column value in a table. Value is column number. TableRowIndexFeatureExtractor creates a feature for every row value in a table. Value is row number. CandidateFeatureExtractorsConfiguration contains base Feature Extractors configuration used in GenericIeHypermodelConfiguration. * GenericMultiClassificationFEConfiguration contains base Feature Extractors configuration used in GenericMultiClassificationHypermodelConfiguration. Post Processors See the processing folder of the project and QuickModelConfiguration.getProcessors(). AmountNormalizationPostProcessor normalizes numbers to the .00 pattern. Adds 0 in front of dot sign, if missed. If final value is not parsed as double it is removed from the document. AppendCurrencyPostProcessor appends to a value provided value: e.g. 'USD'. DatePostProcessing tries to normalized specific custom date format to MM dd yyyy first. Then, clears out all forbidden symbols, applies OOTB date normalizer. Finally, removes all dates that start with '00' in year. EmailPostProcessor validates emails with a specific pattern, removes if data doesn't match. ProductPostProcessor custom post processor. Multiple spaces are removed. Also there are some specific values to replace or remove. Keep in mind that data should be taken from table cells. RemoveCommasFromPrice removes commas from numbers. RemoveSInTheBeginningFromPrice removes OCR errors when a sign is incorrectly recognized as 's' or 'S'. SupplierNamePostProcessor uses Jaro Winker similarity to repair corrupted data based on dictionary provided. If similarity value is below threshold value is removed. ToUpperOrLowerCasePostProcessor fixes values case, if needed. Local Model Training Run ModelTrainingRunner as java application. For details, refer to Model Training. Local Model Run Run ModelExecutionRunner as java application. For details, refer to Local Run. Model Deployment to SPA To use your model in Control Tower, refer to the Publish and Execute section of the Getting Started page. "},{"version":"10.0","date":"Sep-20-2019","title":"feature-engineering","name":"Feature Engineering","fullPath":"iac/automl/automl-sdk/feature-engineering","content":" Features) are individual measurable properties or characteristics of data being observed. Selecting relevant features and deciding how to encode them for a training can have a great impact on model results. The process of feature development is known as feature engineering or feature generation. Simply put, it is a process of manually constructing new attributes from raw data. It involves intelligent combining or splitting the existing raw attributes into new ones which have a higher predictive power. A particular target data element can have several features tagging it. It is important to select only the features that are relevant to the problem so that the accuracy of the model improves. It also reduces the complexity of the model as we avoid the least significant or unnecessary features. Feature engineering steps Review the training set and define field patterns. Create a Feature. Run training and evaluate results. Review training set and define field patterns As input data you have list of fields that should be extracted and Training Set with already tagged fields. In the example below you can see tagged Invoice and tagged fields that should be extracted: Client Name, Invoice Date, and group of fields Products with fields Item, Description, Quantity, Discount, Tax Rate and Price. This step is aimed to find out attributes that better differentiate the patterns for each field. Each field should be carefully analyzed to understand its nature (whether it is time, date, email, address etc.) and possible position and context. Context is basically everything that surrounds the tagged piece of text, its position on the page and in some cases its position relative to other important pieces of information. Context is the most important source of features and is used to determine where a gold value should be searched. Field analysis usually contains the following steps: tags distribution (how many objects for each field are in the data set ): check their consistency (for example, in case of Address field) whether all values look similar; check whether some values are missed (in case of not required fields). This information is useful in understanding which fields are badly represented. In the example below the training set size is 750 documents, but there are only 30 objects of mot _code field. This field is badly represented. required vs. not required fields; group values, e.g. like table with list of products with their name, quantity, price; multiple values for one field in the document; other examples for analysis are total number of tags, frequency of a given tag, tag with the greatest count, the N most common tags and their frequencies etc. define whether word shape patterns is applied to the field, and how many occurrences of each pattern is met in the documents: example 1: for field Invoice Number we should pay attention to shape and numeric Invoice: xx xxxx met in 50 cases. Invoice: xx _xxxx met in 45 cases. Invoice: xx Xxxx met in 5 cases. You can analyse data using various utilities. Analyse Context Compare tagged value and data value, and get the context where its met tagged value data value Context EUR Euros or sign Get the first element in line from left side: In case some of patterns are met once but not tens or hundreds of times in the whole training set, then TBD. Feature engineering tips Start with a minimum set of features: include features suggested by domain knowledge; test these out individually, build from the bottom up. Keep in mind limits on Feature Engineering. Too much features can lead to overfitting when model will provide good extraction results and statistics on training set but will have bad results on production (new) set. This can be caused by selecting useless features or dependent correlated features. Create features Having field patterns and context from the previous step we can proceed with creation of features. Generally all features can be split into several types: Gold Data Specific Features. This defines the features related to the gold value itself. Examples: the gold value type (price, percentage, address, year, time, date, phone, email etc.), in case we do not have specific letter pattern for the words. Examples: identify if the token contains currency characters; determine whether the last N chars of the token contain digits; determines if the covered text contains a number, must contain comma or dot for every 1 3 digits \"( s €£₤ s ) 0 9 ) *( : D )\"; presence of hyphen. define whether token correspond to specific letter pattern (capitalization, numeric, length, special chars in between like hyper, dot, etc.). Word shape features are used to represent such patterns by mapping lower case letters to ‘x’, upper case to ‘X’, numbers to ’d’, and punctuation. Thus for example A.B.C would map to A.B.C. and AB 01 03 would map to XX dd dd. Examples: determine if the token is a CUSIP code; determine whether token wordshape corresponds to specific pattern: Xxx _ddddd, xx xx ddd xx, XXXXXX; N gram is a sequence of words: a 2 gram (or bigram) is a two word sequence of words like “please click”, or ”click button”, and a 3 gram (or trigram) is a three word sequence of words like “please click button” etc. Example 1 In the example below, in case focus element is a number, then feature will be created with a value equals to 1. Define if the focus element contains a number import java.util.Collection; import java.util.Collections; import com.workfusion.vds.sdk.api.nlp.fe.Feature; import com.workfusion.vds.sdk.api.nlp.fe.FeatureExtractor; import com.workfusion.vds.sdk.api.nlp.fe.annotation.FeatureName; import com.workfusion.vds.sdk.api.nlp.model.Document; import com.workfusion.vds.sdk.api.nlp.model.Element; @FeatureName(IsNumberIncludedFE.FEATURE_NAME) public class IsNumberIncludedFE implements FeatureExtractor { public static final String FEATURE_NAME = \"IsNumberIncluded\"; @Override public Collection extract(Document document, T element) { String text = element.getText(); if (text.matches(\". d.\")) { if text contains a digit, return a feture return Collections.singletonList(new Feature(FEATURE_NAME, 1)); } otherwise return empty list return Collections.emptyList(); } } Local Context Features: the information around the gold value. Examples: the N tokens lines cells before after above below the gold value. Examples: check whether wordshape of neighboring words corresponds to some specific pattern; define whether number of tokens covered by cell is equal to some specific value; define whether token is situated in a specifc line of the focus cell; check if there are keyword matches in the same line in the non empty block preceding the block covering the focus line; determine if the numeric value of the focus cell is equal to the cell above; determine if number of columns from the current cell to the end of the table is equal to N; determine whether keywords occur in the same column as the focus cell; Example 2 The below example shows, that feature will be created with value equal to 1, in case when token, next to target token, in the same cell contains keyword. Define whether next token in the same cell contains keyword import java.util.ArrayList; import java.util.Collection; import java.util.List; import com.workfusion.vds.sdk.api.nlp.fe.Feature; import com.workfusion.vds.sdk.api.nlp.fe.FeatureExtractor; import com.workfusion.vds.sdk.api.nlp.fe.annotation.FeatureName; import com.workfusion.vds.sdk.api.nlp.fe.annotation.Index; import com.workfusion.vds.sdk.api.nlp.fe.annotation.IndexType; import com.workfusion.vds.sdk.api.nlp.fe.annotation.Indexes; import com.workfusion.vds.sdk.api.nlp.model.Cell; import com.workfusion.vds.sdk.api.nlp.model.Document; import com.workfusion.vds.sdk.api.nlp.model.Element; import com.workfusion.vds.sdk.api.nlp.model.Token; feature extractor analyze cell >token covered covering relations, so index will be added. BIDIRECTIONAL index type is required, because feature extractors search cells by token, and tokens by cells. @Indexes() @FeatureName(NextCellContainsKeyword.FEATURE_NAME) public class NextCellContainsKeyword implements FeatureExtractor { public static final String FEATURE_NAME = \"NextTokenInCellContainsKeyword\"; private String keyword; public NextCellContainsKeyword(String keyword) { super(); specify keyword this.keyword = keyword; } @Override public Collection extract(Document document, T element) { List result = new ArrayList(); find parent cells on current token List cells = document.findCovering(Cell.class, element); if (!cells.isEmpty()) { first cell will be a target cell Cell firstCell = cells.stream().findFirst().get(); find all tokens inside cell Collection nextTokens = document.findCovered(Token.class, element.getEnd(), firstCell.getEnd()); nextTokens.forEach(token > { if one of the token in cell contains a keyword, add a feature if (token.getText().contains(keyword)) { Feature feature = new Feature(FEATURE_NAME, 1.0); result.add(feature); } }); } return result; } } Global Context Features: the global information such as the location within specific element (in a line cell table block page). Examples: check whether a token is on the same page with some keywords NERs; determines whether any keyword(s) of the user provided list are found in the blocks to the left of focus; check whether number of tokens of needed type in all tables is equal to N; define if token is in the longest row of all the tables in the document; check whether number of cells in focus table is equal to N; Example 3 The following example defines how many times the text from the focus element is met in the Document. The OnDocumentStart** method is used to prepare data for extraction. Define how many times the target element is met in document import java.util.ArrayList; import java.util.Collection; import java.util.HashMap; import java.util.List; import java.util.Map; import com.workfusion.vds.sdk.api.nlp.annotation.OnDocumentComplete; import com.workfusion.vds.sdk.api.nlp.annotation.OnDocumentStart; import com.workfusion.vds.sdk.api.nlp.fe.Feature; import com.workfusion.vds.sdk.api.nlp.fe.FeatureExtractor; import com.workfusion.vds.sdk.api.nlp.fe.annotation.FeatureName; import com.workfusion.vds.sdk.api.nlp.model.Document; import com.workfusion.vds.sdk.api.nlp.model.Element; @FeatureName(TextOccurrenceInDocument.FEATURE_NAME) public class TextOccurrenceInDocument implements FeatureExtractor { public static final String FEATURE_NAME = \"ToknenMetInDocument\"; private Map textOccurrence = new HashMap(); private int totalFocusElementNumber; @OnDocumentStart public void onStart(Document document, Class focusClass) { find all focus elements (by default focusElement is Token) Collection elements = document.findAll(focusClass); store a total number of elements in document. it will be used for feature normalization totalFocusElementNumber = elements.size(); Map textOccurrence = new HashMap(); calculate how many times text present into the document for (T element : elements) { Integer occurrence = textOccurrence.get(element.getText()); if (occurrence == null) { occurrence = 0; } else { occurrence ; } textOccurrence.put(element.getText(), occurrence); } } @Override public Collection extract(Document document, T element) { List result = new ArrayList(); get precalculated number of how many times text of element is met in the document Integer number = textOccurrence.get(element.getText()); it is strongly recommended to use normalized features (values between 0 and 1), divide to the total number of elements in document to get feature value normalized double featureValue = (double)number totalFocusElementNumber; add feature result.add(new Feature(FEATURE_NAME, featureValue)); return result; } @OnDocumentComplete public void complete() { clear map after usage textOccurrence.clear(); } } Named Entity Features: whether a word is a part of any relative Named Entities (NERs), the start middle end of NER, the distance to a specific NER. check if the text of the focus annotation is fully contained within the text of NERs; check whether distance between token and specific NER equals to N characters; determine if focus table contains full address (>=1 each of city, state, and postal code NERs). Example 4 In the example below, feature will be created with value equal to 1, in case element is covered by NER of specific type. Define if target element covered by NER import java.util.ArrayList; import java.util.Collection; import java.util.Collections; import java.util.List; import com.workfusion.vds.sdk.api.nlp.fe.Feature; import com.workfusion.vds.sdk.api.nlp.fe.FeatureExtractor; import com.workfusion.vds.sdk.api.nlp.fe.annotation.FeatureName; import com.workfusion.vds.sdk.api.nlp.fe.annotation.Index; import com.workfusion.vds.sdk.api.nlp.fe.annotation.IndexType; import com.workfusion.vds.sdk.api.nlp.fe.annotation.Indexes; import com.workfusion.vds.sdk.api.nlp.model.Document; import com.workfusion.vds.sdk.api.nlp.model.Element; import com.workfusion.vds.sdk.api.nlp.model.NamedEntity; feature extractor logic analyze Token >Named Entity relations, to increase performance index is recommended @Indexes() @FeatureName(IsNEPresent.FEATURE_NAME) public class IsNEPresent implements FeatureExtractor { public static final String FEATURENAME = \"isnamed_entity\"; private final String mentionType; public IsNEPresent(String mentionType) { specify required named entity type this.mentionType = mentionType; } @Override public Collection extract(Document document, T element) { List result = new ArrayList(); find all named entities inside token List namedEntity = document.findCovering(NamedEntity.class, element); if (namedEntity.stream() .filter(n > mentionType.equalsIgnoreCase(n.getType())) .findAny() .isPresent()) { if named entity with required type exist, then return a feature return Collections.singletonList(new Feature(FEATURE_NAME, 1)); } return Collections.emptyList(); } } All the Features listed above can be used in any combinations. Run training and evaluate results When you added necessary features, then you can start model training for the field. Once training is finished you should evaluate it. :::note Model evaluation should be implemented on unseen data set. Otherwise model evaluation results will be overstated. ::: "},{"version":"10.0","date":"Sep-20-2019","title":"feature-extractors","name":"Feature Extractors","fullPath":"iac/automl/automl-sdk/feature-extractors","content":" A Feature Extractor is an AutoML SDK component that has in built logic to analyze each Token in a Document and produce a set of independent and discriminating features). A Feature represents a simple question such as \"Is the current cell located in the first column \" with a clear non ambiguous answer \"Yes No\" or \"1 0\". Though, it is also possible to store double values as an answer, such as \"0.25\" to a question like \"How similar is the text of this token to the particular keyword (like \"total\") \". Features are used by the AutoML algorithm to predict which Tokens should be tagged as correct answers. Results of a Feature Extractor may look like this: CellInFirstColumn = 1 or SimilarityTo_Total = 0.25. This stage of the pipeline is crucial as the quality and quantity of the produced features have a major influence on the overall quality of the model. :::tip AutoML SDK contains an TBD in built set of Feature Extractors available via TBD AutoML SDK API. Make sure to have a look before proceeding with the next steps, such as Feature Extractor creation or TBD Feature engineering. ::: The following figure shows the way two Feature Extractors (IsFirstNameFE and IsDateFE) increment through the Sentence element \"Peter Nilson 17 October 1937\". In this example Feature Extractors take one Token into focus at time, and then extract a Feature if it corresponds to the Feature Extractor logic. As negative values are typically omitted the final result will look as follows. Token FE Value : : Peter IsFirstNameFE = 1 17 IsDate = 1 Creating a Feature Extractor If none of the out of the box Feature Extractors satisfy your need, create a new one. Implement the FeatureExtractor interface which contains one required method extract(). FeatureExtractor implementation can have optional methods annotated with a corresponding Java annotation as well. These methods represent different stages of the FeatureExtractor lifecycle: OnInit OnDocumentStart OnDocumentComplete OnDestroy etc. For complete usage information, refer to FeatureExtractor documentation. :::warning All fields of a Feature Extractor must be serializable. Although, the Feature Extractor does not have to implement the Serializable interface. Keep in mind that lambdas are not serializable by default. ::: If you cannot avoid using non serializable fields for some reason, then you can do the following trick: Define a field and keep it null. Initialize the field in the @OnInit method. Do not initialize the field in a constructor. Feature Extractor with non serializable date formatter Your Feature Extractor converts dates from one format to another with the help of DateTimeFormatter which is not serializable. The correct initialization of the Feature Extractor would be as follows. public class MyFeatureExtractor implements FeatureExtractor { private DateTimeFormatter dateTimeFormatter; @OnInit public void init() { dateTimeFormatter = DateTimeFormatter.ofPattern(\"dd MM yyyy\"); } * The Feature Extractor implementation goes here * } Feature Extractor with a lambda Let's say, you want to create a Feature Extractor that has a function as a class member. In your Feature Extractor, you want to refer to one of the functions using a lambda. Then, in order to avoid issues with serialization the code should look the following way. public class MyFeatureExtractor implements FeatureExtractor { private Function function; @OnInit public void init() { function = e > e.getBegin() } * The FeatureExtractor implementation goes here * } Extract The extract() method applies the main algorithm for feature extraction which performs the analysis of the provided Document structure, and then extracts features for a specified Token element. The extract() method is called for each Token and returns a list of features. The extraction process is parallel for all fields in a Document. The following Feature Extractor produces a feature when a Token contains only digits. import java.util.ArrayList; import java.util.Collection; import java.util.List; import com.workfusion.vds.sdk.api.nlp.fe.Feature; import com.workfusion.vds.sdk.api.nlp.fe.FeatureExtractor; import com.workfusion.vds.sdk.api.nlp.model.Document; import com.workfusion.vds.sdk.api.nlp.model.Element; public class DigitsOnlyFE implements FeatureExtractor { private final static String FEATURE_NAME = \"DigitsOnly\"; @Override public Collection extract(Document document, T element) { List features = new ArrayList(); if (element.getText().matches(\" 0 9 \")) { features.add(new Feature(FEATURE_NAME, 1.0)); } return features; } } OnInit OnInit is a method level annotation for FeatureExtractor used to define a method that should be invoked after the constructor. A method annotated with OnInit should follow these conventions: Called only once when a FeatureExtractor instance is created. Accepts a Map of parameters. These parameters are context specific and typically not required for a common Feature Extractor. Accepts a special builder for index caching and type of focus Element, by default — Token. For details, refer to the Indexes and Cache section. Method is optional. A method annotated with OnInit is typically used to initialize regex patterns, add dynamic cached indexes, etc. :::warning If a class has more than one @OnInit method, an exception will be thrown. If both a class and its sub class have @OnInit method, then only the method in sub class is invoked. Use super().parentInitMethodName() to invoke the parent method as well. In case, @OnInit creates complex memory collections, clean them inside the @OnDestroy method. See the following section for details. ::: OnDestroy A method annotated with OnDestroy follows these conventions: Called only once when the information extraction process is fully complete. Used to release resources by removing objects references, that the OnDestroy method is holding. Method is optional. :::warning If a class has more than one @OnDestroy method, an exception will be thrown. If both a class and its sub class have @OnDestroy method, then only the method in sub class is invoked. Use super().parentDestroyMethodName() to invoke the parent method as well. ::: The following example finds the similarity score between the element text and all keywords from a dictionary file. OnInit and OnDestroy methods are used to read this dictionary file. The target Token is compared with data from the dictionary file using Levenshtein distance as a string similarity algorithm. import java.io.InputStreamReader; import java.io.Reader; import java.util.ArrayList; import java.util.Collection; import java.util.List; import org.apache.commons.io.IOUtils; import com.workfusion.vds.nlp.similarity.StringSimilarityUtils; import com.workfusion.vds.sdk.api.exception.SdkException; import com.workfusion.vds.sdk.api.nlp.annotation.OnDestroy; import com.workfusion.vds.sdk.api.nlp.annotation.OnInit; import com.workfusion.vds.sdk.api.nlp.fe.Feature; import com.workfusion.vds.sdk.api.nlp.fe.FeatureExtractor; import com.workfusion.vds.sdk.api.nlp.model.Document; import com.workfusion.vds.sdk.api.nlp.model.Element; public class DictionarySimilarityFE implements FeatureExtractor { public static final String FEATURE_NAME = \"SimilarityToKeyword\"; private String pathToResource; private List keywords = new ArrayList(); public DictionarySimilarityFE(String pathToResource) { this.pathToResource = pathToResource; } @OnInit public void init() { read resource from classpath and collect keywords for future processing. try (Reader reader = new InputStreamReader(this.getClass().getResourceAsStream(pathToResource))) { keywords = IOUtils.readLines(reader); } catch (Exception e) { throw new SdkException(String.format(\"Error reading file %s.\", pathToResource), e); } } @Override public Collection extract(Document document, T element) { List result = new ArrayList(); for (String keyword : keywords) { calculate similarity between element text and keyword using Similarity Utils double similarityScore = StringSimilarityUtils.levenshtein(element.getText(), keyword); aggregate constant FEATURE_NAME and keyword into final feature name String featureName = String.join(\"\", FEATURENAME, keyword); add a feature result.add(new Feature(featureName, similarityScore)); } return result; } @OnDestroy public void destroy() { clean collection after usage keywords.clear(); } } OnDocumentStart A method annotated with OnDocumentStart follows these conventions: Called only once for each document before extract() method. Accepts a Document and element type (Token by default) to perform feature extraction. Method is optional. Typical usage: If the feature extraction logic requires to analyze large structures or entire documents for each Token, it is a good practice to do pre calculations on the Document level, and then use it with the extract() method to improve the performance. :::warning If a class has more than one @OnDocumentStart method, an exception will be thrown. If both a class and its sub class have @OnDocumentStart method, then only the method in sub class is invoked. Use super().parentMethodName() to invoke the parent method as well. In case, the @OnDocumentStart method creates complex memory collections, clean them inside the @OnDocumentDestroy method. See the following section for details. ::: All class level variables initialized inside the OnDocumentStart method should be used in read only mode inside the extract() method, because this method can be executed in multiple threads. OnDocumentComplete A method annotated with OnDocumentComplete follows these conventions: Called only once for a whole Document to release the resources, while the extract() method is called for all Tokens in a Document. Method is optional. :::warning If a class has more than one @OnDocumentComplete method, an exception will be thrown. If both a class and its sub class have @OnDocumentComplete method, then only the method in sub class is invoked. Use super().parentMethodName() to invoke the parent method as well. ::: :::note Both OnDocumentStart and OnDocumentComplete are used to prepare finalize data for a Document. Usually OnDocumentStart will be used to prepare big data from a full document to create local cache. This data will be consumed inside the extract() method to improve the speed of feature extraction. ::: The following example defines how many times the text from the focus element is met in the Document. The OnDocumentStart method is used to prepare data for extraction. import java.util.ArrayList; import java.util.Collection; import java.util.HashMap; import java.util.List; import java.util.Map; import com.workfusion.vds.sdk.api.nlp.annotation.OnDocumentComplete; import com.workfusion.vds.sdk.api.nlp.annotation.OnDocumentStart; import com.workfusion.vds.sdk.api.nlp.fe.Feature; import com.workfusion.vds.sdk.api.nlp.fe.FeatureExtractor; import com.workfusion.vds.sdk.api.nlp.model.Document; import com.workfusion.vds.sdk.api.nlp.model.Element; public class TextOccurrenceInDocument implements FeatureExtractor { public static final String FEATURE_NAME = \"TokenMetInDocument\"; private Map textOccurrence = new HashMap(); private int totalFocusElementNumber; @OnDocumentStart public void onStart(Document document, Class focusClass) { Find all focus elements (by default, focusElement is Token) Collection elements = document.findAll(focusClass); Store a total number of elements in a Document. Used for feature normalization totalFocusElementNumber = elements.size(); Map textOccurrence = new HashMap(); Calculate the number of text occurrences in the Document for (T element : elements) { Integer occurrence = textOccurrence.get(element.getText()); if (occurrence == null) { occurrence = 0; } else { occurrence ; } textOccurrence.put(element.getText(), occurrence); } } @Override public Collection extract(Document document, T element) { List result = new ArrayList(); Get a pre calculated number of text occurrences in the Document Integer number = textOccurrence.get(element.getText()); We strongly recommend using normalized features (values between 0 and 1). To normalize a feature value, divide it by the total number of elements in a Document. double featureValue = (double)number totalFocusElementNumber; Add a feature result.add(new Feature(FEATURE_NAME, featureValue)); return result; } @OnDocumentComplete public void complete() { Clear the map after usage. textOccurrence.clear(); } } Indexes and Cache The feature extraction process analyzes the location and elements around the analyzed focus element. To perform the analysis, use findCovered() and findCovering() methods on a Document to find all the elements inside or outside the processed element. These operations usually require significant time to be processed. To improve the performance, add an @Indexes declaration on the class level. In this case, the Document will store the results of all \"covered covering\" relations before the feature extraction process. When the FeatureExtractor invokes the findCovering findCovered method and has a corresponding index, it produces the result from cache without any calculation. See the following example for details. import java.util.Collection; import java.util.List; import com.workfusion.vds.sdk.api.nlp.annotation.DependsOn; import com.workfusion.vds.sdk.api.nlp.fe.Feature; import com.workfusion.vds.sdk.api.nlp.fe.FeatureExtractor; import com.workfusion.vds.sdk.api.nlp.fe.annotation.FeatureName; import com.workfusion.vds.sdk.api.nlp.fe.annotation.Index; import com.workfusion.vds.sdk.api.nlp.fe.annotation.IndexType; import com.workfusion.vds.sdk.api.nlp.fe.annotation.Indexes; import com.workfusion.vds.sdk.api.nlp.model.Document; import com.workfusion.vds.sdk.api.nlp.model.Element; import com.workfusion.vds.sdk.api.nlp.model.Line; import com.workfusion.vds.sdk.api.nlp.model.Token; @DependsOn({Line.class, Token.class}) @Indexes({ @Index(covering = Line.class, covered = Token.class, type = IndexType.BIDIRECTIONAL) }) @FeatureName(SimilarityKeysInNextLineFE.FEATURE_NAME) public class SimilarityKeysInNextLineFE implements FeatureExtractor { public final static String FEATURE_NAME = \"similarityInNextLine\"; @Override public Collection extract(Document document, Token element) { Lines to be taken from cache List lines= document.findCovering(Line.class, element); Create features here } } Alternatively, you can declare an initialization method inside the FeatureExtractor class. See the following example for details. import java.util.Collection; import java.util.List; import com.workfusion.vds.sdk.api.nlp.annotation.OnInit; import com.workfusion.vds.sdk.api.nlp.cache.CacheBuilder; import com.workfusion.vds.sdk.api.nlp.fe.Feature; import com.workfusion.vds.sdk.api.nlp.fe.FeatureExtractor; import com.workfusion.vds.sdk.api.nlp.model.Document; import com.workfusion.vds.sdk.api.nlp.model.Element; public class CustomFeatureExtractor implements FeatureExtractor { private Class searchClass; public CustomFeatureExtractor(Class searchClass) { super(); This Feature Extractor can be configured to search for Tokens (Type T) inside the elements with Type S. For example, Table, Line, Cell, Sentence, etc. this.searchClass = searchClass; } @OnInit public void init(CacheBuilder builder, Class focusClass) { builder.covering(focusClass, searchClass); } @Override public Collection extract(Document document, T element) { Lines will be taken from cache List parentElements = document.findCovering(searchClass, element); Create features here } } We strongly recommend adding an @Indexes declaration in the following cases: A Feature Extractor uses findAllCovered or findAllCovering methods. A Feature Extractor frequently uses findCovered or findCovering applied to different elements. In this case, @Indexes should contain a declaration of the covered covering element type, otherwise the Feature Extractor will use a basic focus type — Token. Additional materials Out of the box feature extractors Feature engineering "},{"version":"10.0","date":"Jul-08-2019","title":"introduction","name":"Introduction","fullPath":"iac/automl/automl-sdk/introduction","content":" AutoML SDK is a WorkFusion Java framework for creating and customizing Machine Learning models. AutoML SDK has the following features: Model customization by configuring its components: Annotators, Feature Extractors, Algorithms, etc. Post Processing—normalization and validation logic, fixing OCR errors—to improve model results. Run TBD evaluation extract process and publish improved model. Results analytics and statistics. AutoML SDK is a powerful tool for TBD Classification and TBD Information Extraction use cases. "},{"version":"10.0","date":"Sep-20-2019","title":"local-run","name":"Local Run","fullPath":"iac/automl/automl-sdk/local-run","content":" Once you developed the model and trained it, you should run extraction locally to get extraction statistics. Using extraction statistics you can analyze model quality and decide whether you need improve it. For that purpose you should use the ModelExecutionRunner class, which is generated by the archetype. Here you can find main method where you should provide the following paths: Trained model folder (model folder from the output) Data set folder Output folder import java.nio.file.Path; import java.nio.file.Paths; import java.util.HashMap; import java.util.Map; import com.workfusion.vds.sdk.run.ModelRunner; import com.workfusion.vds.sdk.run.config.LocalExecutionConfiguration; public class ModelExecutionRunner { public static void main(String args) throws Exception { Path trainedModelPath = Paths.get(\" home post processing trained model \"); Path inputFolderPath = Paths.get(\" home post processing input \"); Path outputFolderPath = Paths.get(\" home post processing output \"); Map parameters = new HashMap(); LocalExecutionConfiguration configuration = LocalExecutionConfiguration.builder() .inputDir(inputFolderPath) .outputDir(outputFolderPath) .trainedModelDir(trainedModelPath) .parameters(parameters) .build(); ModelRunner.run(ExampleIeModel.class, configuration); } } :::warning If you use Windows on your machine, take the following conditions into account: Your Java, IDE and working folder should be installed on the same logical drive (e.g. C). Do not provide the drive name in the path, use \" home data set \" instead of \"C: home data set \". ::: After execution, you will find following structure in the output folder: model result — folder contains .JSON files with results of model extraction. processing result — folder with .JSON file where results of post processing are stored. statistics.csv — file with statistics. Inside the ModelRunner class you can find an instance of ProcessingRunner. This is a main class which executes extraction and post processing logic on your local machine. We expect that data set folder contains set of separate files (documents), but if you have only .CSV file (e.g. snapshot from Manual Task) instead of calling the run method you can call the runFromCsv method providing a path to the .CSV file and the name of the column which contains document content (html, xml, text). Our library also contains the utility class TrainingSetUtils, which provides useful methods to convert data set from to .CSV and txt files. Statistics Let's look closer at statistics.csv file. Using this file you can analyze model extraction results and post processing output. For each file in a data set there is a separate line. For each field in a data set document there is 7 columns where ` is a corresponding field name e.g. invoice_amount`. Column Name Description 1 _gold Value from data set file. 2 _extracted Value extracted by trained model. 3 postprocessed Value after post processing. 4 modelscore Score provided by model during extraction. 5 finalscore Score changed by post processing (same as the model score if was not changed). 6 errortype Type of error (possible values: TP, TN, FP, FN). 7 reason Reason why error occurred, if any. Possible values: EXTRACTMODEL, EXTRACTRULES, FAILMODEL, FAIL_RULES. Once you updated the model (annotators, feature extractors or post processing), you should train model again, run extract and analyze updated statistics. Training a Model With Limited Resources In some cases, you may need to train a model having limited CPU resources and less then 8GB RAM. For that case, we recommend to prepare a ModelTrainingRunner class to organize training field by field nested into a try catch block. Please note, this approach takes more time. package ru.sberbank.run; import java.nio.file.Path; import java.nio.file.Paths; import java.util.ArrayList; import java.util.HashMap; import java.util.List; import java.util.Map; import com.workfusion.nlp.uima.pipeline.constants.ConfigurationConstants; import com.workfusion.vds.sdk.api.nlp.configuration.FieldInfo; import com.workfusion.vds.sdk.api.nlp.configuration.FieldType; import com.workfusion.vds.sdk.run.ModelRunner; import com.workfusion.vds.sdk.run.config.LocalTrainingConfiguration; import ru.sberbank.model.RussianIE; public class ModelTrainingRunner { public static void main(String args) throws Exception { System.setProperty(\"WORKFLOWLOGFOLDER\", \". logs \"); System.setProperty(\"EVALWORKINGDIRISUNDEFINED\", \". eval \"); List fields = new ArrayList(); Path inputDirPath = Paths.get(\"D: generic input\"); String outputPathFolder = \"D: dev train generic\"; LocalTrainingConfiguration configuration = null; String fieldName = \"\"; FieldType type = FieldType.FREE_TEXT; Map parameters = new HashMap(); parameters.put(ConfigurationConstants.HPOTIMELIMIT, 60 * 60 * 4); try { train with answer type Address fieldName = \"full_address\"; type = FieldType.ADDRESS; fields.add(new FieldInfo.Builder(fieldName).type(type).build()); configuration = LocalTrainingConfiguration.builder().inputDir(inputDirPath).outputDir(Paths.get(outputPathFolder \" model\" \"\" fieldName)) .fields(fields).parameters(parameters).build(); ModelRunner.run(RussianIE.class, configuration); fields.clear(); } catch (Exception ex) { System.out.println(ex); } try { train with answer type Free Text the same field fieldName = \"full_address\"; type = FieldType.FREE_TEXT; fields.add(new FieldInfo.Builder(fieldName).type(type).build()); configuration = LocalTrainingConfiguration.builder().inputDir(inputDirPath).outputDir(Paths.get(outputPathFolder \" model\" type \"\" fieldName)).fields(fields).parameters(parameters).build(); ModelRunner.run(RussianIE.class, configuration); fields.clear(); } catch (Exception ex) { System.out.println(ex); } try { train with answer type Number for total amount field fieldName = \"total_amount\"; type = FieldType.NUMBER; fields.add(new FieldInfo.Builder(fieldName).type(type).build()); configuration = LocalTrainingConfiguration.builder().inputDir(inputDirPath).outputDir(Paths.get(outputPathFolder \" model\" type \"\" fieldName)).fields(fields).parameters(parameters).build(); ModelRunner.run(RussianIE.class, configuration); fields.clear(); } catch (Exception ex) { System.out.println(ex); } } } "},{"version":"10.0","date":"Sep-20-2019","title":"getting-started","name":"Developing with AutoML SDK","fullPath":"iac/automl/automl-sdk/getting-started","content":" This page is a good place to start with and refer to when developing with AutoML SDK. It explains the process of development and publishing a model on a high level with referring to more detailed documents. Create New Model First, let's create an AutoML SDK project to start developing a new ML model. Proceed with these steps: Make sure you have access to an AutoML SDK installation. Either from WorkFusion Maven repository, or your local on premise repository. Configure the local environment. For details, refer to Environment Configuration. Configure the project as described in Project Configuration. Implementation Now that you have a ready to go AutoML SDK project, implement a generic model. Do the following steps: Configure ModelTrainingRunner to extract the necessary fields. Run ModelTrainingRunner. Check model statistics in the output model results stats folder. Proceed with TBD Step 4 of the ML Engineer Manual. In some cases, you may need to improve some field( s) in an already existing trained model. Re training the whole model is a time consuming operation. To save time, you can improve a field and then replace the old field artifact folder with a new one. If your model needs some improvements, consider the following steps: Make a separate copy of your AutoML SDK project. Import fixed serialized configuration( s) to the AutoML SDK configuration file for all fields. AutoML skips smart selection of components and thus significantly reduces time of further re training. Comment all fields in your ModelTrainingRunner, uncomment only the field( s) that needs to be improved. :::tip If your model extracts fields but they are formatted incorrectly, consider adding Post Processors to your AutoML SDK configuration. If your model extracts wrong fields, consider adding Feature Extractors and or Annotators to your AutoML SDK configuration. Run ModelTrainingRunner with your updated AutoML SDK configuration. Proceed with TBD Step 4 of the ML Engineer Manual, if you're satisfied with the result. ::: Reuse Existing Model Developing or reusing the existing model is almost identical to the way a new model is created. The difference is that instead of configuring a new project you'll need to import an existing one. Proceed with these steps to start developing the existing model: Make sure you have access to an AutoML SDK installation. Either from WorkFusion Maven repository, or your local on premise repository. Configure the local environment. For details, refer to Environment Configuration. Import your existing AutoML SDK project to WorkFusion Studio. Then, either use the model as is, or make improvements as described above. Publish and Execute After you successfully implemented a new model or reused an already existing one, you need to publish it for production purposes. This means you need to copy the resulting artifacts to a remote file storage and make it available on a remote Control Tower for further usage. Your trained model project may look like this. ProjectForTest ├── data ├── deploy └── ProjectForTest └── 1.0 ├── config ├── lib └── model.description ├── process ├── results ├── extract └── training ├── input ├── output ├── process └── work ├── src ├── target └── pom.xml There are two options to publish your model: Publish only model configuration. Training will be performed on a remote Control Tower from scratch. This takes more time, but once the training is complete, your model is available for execution right away. Publish a trained model with all training artifacts. This allows to avoid model re training but requires extra manual S3 bucket manipulations. Publish Model Configuration Publish the model to S3 (or your on premise storage) by proceeding with these steps: Run maven install using the pom.xml file in the root of the project to generate the deploy folder. Copy the deploy ProjectForTest folder to s3: vds models . Add a new record with a value ProjectForTest:1.0 to the s3: vds models models. file. This will make your model visible in Control Tower configured to work with this S3 bucket. Check model availability in Control Tower. In Manual Task, click the Configure AutoML tab. Your model is now available in the Machine Learning Pipelines menu. Alternatively, check model availability via TBD API by sending a listHyperModels request. Your newly published model configuration should now be available on the list of models. At this stage, you can configure a training in Control Tower using your new model. In Manual Task, click the Configure AutoML tab, select Train new model, and then select your Machine Learning Model. Following the configuration below you will use only the model's configuration. Model training will be performed on a remote Control Tower instance from scratch. Publish Trained Model To avoid remote re training and use the model for extraction classification right away, you can publish your trained model directly to S3. Follow this procedure: Publish Model Configuration as described above. Copy the training output folder to s3: vds resources . :::note Note, that the name of the S3 folder (in this case, training) should be exactly the same as in your AutoML SDK project in WorkFusion Studio. Make sure, there are no backslashes ( ) in the path to parameters.json. In case, there are, replace them with a regular slash ( ). ::: Execute Model To use your model in a Business Process, add this new model to an AutoML Business Process in Control Tower. At this point, you can create a new Manual Task or reuse an existing one. In Manual Task, navigate to the Configure AutoML tab, and then fill in all fields using the rules below. Machine Learning Model dropdown menu fetches a title from the model.description file of your AutoML SDK project. Make sure to give it a meaningful name before you deploy your model. Training Model ID is the name of the folder from your project in WorkFusion Studio (in this case, training) which was copied to s3: vds resources . Group ID is combined results for Information Extraction. Click Save. Click on the AutoML tab, and then click Activate Cognitive Bot. This opens an Automation BP wizard. "},{"version":"10.0","date":"Jul-08-2019","title":"model-configuration","name":"Model Configuration","fullPath":"iac/automl/automl-sdk/model-configuration","content":" To make the most of AutoML SDK, components need to be used together in one configuration. The main hyper model class located at ModelPackage model ModelName Model.java contains a @HypermodelConfiguration annotation with reference to the AutoML SDK configuration class. import com.workfusion.vds.nlp.hypermodel.ie.generic.GenericIeHypermodel; import com.workfusion.vds.sdk.api.hypermodel.annotation.HypermodelConfiguration; @HypermodelConfiguration(TrainingIeModelConfiguration.class) public class TrainingIeModel extends GenericIeHypermodel { } While configuring a model ML Engineer should not edit the main hyper model class. All configuration should be done in TrainingIeModelConfiguration.java as in the following example. The following code is generated for a model configuration in case the parameter importConfigurationFromGenericModel is set to N. Generated model configuration contains 3 methods. Each of these methods provides list of AutoML SDK components specified by type: Feature Extractors, Annotators and Post Processors. import java.util.ArrayList; import java.util.Arrays; import java.util.List; import com.workfusion.vds.sdk.api.hypermodel.annotation.ModelConfiguration; import com.workfusion.vds.sdk.api.hypermodel.annotation.Named; import com.workfusion.vds.sdk.api.nlp.annotator.Annotator; import com.workfusion.vds.sdk.api.nlp.configuration.IeConfigurationContext; import com.workfusion.vds.sdk.api.nlp.fe.FeatureExtractor; import com.workfusion.vds.sdk.api.nlp.model.Document; import com.workfusion.vds.sdk.api.nlp.model.Element; import com.workfusion.vds.sdk.api.nlp.model.IeDocument; import com.workfusion.vds.sdk.api.nlp.processing.Processor; import com.workfusion.vds.sdk.nlp.component.annotator.tokenizer.SplitterTokenAnnotator; @ModelConfiguration public class TrainingIeConfiguration { @Named(\"annotators\") public List> getAnnotators(IeConfigurationContext context) { TODO configure annotators here. List> annotators = new ArrayList(); annotators.add(new SplitterTokenAnnotator(\" s\")); return annotators; } @Named(\"featureExtractors\") public List> getFeatureExtractors(IeConfigurationContext context) { TODO configure feature extractors here. return Arrays.asList(new FeatureExtractorExample()); } @Named(\"processors\") public List> getProcessors() { TODO configure post processors here. return Arrays.asList(new PostProcessorExample()); } } In case the parameter importConfigurationFromGenericModel is set to Y the following annotation will be added to the configuration class to provide a reference to the generic Information Extraction model. import com.workfusion.vds.nlp.hypermodel.ie.generic.config.GenericIeHypermodelConfiguration; import com.workfusion.vds.sdk.api.hypermodel.annotation.Import; import com.workfusion.vds.sdk.api.hypermodel.annotation.ModelConfiguration; @ModelConfiguration @Import(configurations = { @Import.Configuration(GenericIeHypermodelConfiguration.class) reference to generic information extraction model }) public class TrainingIeConfiguration { ... the same content } For Information Extraction ML Engineer analyzes the data set and specifies the list of fields that should be extracted (e.g. address, invoice number, email, etc.). During model development different fields will require different logic to extract values from documents and produce features for the ML algorithm. This means different Feature Extractors and Annotators should be applied to different fields. To deal with this, each method needs an injected instance of a ConfigurationContext class which contains information about the current field. This way the method can produce different sets of components for different fields as the configuration class will be executed multiple times, once for each field. In the following example Annotators SplitterTokenAnnotator and EntityBoundaryAnnotator are added to all field configurations, and special Annotators are added for fields invoicenumber and totalamount. For more information, refer to AutoML SDK Configuration page. import java.util.ArrayList; import java.util.List; import com.workfusion.vds.sdk.api.hypermodel.annotation.ModelConfiguration; import com.workfusion.vds.sdk.api.hypermodel.annotation.Named; import com.workfusion.vds.sdk.api.nlp.annotator.Annotator; import com.workfusion.vds.sdk.api.nlp.configuration.IeConfigurationContext; import com.workfusion.vds.sdk.nlp.component.annotator.EntityBoundaryAnnotator; import com.workfusion.vds.sdk.nlp.component.annotator.ner.BaseRegexNerAnnotator; import com.workfusion.vds.sdk.nlp.component.annotator.tokenizer.SplitterTokenAnnotator; @ModelConfiguration public class TrainingIeModelConfiguration { @Named(\"annotators\") public List annotators(IeConfigurationContext context) { List annotators = new ArrayList(); annotators.add(new EntityBoundaryAnnotator()); annotators.add(new SplitterTokenAnnotator(\"( s: _;' )\")); switch (context.getField().getCode()) { case \"invoice_number\": { annotators.add(BaseRegexNerAnnotator.getJavaPatternRegexNerAnnotator(\"invoice_number\", \"( d {10,11})\")); break; } case \"total_amount\": { annotators.add(BaseRegexNerAnnotator.getJavaPatternRegexNerAnnotator(\"amount\", \"0 9l {1,3} , . 0 9 {2,3} .*\")); break; } } } } "},{"version":"10.0","date":"Jul-08-2019","title":"model-execution","name":"Model Execution","fullPath":"iac/automl/automl-sdk/model-execution","content":" To check model quality, you should prepare a test set of documents and run the execution process. ModelEvaluationRunner.java contains similar logic as the training runner. Once the execution process is finished, model results are available in the output directory. Statistics are located in outputDir statistics.csv and contain information about what actually was extracted by the model against what was expected. For more information about runners, see the Local Run page. import java.nio.file.Path; import java.nio.file.Paths; import java.util.HashMap; import java.util.Map; import com.workfusion.vds.sdk.run.ModelRunner; import com.workfusion.vds.sdk.run.config.LocalExecutionConfiguration; public class ModelExecutionRunner { public static void main(String args) throws Exception { System.setProperty(\"WORKFLOWLOGFOLDER\", \". logs \"); TODO put correct values for the paths Path trainedModelPath = Paths.get(\"model train training output model\"); Path inputDirPath = Paths.get(\"model docs\"); Path outputDirPath = Paths.get(\"model extract\"); TODO add parameters if needed. Map parameters = new HashMap(); LocalExecutionConfiguration configuration = LocalExecutionConfiguration.builder() .inputDir(inputDirPath) .outputDir(outputDirPath) .trainedModelDir(trainedModelPath) .parameters(parameters) .build(); ModelRunner.run(IeModelModel.class, configuration); } } "},{"version":"10.0","date":"Jul-31-2019","title":"ootb-fes","name":"OOTB Feature Extractors","fullPath":"iac/automl/automl-sdk/ootb-fes","content":" The majority of all use case implementations apply a number of common Feature Extractors most of which are reusable. AutoML SDK contains most used Feature Extractors optimized and ready out of the box with AutoML SDK API. These Feature Extractors are intended to cover general use cases without any need to write your own from scratch. This page briefly describes pre made Feature Extractors providing links to usage documentation and visualizing feature extraction in some cases. AreAllFollowingTokensCapitalizedBeforeNerTypeFE AreAllFollowingTokensCapitalizedBeforeNerTypeFE class determines if ALL tokens between the focus Element and the first following Named Entity with the given type are capitalized. For usage details, refer to AreFollowingTokensCapitalizedBeforeNerFE documentation. BaseKeywordsInElementFE BaseKeywordsInElementFE is a base class for extracting features from the search element that covers the target focus element. It is a base abstract class for FocusElementKeywordsFE and KeywordsFrequencyInElementFE implementations. For usage details, about the base class refer to BaseKeywordsInElementFE documentation. FocusElementKeywordsFE FocusElementKeywordsFE class produces a feature if the search element that covers the current focus element contains at least one of the keywords. For usage details, refer to FocusElementKeywordsFE documentation. FocusElementKeywordsFrequencyFE KeywordsFrequencyInElementFE class counts the amount of keywords inside a search element that covers the current focus element. For usage details, refer to FocusElementKeywordsFrequencyFE documentation. BaseNerInFocusElementFE BaseNerInFocusElementFE is a base abstract class for IsFirstNerInFocusElementFE and IsLastNerInFocusElementFE implementations. For usage details, refer to BaseNerInFocusElementFE documentation. The following example shows a typical Sentence in an AutoML SDK Document. IsFirstNerInFocusElementFE IsFirstNerInFocusElementFE class produces a feature for a focus element if it contains exactly the first Named Entity marker of the given type in the specified search element. In other words, it determines if the first Named Entity in the search class is in the user defined focus class. For usage details, refer to IsFirstNerInFocusElementFE documentation. Example In the previous figure IsFirstNerInFocusElementFE with a focus element Token and search element Sentence will produce a feature for Token \"Pinnacle\" because this part of Sentence contains the first Named Entity of type Company. It will also produce features for all Tokens that intersect with the first Named Entities of all types. For Token \"Vlasic\" it will NOT produce a Feature because it intersects with the second (last) Named Entity of type Company. IsLastNerInFocusElementFE IsLastNerInFocusElementFE class produces a feature for a focus element if it contains exactly the last Named Entity marker of the given type in the specified search element. In other words, it determines if the last Named Entity in the search class is in the user defined focus class. For usage details, refer to IsLastNerInFocusElementFE documentation. Example In the previous figure IsLastNerInFocusElementFE with a focus element Token and search element Sentence will produce a feature for Token \"Vlasic\" because it contains the last Named Entity of type Company for this Sentence. If the search element has only one Named Entity of a certain type then it is considered both the first and the last at the same time. IsFocusElementExactSameToNerFE IsFocusElementExactSameToNerFE class determines if the covering focus element (or search class) has the same content as the covered Named Entity. In other words, it produces Features for Tokens that contain exactly the same text as the covered Named Entity. It can also check if the Named Entity is the first in a Document. For usage details, refer to IsFocusElementExactSameToNerFE documentation. IsFocusElementWithNerFE IsFocusElementWithNerFE determines if the search element covering the focus element contains a Named Entity( ies). It can also check if the Named Entity is the first in a Document. In the following example IsFocusElementWithNerFE produces Features for all Tokens that contain a Named Entity in a given search element Sentence. For usage details, refer to IsFocusElementWithNerFE documentation. LowerCaseWordshapeFE LowerCaseWordshapeFE finds the word shape of the covered text removing leading or trailing whitespaces and making all characters lowercase and digits '1'. Example \" ,. abc ABC 123\" > \" ,. abc abc 111\" For usage details, refer to LowerCaseWordshapeFE documentation. PositionInCoveringFE PositionInCoveringFE class gets the index (position) of the focus element in the covering search element. For usage details, refer to PositionInCoveringFE documentation. PreviousElementSameLineKeywordMatchFE PreviousElementSameLineKeywordsMatchFE class determines if there are Keyword matches in the same line in a non empty Element preceding the current Element covered by the focus Line. Search Element (Cell, LayoutBlock, etc.) should be specified in the constructor. The search element is not case sensitive. Example The following figure shows some typical Blocks, Cells, or other Elements in a AutoML SDK Document. PreviousElementSameLineKeywordsMatchFE class with search Element Block and Keywords set \"Vlasic Foods\" will produce a feature for Line 1 in Block 2 because the same line—Line 1 in the previous Block 1—contains the same text. For usage details, refer to PreviousElementSameLineKeywordsMatchFE documentation. SimilarityToKeywordFE SimilarityToKeywords class gets the similarity of a focus element to provided Keywords. For usage details, refer to SimilarityToKeywordsFE documentation. "},{"version":"10.0","date":"Sep-20-2019","title":"parser","name":"Parser","fullPath":"iac/automl/automl-sdk/parser","content":" The first stage of document processing is a Parser — an AutoML SDK component which removes HTML tags from training set documents, and then creates Elements in an AutoML SDK Document based on tags, defining the begin position and end position for each Element. Elements created at this stage: Cell, Line, Page, Row, Table, Tag. :::note This step is configured automatically, there's no need to manually update anything. ::: Table Element is created with all the data within the `` tag. The following figure shows how the data from HTML code is parsed and which elements are created: Two Row elements are created with text inside `` tags, because there are two rows in initial document. Four Cell elements are created with text inside `` tags, because there are four cells in initial document. One Line element is created with text inside the `` tag. "},{"version":"10.0","date":"Jul-08-2019","title":"model-training","name":"Model Training","fullPath":"iac/automl/automl-sdk/model-training","content":" A model generated from an archetype contains a local runner to allow the developer to train the configured model. The runner is located at ... run ModelTrainingRunner.java. The ModelTrainingRunner class contains a single method. You need to provide a path to the dataset and a working directory, and then specify a list of FieldInfo for the Information Extraction use case. When training is finished, the resulting model will be available at workingDir training output model. This path should be passed to the execution runner to check model quality. import java.nio.file.Path; import java.nio.file.Paths; import java.util.ArrayList; import java.util.HashMap; import java.util.List; import java.util.Map; import com.workfusion.vds.sdk.api.nlp.configuration.FieldInfo; import com.workfusion.vds.sdk.api.nlp.configuration.FieldType; import com.workfusion.vds.sdk.run.ModelRunner; import com.workfusion.vds.sdk.run.config.LocalTrainingConfiguration; public class ModelTrainingRunner { public static void main(String args) throws Exception { System.setProperty(\"WORKFLOWLOGFOLDER\", \". logs \"); Configure input and output Path inputDirPath = Paths.get(\"TODO add path to folder with training set\"); Path outputDirPath = Paths.get(\"TODO add path to output folder\"); Configure the fields according to your use case List fields = new ArrayList(); fields.add(new FieldInfo.Builder(\"invoice_number\") .type(FieldType.INVOICE_TYPE) .build()); fields.add(new FieldInfo.Builder(\"total_amount\") .type(FieldType.PRICE) .build()); Add parameters, if needed Map parameters = new HashMap(); LocalTrainingConfiguration configuration = LocalTrainingConfiguration.builder() .inputDir(inputDirPath) .outputDir(outputDirPath) .fields(fields) .parameters(parameters) .build(); Run your model specified by the Hypermodel class and configuration ModelRunner.run(TrainingIeModel.class, configuration); } } Local Model Training Running ModelTrainingRunner To make the runner work, provide the following parameters: Path to the folder with training set files — inputDirPath. Path to the output folder where trained model artifacts will be stored — outputDirPath. In case of Information Extraction, provide a list of FieldInfo fields which represent your IE configuration. Provide your custom model class to ModelRunner.run(). Optional. Add additional parameters (for example, HPO limits, additional training configuration, etc.). Optional. For v9.1 . Add additional inputs, such as dictionaries, reference data, etc. using the addResource(targetFolder, sourceFile Folder) method. Here is an example of a ModelTrainingRunner configuration for one field invoice_number and a corresponding type: import java.nio.file.Path; import java.nio.file.Paths; import java.util.ArrayList; import java.util.HashMap; import java.util.List; import java.util.Map; import com.workfusion.vds.sdk.api.nlp.configuration.FieldInfo; import com.workfusion.vds.sdk.api.nlp.configuration.FieldType; import com.workfusion.vds.sdk.run.ModelRunner; import com.workfusion.vds.sdk.run.config.LocalTrainingConfiguration; public class ModelTrainingRunner { public static void main(String args) throws Exception { System.setProperty(\"WORKFLOWLOGFOLDER\", \". logs \"); Path inputDirPath = Paths.get(\" home ie example input\"); Path outputDirPath = Paths.get(\" home ie example output\"); List fields = new ArrayList(); fields.add(new FieldInfo.Builder(\"invoice_number\") .type(FieldType.INVOICE_NUMBER) .required(true) .multiValue(false) .build()); Map parameters = new HashMap(); LocalTrainingConfiguration configuration = LocalTrainingConfiguration.builder() .inputDir(inputDirPath) .outputDir(outputDirPath) .fields(fields) .parameters(parameters) .addResource(\"dictionary\", Paths.get(\"home ie example dictionary InvoiceNumbers.csv\")) .build(); ModelRunner.run(ExampleIeModel.class, configuration); } } Here is an example of a ModelTrainingRunner configuration for a group field invoice_group and a corresponding type: import java.nio.file.Path; import java.nio.file.Paths; import java.util.ArrayList; import java.util.HashMap; import java.util.List; import java.util.Map; import com.workfusion.vds.sdk.api.nlp.configuration.FieldInfo; import com.workfusion.vds.sdk.api.nlp.configuration.FieldType; import com.workfusion.vds.sdk.run.ModelRunner; import com.workfusion.vds.sdk.run.config.LocalTrainingConfiguration; public class ModelTrainingRunner { public static void main(String args) throws Exception { System.setProperty(\"WORKFLOWLOGFOLDER\", \". logs \"); Path inputDirPath = Paths.get(\" home ie example input\"); Path outputDirPath = Paths.get(\" home ie example output\"); List fields = new ArrayList(); fields.add(new FieldInfo.Builder(\"invoice_group\").type(FieldType.GROUP) .child(new FieldInfo.Builder(\"invoice_number\") .type(FieldType.INVOICE_TYPE) .build()) .child(new FieldInfo.Builder(\"invoice_date\") .type(FieldType.INVOICE_DATE) .build()) .build()); Map parameters = new HashMap(); LocalTrainingConfiguration configuration = LocalTrainingConfiguration.builder() .inputDir(inputDirPath) .outputDir(outputDirPath) .fields(fields) .parameters(parameters) .addResource(\"dictionary\", Paths.get(\"home ie example dictionary InvoiceNumbers.csv\")) .build(); ModelRunner.run(ExampleIeModel.class, configuration); } } Refer to the Resources Estimation documentation for more details. Model Training Results After a successful model training your output folder should contain a training sub folder with following folders: output — A folder with model output (statistics, trained model artifact, etc.). process — A folder with model training details (status files, training logs, etc.). work — A working folder for model training (feature files, temporary files, etc.). The trained model artifact will be placed in the following folder: training output model. Cluster Model Training Versioning Info Available from version 9.0.0 A model generated from an archetype contains a special class for model training — ModelTrainingRunner. Running ModelTrainingRunner To make the runner work, proceed with the following steps: Generate public and private SSH keys. You can use PuTTYgen as a key generation tool for creating SSH keys for PuTTY. It is similar to the ssh keygen tool used in other SSH implementations. Send your public key to your system administrator and request access to the cluster. Provide the following information to MarathonTrainingConfiguration.builder(): Path to a folder with training set files. In case of Information Extraction use case: List of FieldInfo fields which represent your IE configuration. Provide your custom model Class to method run. Configuring Model Training on Marathon Cluster The configuration class for model training on the Marathon cluster has the following parameters. Required: inputDir — A path to a folder with training set files. endpoint — URL to cluster. sshHost — Name of Apache Mesos master node. sshKey — SSH key path. clusterSharedWorkerDir — Path to workers handled by the worker management service. Defaults to opt workfusion vds data workers app . clusterWorkingDir — Path to Marathon and Mesos on a cluster where your training starts. Defaults to opt workfusion vds data eval . id — Training ID. Set a unique and verbose name to avoid collisions. fields — List of FieldInfo fields which represent your IE configuration. Optional: sshPort — Gets SSH port. Default: 22. sshKeyPassphrase — SSH key passphrase, if your SSH key is password protected. testSetDir — A path to a folder with test set files. sshUserName — SSH username for connection. Default: wfuser. parameters — Additional parameters (for example, Search Engine limits, additional training configuration, etc.). Here is an example of cluster model runner configuration. import java.nio.file.Paths; import java.util.ArrayList; import java.util.HashMap; import java.util.List; import java.util.Map; import com.workfusion.vds.sdk.api.nlp.configuration.FieldInfo; import com.workfusion.vds.sdk.api.nlp.configuration.FieldType; import com.workfusion.vds.sdk.run.ModelRunner; import com.workfusion.vds.sdk.run.config.MarathonTrainingConfiguration; public class ModelTrainingRunner { public static void main(String args) throws Exception { System.setProperty(\"WORKFLOWLOGFOLDER\", \". logs \"); List fields = new ArrayList(); fields.add(new FieldInfo.Builder(\"transaction_data\").type(FieldType.DATE).build()); Map parameters = new HashMap(); ClusterTrainingConfiguration configuration = ClusterTrainingConfiguration.builder() .inputDir(Paths.get(\"training_set\")) .testSetDir(Paths.get(\"test_set\")) .endpoint(\"urltocluster\") .sshHost(\"host\") .sshUsername(\"user\") .sshPort(22) .sshKey(\"sshprivatekey_path\") .sshKeyPassphrase(\"ssh_password\") .clusterSharedWorkerDir(\"wmssharedfolder\") .clusterWorkingDir(\"clusterworkingdir\") .id(\"trainingId\") .parameters(parameters) .fields(fields) .build(); ModelRunner.run(YourIeModel.class, configuration); } } Model Training Results After a successful model training the Marathon folder will contain a training sub folder with the following folders: output — a folder with model output (statistics, trained model artifact, etc.). process — a folder with model training details (status files, training logs, etc.). work — a working folder for model training (feature files, temporary files, etc.). The trained model artifact will be placed in the following folder: training output model. Additional Resources Resource Estimation Local Run "},{"version":"10.0","date":"Jul-08-2019","title":"post-processing-acceleration","name":"Post-Processing Acceleration","fullPath":"iac/automl/automl-sdk/post-processing-acceleration","content":" Post processing utilizes normalizers. Before 9.3, AutoML included a number of out of the box normalizers, but the final post processing configuration was written by Machine Learning Engineer. MLE had to choose the combination of normalizers themselves for each case. Normalizer Search Engine, introduced in the 9.3 release, tests different combinations of normalizers for each field to select the optimal chain. The final choice is the most useful and the safest for particular case. This minimizes the practice of designing custom post processors, so MLE doesn't have to write a post processor in the majority of the cases. Normalizer Search Engine works especially well on numeric fields: numbers, dates, amount, and price. Quality of these fields can improve up to 70% in comparison with the same IE flow without the post processing. Normalizer Search Engine is also useful for select one, email, currency, address, text fields etc. Text Normalization For All Text Fields UpperCaseNormalizer Modifies text to contain only upper case letters for the specified locale. Before: aBCdE After: ABCDE LowerCaseNormalizer Modifies text to contain only lower case letters for the specified locale.s Before: aBCdE After: abcde CapitalizeNormalizer Capitalizes all the whitespace separated words in a string. Only the first letter of each word is changed. Before: aBC bcC cCD After: ABC BcC CCD TrimmerNormalizer Removes spaces at the beginning and the end of the string. Before: \" aa a \" After: \"aa a\" WhitespaceNormalizer Replaces multiple occurrences of any whitespace symbol with one standard whitespace symbol. Before: \"cat dog\" After: \"cat dog\" WhitespaceRemoveNormalizer Replaces all occurrences of any whitespace symbol. Before: \"cat dog \" After: \"catdog\" PunctuationRemoveNormalizer Removes all occurrences of punctuation marks. Before: cat,dog After: catdog MapDocumentTextToValueNormalizer Text normalization for select one optional one field types based on the automatic dictionary provided in resources and the nearest neighbors strategy. Substitutes the answer for the value from the dictionary, if the similarity score between them is acceptable. Formatting DateFormatNormalizer Date format normalization for date fields based on automatic date month order determination for given locale. If date format pattern is not provided, the default one (\"MM dd yyyy\") will be used. Before: 1989,10,22 After: 10 22 1989 AmountFormatNormalizer Amount formatting normalization for amount fields based on automatic locale determination used for delimeter choice. If amount format pattern is not provided, the default one (\" .00\") will be used. Before: 4,32 After: 4.32 DecimalNumberFormatNormalizer Number formatting normalization for number fields based on automatic locale determination (used for delimiter choice). Before: 3,14159265359 After: 3.14159265359 OCR Error Correction OcrDateFixNormalizer Removes OCR errors from the date string. Before: Ju1 20, 2015 After: Jul 20, 2015 DictionaryCorrectionNormalizer Removes OCR errors from the string based on the dictionary automatically gathered by GoldDataAnalyzers. Before: l 5QO After: 1 500 UsStatesAbbreviationNormalizer Replaces the name of the US state by its abbreviation for address fields. Before: Oklahoma After: OK For Data, Amount and Number fields, formatting pattern should be provided in the field info normalizationPattern attribute, otherwise the default pattern (\"MM dd yyyy\" for Date fields, \" .00\" for Amount fields) will be used: new FieldInfo.Builder(\"invoice_date\") .type(FieldType.DATE) .multiValue(false) .property(GenericPipelineConfiguration.PARAMNORMALIZATIONPATTERN, \"MM dd yyyy\") .build(); Configuration The new components include: GoldDataAnalyzersConfiguration: a set of analyzers grouped by field type. Gold Data Analyzer finds patterns and similarities in training data and stores them in resources. GenericNormalizersConfiguration: a set of normalizers grouped by field type. Normalizers Search Engine uses it to find the best chain of normalizers. Normalizers use the output of gold data analyzers. BestNormalizersProcessor: reads the serialized best normalizers chain and applies them to corresponding fields. @ModelConfiguration() @Import(configurations = { @Import.Configuration(CandidateFeatureExtractorsConfiguration.class), @Import.Configuration(GenericIeAnnotatorConfiguration.class), @Import.Configuration(GenericNormalizersConfiguration.class), @Import.Configuration(GoldDataAnalyzersConfiguration.class), @Import.Configuration(GenericHpoConfiguration.class), @Import.Configuration(GenericIePipelineConfiguration.class), @Import.Configuration(GenericIeMlConfiguration.class), @Import.Configuration(GenericIeParserConfiguration.class) }) public class GenericIeHypermodelConfiguration { @Named(\"basePostProcessors\") public List getPostProcessors(DefaultConfigurationContext configurationContext) { return Lists.newArrayList(new BestNormalizersProcessor(configurationContext.getResource(\" lookup normalizers\")), new CheckerModelPostProcessor(), new DataValueNormalizationProcessor(), new RowBasedGroupingProcessor()); } } Execution Flow Gold Data Analyzer specified for the selected field is executed. The output files (local.json, ocr corrections.csv) are stored in the resources directory. All possible combinations and permutations of fields normalizers are generated. Statistics is calculated and stored for all generated combinations in the normalizer statistics.csv file. Best normalizers are serialized in the best normalizers.json file. The best normalizers.json file is used by BestNormalizersProcessor for post processing. Here's the files structure. output └──resources └──lookup └──normalizers ├──email │ └──normalizer statistics.csv ├──invoice_date │ ├──best normalizers.json │ ├──date format priority.txt │ └──normalizer statistics.csv ├──invoice_number │ ├──best normalizers.json │ ├──normalizer statistics.csv │ └──ocr corrections.csv ├──price │ ├──best normalizers.json │ ├──locale.json │ └──normalizer statistics.csv ├──product │ └──normalizer statistics.csv ├──quantity │ ├──best normalizers.json │ ├──locale.json │ └──normalizer statistics.csv └── total_amount ├──best normalizers.json ├──locale.json └──normalizer statistics.csv "},{"version":"10.0","date":"Sep-20-2019","title":"post-processing","name":"Post-Processing","fullPath":"iac/automl/automl-sdk/post-processing","content":" Post Processing is the final stage of the AutoML SDK pipeline performed after the model training. Post Processing modifies the trained model output to fit the requirements of a customer by applying pre defined rules. In rare cases, based on a separate ML model. Generally, Post Processing handles such field value modifications as adding, removing, changing (80% of cases), and grouping. Example: A Post Processor can change a date format from 11.07.2018 to a standard US format 07 11 2018. import java.time.LocalDate; import java.time.format.DateTimeFormatter; import java.util.Locale; import java.util.Optional; import com.workfusion.vds.sdk.api.nlp.annotation.OnInit; import com.workfusion.vds.sdk.api.nlp.model.Field; import com.workfusion.vds.sdk.api.nlp.model.IeDocument; import com.workfusion.vds.sdk.api.nlp.processing.ProcessingException; import com.workfusion.vds.sdk.api.nlp.processing.Processor; public class DatePostProcessor implements Processor { private DateTimeFormatter inputDateFormatter; private DateTimeFormatter outputDateFormatter; @OnInit public void init() { initialize formatter inside onInit method to prevent serialization issues inputDateFormatter = DateTimeFormatter.ofPattern(\"dd.MM.yyyy\", Locale.ENGLISH); outputDateFormatter = DateTimeFormatter.ofPattern(\"MM dd yy\"); } @Override public void process(IeDocument document) throws ProcessingException { find field for code 'date' Optional fieldOptional = document.findField(\"date\"); if (fieldOptional.isPresent()) { Field field = fieldOptional.get(); LocalDate date = LocalDate.parse(field.getValue(), inputDateFormatter); field.setValue((date).format(outputDateFormatter)); } } } In AutoML SDK, Post Processors have a number of implementations depending on the problem being solved. :::warning All fields of a Post Processor must be serializable. Although, the Post Processor does not have to implement the serializable interface. Keep in mind that lambdas are not serializable by default. ::: If you cannot avoid using non serializable fields for some reason, then you can do the following trick: Define a field and keep it null. Initialize the field in the @OnInit method. Do not initialize the field in a constructor. A Post Processor that uses non serializable date formatter Your Post Processor converts dates from one format to another with help of DateTimeFormatter which is not Serializable. The correct initialization of the Post Processor would be as follows. public class DatePostProcessor implements Processor { private DateTimeFormatter dateTimeFormatter; @OnInit public void init() { dateTimeFormatter = DateTimeFormatter.ofPattern(\"dd MM yyyy\"); } * The Post Processor implementation goes here * } A Post Processor that uses a lambda Let's say you have a bunch of common normalization functions grouped into the PostProcessingFunctions class. In your Post Processor, you want to refer to one of the functions using a lambda. Then, in order to avoid issues with serialization the code should look the following way. public class MyPostProcessor implements Processor { private Normalizer normalizer; @OnInit public void init() { normalizer = PostProcessingFunctions::removeLegalEndings; } * The Post Processor implementation goes here * } Normalization Normalization is the process of changing the extracted values to a certain format. The extracted values may include: Numbers: 1k > 1000 Dates: 11.07.2018 > 07 11 2018 Prices: 100 > 100.00 Currencies: > USD Addresses: NY Brooklyn 02356 Ralph Ave. > 02356 Ralph Ave. Brooklyn NY Organizations: WF or Workfusion Systems > WorkFusion The following Normalizer converts a date format to ISO standard YYYY MM DD. OcrDateNormalizer normalizer = new OcrDateNormalizer(); normalizer.normalize(\"11 Jul 18\") => \"2018 07 11\"; normalizer.normalize(\"07 31 2015\") => \"2015 07 31\"; For more normalization examples, refer to Post Processing Examples. API Basics :::warning Normalizers are not standalone components and thus should be used within a Post Processor instance. ::: All Normalizers implement the Normalizer interface and have a normalize() method, which takes a string as a parameter and returns a normalized representation of this string. For normalizing numbers use the NumberNormalizer interface with a normalize(String text, String numberFormat) method. Refer to Java numberFormat documentation for details. Normalizers follow these rules: Return null for a null string. Return empty string for an empty string. Return the original string if errors occur during normalization. Here's an example of a Post Processor which normalizes the Price value from 1,234,56 to 1234.56. import java.util.Optional; import com.workfusion.vds.sdk.api.nlp.model.Field; import com.workfusion.vds.sdk.api.nlp.model.IeDocument; import com.workfusion.vds.sdk.api.nlp.processing.ProcessingException; import com.workfusion.vds.sdk.api.nlp.processing.Processor; import com.workfusion.vds.sdk.nlp.component.processing.normalization.OcrAmountNormalizer; public class AmountPostProcessor implements Processor { public void process(IeDocument document) throws ProcessingException { Optional amount = document.findField(\"invoice_amount\"); if (amount.isPresent()) { Field amountField = amount.get(); String value = amountField.getValue(); OcrAmountNormalizer amountNormalizer = new OcrAmountNormalizer(); amountField.setValue(amountNormalizer.normalize(value)); } } } For full list of AutoML SDK OOTB Normalizersand details, refer to Javadocs. OCR Errors Correction Processed documents are generally scanned original invoices, checks, etc. During the document processing OCR may introduce errors due to damaged documents and low quality of scans. Post Processing steps in to either fix the incorrect value, or remove it. Fixing Value For example, OCR output may contain the same recognition error in all documents. Post Processing can fix incorrect values by introducing a rule (usually a regex replacing one character with another). The following example shows an implementation of a Post Processor for replacing G to 6, B to 8, O to 0 in Zip Codes. import java.util.Optional; import com.workfusion.vds.sdk.api.nlp.model.Field; import com.workfusion.vds.sdk.api.nlp.model.IeDocument; import com.workfusion.vds.sdk.api.nlp.processing.ProcessingException; import com.workfusion.vds.sdk.api.nlp.processing.Processor; public class ZipPostProcessor implements Processor { public void process(IeDocument document) throws ProcessingException { Optional zipCode = document.findField(\"zip_code\"); if (zipCode.isPresent()) { Field zipCodeField = zipCode.get(); String value = zipCodeField.getValue(); String correctedValue = value.toUpperCase() .replaceAll(\"G\", \"6\") .replaceAll(\"B\", \"8\") .replaceAll(\"O\", \"0\"); zipCodeField.setValue(correctedValue); } } } :::note Use this approach if the probability of positive correction is high enough for your case. In many cases, the probability threshold is calculated based on the results of model training. ::: Consider the following example. Comparing model training results (tagged value against data value) shows that character \"G\" is replaced with \"6\" in 98% of cases. This may be considered as a high positive correction probability and thus can be used in a Post Processor to fix OCR errors. In other case, model training results may show that character \"B\" is replaced with \"8\" in 55% of cases and with \"6\" in 45% of cases. This is quite a low probability level and should not be used in a Post Processor. Depending on the use case requirements, it is generally recommended that values with low to medium level of positive correction probability should be skipped or removed by the Post Processor. Removing Value Sometimes OCR may introduce a recognition error in 50% of cases while the other 50% are correct. If it's impossible to fix a field value, then Post Processing can remove it. Validation When extracting Bank Card Numbers, IBANs, Zip Codes, CUSIPs, etc. it’s crucial to check them to make sure the extracted value is valid. In case, the extracted value is not valid, it’s usually removed. For more information about CUSIP validation, refer to CUSIP documentation. The following example validates the credit card number. Validation criteria: all characters are digits. import java.util.Collection; import com.workfusion.vds.sdk.api.nlp.model.Field; import com.workfusion.vds.sdk.api.nlp.model.IeDocument; import com.workfusion.vds.sdk.api.nlp.processing.ProcessingException; import com.workfusion.vds.sdk.api.nlp.processing.Processor; public class CreditCardPostProcessor implements Processor { private static final String FIELDNAME = \"creditcard\"; @Override public void process(IeDocument document) throws ProcessingException { Collection fields = document.findFields(FIELD_NAME); fields.forEach(field > { String value = field.getValue(); if (!value.chars().allMatch(Character::isDigit)) { document.remove(field); } }); } } Mapping to Reference Data A Post Processor can extract additional data not contained in the original Document using the reference data. For example, employer’s Name and Surname can help extract their unique company ID. Having extracted only zip code, you can map it to the city, and then extract this information even though it’s not initially present in the Document. Reference data can be taken from a database, an API request, a local dictionary, etc. :::note For the following example to work, provide a CSV dictionary zipcodes.csv file, and then put it into Classpath. Each record in the dictionary contains 3 columns, where the first column is the name of a city, and the next two represent a range of zip codes. For example: Huntsville, 35801, 35816 Anchorage, 99501, 99524 etc. ::: import java.io.File; import java.nio.file.Paths; import java.util.List; import java.util.Optional; import org.apache.commons.lang3.math.NumberUtils; import com.google.common.collect.Range; import com.google.common.collect.RangeMap; import com.google.common.collect.TreeRangeMap; import com.workfusion.vds.sdk.api.nlp.annotation.OnInit; import com.workfusion.vds.sdk.api.nlp.model.Field; import com.workfusion.vds.sdk.api.nlp.model.IeDocument; import com.workfusion.vds.sdk.api.nlp.processing.ProcessingException; import com.workfusion.vds.sdk.api.nlp.processing.Processor; import com.workfusion.vds.sdk.nlp.component.dictionary.CsvDictionaryProvider; public class ZipToCityPostProcessor implements Processor { private RangeMap zipToCities; @OnInit public void init() { try { read zip dictionary from classpath File file = Paths.get(this.getClass().getResource(\"zipcodes.csv\").toURI()).toFile(); use csv reader from components to read this file CsvDictionaryProvider provider = new CsvDictionaryProvider(file); List> records = provider.getDictionary(); create guava range map based on file records to store zip >city relations zipToCities = TreeRangeMap.create(); fill this map records.forEach(record > { String city = record.get(0); Integer beginZip = Integer.valueOf(record.get(1)); Integer endZip = Integer.valueOf(record.get(1)); zipToCities.put(Range.closed(beginZip, endZip), city); }); } catch (Exception e) { throw new ProcessingException(e); } } @Override public void process(IeDocument document) throws ProcessingException { Optional zipCode = document.findField(\"zip_code\"); if (zipCode.isPresent()) { Field zipCodeField = zipCode.get(); String value = zipCodeField.getValue(); Integer zipCodeValue = NumberUtils.toInt(value, 0); String city = zipToCities.get(zipCodeValue); if (city != null) { add new field for city, does not set begin and end because positions in text are not exists document.add(Field.descriptor() .setName(\"city\") .setValue(city) .setScore(zipCodeField.getScore())); } } } } Transformation Transformation represents simple procedures like trimming, lower upper case, capitalization, removing punctuation, special characters, etc. Example: MINSK, BELARUS can be transformed into Minsk, Belarus. The following code snippet replaces all fields with type ADDRESS with normalized letter case values using out of the box Post Processors: NormalizerProcessor and TextNormalizer. import java.util.ArrayList; import java.util.List; import com.workfusion.vds.nlp.model.configuration.DefaultConfigurationContext; import com.workfusion.vds.sdk.api.hypermodel.annotation.Named; import com.workfusion.vds.sdk.api.nlp.configuration.FieldInfo; import com.workfusion.vds.sdk.api.nlp.normalization.Normalizer; import com.workfusion.vds.sdk.api.nlp.processing.Processor; import com.workfusion.vds.sdk.nlp.component.processing.NormalizerProcessor; import com.workfusion.vds.sdk.nlp.component.processing.normalization.TextNormalizer; import com.workfusion.vds.sdk.nlp.component.util.FieldInfoUtils; @Named(\"basePostProcessors\") public List getPostProcessors(DefaultConfigurationContext configurationContext) { List processors = new ArrayList(); List allFieldCodes = FieldInfoUtils.getAllChildren(configurationContext.getField()); allFieldCodes.forEach(field > { switch (field.getType()) { case ADDRESS: make only first letter capital, e.g. mInsk >Minsk Normalizer normalizer = TextNormalizer.builder() .lowerCase() .capitalize() .build(); processors.add(new NormalizerProcessor(field.getCode(), normalizer)); } }); return processors; } Grouping A model can extract multiple field values of the same type (for example, Currency, Quantity, and Price) that should be joined into a group according to their logical connection. For example, you may need to group Product Name, Amount and Price. Field values can be grouped based on their position in a document or words in a sentence, based on a table line, or based on some underlying custom logic. Words Field values grouped based on their position in a sentence. Table Cells Field values grouped based on a line in a table. The group number information is saved in the tabnumber attribute of an appropriate tag. This attribute is then used in Control Tower to display tabs with grouped fields, as in the previous example (tabs Product 1, Product 2). :::note The tabnumber attribute starts with zero (0) and is incremented by one (1) with each next field group. ::: For more information, refer to the Examples page and documentation. Additional Resources Post Processing Examples Post Processing Acceleration "},{"version":"10.0","date":"Sep-20-2019","title":"post-processing-examples","name":"Post-Processing Examples","fullPath":"iac/automl/automl-sdk/post-processing-examples","content":" This page provides a number of Post Processor examples covering a variety of use cases. Normalization Examples :::note Normalizers are not standalone components and should be used within a Post Processor instance. ::: Amount Normalization For amount normalization, use the OcrAmountNormalizer class. This Normalizer has two main options: Fixing OCR errors in numbers. Formatting numbers and removing currency signs, if any. By default, the .00 format is used for numbers formatting, but you can provide your format. See the following example for details. OcrAmountNormalizer normalizer = new OcrAmountNormalizer(); normalizer.normalize(\"1,178.42\") => \"1178.42\"; normalizer.normalize(\"2,154,60\") => \"2154.60\"; normalizer.normalize(\" 2,154,60\") => \"2154.60\"; normalizer.normalize(\"700435.85\", \" , . \") => \"700,435.8\"; Sometimes a model extracts fields with a currency sign, or there may be some OCR errors in the amount field. The following implementation of a Post Processor normalizes 1,234,56 to 1234.56. For more details, refer to the OcrAmountNormalizer class documentation. public void process(IeDocument document) throws ProcessingException { Optional amount = document.findField(\"invoice_amount\"); if (amount.isPresent()) { Field amountField = amount.get(); String value = amountField.getValue(); OcrAmountNormalizer amountNormalizer = new OcrAmountNormalizer(); amountField.setValue(amountNormalizer.normalize(value)); } } Dates Normalization To modify Dates to the appropriate format and fix some OCR errors, use the OcrAmountNormalizer class. By default, ISO 8601 Date Standard is used — YYYY MM DD. You can set your custom date format. See the following examples. OcrDateNormalizer normalizer = new OcrDateNormalizer(); normalizer.normalize(\"30 Jun 12\") => \"2012 06 30\"; normalizer.normalize(\"07 31 2015\") => \"2015 07 31\"; Custom date format. OcrDateNormalizer normalizer = new OcrDateNormalizer(\"MM dd yyyy\"); normalizer.normalize(\"23 Au0 201 5\") => \"08 23 2015\"; normalizer.normalize(\"30 ABRIL n >015\") => \"04 30 2015\"; Anchor date for incomplete dates. OcrDateNormalizer normalizer = new OcrDateNormalizer(\"MM dd yyyy\", LocalDate.of(2017, 1, 1)); normalizer.normalize(\"May 10\") => \"05 10 2017\"; Year month priority. OcrDateNormalizer normalizer = new OcrDateNormalizer(\"MM dd yyyy\", LocalDate.of(2017, 1, 1)); normalizer.normalize(\"May 10\") => \"05 10 2017\"; normalizer.setYearPriorDay(true); normalizer.normalize(\"May 10\") => \"05 01 2010\"; normalizer.setFirstDayOfMonth(false); normalizer.normalize(\"May\") => \"05 31 2017\"; Text to Date Normalization To normalize text to dates, use the TextToDateNormalizer class. It normalizes the text representation of dates to formatted dates. You can provide date format and anchor date for incomplete dates, or next previous cases. LocalDate anchorDate = LocalDate.of(2011, 12, 13); TextToDateNormalizer normalizer = new TextToDateNormalizer(anchorDate); normalizer.normalize(\"in 3 month\") => \"2012 03 12\"; normalizer.normalize(\"5 days ago\") => \"2011 12 08\"; normalizer.normalize(\"after forty one day\") => \"2012 01 23\"; normalizer.normalize(\"this Friday\") => \"2011 12 16\"; normalizer.normalize(\"Monday last week\") => \"2011 12 05\"; Custom format. LocalDate anchorDate = LocalDate.of(2011, 12, 13); normalizer = new TextToDateNormalizer(\"dd MM yyyy\", anchorDate); normalizer.normalize(\"in 3 month\") => \"12 03 2012\"; Text to Number Normalization This Normalizer provides the same functionality as the previous one, but for numbers instead of dates. For this purpose, use the TextToIntegerNormalizer class. TextToIntegerNormalizer normalizer = new TextToIntegerNormalizer(); normalizer.normalize(\"one thousand two hundred eleven\") => \"1211\"; normalizer.normalize(\"ninety nine thousand nine hundred ninety nine\", \" , .00\") => \"99,999.00\"; normalizer.normalize(\"minus one\") => 1; OCR Error Correction Example After the model results analysis, we see that in the zipcode field we have some OCR errors. The following code helps us replace G > 6, B > 8, O > 0_. import java.util.Optional; import com.workfusion.vds.sdk.api.nlp.model.Field; import com.workfusion.vds.sdk.api.nlp.model.IeDocument; import com.workfusion.vds.sdk.api.nlp.processing.ProcessingException; import com.workfusion.vds.sdk.api.nlp.processing.Processor; public class ZipPostProcessor implements Processor { public void process(IeDocument document) throws ProcessingException { Optional zipCode = document.findField(\"zip_code\"); if (zipCode.isPresent()) { Field zipCodeField = zipCode.get(); String value = zipCodeField.getValue(); String correctedValue = value.toUpperCase() .replaceAll(\"G\", \"6\") .replaceAll(\"B\", \"8\") .replaceAll(\"O\", \"0\"); zipCodeField.setValue(correctedValue); } } } Field Grouping An AutoML model can extract multiple fields of the same type (e.g. Currency, Quantity, Price) that should be joined into groups according to their connection. AutoML SDK provides 2 strategies to assign group numbers which come as out of the box Post Processors: RowBasedGroupingProcessor PositionBasedGroupingProcessor RowBasedGroupingProcessor Row based strategy can be applied to documents containing tables. RowBasedGroupingProcessor iterates through all rows in tables, and then finds field groups. If a row contains at least one group field, it assigns the same group number to all fields in this row. In the following example, RowBasedGroupingProcessor will assign 3 group numbers tabnumber to each row consisting of Account Number, Bank, and SWIFT. Group Number Account Number Bank SWIFT 0 100500 Private Bank BOPIPHMM 1 100608 Super Bank BNORPHMM 2 100609 Super Bank CHBKPHMM PositionBasedGroupingProcessor Position based strategy can be applied to any document. PositionBasedGroupingProcessor is based on field indices in a Document. It finds group fields in a Document, and then assigns group numbers according to their indices in text. Fields with the same name can't exist in one group. The algorithm adds fields to a current group until the first field with the same name is found. Then this field is added to a new group (with incremented number), and all other fields are added to this new group. Consider the following example of a tagged text: Samuel Langhorne Clemens (November 30, 1835 – April 21, 1910), better known by his pen name Mark Twain, was an American writer, humorist, entrepreneur, publisher, and lecturer. Among his novels are The Adventures of Tom Sawyer (1876) and its sequel, the Adventures of Huckleberry Finn (1885), the latter often called &quot;The Great American Novel&quot;. Eric Arthur Blair (25 June 1903 – 21 January 1950), better known by his pen name George Orwell, was an English novelist, essayist, journalist, and critic. He is best known for the allegorical novella Animal Farm (1945) and the dystopian novel Nineteen Eighty Four (1949).Fields:NamePen NameLife YearsGrouping Algorithm steps:Set the current Group Number to zero (0)Go to the 1st field (tag): Samuel Langhorne Clemens (Name)Ckeck whether Group 0 already has a Name field → NoSet Group Number = 0 for this fieldGo to the 2nd field: November 30, 1835 – April 21, 1910 (Life Years)Ckeck whether Group 0 already has a Life Years field → NoSet Group Number = 0 for this fieldGo to the 3rd field: Mark Twain (Pen Name)Ckeck whether Group 0 already has a Pen Name field → NoSet Group Number = 0 for this fieldGo to the 4th field (tag): Eric Arthur Blair (Name)Ckeck whether Group 0 already has a Name field → YesSet the current Group Number to one (1)Set Group Number = 1 for this fieldGo to the 5th field (tag) ... Custom Grouping Processor In some cases, you might need to implement a custom grouping strategy. Custom grouping processors are applied to documents with complex structure, such as: Mix of text paragraphs and tables. Records are spread over several table rows. One table column is common for all records. To handle such cases, you can override BaseGroupingProcessor and implement your custom logic in the findGroups() method. You can use the insertGroupIndices() method to set the group index for a field. For example, you have a document with tables, where one table column Country is common for all records. Country Account Number Bank Name SWIFT Belgium 100500 Private Bank BOPIPHMM Belgium 100608 Super Bank BNORPHMM Belgium 100609 Super Bank CHBKPHMM In this case, field grouping can be implemented using the following code sample. public class CustomGroupingProcessor extends BaseGroupingProcessor { @Override protected void findGroups(IeDocument document, Set groupCodes) { Collection rowAnnotations = document.findAll(Row.class); int groupIdx = 0; for (Row row : rowAnnotations) { List fields = document.findCovered(Field.class, row); List groupFields = fields.stream() .filter(field > groupCodes.contains(field.getName())) .collect(Collectors.toList()); if (!groupFields.isEmpty()) { insertGroupIndices(groupFields, groupIdx); groupIdx ; } } country belongs to all groups Field countryField = document.findField(\"country\"); insertGroupIndices(countryField, IntStream.rangeClosed(0, groupIdx).toArray()); } } Post Processing for Classification Models Below you can find an example of processing for Classification Model where we change the Document label from 'Deutsche bank' to 'DB', and then set score to 1. public void process(ClassificationDocument document) throws ProcessingException { Label bank = document.findLabels().stream() .findFirst() .orElse(null); String name = bank.getName(); if (bank != null && \"Deutsche bank\".contains(name)) { document.remove(bank); document.add(Label.descriptor().setName(\"DB\").setScore(BigDecimal.ONE)); } } "},{"version":"10.0","date":"Jul-10-2019","title":"project-configuration","name":"Project Configuration","fullPath":"iac/automl/automl-sdk/project-configuration","content":" To configure an AutoML project, use either of the following two methods: Using an AutoML SDK Wizard (default) Using WorkFusion Studio Using the command line Using an AutoML SDK Wizard AutoML SDK Wizard is the simplest way to configure a new AutoML SDK project in WorkFusion Studio. Follow these instructions to quickly get started with a new project: In WorkFusion Studio, click File > New > AutoML SDK Project. Select the Archetype Catalog: Remote. Provide a URL to a pre installed Nexus repository with all artifacts on your on premise installation or a WorkFusion Maven repository for training purposes. Provide your username and password. Local. Provide a Path to the archetype catalog.xml file. Select the way you want your project to be created: Create an empty project if you want to have an empty model without any configuration. Extend generic models if you want your model to have a generic model configuration. Select the Type of ML Project and AutoML SDK Version. Specify the properties and parameters for your AutoML SDK project. Note, that the parameters are already predefined. groupId — Group identifier of your artifact. artifactId — Unique identifier of your artifact. version — Artifact version. package — Default package for code generation. modelClassName — Class name of your model. modelCode — Hypermodel code, only a z, 0 9, _, are allowed for model code. modelDescription — Hypermodel description which will be used in WorkFusion during automation setup as a hint. modelTitle — Hypermodel title which will be used in WorkFusion during automation setup as value in drop down. modelVersion — Hypermodel version. Click Finish and wait till your project is set up. Now you're ready to start using your AutoML SDK project. Using WorkFusion Studio Project Setup In WorkFusion Studio, click Window > Preferences > Maven > Archetypes. Click Add Remote Catalog... to use a pre installed Nexus repository with all artifacts on your on premise installation or WorkFusion Maven repository for training purposes. Provide a URL to the repository, add repository description (optional), and then click OK. Click OK to save the changes. Generating a Project from Archetype In WorkFusion Studio, click File > New > Other > Maven. Select Maven Project, and then click Next. Select Use default Workspace location, and then click Next. Select the Catalog that you created in the previous section, select the archetype for your use case (Information Extraction or Classification), and then click Next. Specify the Archetype parameters. groupId — Group identifier of your artifact. artifactId — Unique identifier of your artifact. version — Artifact version. package — Default package for code generation. importConfigurationFromGenericModel — enter y if you want your model to have generic model configuration or enter n if you want to have empty model without any configuration. modelClassName — Class name of your model. modelCode — Hypermodel code, only a z, 0 9, _, are allowed for model code. modelDescription — Hypermodel description which will be used in WorkFusion during automation setup as a hint. modelTitle — Hypermodel title which will be used in WorkFusion during automation setup as value in drop down. modelVersion — Hypermodel version. Click Finish to generate a new project with the provided values. In the Project tree, right click the pom.xml file. Click Run As > Maven install and wait for project to build. As soon as you see the BUILD SUCCESS message in the Console, you are ready to use your custom model. Using Command Line To generate a stub using a command line, use one of the following Maven commands depending on the model type and Search Engine version. Search Engine 1.0 Models Information Extraction mvn archetype:generate DarchetypeGroupId=com.workfusion.ml DarchetypeArtifactId=ml sdk ie archetype DarchetypeVersion= Classification mvn archetype:generate DarchetypeGroupId=com.workfusion.ml DarchetypeArtifactId=ml sdk classification archetype DarchetypeVersion= Binary Classification mvn archetype:generate DarchetypeGroupId=com.workfusion.ml DarchetypeArtifactId=ml sdk binary classification archetype DarchetypeVersion= Search Engine 2.0 Models Information Extraction mvn archetype:generate DarchetypeGroupId=com.workfusion.ml DarchetypeArtifactId=ml sdk ie se 20 archetype DarchetypeVersion= Classification mvn archetype:generate DarchetypeGroupId=com.workfusion.ml DarchetypeArtifactId=ml sdk classification se 20 archetype DarchetypeVersion= Binary Classification mvn archetype:generate DarchetypeGroupId=com.workfusion.ml DarchetypeArtifactId=ml sdk binary classification se 20 archetype DarchetypeVersion= Generating a Project Copy one of the above mentioned commands. Replace ` by the current version of your AutoML SDK (for example, 10.0.x.x`). Next, provide the following fields: groupId — Group identifier of your artifact. artifactId — Unique identifier of your artifact. version — Artifact version. package — Default package for code generation. importConfigurationFromGenericModel — enter y if you want your model to have a generic model configuration or enter n if you want to have an empty model without any configuration. modelClassName — Class name of your model. modelCode — Hypermodel code, only a z, 0 9, _, are allowed for model code. modelDescription — Hypermodel description which will be used in WorkFusion during automation setup as a hint. modelTitle — Hypermodel title which will be used in WorkFusion during automation setup as value in drop down. modelVersion — Hypermodel version. Your Hypermodel stub is generated. It's a Maven project that can be imported in your favorite Java IDE. The generated stub has following structure: example ie ├── src │ ├── main │ │ └── java │ │ │ └── com │ │ │ └── wf │ │ │ └── example │ │ │ ├── example_ie │ │ │ │ ├── config │ │ │ │ │ └── ExampleIeModelConfiguration.java │ │ │ │ ├── model │ │ │ │ │ └── ExampleIeModel.java │ │ │ │ └── run │ │ │ │ ├── ModelProcessingRunner.java │ │ │ │ └── ModelTrainingRunner.java │ │ │ └── FeatureExtractorExample.java │ │ │ └── PostProcessorExample.java │ │ └── resources │ │ └── logback local.xml │ └── test │ ├── java │ └── resources └── target └── pom.xml Project Artifacts In the generated stub, you can find the following classes: Model — Class with a custom Hypermodel, here you can find a basic configuration of your Hypermodel — code, version, title, etc. Also here you can find generated annotation ModelConfiguration with pre generated model configuration. The generated Hypermodel extends one of generic models GenericIeHyperModel for IE models and GenericMultiClassificationHyperModel for Classification models. Configuration — Class with your configuration for your model. It has methods like processor, featureExtractors, annotators where you can define your own model configuration. PostProcessorExample — Example of post processor. FeatureExtractorExample — Example of feature extractor. ModelTrainingRunner — Class for local model training run. ModelProcessingRunner — Class for local model execution run, to develop debug post processing. "},{"version":"10.0","date":"Jul-08-2019","title":"resource-estimation","name":"Resource Estimation","fullPath":"iac/automl/automl-sdk/resource-estimation","content":" To train an IE model on a local desktop or terminal, your machine needs at least 8 GB RAM and 2 CPU. Depending on the IE model, here's the recommended configuration: Model RAM CPU v9.2 Generic IE 16 GB 4 v9.2 Generic Search Engine 2.0 8 GB 4 v9.2 Generic Binary Classification * 16 GB 8 v9.2 Generic Multi class Classification 16 GB 8 v9.2 Extended Generic IE Search Engine 2.0 or Generic IE 16 GB 8 * depends on the documents volume and amount of selected classes, estimated for a training set with less than 3000 documents and 6 classes. In some cases with huge amounts of data in a training set, local training may throw OutOfMemoryException. To avoid this, follow these steps: Edit the ModelTrainingRunner class as in the following code example. Model training is performed per field only. package com.workfusion.model.run; import java.nio.file.Path; import java.nio.file.Paths; import java.util.ArrayList; import java.util.HashMap; import java.util.List; import java.util.Map; import com.workfusion.nlp.uima.pipeline.constants.ConfigurationConstants; import com.workfusion.vds.sdk.api.nlp.configuration.FieldInfo; import com.workfusion.vds.sdk.api.nlp.configuration.FieldType; import com.workfusion.vds.sdk.run.ModelRunner; import com.workfusion.vds.sdk.run.config.LocalTrainingConfiguration; import com.workfusion.model.ExtendedModel; public class ModelTrainingRunner { public static void runFieldTraining(String fieldName, FieldType type, String outputPathFolder, Path inputDirPath) { List fields = new ArrayList(); Map parameters = new HashMap(); parameters.put(ConfigurationConstants.HPOTIMELIMIT, 60 * 60 * 4); try { fields.add(new FieldInfo.Builder(fieldName).type(type).build()); LocalTrainingConfiguration configuration = LocalTrainingConfiguration.builder().inputDir(inputDirPath).outputDir(Paths.get(outputPathFolder \" model\" \"\" fieldName)).fields(fields).parameters(parameters).build(); ModelRunner.run(ExtendedModel.class, configuration); } catch (Exception ex) { System.out.println(ex); } } public static void main(String args) throws Exception { System.setProperty(\"WORKFLOWLOGFOLDER\", \". logs \"); System.setProperty(\"EVALWORKINGDIRISUNDEFINED\", \". eval \"); Path inputDirPath = Paths.get(\"C: generic input\"); runFieldTraining(\"full_address\", FieldType.ADDRESS, inputDirPath, \"C: dev train generic output\"); runFieldTraining(\"total_amount\", FieldType.NUMBER, inputDirPath, \"C: dev train generic output\"); runFieldTraining(\"fullname\", FieldType.FREETEXT, inputDirPath, \"C: dev train generic output\"); } } Once ModelTrainingRunner has all fields training completed, merge all separate models into a single model. Refer to the Combining Several Models Into Single One document. "},{"version":"10.0","date":"Sep-20-2019","title":"search-engine","name":"Search Engine","fullPath":"iac/automl/automl-sdk/search-engine","content":" Search Engine is a framework for optimizing Hyper Model parameters and finding the \"optimal\" set of Components: Annotators, Feature Extractors (FEs), Post Processors, algorithms and their parameters to train the model with. Here are the best pre requisites for Search Engine experiment: To run a Search Engine, use a set of gold labelled training data. In many cases, it's better to have a baseline implementation to optimize. The model should be able to train and produce results using the training data set described above with an arbitrary set of FEs. One of the primary purposes of Search Engine is to find the optimal subset of FEs from an initial superset. Search Engine uses the initial set of FEs as the candidate space to search against to find the best subset of FEs available. Algorithmic Details Search Engine is a proprietary implementation of the best approaches from the recent researches around the Hyperparameter optimization problem. At a high level Search Engine performs the following iterations: Training an approximation function of the space of hyper parameters and FEs. We use a deep learning model as the approximation function. Conducting MCMC sampling against this approximate function using Metropolis Hastings acceptance testing. 1.Execute actual experiments with the base ML model using the top scoring results found by sampling against the approximate function. These steps are repeated for a number of iterations until no better result can be found. Approximation Function In most of the cases, full grid search of experiments can't be run due to the large space of the hyper parameters and FEs. :::tip A native (brute force) search of 20 choices of FEs with 5 model parameters that can each take one of three values would produce 2 20 3 5 = 1,048,819 experiments. ::: Obviously, this is way too many experiments to run. Even if each experiment took only one second this would still require over 12 days of computing time. To get results within limited reasonable time, we use a function to model the full space of FEs and hyper parameters, and then use it to search within the space. Initialization In order to use the approximate function, we must train it with samples from the full search space. To do this, we obtain some number of initial random samples using the base ML model. Each initial sample uses a different random subset of FEs, and thus gets different scores. We can represent each initial sample as a binary vector where each dimension corresponds to one of the FEs. A dimension of the binary vector that is \"on\" represents an FE that is used for the current sample, whereas a zero dimension corresponds to an \"off\" FE that is not used in the current experiment. After we perform some number of initial seed experiments we train our approximation model using the binary vector representations that correspond to each experiment with the base ML model score as the dependent variable. Deep Learning Model We are currently using a multi layer perceptron as the approximation function since our search space is highly non linear. The deep learning model may change in the near future. Metropolis Hastings Sampling The approximation function is trained with the last set of samples obtained from the model. It generates a preset number of random binary vectors and scores them against the approximate function. Top N of these samples are selected for MCMC sampling using the Metropolis Hastings (MH) algorithm. Each sample consists of flipping one dimension of the binary vector from \"on\" to \"off\" or vice versa and then scoring against the approximation function. As per the standard MH algorithm all samples that result in higher scores are immediately accepted and samples that result in lower scores are only accepted with a certain probability. Simulated Annealing Simulated annealing is used to speed up the search via sampling. It creates the ability for the search to jump in larger steps to a different area of the space by raising the acceptance probabilities by \"temperature\". This results in more acceptances in ratio to the current \"temperature\". After each iteration the \"temperature\" is dropped to reduce the size of sampling steps around the search space. Running Search Engine Search Engine experiments run in a distributed cluster which can be hosted in cloud or on premise. Search Engine Output Search Engine experiments produce a number of files with details of optimizations performed. The important ones to consider are all output to the work pre eval parent directory. Note that there may be subfolders under this directory if multiple models are optimized in one Search Engine run. statistics.csv — contains per iteration statistics such as top score, average score, standard deviation, etc. extractors.json — the current best set of FEs in JSON format. parameters.json — the full serialized configuration used for the current best experiment including Tokenizers and Annotators. scores.csv — the \"on off\" configuration of all experiments with the corresponding score from the approximation function. best. csv json — the \"on off\" configuration of FEs used for the best experiment. Each \"on\" FE is listed in the extractors.json. Finally, after Search Engine cycle is complete there is an output directory located at output which contains the following artifacts: model — a directory with the final best model. avg evaluation results.txt — the P R F1 evaluation scores for the best model. evaluation results.zip — an archive of the detailed evaluation output for the best model that can be loaded into Tableau. "},{"version":"10.0","date":"Sep-20-2019","title":"python-classifiers","name":"Python Classifiers","fullPath":"iac/automl/automl-sdk/python-classifiers","content":" Starting with WorkFusion v10.0, AutoML SDK can be extended with a set of Python classifiers based on Scikit learn, MXNet, etc. AutoML SDK extension allows using ML algorithms implemented in Python with AutoML pipelines. Here's a list of supported classifiers: GenericPythonClassifier DpLogisticRegressionClassifier DpSvmClassifier AdaBoostClassifier BaggingClassifier DecisionTreeClassifier ExtraTreeClassifier ExtraTreesClassifier GradientBoostingClassifier KNeighborsClassifier LinearSupportVectorClassifier LogisticRegression LogisticRegressionCV RandomForestClassifier StochasticGradientDescentClassifier Development Environment Setup This section contains instructions for setting up the local development environment for proper building and running a custom model with Python model support. Install System Packages Install the required system packages (Python, libraries, compiler, etc.). On a Windows machine, follow these steps: Install Visual Studio 2017 Community Edition. Install Python 2.7.16 or higher. Check that pip is installed. python m pip If an error occurs, install pip: Download get pip.py with curl or via browser. curl https: bootstrap.pypa.io get pip.py o get pip.py Install pip. python get pip.py On a Mac, proceed with these steps: Download and install Python 2.7.16 . Install the Compiler (GNU Compiler Collection). brew install gcc On Linux, run the following commands: Install Python on Fedora. dnf install gcc python2 python devel libstdc Install Python on Ubuntu. apt get install gcc python2.7 python dev python pip Finally, add Python 2.7 to the environment variable PATH, if it's not there already. Install Python Packages Before proceeding with packages installation quit all instances of terminal and start a new one or IDE with integrated terminal. To install all required Python packages, run the following command: :::note If you're planning to use a virtual environment, first activate it before running this command, and then proceed to the next section. ::: python m pip install user pip==19.0.2 setuptools==40.8.0 wheel==0.33.0 mxnet==1.3.1 scikit learn==0.20.2 mock==2.0.0 Using Virtual Environment If you want to use virtual environment, then set the environment variables: AUTOMLPYTHONBIN= Use these steps to set variables: MacOS: ~ .bash_profile → export = Linux: ~ .bashrc → export = Windows: My Computer > Properties Environment variables → = Quit and restart terminal or your IDE. :::note If you're using Nvidia GPU with configured CUDA, or Intel CPU with mkl driver, or both, you can install mxnet cu92 or mxnet mkl or mxnet cu92mkl instead of the mxnet package. ::: Using Python Classifiers To implement a Python classifier, use the mlConfig Named Entity in your AutoML SDK configuration. Here's an example implementing RandomForestClassifier with default parameters. @Named(\"mlConfig\") public Classifier mlConfig() { return RandomForestClassifier.builder() .build(); } Here's an example implementing RandomForestClassifier with some custom parameters. @Named(\"mlConfig\") public Classifier mlConfig() { return RandomForestClassifier.builder() .maxDepth(10) .estimators(100) .maxFeatures(MaxFeaturesType.LOG2) .build(); } Here's an example of GenericPythonClassifier — a generic customizable classifier. It can be used to set up any classifier manually by setting parameters and Python model name directly. @Named(\"mlConfig\") public Classifier mlConfig() { return GenericPythonClassifier.builder() .modelName(\"sklearn.RandomForestClassifier\") .parameter(\"max_depth\", 10) .parameter(\"n_estimators\", 100) .parameter(\"random_state\", 123) .parameter(\"verbose\", true) .build() } 1 : https: workfusion docs.s3.amazonaws.com vds ml sdk latest com workfusion vds sdk ml generic GenericPythonClassifier.html 2 : https: workfusion docs.s3.amazonaws.com vds ml sdk latest com workfusion vds sdk ml dp bolton linear logisticregression DpLogisticRegressionClassifier.html 3 : https: workfusion docs.s3.amazonaws.com vds ml sdk latest com workfusion vds sdk ml dp bolton linear svm DpSvmClassifier.html 4 : https: workfusion docs.s3.amazonaws.com vds ml sdk latest com workfusion vds sdk ml sklearn adaboost AdaBoostClassifier.html 5 : https: workfusion docs.s3.amazonaws.com vds ml sdk latest com workfusion vds sdk ml sklearn bagging BaggingClassifier.html 6 : https: workfusion docs.s3.amazonaws.com vds ml sdk latest com workfusion vds sdk ml sklearn decisiontree DecisionTreeClassifier.html 7 : https: workfusion docs.s3.amazonaws.com vds ml sdk latest com workfusion vds sdk ml sklearn extratree ExtraTreeClassifier.html 8 : https: workfusion docs.s3.amazonaws.com vds ml sdk latest com workfusion vds sdk ml sklearn extratrees ExtraTreesClassifier.html 9 : https: workfusion docs.s3.amazonaws.com vds ml sdk latest com workfusion vds sdk ml sklearn gradientboosting GradientBoostingClassifier.html 10 : https: workfusion docs.s3.amazonaws.com vds ml sdk latest com workfusion vds sdk ml sklearn kneighbors KNeighborsClassifier.html 11 : https: workfusion docs.s3.amazonaws.com vds ml sdk latest com workfusion vds sdk ml sklearn linearsvc LinearSupportVectorClassifier.html 12 : https: workfusion docs.s3.amazonaws.com vds ml sdk latest com workfusion vds sdk ml sklearn logisticregression LogisticRegression.html 13 : https: workfusion docs.s3.amazonaws.com vds ml sdk latest com workfusion vds sdk ml sklearn logisticregression LogisticRegressionCV.html 14 : https: workfusion docs.s3.amazonaws.com vds ml sdk latest com workfusion vds sdk ml sklearn randomforest RandomForestClassifier.html 15 : https: workfusion docs.s3.amazonaws.com vds ml sdk latest com workfusion vds sdk ml sklearn sgd StochasticGradientDescentClassifier.html "},{"version":"10.0","date":"Sep-20-2019","title":"statistics","name":"Statistics","fullPath":"iac/automl/automl-sdk/statistics","content":" To better understand the model results, AutoML SDK provides comprehensive statistics for further analysis and model tuning. For statistics calculation, AutoML SDK applies a standard MapReduce approach. Statistics calculator processes one document with gold and extracted fields at a time, and then creates a data transfer object (DTO) — a lightweight representation of statistics for the processed document. Statistics is calculated based on a collection of DTOs created previously. With a number of interfaces for custom calculations we also provide a basic reference implementation of statistics calculator for Information Extraction and Classification use cases. See the following sections for details. Visual representation of the calculated statistics is produced via a set of OOTB Printers. Although, ML Engineers can create their own printers and plug them into the reference implementation of statistics calculator to generate various types of reports: CSV, Excel document, etc. Main Interfaces ** StatisticsCalculator&lt;I extends Document, O extends DocumentResult&gt; (https: workfusion docs.s3.amazonaws.com vds ml sdk latest com workfusion vds sdk api nlp statistics StatisticsCalculator.html)* is the main interface for statistics calculation and should be as well implemented for custom* statistics calculation. O process(I document) O process(I document) gets Document as an input parameter and returns a DTO which extends DocumentResult. ML Engineer manually implements mapping from Document to DocumentResult. Method is called for each document in a training set. void calculate(Collection results) With void calculate(Collection results) the input parameter is a collection of DocumentResults calculated during the above mentioned Process method call. ML Engineer implements the main statistics calculator and execution of the provided Printers. Method is called only once in the end of execution. DocumentResult is an interface for DTO returned by the Process method during the mapping of Document to its statistics result. String getDocumentId() String getDocumentId() returns ID of the Document for which the results were calculated. Information Extraction IeStatisticsCalculator is an implementation for StatisticsCalculator for the Information Extraction use case. It accepts IeDocument as an input parameter and outputs IeDocumentResult. Using the Enable Disable methods of the calculator you can enable disable the following 3 different evaluation types: Value based (enabled by default). Gold and extracted results evaluated by the data value attribute of the corresponding Field and GoldField. Position based. Gold and extracted results evaluated by the begin and end position of the corresponding Field and GoldField. Text based. Gold and extracted results evaluated by the extracted text of the corresponding Field and GoldField. See StatisticsEvaluationType for more details. The Calculator produces statistics based on the provided strategies. It maps the extracted result of all Fields and gold result of all GoldFields for each FieldInfo to create an object based DTOs structure representing the result of mapping. IeDocumentResult contains information about the mapping result of extracted and gold fields. This information is represented as DTO to store all the needed data from the corresponding Field GoldField with a corresponding result type. :::note StatisticCalculator transforms the Field GoldField data into a lightweight DTO that is included into DocumentResult to prevent memory issues. ::: Method Description Collection getGroupResults() Returns a mapping results for group fields. Group field is a field represented as a collection of children fields. Collection getResults() Returns mapping results between Field and GoldField aggregated into DTO. Classification ClassificationStatisticsCalculator is an implementation of StatisticsCalculator for the Classification use case. It accepts ClassificationDocument as an input parameter and outputs ClassificationDocumentResult. ClassificationDocumentResult contains information about the mapping result between the extracted Label and GoldLabel. This information is represented as DTOs to store data from the corresponding Label GoldLabel. Method Description Collection getLabels() Returns a mapping result for Label GoldLabel classification. Printers Reference implementation contains a basic interface for all printers. Any number of printers can be plugged into the corresponding StatisticsCalculator. StatisticsPrinter&lt;D extends DocumentResult&gt; is an interface for statistics printer. It should be implemented in order to print statistics in a custom format. Method Description void print(Collection&lt;D&gt; results, Path output) Input parameters: collection of statistics DTOs and a path to a target folder. By default, this method is called for each evaluation type and in each statistics calculator where it's plugged in. This method is called during the statistics calculation. IeGoldVsExtractedCSVPrinter IeGoldVsExtractedCSVPrinter provides statistics for the Information Extraction use case and represents the mapping result between the gold and model extracted values. Input: collection of IeDocumentResult. Output: mapping result between each Gold and Extracted field. This printer can be configured to store results for all field names into one file (enabled by default), or into separate files where each file corresponds to one field name (set filePerField(true)). The result will be stored in gold vs extracted.csv or fieldName gold vs extracted.csv where fieldName is a name of field when filePerField is set to true. FILE _NAME — a name of document file from training evaluation set. FIELD _NAME — a name of field. GOLD — gold value of field. Value of this column depends on the evaluation type. EXTRACTED — extracted value of field. Value of this column depends on the evaluation type. RESULT — type of mapping between the Gold and Extracted value. It can have the following values: TP (True Positive) means that Gold and Extracted values are identical. FP (False Positive) means that the field was extracted, but does not map to any Gold value. FP, FN (False Positive, False Negative) means that both Gold and Extracted values are present, but differ. FN (False Negative) means that the Gold value is present but does not map to any Extracted field. REASON represents information about what caused the RESULT. It can have the following values: EXTRACT _MODEL means that Gold and Extracted values are identical, and were extracted by a model. Extracted by model means that value and text in extracted field are identical. EXTRACT _RULES means that Gold and Extracted values are identical, and were extracted by rules. Extracted by rules means that value and text in extracted field are different. FAIL _MODEL means that Gold and Extracted values are different, and failed by a model. Failed by model means that value and text in extracted field are identical. FAIL _RULES means that Gold and Extracted values are identical, and failed by rules. Failed by rules means that value and text in extracted field are different. IeGoldVsExtractedPerGroupCSVPrinter IeGoldVsExtractedPerGroupCSVPrinter provides statistics for the Information Extraction use case and represents the mapping result between the gold and model extracted group field values. Input: collection of IeDocumentResult. Output: mapping result between each Gold and Extracted group fields. Group field is a collection of multi value child fields associated via the tabNumber attribute. This printer can be configured to store results for all field names into one file (enabled by default), or into separate files where each file created for one field name (set filePerField(true)). The result will be written into group gold vs extracted.csv or fieldName group gold vs extracted.csv where fieldName is a name of field when filePerField is set to true. FILE _NAME — name of a document file from training evaluation set. GROUP _NAME — name of a group field. GOLD _TABNUMBER — value of the tabnumber attribute of all child fields of gold group field. EXTRACTED _TABNUMBER — value of the tabnumber attribute of all child fields in the extracted group field. EXTRACTED — extracted value of field. Value of this column depends on the evaluation type. RESULT — type of mapping between the Gold and Extracted value. It can have the following values: TP (True Positive) means that Gold and Extracted values are identical. FP (False Positive) means that the field was extracted, but does not map to any Gold value. FP, FN (False Positive, False Negative) means that both Gold and Extracted values are present, but differ. FN (False Negative) means that the Gold value is present but does not map to any Extracted field. CHILD _NAME — name of a child field. CHILD _GOLD — Gold value of a child field. Value of this column depends on the evaluation type. CHILD _EXTRACTED — extracted value of a child field. Value of this column depends on the evaluation type. IePerFieldCSVPrinter IePerFieldCSVPrinter provides statistics for the Information Extraction use case. Input: collection of IeDocumentResult. Output: metrics for each field name based on the mapping result between each Gold and Extracted field. The result is written to the per field.csv file, as in the following example. FIELD — name of a field. TP — True Positive. Number of correctly extracted fields. TN — True Negative. Always zero for Information Extraction tasks. FP — False Positive. Number of extracted fields that are not identical to any gold field. FN — False Negative. Number of gold fields that are not identical to any extracted fields. P — Precision. R — Recall. F1 — F1score. A — Accuracy. IePerGroupFieldCSVPrinter IePerGroupFieldCSVPrinter provides statistics for the Information Extraction use case. Input: collection of IeDocumentResult. Output: metrics for each group field name based on the mapping result between each Gold and Extracted group field. The result is written into per group field.csv file. FIELD — name of a field. TP — True Positive. Number of correctly extracted fields. TN — True Negative. Always zero for Information Extraction tasks. FP — False Positive. Number of extracted fields that are not identical to any gold field. FN — False Negative. Number of gold fields that are not identical to any extracted fields. P — Precision. R — Recall. F1 — F1 score. A — Accuracy. ClassificationGoldVsExtractedCSVPrinter ClassificationGoldVsExtractedCSVPrinter provides statistics for the Classification use case. Input: collection of ClassificationDocumentResult Output: mapping result between each Gold and Extracted label for each file. Results are written to label gold vs extracted.csv file. FILE _NAME — name of a document file from training evaluation set. GOLD _LABEL — Gold value of label for a document. EXTRACTED _LABEL — Extracted value of label for a document. RESULT — type of mapping between the Gold and Extracted value. It can have the following values: TP (True Positive) means that Gold and Extracted values are identical. FP (False Positive) means that the field was extracted, but does not map to any Gold value. FP, FN (False Positive, False Negative) means that both Gold and Extracted values are present, but differ. FN (False Negative) means that the Gold value is present but does not map to any Extracted field. REASON represents information about what caused the RESULT. It can have the following values: EXTRACT _MODEL means that Gold and Extracted values are identical, and were extracted by a model. FAIL _MODEL means that Gold and Extracted values are different, and failed by a model. ClassificationPerLabelCSVPrinter ClassificationPerLabelCSVPrinter provides statistics for the Classification use case. Input: collection of ClassificationDocumentResult. Output: metrics for each label in the mapping result between each gold and extracted labels for each document. Results are written to per label.csv file. LABEL — Name of a label. TP — True Positive. Number of correctly extracted fields. TN — True Negative. Always zero for Information Extraction tasks. FP — False Positive. Number of extracted fields that are not identical to any gold field. FN — False Negative. Number of gold fields that are not identical to any extracted fields. P — Precision. R — Recall. F1 — F1 score. A — Accuracy. Statistics Configuration To configure statistics calculators, define a @Named component in your model configuration which returns a list of StatisticsCalculator. The following code sample can be applied to an Information Extraction model. import java.util.ArrayList; import java.util.List; import com.workfusion.vds.sdk.api.hypermodel.annotation.ModelConfiguration; import com.workfusion.vds.sdk.api.hypermodel.annotation.Named; import com.workfusion.vds.sdk.api.nlp.statistics.StatisticsCalculator; import com.workfusion.vds.sdk.nlp.component.statistics.IeStatisticsCalculator; import com.workfusion.vds.sdk.nlp.component.statistics.StatisticsEvaluationType; import com.workfusion.vds.sdk.nlp.component.statistics.printer.IeGoldVsExtractedCSVPrinter; import com.workfusion.vds.sdk.nlp.component.statistics.printer.IeGoldVsExtractedPerGroupCSVPrinter; import com.workfusion.vds.sdk.nlp.component.statistics.printer.IePerFieldCSVPrinter; import com.workfusion.vds.sdk.nlp.component.statistics.printer.IePerGroupFieldCSVPrinter; @ModelConfiguration public class CustomStatisticConfiguration { @Named(\"customStatistics\") public List statisticsCalculators() { List calculators = new ArrayList(); add any number of OOTB or custom calculators calculators.add(new IeStatisticsCalculator() enable evaluation type (VALUE_BASED is enabled by default) .enable(StatisticsEvaluationType.VALUEBASED).enable(StatisticsEvaluationType.POSITIONBASED) .enable(StatisticsEvaluationType.TEXT_BASED) add any number of OOTB or custom printers field printers .printer(new IeGoldVsExtractedCSVPrinter()) .printer(new IePerFieldCSVPrinter()) group field printers .printer(new IeGoldVsExtractedPerGroupCSVPrinter()) .printer(new IePerGroupFieldCSVPrinter())); return calculators; } } The next code sample can be applied in a Classification model. import java.util.ArrayList; import java.util.List; import com.workfusion.vds.sdk.api.hypermodel.annotation.ModelConfiguration; import com.workfusion.vds.sdk.api.hypermodel.annotation.Named; import com.workfusion.vds.sdk.api.nlp.statistics.StatisticsCalculator; import com.workfusion.vds.sdk.nlp.component.statistics.ClassificationStatisticsCalculator; import com.workfusion.vds.sdk.nlp.component.statistics.printer.ClassificationGoldVsExtractedCSVPrinter; import com.workfusion.vds.sdk.nlp.component.statistics.printer.ClassificationPerLabelCSVPrinter; @ModelConfiguration public class CustomStatisticConfiguration { @Named(\"customStatistics\") public List statisticsCalculators() { List calculators = new ArrayList(); add any number of calculators OOTB or custom calculators.add(new ClassificationStatisticsCalculator() add any number of printers OOTB or custom .printer(new ClassificationGoldVsExtractedCSVPrinter()) .printer(new ClassificationPerLabelCSVPrinter())); return calculators; } } "},{"version":"10.0","date":"Sep-20-2019","title":"automl-delivery","name":"Complete View on AutoML Delivery","fullPath":"iac/automl/concepts/automl-delivery","content":" In this chapter, we will highlight the ML project complete delivery process. There are different complexities of ML requirements we face. This leads to different implementation approaches, i.e. different Roles can solve use cases, and collaboration between Roles may vary. Project Roles For a full list of available guides by project role, refer to the User guides section. AutoML Complexity Analysis Not all Use Cases are available for implementation with the out of the box models or custom models created by ML Engineers. Some require the involvement of WorkFusion Data Scientists. See the complexity decision diagram below. Search Engine Training In WorkFusion, training of new ML models is done in two stages: Search Engine finds the best parameters metamodel for the supplied training set. Model training itself uses a metamodel to train the ultimate model. The default HPO time limit configuration is 2 hours per field since SPA v9.0; until SPA 9.0 it was 10 hours per field. It is possible, but highly unlikely, that HPO for a field would finish before this time limit, because the optimization aim is to reach 100% quality (F1 metric), which is unlikely to happen. It is not recommended to lower the time limit, either, as the quality of the resulting features will likely be lower if you do not allow HPO to run enough experiments. Model Training and Tuning Tuning of the model by MLE is done through AutoML SDK. While MLEs can run training from AutoML SDK, Control Tower has a set of AutoML components used for the following purposes: Create data set (Manual Task, Automation training set) Manage quality of data set (Adjudication Rules, Qualification Task) Run training and extraction (Automation Business Process) Monitor & troubleshoot training (Marathon & Mesos, WFMLJobdata DataStore) View & analyze results (Automation Chart and Dashboards) Running Training from Control Tower To start model training from Control Tower, proceed with the following steps: Create a manual task selecting ML Model, BP Use Case and Training Set (assuming training set is ready and uploaded as Automation Training Set to CT) in the Automation options: TBD Creating a Manual Task. Run and stop the task to start model evaluation: TBD Starting a Training Process. Note that to see the training BP, you need to select Automation Training filter in Business Processes view — by default, training processes are hidden. Once training is finished, you will see an Automation Available label on the manual task. Then you can click on the label, set accuracy threshold, and apply recommendation to create a process with Automation sub process in it: TBD Setting Up an Automation Chart. Run the Automation process with the trained model extracting data: TBD Running a Cognitive Business Process. :::note Note that you can automate manual tasks that are steps of a business process in the same way as you automate standalone manual tasks (see TBD Automate Manual BP Steps). ::: Automation Business Process in Detail Automation Business Process is a collection of steps through which training, extraction, automatic quality control, statistics calculation and all the other surrounding activities of automation are executed. The Automation settings step is where BP parameters are configured (e.g. enable disable Statistical Quality Control and Statistics calculation). After the Settings step, BP divides into two flows: Training and Production. Training is launched first through the steps that were described above. Once a trained model is ready and you apply a recommendation, the manual task on which it was started (either within BP or standalone) is replaced with this Automation (sub)process and the Production flow in it is activated. Production flow of the Automation BP is where the trained model is applied for extracting data. It has the following structure: At Extract Information step, ML algorithms try to extract data automatically. The Post Extract step does post processing and normalization of data extracted by the ML model. Prepare data, basically, prepares data for the Statistics Calculation step. A special Composite Rule (Skip manual steps or is the data extracted automatically ) checks the Extract Information step results: If failure or partial success, then records are sent to human workers. Note that in case of failure, the data extracted by the model is cleared and humans tag all from scratch; in case of partial success, the data extracted by the model is shown to a human and they complete what was missed. If success, then apply SQC (Statistical Quality Control) or go to Statistics calculation if SQC is disabled. If SQC is enabled, and inspection is required, then send a batch of records to human workers (the same original manual task is present inside the SQC subprocess). If a BP uses gold data (documents tagged by human SMEs), the execution goes directly to the Statistics Calculation step without SQC. The Statistics Calculation step provides calculations for quality statistics and saves results to data store. Enabling Statistics Calculation is required to use Automation dashboards because they use statistics that are saved in the database at this step. To find details on all the steps, go to Automation BP Description. Records where the model is not confident are verified by human — but how do I know if the \"success\" decisions of the model are, in fact, correct SQC or AutoQC uses statistical methods to monitor and maintain the quality of model decisions. AutoQC subprocess chooses the optimally cost effective combination of automated machines and cloud workers that always deliver at or above the acceptable quality level. The main concept of AutoQC is to take a Sample from a defined Batch of items (BP records, documents, etc.) and verify each item in that sample (which is done by a human in the manual step inside the AutoQC sub process). After the sample verification, the whole batch is considered accepted or rejected depending on the Rejection Limit. This way, AutoQC makes conclusions on correctness of what the model considers to be success based on a sample. Read AutoQC to understand this concept and its mechanics in WorkFusion. Model Release and Go to Production The trained model can be moved from one environment to another. For guidance on how to do it, refer to Migrate Trained Model. When more data is accumulated, Model Retraining can be launched manually or set up to launch automatically. For a production process, the latter case would mean that the model is retrained in a production environment and automatically applied in place of the model that is currently running, therefore it is rarely used. Depending on customer architecture and security requirements, two Model (Re )Training Strategies can be applied: re training in DEV or in PROD. "},{"version":"10.0","date":"Jul-26-2019","title":"automl-glossary","name":"AutoML Glossary","fullPath":"iac/automl/concepts/automl-glossary","content":" Machine Learning Algorithm An algorithm that can learn from data and make predictions on it. Such algorithms overcome strictly static program rules by making data driven predictions or decisions. Examples of ML algorithms that are supported in AutoML SDK: SVM, DNN, regression etc. Tasks Classification The task of predicting class for the items. There are two types of Classification: Binary classification: you need to classify elements in two groups. Multiclass classification: you need to classify the elements into several groups. See more about Statistical Classification. Information Extraction The task of extracting structured information or key facts from unstructured and or semi structured documents. For example, extracting financial information from invoices, claims and other financial documents. Basic elements Token The smallest elementary element of a document, usually word separated by spaces or other special characters. Tokens are produced by Tokenizer component. Tokens are further used for content analysis in AutoML SDK framework pipeline. Element Elements are units that created while documents pass through the AutoML SDK pipeline. Elements are represented as tokens, tables, sentences, pages, Named Entities, or Entity Boundary Elements and are used in content analysis. Some elements are created based on tags from manual labeling, while others as a result of analysis of words, phrases, named entities in unstructured content. Feature A property of element being evaluated. Generally feature is a simple question with a Yes No answer which is used by the ML algorithm to predict which tokens should be tagged as correct answers. See more about Features). Covered Covering Designation used in AutoML SDK API that defines relationship between elements to find all the elements inside or outside the processed element in document. Covered covering relationship is used during AutoML SDK Feature Extractors configuration. For example: for each word token in a sentence, sentence is a covering element for token. For a sentence token, each word is a covered token. Model Model An artifact that represents a combination of AutoML SDK components, like algorithms and their parameters, annotators, feature extractors, pre and post processors. Model can be built from archetype—extended model—or created from scratch. Model can involve either AutoML Search Engine configuration or fixed configuration. Once model is configured, ML Engineer can proceed with model training. Generic model Out of the box machine learning model, designed to process semi structured documents. There are two types of generic models according to specific tasks: Information Extraction Classification Generic models can be used to process invoices, claims, emails and other documents. Extended Model Model configuration that is copied from generic model configuration and enforced with custom components. Tagged text A string, each values of which was assigned 1 or more tags. This text is used for model training. Trained Model The result of model training process, comprised of training set and model that fits the training set. Once ML Engineer obtains trained model with proper model results, it can be further used for model execution process. Training Set A set of documents (invoices, e mails, contracts, etc.) used for training an ML model. For Information Extraction, a training set consists of documents which contain gold values for a set of fields that should be extracted. For Classification, training set consists of documents accompanied by target class. AutoML AutoML A proprietary WorkFusion technology for Cognitive Automation. AutoML applies human in the loop approach and AutoML Search Engine to automatically find the best method of automation for the use case. AutoML SDK A WorkFusion Java framework for creating and extending machine learning models for any Information Extraction or Classification use case. Archetype A model project template that is available for configuration and provides possibility to make extensions for machine learning models. AutoML SDK provides Classification and Information Extraction Maven project archetypes. Archetypes provide ML Engineers with a great way to quickly get prototypes up and running. See more about Archetypes AutoML SDK Component A configurable part of AutoML SDK framework. AutoML SDK comprises following components: Parser Annotator Feature Extractors Algorithms Post processing Each component is responsible for a specific task. Components combination depends on the use case and documents structure. For example, for information extraction task, choice of components depends on field type (date, amount, address), field location in the document etc. Proceeding through the components pipeline AutoML SDK framework delivers a ready to use trained model. See more about Components AutoML SDK Pipeline The sequence of steps needed to fit a model to a data set. A pipeline is defined within model configuration and can consist of any combination of AutoML SDK components. Typically components have the following order within AutoML SDK pipeline: Parser, Annotator, Feature Extractors, Algorithms, Post processing. For each step in a pipeline there can be several components of a specific type. For example, two annotators are configured to extract \"Date\" field from specific sentences of a document: first to create \"Sentence\" elements, second to create Named Entity of type \"Date\". Fixed Configuration The model configuration with strictly pre defined set of components without including AutoML Search Engine configuration. In case fixed configuration is used for model configuration, then model training consists of running a single experiment. See more about Fixed Configuration. AutoML Search Engine A framework for searching the \"optimal\" set of components: annotators, feature extractors, post processors, algorithms and their parameters to train the model with. Search of \"optimal\" set of components is performed in a series of model experiments. In case AutoML Search is used in configuration, then each experiment involves systematically varying one or more components and examining the effect on result. AutoML Search Engine Configuration The process of defining dimensions of components and their combination. For example, model can have a number of declared annotators, but only a particular combination of these annotators performs the best result. Dimensions A strategy for component selection within AutoML Search Engine. For example: define whether feature extractor BaseKeywordsInElementFE is optional or required in AutoML SDK configuration. See more Machine Learning Workflow Feature Engineering The process of extracting or selecting features related to a training set in order to improve results for trained model. Model Configuration Process of defining AutoML SDK components. Model Experiment Single iteration of machine learning model training with specific combination of AutoML SDK components. Model Training The process of defining a model that fits the training set. In case model configuration includes AutoML Search Engine, model training means the search of the best combination of components. Result of model training process is trained model. Statistics Step An optional step in AutoML SDK pipeline which provides model results. Statistics helps ML Engineer to analyze model results for better understanding how to tune model configuration. Statistics is available for both Information Extraction and Classification use cases. Model Execution The process of applying a trained model to a data set. Usually model execution process applied on production data set and production environment. Cognitive Bot Workflow Use case that is used in SPA to run ML models training and execution. See more about Cognitive Bot Automatic Quality Control AutoQC refers to the use of statistical methods in the monitoring and maintaining of the quality of services. In Cognitive Bot Workflow, the AutoQC sub process chooses the optimally cost effective combination of cognitive bots and people that always deliver at or above the acceptable quality level. For more details, refer to in depth AutoQC description. Results Score A predicted value generated from a trained model. Precision Metric that is calculated to analyze model results, and defined as the number of items correctly predicted extracted divided by the total number of predicted extracted items. See more about Precision. Recall Metric that is calculated to analyze model results, and defined as the number of items correctly predicted extracted divided by the total number of items that actually belong to the class (gold results). See more about Recall. F1 A measure of a model's accuracy, calculated from Precision and Recall. See more about F1 Score. "},{"version":"10.0","date":"Aug-29-2019","title":"checker-model","name":"Checker Model","fullPath":"iac/automl/concepts/checker-model","content":" > Note: Checker model should be used by an ML Engineer to improve Information Extraction model results, if needed. WorkFusion AutoML uses estimated accuracy and cut off rules to deliver customer quality results. The use of cut off rules is always the choice of balance between precision or recall. In most cases, to improve the precision of the model, you need to sacrifice recall. This means, everything below a certain threshold is rejected and everything above is accepted. In real world scenarios, some accepted records may also contain errors as it's nearly impossible to separate correct and incorrect records using a threshold and at the same time not to drop recall dramatically. The concept behind the checker model is to try and identify incorrect values by training a separate meta model, and then use it to post process—that is validate and remove—incorrectly predicted values produced by the original model. Estimated accuracy model is then trained considering this information. Checker model is a binary classification model which labels each token of the input document as either correct (pos 1) or incorrect (neg 0). Training data includes features of the original model collected from .fe files combined with BIESO scores produced by the original model. BIESO is a tagging approach for multi word named entities where a model tags each token of the named entity with one of the following tags: B – First token of a multi word entity I – Inner token E – Last token of a multi word entity S – A single token entity O – A non entity token BIESO score represents model predictions for each token resulting in 1. Token B I E S O date 0.05 0.05 0.05 0.05 0.8 19 0.5 0.1 0.2 0.1 0.1 For more information, refer to the original paper (https: dl.acm.org citation.cfm id=2955027). Workflow Overview Checker model is applied both at training and execution stages. Training The following steps take place during model training with a checker model: Feature extraction. Cross validation. Train Classify Raw statistics calculation. Checker model training. Use features, scores from cross validation and correctness information of IE model as input training data to checker model. Checker model statistics calculation. Applying Post Processors. The first Post Processor should apply the checker model and filter results. Processed statistics calculation. Estimated accuracy model training. Execution Feature Extraction Information Extraction model classification. Values Post Processing. Checker model Post Processor. All other Post Processors. Estimated accuracy model extraction. Using Checker Model Checker model is disabled by default and can be enabled *for each field separately*. To enable the checker model from the Business Process, use the checkerModelEnabled boolean parameter, it should be added in \"Start Train and Eval\" bot configuration in Cognitive BP during answer info creation in createAnswerInfo method. In AutoML SDK model training runner, use the GenericPipelineConfiguration.CHECKERMODELENABLED parameter and put it to FieldInfo properties. Cut off rules can be applied to checker model using the checkerModelPrecisionThreshold parameter (0.85 by default), this parameter is defined for each field separately. For implementation details, see the following examples. Model Training Runner Add the following code to your Model Training Runner. ModelRunner new FieldInfo.Builder(Fields.INVOICE_NUMBER) .type(FieldType.INVOICE_NUMBER) .multiValue(false) .property(GenericPipelineConfiguration.CHECKERMODELENABLED, \"true\") .build() Business Process In Control Tower, navigate to Advanced &gt; Bot Configuration, and then use the following code to enable Checker model. Bot Configuration: Start Train and Evaluation Step applied for each field changes should be done in createAnswerInfo(AnswerInfoDTO answer) method Map properties = new HashMap(); properties.putAll(getOptionsMap(answer)); properties.put(\"checkerModelEnabled\", \"true\"); properties.put(\"checkerModelPrecisionThreshold\", \"0.95\"); Optional res.put(\"properties\", properties); Here's a Business Process example with a Checker model. Note: Checker model works only with Liblinear. Any error or exception during checker model training doesn't affect the original model training. Checker model training will be skipped in case of insufficient data. Checker model is applied using com.workfusion.vds.sdk.nlp.component.processing.CheckerModelPostProcessor. "},{"version":"10.0","date":"Jul-08-2019","title":"classification","name":"Classification","fullPath":"iac/automl/concepts/classification","content":" WorkFusion AutoML models provide an ability to automatically classify texts by analyzing its parts (tokens) and their combinations (features). Binary Classification Binary classification is the task of classifying the elements of a given set into two groups (predicting which group each one belongs to) Examples of Binary Classification: Dividend Announcements (is given text related or not related to dividends) Sentiment Analysis (positive or negative tweet analysis) Title Classification (compare two titles of a person and tell whether they match or not) Manual Task in WorkFusion Binary Classification can be done by human workers in WorkFusion: You provide a set of texts as a source. Workers need to read analyze the text and select one of two options (typically it is a radio button group): Yes or No (Valid or Invalid, Spam or Not Spam). Source texts and results (worker answers) are submitted to WorkFusion AutoML and Binary Classification ML models are trained on these data Task 1 Task 2 Results Results are stored in a separate column (match _result in the picture below): Depending on your business problem, you might be more interested in a model that performs well for a specific subset of these metrics. For example, two business applications might have very different requirements for their ML models: One application might need to be extremely sure about the positive predictions actually being positive (high precision) and be able to afford to misclassify some positive examples as negative (moderate recall). Another application might need to correctly predict as many positive examples as possible (high recall) and will accept some negative examples being misclassified as positive (moderate precision). Multi Class Classification Multi class classification is very similar to the Binary Classification, the only difference is that you need to classify the elements of a given set into several groups (more than two). Examples of Multi class Classification: Product Description (which product does the given text describe: Computers, Food, Clothes, Books) Text Style Classification (identify the style of the given text: Romance, Thriller, Adventure, etc.) Company News (match given news to one of the companies in the list: Apple, Microsoft, or Intel) Manual Task in WorkFusion Task 1 Task 2 Results "},{"version":"10.0","date":"Jul-08-2019","title":"data-set","name":"Data Set","fullPath":"iac/automl/concepts/data-set","content":" Training Set Training set is a set of documents (invoices, emails, contracts, etc.) used for training an ML model. For Information Extraction, a training set consists of documents which contain gold values (values surrounded by special tags) for a set of fields. Fields — attributes that should be extracted from the documents (for example, date, email, company name, company address). Each tagged value is an object for some particular field. It is surrounded by a special tag with special attributes. This text and the tag are called a gold tag or a gold value. Here's an example of extraction tag appearance on UI. And here's the extraction tag appearance in HTML structure. \"PGS 1 &nbsp; &nbsp; &nbsp; 13.00 DF \" 24, 20 == Field name is used as a tag name and its attributes contain the mark that it is a special tag class=\"extraction tag\" the text in which is a gold value, as well as some information about this value. The most important attributes of a tag are the following: data value — a gold tag attribute that will be returned as extraction result (what a user will see, if they try to save extraction results as JSON or CSV). May be changed in a manual task or by post processing as final result does not necessarily equal to the tagged string. Note that ML will be focused on tagged string, not data value. On the picture above you see data value=\"24.20\". Below is the case when the data value is not equal to the tagged text. tabnumber — attribute showing that the tag belong to some group of tags and the number of this group. We will give more details about this attribute in Tables Processing. appendorder — attribute defining in which order different chunks of the same tag should be appended to each other. Data Set Requirements The first requirement for data set concerns its size. It should consist of at least 500 1000 documents for unstructured data. For highly structured data the training set can be smaller e.g. if your population contains only one layout and all documents have same structure, 100 of them would be enough for training. A data set for machine learning should be tagged in a particular way only the model does not treat text same way as people do. While people can perceive text (be it names, separate words, numbers, etc.) in its entirety, understanding the meaning of what they see and being able to identify why it makes sense, to the model any text is just strings of symbols organized in an html or xml tree. Its goal is to identify a value and the context surrounding the value. Context is basically everything that surrounds the tagged piece of text, its position on the page and in some cases its position relative to other important pieces of information. Context is the most important source of features and is used to determine where a gold value should be searched. On this picture \"R\" is tagged as \"currency\" and 316,920.00 as total_amount. All the rest of the text is a short context for these two values. Gold values in the data set are the objects that should be classified as \"the value that should be extracted\". The set of features is defined on the whole data set (all the documents). In each document for each gold value feature values will be calculated and weighed during the training process and the model (the function) will be built. When the model is trained and you run it on some data, it will extract the strings that can be classified as \"the value should be extracted\" by their features (they are situated in the part of the feature space that contains objects that should be extracted). Values themselves are the source of features as well. For example, CUSIP number that is used for classifying financial instruments worldwide has a definite shape and check sum, so any extracted value for CUSIP can be validated by these criteria. Suppose that our documents contain more than one CUSIP and we are interested in some particular one for example, facility CUSIP. In this case CUSIP shape and check sum can be used for identifying all CUSIPs in the document while their context can be used for identifying the specific CUSIP we are interested in. The data set quality is very important for machine learning as wrong values or wrong context are the source of incorrect features and weights, which affects the model's quality. \"Garbage in garbage out\" is the main principle of supervised machine learning, which means if the training set is not good enough and big enough, the model won't show good results. And the data set quality is one of the most important DA responsibilities. A good data set meets the following requirements: Diversity Objects from bad represented or not represented layouts may have feature values which make the model treat them like outliers. In best case they will be ignored, in worse case they will generate additional false positives. That's why it's important to have enough representation of all layouts and features we aim to extract, as in the reality. Consistency Consistency is one of the most important requirements to your data set. Each field should be consistently tagged across all the documents (or a certain layout): for example, if an invoice has the same \"po _number\" occurring on all the pages of the document, we should always label the one at the same location (on the same one page across all the data set; for example, on the first page). Otherwise, it will send a confused signal to Machine Learning algorithm: If a layout contains the same value several times in different parts of a document (it's rather common case for some fields, e.g total invoice amount, taxes), it's recommended to chose one context where it will be tagged in advance and stick to this context across all the documents of this layout. If we have the following situation for some field: Context Value Value 1 Value 2 Value 3 Value 4 Value 5 : : : : : : Context 1 tagged tagged Context 2 tagged Context 3 tagged tagged The model may treat it in the following way: Context Value Value 1 Value 2 Value 3 Value 4 Value 5 Conclusion Context 1 Correct Incorrect Incorrect Incorrect Correct Not extracted in 60% =&gt; most probably should not be extracted =&gt; \"not to extract\" =&gt; FN Context 2 Incorrect Incorrect Correct Incorrect Incorrect Not extracted in 80% =&gt; most probably should not be extracted =&gt; \"not to extract\" =&gt; FN Context 3 Incorrect Correct Incorrect Correct Incorrect Not extracted in 60% =&gt; most probably should not be extracted =&gt; \"not to extract\" =&gt; FN Here the fact that same value was tagged in different context across documents resulted in the model deciding that the value should not be extracted at all. As we mentioned, context and a document structure around the tagged value is an important source of features. If the value is not extracted in the context the model expects it to be, it may learn that the values in this context may be incorrect and should not be extracted at all. It will generate additional false negatives in extraction results and affect Recall. Another bad effect may take place if the model defined the features for context in not an optimal way. For example, in one type invoicedate is given in the document after \"Invoice date:\" and after \"Date:\" and is tagged in both contexts. Documents also contain \"Payment date: {date in the same format}\". In this case the model may find \"the context like required\" and extract payment date instead of invoice date as has incorrect feature \"date: {value that should be extracted}\". It will generate additional false positives in extraction results and affect Precision._ If a Data Scientist requires to tag all the appearances of the field (in case they want to let the model learn all the possible contexts), a field should be tagged in each of its appearances (missing any of them is a gross error). However, it's a rare case as long as we understand that a human factor may result in missed and untagged appearances and, futhermore, in additional false values, hence, it's a very rare practice. Completion Completion has two different aspects: If a correct field is given in the document, it must be tagged. Otherwise: The model may treat it as incorrect and generate additional FN by the same logic as in the previous example (affects Recall). If its feature values will be enough to be extracted and it's extracted, when counting evaluation statistics we will have a FP here because the gold is empty (affects Precision). If a field contains multiple words (in ML space, we call it a \"token\"), we should always label the entire string (with all the words) belonging to the field. For example, we should extract suppliername and it's always a company name. We see \"WorkFusion Inc.\" and think that \"WorkFusion\" is important information and should be tagged and \"Inc.\" is not very important and we don't tag it. The model gets the information that only a part of a string that contains suppliername should be extracted as supplier name. And when the model \"sees\" \"The Bank of America\", it will extract \"The Bank of\" because previously we \"told\" it that a part of company name is correct value and entire string is incorrect value. Gold data Extraction results : : WorkFusion Inc. The Bank of America In case you need to exclude some part of the string from the values, for example there is a requirement that supplier_name shouldn't contain legal endings, it's recommended to tag the full value, correct data value and apply post processing (in case of legal endings, it's better to use dictionary based approach) to the model results, so that you have full control of what is changed. Normalization If the value can be given in many formats, it should be normalized to the same one format across all the gold data. For example, date can be given as 01 18 2017, January 18, 2017, Jan 18 2017, etc. All these option are correct but presented in different format so we need to normalize them to a common format by selecting a mask for date in the manual task designer. If the values should be mapped and some reference data is planned to be used, the reference data should be added as a dropdown in a manual task. If a value should be normalized in some special way (for example legal endings should be removed from a company name) the data value should be corrected in a manual task (or later in post processing). If the values are not normalized, additional false positives may appear in extraction statistics (which affects Precision). Gold Value Extracted Value Decision Comment Jan 18 2017 01 18 2017 FP Cannot be compared with the extracted result correctly 18.01.2017 01 18 2017 FP The date wasn't normalized in gold data and cannot be compared with the exacted result correctly WorkFusion Inc. WorkFusion FP The extracted value is correct and normalized but the gold value is not normalized, therefore, they cannot be compared correctly. WorkFusion Inc. WorkFusion (UK) Limited FP The extracted value was derived from the company name and its address (which is UK address) using reference data, but it wasn't done in gold value, therefore, gold and extracted values cannot be compared correctly. We covered the most common requirements to a data set. A few other requirements for a training set include the following: make sure that there are no conflicting values in different fields (e.g. “Male” and “Pregnant”), number values are not out of range (“Age=1000”), missing values (either empty fields, or the fields that are recognized as containing backspaces but which are actually empty) etc. Typical Cases of Incorrect Tagging Extra Data Tagged Tag E mail contains phrase \"E mail\" which doesn’t relate to the tag. Incomplete Data Tagged Tag Invoice number doesn’t contain full information from the required field. Optical Character Recognition (OCR) In 80% of use cases WorkFusion AutoML processes documents that are scans, unsearchable PDF files or images. All WorkFusion models require HTML, XML or plain text documents as an input. Therefore, to get the documents in suitable format, we use 3rd party OCR (Optical Character Recognition) software in order to convert images and PDF documents into text (plain text, XML, or HTML). To achieve that, WorkFusion uses ABBYY OCR which offers an out of the box solution: we don’t need to write or maintain any custom code. However, each iteration is chargeable: we pay for each page. Before sending documents for manual tagging and preparing the data set, OCR quality should be checked, because OCRed documents structure is an important source of features. The original document structure is not always exactly reproduced by OCR. There can be some difference between original and OCRed document. For example, different number of lines in text, different tables formatting, additional new lines and paragraphs. So, OCRed document looks not exactly the same as original document, but it doesn't prevent ML to process it and handle these changes correctly, especially if they are reproduced across all or most of the documents within one layout. OCR can make mistakes in recognition of some characters. For example, zeros (\"0\") can be recognized as a letter \"O\". Such mistakes can be handled by adding post processing logic that replaces incorrect symbols or symbol sequences with correct ones. If OCR mistakes in values or changes in document structure can be fixed by post processing, they don't require changing OCR settings. In some cases OCR results are not accurate enough (hand written text on documents, images with low DPI, twisted scans, crumpled paper, scans of photocopies, etc.) which causes certain problems (table margins, wrong or missing characters etc., appearance of additional structures that corrupt values in the way that ML cannot find them and extract correctly). Therefore, OCR results may be not accurate. Below are some common problems that you may see after OCR: Problem Solution Data Set Compliance Text is missed. Change OCR parameters. Exclude Characters are wrong, but can be recognized by human. If it's an occasional mistake, that appears in some documents of a layout, it should be corrected manually in HT when creating the gold set, and handled by post processing in automation results. If some value is corrupted in the whole layout and cannot be corrected by post processing (only removed), OCR should be improved. There can be some exclusions. E.g.: company name is given only in logo and it cannot be recognized well by OCR. Include in case of occasional mistake. The value is totally corrupted and cannot be recognized by human. If the value itself and it's context is totally corrupted and without checking the original document you cannot understand what should be in this part of document, it should not be tagged. The document should be excluded from the training set, because it won't let the model learn anything useful and may be the source of wrong features and weights. If such documents are rather regular cases in the common documents flow and are not caused by the original documents quality, it's recommended to improve OCR quality. Exclude Corrupted, irregular structure. Text may be converted into a tables with random structure that won't be reproduced in other documents. If the value to be extracted is in the table, it's recommended to exclude such documents from the training set as they may be a source of wrong features and weights. If such documents are rather regular cases in the common documents flow and are not caused by the original documents quality, it's recommended to improve OCR quality. Exclude It's recommended to exclude the document's with strongly corrupted structure, missed text and values that cannot be recognized by human from the training set, but they should be included into the test set (in ideal case in the same proportion as they are represented in regular documents flow). If they are excluded from ML results evaluation, the statistics will be overestimated and won't be reproduced on the customer testing phase. Remember, a good enough and big enough training set is an important factor in ML success. \"Garbage in – garbage out\" is how ML works. If OCR quality is low, wrong features will be defined and the model quality will be low. If the training set contains mistakes, they will be reproduced in evaluation and you may not achieve your targets (each use case has targets for Precision and Recall). If gold data precision is lower than the target for the use case (for example, your target is P=95% and gold data contains 15% errors), you will never be able to achieve such target. So, be mindful about the training set quality. "},{"version":"10.0","date":"Jul-08-2019","title":"information-extraction","name":"Information Extraction","fullPath":"iac/automl/concepts/information-extraction","content":" Information Extraction is a process of extracting structured information (or key facts) from unstructured and or semi structured documents (invoices, claims, dividend news, etc.). Input Data In real cases, customers have diverse paper or scanned documents in PDF, PNG, JPEG, TIFF or other formats. These scanned documents are stored on a shared network drive, on a server file system, or in some application (SAP, Salesforce, SharePoint). Business Problem AutoML's Information Extraction feature solves a variety of problems. The main ones can be narrowed down to the following: Convert scanned documents into digital text documents. Extract structured content from these documents (account ID, amount, currency, etc.). Provide the structured content to some data store. All these steps need to be automated, it means made without or with a minimal human work. Workflow To solve the business problem stated above, the following steps need to be completed in WorkFusion: Execute OCR (Optical Character Recognition) for all scanned documents. Input: Scanned PDF Output: Digital HTML Create and execute an Information Extraction manual task in Control Tower. In this task, human workers select text chunks and tag them using mouse or hotkeys. Previously OCRed digital documents are used as an input data for Information Extraction. After human workers have manually tagged the documents, the tagged text can be used for training AutoML models so that the next document batch can be partially extracted by a cognitive bot. The structured tagged results can be exported from Control Tower to any other system. "},{"version":"10.0","date":"Jul-31-2019","title":"machine-learning-basics","name":"Machine Learning Basics","fullPath":"iac/automl/concepts/machine-learning-basics","content":" Types of ML Algorithms There are two types of machine learning: Supervised learning – analyzes the training data and produces an inferred function. The training data consist of a set of training examples. Each example is a pair consisting of an input object and a desired output value. Examples: logistic regression, decision tree, naive bayes classifier, support vector machine, bagging (e.g. random forest), some types of neural networks. Unsupervised machine learning – infers a function to describe hidden structure from \"unlabeled\" data. Since the examples given to the learner are unlabeled, there is no evaluation of the accuracy of the structure that is output by the relevant algorithm—which is one way of distinguishing unsupervised learning from supervised learning. Examples: k means, other types of neural networks :). Supervised Learning Supervised machine learning: WorkFusion models are based on supervised ML algorithms which use training data to analyze hidden patterns and then predict results on the basis of its findings. Building a supervised ML model usually includes the following steps: Determine what type of objects should be used for training. In WorkFusion, most common problems are classification and information extraction. For classification, the object is a pair \"a document → class\", for information extraction an object is a pair \"gold value (information that should be extracted) → field\" Collect the training set which represents how the function can be used in real world. In WorkFusion use cases human in the loop is used and training sets are usually collected by manual labeling documents for classification and gold values for information extraction. In rare cases training sets are tagged automatically, using provided historical data. Determine the features that represent the learned function. The model performance quality depends strongly on how the input objects are represented. Typically, the input object is transformed into a feature vector, which contains a number of features that describe this object. The number of features should not be too large, but should contain enough information to predict objects correctly. In WorkFusion out of the box models' features are defined automatically while in custom models they are created manually. Determine the structure of the learned function and corresponding learning algorithm. Complete the development phase (isn't required for WF out of the box models). Train the learning algorithm on available training set and get the model (function). Evaluate the performance of the learned function. After training process performance of the resulting function should be measured. In order to ensure the objectiveness of measurement, it is better to use a test set that is not part of the training set. These steps are completed on each use case where machine learning is used. Now let's see a simplified version of what is a model on an example of Support Vector Machine (SVM). Support Vector Machines is a binary classification algorithm. Being given a set of objects belonging to two classes, it builds a function which divides objects accordingly in an n dimensional feature space. Suppose we have a training set that consists of objects belonging to classes “red squares”and “blue circles”. Our goal is to predict the class for a new unseen object. Support Vector Machine (1 dimensional hyper plane in 2 dimensional space) To solve this problem, we need a training set of red squares and blue circles representative enough to describe all real world object of these classes. If the training set doesn't \"illustrate\" the real world adequately, the model trained on it may work incorrectly on the objects that are not included into the training set. Next step is to define the features that will describe each object and give information that allows to build a function that can work correctly in the real world, not only on the training set. As we mentioned earlier, in WorkFusion models' features are generated automatically or created manually. In both cases the whole training set is used as a source of features. Many facts are taken into account, facts are transformed into features that describe objects and define each object's class. Therefore, each object has some features. If we try to describe the objects from the picture below by two features, we will get the following description: In WorkFusion AutoML, all the feature values are numeric values and each object is determined by all its feature values (the vector of features) and is marked by a label (class). In our example (blue circles and red squares objects described by 2 features), all the objects are situated in 2 dimensional feature space, where each dimension corresponds to a certain feature. One object’s features define its coordinates. (Picture 1) In SVM we define the object's class by the following function: Function yi = f(xi1 xi2) = sign(w0 w1xi1 w2xi2) where: xi1, xi2 coordinates or feature values of i th object, yi class of i th object, w0, w1, w2 feature weights. Feature weights are coefficients that show the importance of each feature and the strength of its impact on a classification decision. Class function returns 1 or 1 that defines which class is assigned to the object. Now we need to build a separating hyper plane, that divides objects into two classes. The process of building this function is a *training process* (or training). There is more than one way to build such a hyper plane. The most optimal ones are the following: If the data set is linearly separable, the hyper plane should classify all the objects correctly and with maximum margin (red line on the picture). If the data set is not linearly separable, the number of incorrectly classified objects should be minimized. Separating hyper plane is described by the following formula: Function w1 x1 w2 x2 ... wn *xn b = 0 where: xi feature value, wi weights, b constant value. Feature values for objects in a dataset are defined before the training starts, so during the training process SVM calculates feature weights, thus, builds a separating hyper plane. As we mentioned before there are many ways of building the hyper plane. How to Evaluate a Model To define which way is better and what is the difference between them, we need to evaluate how correctly they classify objects. To do so, we run our model on some objects and check whether objects are classified correctly. If the dataset is linearly separable, all the objects of one class should be on the same side of a hyper plane. Binary Classification Suppose that both classes are equally interesting to us. For binary classification there are 4 possible results: Predicted class red squares* Predicted class blue circles* Actual class red squares** Correctly classified red squares Incorrectly classified red squares Actual class blue circles Incorrectly classified blue circles Correctly classified blue circles Here we need to separate correctly and incorrectly classified objects: they are true answers and false answers. Objects of two classes are usually called positives (objects of the first class, e.g. red squares) and negatives (objects of the second class, e.g. blue circles). Finally, we have: True positives (TP) and true negatives (TN) objects of the first class and the second class, that were classified correctly. False positives (FP) and false negatives (FN) objects of the first class and the second class, that were classified incorrectly. If one class is more interesting to us, for example we have a classification \"invoice not invoice\" and want to process invoices (target class, relevant elements) further, the classification results will look as follows: Their distribution in the table form is called \"confusion matrix\" and looks like this: Predicted class red squares* Predicted class blue circles* Actual class red squares** TP FN Actual class blue circles** FP TN Binary Classification Calculations Precision (calculated per class only) shows how exact is classification: among all the objects defined by the model as red squares, which of them actually belong to this class It's calculated the following way: P = Correctly classified red squares Total number of objects classified as red squares (the same for blue circles or any other class). Recall (calculated per class only) shows how complete is classification: how many real red squares were classified as red squares It’s calculated the following way: R = Correctly classified red squares Total number of all red squares in a dataset (the same formula for blue circles or any other class). Accuracy (calculated for the whole set) = Total number of correctly classified objects Total number of objects . In confusion matrix it's the number of elements in diagonal the number of elements. Practice Assignment 1 Multiclass Classification of Results In the previous case of binary classification, there were only 2 classes making one class “Positive” and the other class “Negative”. In case of multiclass classification, there are two possible solutions of assigning relevance “Positive Negative, True False” to classes: Define “true false” values for each class separately (analyzes only TPs and FPs in the extracted class sample) Set one class as a major one and treat the rest as an aggregated class “Others” (“negatives”, irrelevant). The second method is used in WorkFusion practice more frequently. Example 1 (first method) Let’s assume we have 30 documents belonging to 3 different classes: 10 documents of A class, 10 documents of B class and 10 documents of C class. The algorithm processed them and gave the following results: 13 documents retrieved as A class documents: 7 documents of A class, 4 documents of B class and 2 documents of C class; 6 documents retrieved as B class documents: 2 documents of A class, 1 document of B class and 3 documents of C class 11 documents retrieved as C class documents: 1 document of A class, 5 documents of B class and 5 documents of C class. Actual class A B C Predicted class A True A False A False A B False B True B False B C False C False C True C If all the classes are equally important. Actual class A B C Predicted class A 7 4 2 B 2 1 3 C 1 5 5 Example 1 confusion matrix: A class B class C class Precision 7 13=53.8% 1 6=16.7% 5 11=45.45% Recall 7 10 = 70% 1 10 = 10% 5 10 = 50% Accuracy (for the whole set): we use only TP values in numerator (documents whose classes were recognized by the algorithm correctly) and all the elements in denominator: Acc=13 30=43.3% Example 2 (second method) If we are focused on one class (e.g. class A), we can transform our multiclass confusion matrix into binary classification confusion matrix. In this case, we redraw the confusion matrix as follows: Actual Class A B C Predicted Class A True Positive False Positive False Positive B False Negative True Negative True Negative C False Negative True Negative True Negative or: Actual Class A Other classes Predicted class A True Positive False Positive Other classes False Negative True Negative According to this, the confusion matrix for Example 2 will look like this: Actual Class A Other Classes Predicted Class A 7 6 Other Classes 3 14 Precision=7 13=53.8% Recall= 7 10 = 70% Accuracy= (7 14) 30=70% Practice Assignment 2 Information Extraction Results Information extraction models classify their results as TPs, FPs, TNs and FNs as well. The document is divided into tokens and, in the very beginning, each token is classified on the basis of whether it should be extracted or not (positives and negatives). Thus, first of all, the model finds the set of objects that can be extracted. There can also be some additional conditions that the classified values should satisfy to be extracted. For example, we need one value for the field and the model found 5 values that it considers as suitable. In this case, it may check which object has the maximum margin to the hyper plane (the margin describes how confident the model is in classifying the object as belonging to this class) and chooses this one – the object with the highest confidence or score. IE model's quality is estimated by comparing gold data and extraction results received after applying a particular model. We use the same terms and the same metrics for quality description as we do for classification: For estimating decision on an object: True positive – the value should be extracted and was extracted correctly (result=gold≠empty). False positive – the value should not be extracted (gold=empty), but was extracted (result≠empty). True negative – the value should not be extracted (gold=empty) and was not extracted (result=empty). False negative – the value should be extracted (gold≠empty) but was not extracted by the model (result=empty). False positive, False negative one value should have been extracted but the model extracted another (wrong) value. It consists of two parts: The model did not extract correct value where correct value was available; machine missed correct value FN Machine extracted value, but that value is incorrect FP **!!! Please, pay attention that this mistake should be included into the set of FPs and the set of FNs to get the correct statistics.** Example: field_name Gold Extracted Decision invoice_date 3 3 2017 3 3 2017 True positive invoice_date 5 2 2017 False negative invoice_date 3 7 2017 3 3 2017 False positive, False negative invoice_date 9 5 2017 False positive invoice_date True negative True negatives are usually considered to be not a very useful metric as customers are usually interested only in the objects that should be extracted. There is a slight difference between estimating classification results and information extraction results. In classification, the number of classified objects and the total number of objects are the same. In case of information extraction task, imagine that you need to process 1000 documents and only 500 of them contain a gold value for a certain field (e. g. e mail). Let’s suppose that our model extracted 400. We see that: the number of documents ≠ the number of gold values ≠ the number of extracted values Hence, in contrast to classification tasks we have two new metrics we are interested in: how many objects we should extract (Gold values), how many objects we extracted (extracted values). In order to know how accurately the model extracts values, we calculate Precision: Precision (P) = Correctly extracted Extracted, or Precision (P) = TP (TP FP), where: FP (FP,FN) ∈ FP In order to know the percent of existing values that can be extracted by the model, we calculate Recall: Recall (R) = Correctly extracted Gold values, or Recall (R) = TP (TP FN), where: FN (FP,FN) ∈ FN Accuracy is not used for information extraction because it describes how accurate the model is for all the classes while in information extraction we have two classes (empty values and not empty values) and usually are interested in quality metrics of “not empty values” only. Let’s see how it works on our documents with e mails. We have: The number of documents Gold values Extracted values TP (correctly extracted) FP (model’s mistakes) FN (missed gold vallues) 1000 500 400 350 50 150 P = TP Extracted=350 400 = 0.875 = 87.5% R = TP Gold = 350 500 = 0.7 = 70% Practice Assignment 3 Natural Language Processing Given: unstructured text (dividend news, invoices, reports) Task: extract particular fields (invoice number, date, amount, currency) from this unstructured text using ML. Phase 1: Annotation (Tagging) Phase 2: Feature Extraction Sample set of Features for the Amount field: Feature Value is a number yes position after Currency field yes position before \" share\" yes can be less than zero no is last word in sentence no "},{"version":"10.0","date":"Aug-23-2019","title":"what-can-automl-do","name":"What Can AutoML Do?","fullPath":"iac/automl/concepts/what-can-automl-do","content":" AutoML is a complex technology and thus requires a deep understanding of core concepts behind machine learning and automation. WorkFusion recognizes two main types of use cases where AutoML can be effectively applied. Cognitive automation is delivered for business workflows which include Information Extraction and Classification. Cognitive Automation AutoML allows to automate at scale and deliver continuously into production without a Data Scientist in a team. Here's a quick visualization of Cognitive Automation flow. Typical Use Cases WorkFusion is used for but not limited to the following use cases: Information Extraction — a process of extracting structured information (or key facts) from unstructured and or semi structured documents. Classification — arrangement in groups or categories according to established criteria. Both binary and multi class are supported. Information Extraction An Information Extraction (IE) use case is used when data defined by business logic is taken out (extracted) from documents and processed according to business rules. In terms of Information Extraction, each use case data point is referred as a \"field\". For example, invoice number, supplier name and quantity of products have to be extracted from all invoices. It means there are three fields (invoice number, supplier name and quantity) to be extracted from documents in this use case. Classification A Classification use case is applied when it is necessary to define the class for the item (document). By class, we usually mean different document types. For example invoices, purchase orders and claims are processed in one workflow and each document type is handled differently. That means we need to classify these documents first before applying automation. Solutions The following video covers typical use cases automated in Insurance, Healthcare, Financial Services and other sectors. It describes problems addressed, how they are solved using the platform and the outcomes. For more in depth information, refer to Machine Learning Basics manual. Resources AutoML Concepts AutoML SDK "},{"version":"10.0","date":"Jul-08-2019","title":"understanding-automl","name":"Understanding AutoML","fullPath":"iac/automl/concepts/understanding-automl","content":" AutoML is a powerful complex technology and thus requires a deep understanding of core concepts behind machine learning and automation. This page contains topics that we found useful when starting with AutoML. Feel free to skip the ones you feel familiar with."},{"version":"10.0","date":"Jul-10-2019","title":"models-overview","name":"Models Overview","fullPath":"iac/automl/concepts/models-overview","content":" WorkFusion is used for but not limited to the following use cases: Classification Classification is arrangement in groups or categories according to established criteria. Both binary and multi class are supported. Examples are email classification, documents identification, classification of pages within documents, approval process. Information Extraction Information Extraction is a process of extracting structured information (or key facts) from unstructured and or semi structured documents. Example are invoice, claims, financial document details extraction. Glossary Search Engine — A mechanism of choosing a set of hyper parameters for a learning algorithm, with the goal of optimizing the algorithm's performance on an independent data set. For details, refer to Search Engine documentation. Model Configuration — A set of parameters used for model training and Search Engine, including algorithms and their parameters, Feature Extractors, pre and post processors. Hyper model — Model Configuration along with a set of dependencies required for training. Trained Model — Final binary artifact which is produced by a Hyper Model via training. Depending on the selected algorithm the following information could be stored: NN structure, scores, coefficient matrix, decision tree configuration, etc. Hyper Model There are two types of Hyper models: Fixed set model which contains \"optimal\" model configuration created by a Data Scientist for a specific use case. Search Engine model finds the \"optimal\" configuration by running a Search Engine. Typically requires more time and resources compared to fixed set model. Each hyper model has a number of parameters which define pre requisites. For example, minDocs — minimum amount of documents required for training, or deltaDocs — amount of documents which can trigger automatic re training. Single Model vs. Multi Model Under the hood WorkFusion allows 2 types of models for handling complex documents: single model and multi model. Single Model Approach This approach implies training a single binary Model for all fields. It works when you need to extract fields with the same semantic meaning. All of the fields are equally important and used in a use case. Post processing is performed for fields. This approach is optimized for performance of training and execution. Multi Model Approach This approach implies training a set of individual binary models per field. it works when the semantic meaning of fields may vary, it means different sub sets of fields are used in a use case. This approach is optimized for higher Automation Rate Accuracy. "},{"version":"10.0","date":"Sep-20-2019","title":"classification-runbook","name":"Classification runbook","fullPath":"iac/automl/mle-manual/classification-runbook","content":" Create a new manual task To create a new Manual Task, click Manual Task > Create > Other > Miscellaneous Tasks > Select This Use Case. This opens a new Manual Task window. Upload input data On the Data tab, click Upload Data, and then click Add to select your input classification .CSV file containing a column either with links to an .XML file or with XML content itself. For demo purposes, let's name this column 'document'. This column will be used for further steps. You will see the following warning message, ignore it for now. Add answer type Now let's add an answer to be displayed to a user when performing an assignment. To add a new answer type, proceed with the following steps: On the Design tab, click Add Answer. In the Unique Code field, type 'document' or the name you gave this column when uploading your .CSV file. If the Unique Code and input data column names are different, save you Manual Task, and then map the column in the Data tab. :::note This Unique Code will be used as part of the identifier for your future model. ::: In the Answer field, type \"Document\". This will be displayed to a user when performing the assignment. In the Answer Type dropdown menu, select Information Extraction. Click Save. On the New Manual Task page the field allows to display information from the input file. You can do either of the following: Substitute blank with the name of the column containing the necessary information. Delete it. If you don't need it. Map some column from the input file to blank. Described in the next 2 steps. Change the name of you Manual Task, and then click Save. In the Data tab, click the system _id header as shown in the following figure, and then select the column to map (in this case, 'document'). Add answer with classes Next, let's add an answer to define classes and how they will be displayed to user. As a result you will get something like this. To create an answer with classes, proceed with the following steps: On the Design tab, click Add Answer. In the Add Answer window, type 'answer' in the Unique Code field and 'Answer' in the Answer field. In the Answer Type dropdown menu, select Check One or Select One. Next, define the classes. Either list classes one by one in the Options field or select Input from Data Store. To list classes manually, provide options for the answer using the key=value format. For example, Eurobond=eurobond. See the example below. To input classes from Data Store, create one using the Data Stores guide. Note that Data Store should contain the id and name columns. Mark this answer as Required. Configure AutoML Next, let's link our Manual Task with AutoML to train a classification model and generate a Cognitive Bot. Machine Learning pipelines First, configure the ML Pipeline by following these steps: On the Configure AutoML tab, select Train New Model. In the Machine Learning Pipelines section, select training parameters: Machine Learning Model: Multi Class Classification Generic Search Engine 2.0 Version: 9.2.0.11 Cognitive Bot Workflow: Cognitive Bot Workflow 9.1.0.9 Training set To train a model, we need some training data — Training Set. If you don't have any training data, use the default option Training Set will be created automatically. This means users will need to complete this Manual Task before training a model so that there's enough human answers to train a model with. If you already have some data, create a Training Set and select it from list. Follow these instructions: In the left sidebar menu, click Advanced > Automation Training Sets. Click Create Training Set. Enter your Training Set name. Add description, if needed. Select Upload File, and then click Upload Data. Click Add, and then navigate to the .CSV file which contains: tagged _text column (plain text or HTML XML code, for example after OCR). Сolumn name provided as the Unique Code of you answer with classes. It should contain corresponding classes (value from the Options * field or id from Input from Data Store field). Refresh the page with your Manual Task when you finish creating the Training Set to see it in the list, and then select it from the dropdown menu. Click Save. Run training Now that we have AutoML configured, let's run the training. Go to the Run tab, and then click Actions. Click Evaluate Cognitive Bot from the actions list. Optional: If you want to add completed records from this Manual Task to your Training Set select Append data records from this task to the Training Set. Click Continue. :::note Evaluation process may take up to few hours. ::: Track progress If all previous steps were correct, you will see a blue icon Cognitive Bot Evaluation next to Manual Task name. You can also check how the training goes in the Business Processes section. In the left sidebar menu, click Business Processes > View All. Select Advanced, and then select Cognitive Bot Training from the Filter dropdown menu. You will see your training in the list. :::tip Also you can use Marathon and Mesos to track the progress. Ask your OPS Team to provide access to these applications, if needed. ::: As soon as your training is finished you will see green icon Cognitive Bot Ready. "},{"version":"10.0","date":"Sep-20-2019","title":"autoqc","name":"AutoQC","fullPath":"iac/automl/mle-manual/autoqc","content":" Automatic Quality Control (AutoQC) refers to the use of statistical methods in the monitoring and maintaining of the quality of products and services. In WorkFusion Automation workflow, the AutoQC sub process chooses the optimally cost effective combination of automated machines and cloud worker that always deliver at or above the acceptable quality level. The main concept of AutoQC is to take a Sample from a defined Batch of items (BP records, documents, etc.) and verify each item in that sample. After the sample verification, the whole batch is considered as accepted or rejected depending on the Rejection Limit. Therefore, AutoQC allows to verify only a part of batch, not 100% but maintain continuous quality check. Theoretical bases and real world usage Walter A. Shewhart \"Economic control of quality of manufactured product\" Six Sigma as a set of techniques and tools for process improvement https: en.wikipedia.org wiki Six_Sigma United States Army Ordnance Corps used statistical quality control among its divisions and contractors at the outbreak of World War II. Read more. Glossary Batch — is a collection of items from which a sample will be drawn, for deciding on its conformance to the acceptance inspection. A batch should include items of the same type, size, etc. and that were produced under the same conditions and time. The batch size is the number of items in a batch. Sample size — The number of items that should be randomly chosen from a batch. Acceptable Quality Level (AQL) — The maximal percent of nonconforming items (or the maximal number of nonconformities per 100 items), which is considered, for inspection purposes, as a satisfying process mean. AQL = 1 Model Accuracy Threshold 100% Rejection Limit — The smallest number of non conforming items in a sample that would lead to the rejection of the entire batch. Inspection Type. There are three types of inspection: Normal inspection is used at the start of the inspection activity. Tightened inspection is used when the vendor's recent quality history has deteriorated (acceptance criteria are more stringent than under normal inspection). Reduced inspection is used when the vendor's recent quality history has been exceptionally good (sample sizes are usually smaller than under normal inspection). Detecting sample size and rejection limit WorkFusion AutoQC is based on the Military Standard 105E (Inspection Level II) which defines the dependence of sample size and rejection limit from batch size, AQL, and inspection type. As an input, we have the following parameters: Batch Size Acceptable Quality Level (AQL) Inspection Type The AutoCQ application uses the following table as a reference to determine the Sample Size and Rejection Limit. Extending and expiring batches The following rules are applied to batches: A batch is created only when a minimum number of items is available (default 2 items). A batch can be extended during a defined time period (default 5 minutes extendTime in the Assemble Batch step). It means that the batch size is increased, thus the sample size and rejection limit can be increased and more items will be sent for inspection. A batch is marked as expired, if NOT all items from the sample are inspected during a defined time period (default Task Expiration Time expirationTime in the Assemble Batch step). Accepting rejecting batches The main purpose and result of AutoQC is to provide an answer: accept or reject some batch. This decision is made after verifying all items in the sample (or a part of a sample in some cases described below). A batch is rejected if the batch expired – not all sample items were inspected during expirationTime, OR if the rejection limit was reached when inspecting the sample. A batch is accepted if the (number of correct sample items) ≥ (sample size rejection limit). If a batch is rejected, all remaining batch items (records in BP) go to the manual step. Switching between inspection types On the Assemble Batch step, a Batch Executor is created for each unique combination of automation Use Case and automated Human Campaign. One Batch Executor can contain multiple batches, which are distinguished by their unique ID. Depending on the number of consecutively accepted or rejected batches, the batch executor can be switched to another inspection type: :::note In the Discontinue Inspection mode, ALL the upcoming items will be routed to human inspection without sampling. It means that the Accuracy drops under the AutoQC acceptable level, and BP author will get an email notification. ::: The following diagram displays an example of a Batch Executor with 3 batches: The Batch Executor has \"production mode: 1\". It means that the AutoQC process will be triggered only for production runs of the automated Human Campaign. Note that batches have different sample size and rejection limit because of their batch size and inspection type. Batch 1 and Batch 2 were rejected because they've reached the rejection limit, therefore Batch 3 has the TIGHTENED inspection type. Batch 1 and Batch 2 were taken into consideration when switching to another inspection type (2 of 5 consecutive batches rejected see the previous figure), therefore they have \"finalized: 1\" :::note Expired batches are not taken into consideration when switching to another inspection type. ::: AutoQC sub process in the automation process Cloud workers statistically verify the automated and delivered results match the predicted accuracy from this historical performance. AutoQC will automatically increase and decrease the amount of work sent to cloud workers to maintain the process performance at the requested acceptable level. AutoQC adapts during periods of repeated over performance of the algorithms by decreasing human cloud work. Repeated periods of algorithm underperformance signal AutoML to retrain the algorithm. AutoQC can be enabled disabled on the 1st BP step. After records pass Extract Information step via Success outcome, they are sent to the SQС sub process. The first Is AutoQC enabled rule checks whether the quality control is needed. If the AutoQC was disabled on the Automation Settings step, all records are routed directly to the end without any inspection. On the Assemble Batch step, a Batch Executor is created for each unique combination of automation use case and automated human campaign. See the assembleBatch operation. Once the batch is assembled, number of records that corresponds to sample size are sent to Human task, the rest of the batch is sent to Wait for Inspection Results step. This condition is verified in the \"Is inspection required \" by using AutoQC REST isInspectionRequired. After Workers have completed the human task, records are sent to Check Inspection Results step where machine answers are compared with human ones. The results of comparison are sent to the AutoQC application via REST addInspectionResult, and the AutoQC app provides inspection type and the batch status getBatchStatus: IN_PROGRESS ADDITIONALSAMPLEREQUIRED ACCEPTED REJECTED EXPIRED Depending on the batch status, the rest of the batch records are routed to the End AutoQC step (if batch is accepted) or to the Manual Task. In case when additional sample is required, only one record is sent to the Manual Task. The last \"Is inspection success \" rule routes records to an appropriate outcome depending on their inspection result (\" sys sqc inspection result\" column): Yes No There was no inspection AutoQC application AutoQC is an application that provides statistical sampling through REST API. It is used in WorkFusion Automation processes to verify automatically extracted information by Workers. Tomcat configuration AutoQC normally is deployed to separate server and its URL should be set in WF server Tomcat configuration: : sqc rest api\" type=\"java.lang.String\" > In case AutoQC service is on another host then only \"sqc applicationUrl\" JNDI required on WF Instance. AutoQC REST API Common parameters description: sourceId is a parameter for multi tenancy, use local in all cases; identifier is a batch group identifier (statistic for this executor is calculated based on related to this identifier batches); executorType helps to logically group executors. Currently used: ML – machine learning; AD – adjudication rule; QC quality check. assembleBatch URL: http s : : sqc rest api assembleBatch Method: POST Headers: Content Type = application json Parameters: { \"sourceId\":\"sourceId\", \"identifier\":\"identifier\", \"executorType\":\"ML\", \"name\":\"name\", \"numOfRecords\": 100, \"extendTime\": 100000, \"expirationTime\": 10000000, \"targetQuality\":85 } Response: { \"status\": \"OK\", \"errorMessage\": \"\", \"result\": { \"batchUuid\": \"9a64afb5 25cf 4d31 a23c de2999e1a3d6\", \"inspectionType\": \"NORMAL\" } } isInspectionRequired URL: http s : : sqc rest api isInspectionRequired Method: POST Headers: – Parameters: batchUuid = 9a64afb5 25cf 4d31 a23c de2999e1a3d6 Response: { \"status\": \"OK\", \"errorMessage\": \"\", \"result\": true } addInspectionResult URL: http s : : sqc rest api addInspectionResult Method: POST Headers: – Parameters: `batchUuid = 9a64afb5 25cf 4d31 a23c de2999e1a3d6 result = 1` Response: { \"status\": \"OK\", \"errorMessage\": \"\", \"result\": { \"inspectionType\": \"NORMAL\", \"prevInspectionType\": \"NORMAL\" } } getBatchStatus URL: http s : : sqc rest api getBatchStatus Method: POST Headers: – Parameters: batchUuid = 9a64afb5 25cf 4d31 a23c de2999e1a3d6 Response: { \"status\": \"OK\", \"errorMessage\": \"\", \"result\": \"IN_PROGRESS\" } projectInfo URL: http s : : sqc rest api projectInfo Method: GET Headers: – Parameters: – Response: { \"version\":\"2.0 SNAPSHOT\", \"repository\":\"https: svn.crowdcomputingsystems.com svn sqc\", \"path\":\"trunk\", \"revision\":\"143\", \"committedDate\":\"2014 12 23 06:56:50 0000 (Tue, 23 Dec 2014)\" } resetDiscontinue This method returns batch executor from DISCONTINUE inspectionType to TIGHTENED and finalizes it's current batches. URL: http s : : sqc rest api resetDiscontinue Method: POST Headers: – Parameters: sourceId BatchExecutor sourceId (String) (e.g. \"local\"); identifier BatchExecutor identifier (String) (e.g. \"84:a513ad77 65eb 4e7c b472 128fd15794da\"); productionMode BatchExecutor productionMode (boolean) (e.g \"1\"); Response: { \"version\":\"2.0 SNAPSHOT\", \"repository\":\"https: svn.crowdcomputingsystems.com svn sqc\", \"path\":\"trunk\", \"revision\":\"143\", \"committedDate\":\"2014 12 23 06:56:50 0000 (Tue, 23 Dec 2014)\" } Continuous sampling (not used in automation) This type of AutoQC does not use batches, the statistics is calculated for all records, and the AutoQC application returns a response whether this particular record needs to be checked or not. isNeedInspect URL: http s : : sqc rest api isNeedInspect Method: POST Headers: Content Type = application json Parameters: { \"sourceId\":\"sourceId\", \"identifier\":\"identifier\", \"executorType\": \"ML\", \"name\": \"name\", \"targetQuality\":90 } targetQuality should be in the range 0–100. Response: true false addResult URL: http s : : sqc rest api addResult Method: POST Headers: Content Type = application json Parameters: { \"sourceId\":\"sourceId\", \"identifier\":\"identifier\", \"result\": 0, \"targetQuality\":90 } targetQuality should be in the range 0–100. result should be 0 or 1. ⇪ Back to Information extraction runbook "},{"version":"10.0","date":"Jul-25-2019","title":"ie-apply-automation","name":"Apply automation","fullPath":"iac/automl/mle-manual/ie-apply-automation","content":" Automation Available Label After the model training is complete, you will see the Automation Available label next to your Manual Task in the tasks list. Automation Chart When the Automation Available label is displayed, proceed with these steps: Go to View Results > Automation tab. Pick a dot on the chart or select the Accuracy Threshold. Accuracy Threshold – minimal accuracy required from automatically extracted data. This parameter determines the quality of output data and is calculated by ML model algorithms after comparing it with gold data patterns. Automation Rate – the percent of Records that can be extracted automatically with the given Minimal acceptable accuracy; percent of manual tasks replaced by automation. Optional: Enable or disable ML model switching by selecting Switch models automatically. Click Apply Recommendation. Automation BP creation wizard will be opened. "},{"version":"10.0","date":"Sep-20-2019","title":"analytics","name":"Analytics","fullPath":"iac/automl/mle-manual/analytics","content":" The following figure is an example of a typical OOTB AutoML Business Process (BP). Analytics in an OOTB AutoML BP allows to collect and rate model predictions and AutoML performance against Gold (human) answers and human performance. If Analytics is enabled, you have two options: Input is raw data without the answers. In this case, the document goes through STP or Exception pipeline, and you get the real time Analytics. Input is a tagged document containing answers for a given manual task, i. e. the gold document for current manual task. In this case processing happens via Evaluation mode. All human work is omitted, because answers are already present. In the Evaluation mode, model answers are compared with gold answers on the Analytics step. Enabling Analytics To enable storing data for further analysis, follow these steps: In the Business Processes section, click on a BP to enable Analytics. On the Workflow tab, double click Work router. In the Analytics section, select Enabled, and then click Save. This setting is applied to each AutoML BP which is created for a given use case. Use Cases The following section contains common use cases with enabled Analytics. Compare Model Predictions with Gold Answers :::note Model prediction is an answer from the AutoML service with applied Post Processing in AutoML BP. ::: To rate model execution using the prepared Gold answers, for example, from the initial training set, follow the steps described for your use case: Information Extraction Classification Information Extraction :::note Gold data for an Information Extraction model is tagged text — a document produced after tagging a Manual Task in Workspace. ::: On the Workflow tab, locate your Manual Task, and then double click it. On the Design tab, click one of your answers, and then find the Unique Code, as in the following example. On the Data tab, click Upload Data to add your tagged text to AutoML BP. When uploading input data for your AutoML BP on the Data tab, two cases are possible: Your tagged text is placed under a column with the same name as the Answer Unique Code. Mapping is performed automatically, no extra actions required. Your tagged text is placed under a column with a different name. In this case, click Map Columns and manually map that column to the Answer Unique Code. :::note If input data has other column with the same name as the IE answer code, you should rename it manually. Otherwise said column will be processed with auto mapping instead of the desired column. ::: Run your BP. Click the Run tab &gt; Actions &gt; Run. All records after the Machine learning sub process flow to the Evaluation mode branch directly to the Analytics step. See the following section for details. Tagged text can be provided as a link to tagged documents. Classification :::note Gold data for a Classification model is expected classified class. ::: On the Workflow tab, locate the Manual Task, and then double click it. On the Design tab, click Add Answer, and then find the Unique Code, as in the following example. In the example above news is a Unique Code for classified data. Note, that the answer type for the Classification should be Information Extraction. Here company_name is a Unique Code for classification decision. When uploading input data for your AutoML BP on the Data tab, make sure your data meets the following requirements: Classification data: one column contains classified data (original text) and has the same column name as the classified data Unique Code. Classification decision: another column contains gold data (classification decision) and has the same column name as the classification answer Unique Code. :::caution There is no automatic column mapping for the classification Unique Code. Automatic mapping is still available for the classified data column. ::: Run your BP. All records after the Machine learning sub process flow to the Evaluation mode branch directly to the Analytics step. See the following section for details. If you use simple model execution, provide only the column with classified data in input data. See the following section for details. Model predictions in daily model execution When Analytics is enabled, AutoML BP collects comparison information for model predictions against human answers. If the processed document goes through the Exception: bot assisted or Exception: fully manual branch, then the model answer is compared against human answers. If the processed document goes through the Straight through processing (STP) branch, then the model answer is compared against itself. If AutoQC is used and the document goes through the Manual Task branch, then the model answer is compared against human answers. Collected Data Usage Comparison data is saved in a data store with name according to the following pattern: automationstat bpcampaignuuid . bp campaign uuid equals to sys bp campaign uuid field after the Machine Learning sub BP with the following replacement: \" \" with \" _\" (underscore). The data store has the following structure by the Column name. run_uuid Current BP instance UUID. Data type: TEXT field_name Answer code. Data type: TEXT Aggregation: per field in document. group_name Group name if field_name relates to a group. Data type: TEXT Aggregation: per field in document. gold_value Gold answer, if any. Data type: TEXT Aggregation: per field in document. extracted_value Model answer, if any. Data type: TEXT Aggregation: per field in document. TP Metric for a given answer code: True Positive. Data type: INTEGER Aggregation: per field in document. Additional information: used INTEGER because BOOLEAN is not supported by datastore plugins. TN Metric for a given answer code: True Negative. Data type: INTEGER Aggregation: per field in document. Additional information: used INTEGER because BOOLEAN is not supported by datastore plugins. FP Metric for a given answer code: False Positive. Data type: INTEGER Aggregation: per field in document. Additional information: used INTEGER because BOOLEAN is not supported by datastore plugins. FN Metric for a given answer code: False Negative. Data type: INTEGER Aggregation: per field in document. Additional information: used INTEGER because BOOLEAN is not supported by datastore plugins. score Model answer metric: score for a given answer code. Data type: TEXT Aggregation: per field in document. Additional information: used TEXT because DECIMAL is not supported by datastore plugins. confidence Model answer metric: estimated Accuracy for a given answer code. Data type: TEXT Aggregation: per field in document. Additional information: used TEXT because DECIMAL is not supported by datastore plugins. document_id Equals to hitUUID. Data type: TEXT Aggregation: per document dochtmllink_tagged Empty by default. Data type: TEXT Aggregation: per document timestamp Record insert timestamp. Data type: TIMESTAMP Aggregation: per document environment WorkSpace environment type: production. Data type: TEXT Aggregation: per document mlprocesstime Process time of the automation extract plugin (post to ML service ML extract etc). In milliseconds. Data type: INTEGER Aggregation: per document humanprocesstime Human spent time. In case of multiple workers, total time spent. In milliseconds. Data type: INTEGER Aggregation: per document humanprocesstype Type of human work: FULLYMANUAL, BOTASSISTED, EXPIRED. FULLY_MANUAL — manual task without ML answer assistance. BOT_ASSISTED — manual task with ML answer assistance. EXPIRED — manual task is not processed and overdue. Manual step in AutoQC is not accounted for at the Analytics stage. Data type: TEXT Aggregation: per document Collected data is used to build Cognitive Automation Breakdown chart. Comparison rules for non empty gold and model answers Use the following comparison algorithm to calculate statistical metrics (TP, TN, FP, FN): Split gold and model answer with ' ' (pipe). Sort the resulting arrays. Compare arrays by elements with the same position. This comparison method is used with multi value fields saved in one string with a pipe separator. If the field is not multi value and still contains a pipe ' ' then such comparison is the same as using equals. ⇪ Back to Information extraction runbook "},{"version":"10.0","date":"Jul-25-2019","title":"ie-model-retraining","name":"Model retraining","fullPath":"iac/automl/mle-manual/ie-model-retraining","content":" Repeating AutoML BP If you have additional input data, you can reuse the AutoML BP by doing one of the following actions: Go to Manual Task, set new accuracy parameters, and then click Apply Recommendation. Copy and run the new AutoML BP. Model Retraining ML model is constantly improving the quality and continues training on all answers provided on the manual steps (when machine failed to extract information or during the AutoQC sub process). When the new retrained model provides better results than the original one, you will see the Cognitive Bot Ready label on the View All Tasks page. To automate your Manual Task with improved quality: Click View Results > Automation tab. You will see a new Automation chart with better parameters (for example, increased Automation rate for the same Minimal acceptable accuracy). Hover over a chart and select a dot with your desired value. Alternatively, select the Minimal acceptable accuracy, and then click Apply Recommendation. Configure and run the AutoML BP again. "},{"version":"10.0","date":"Sep-20-2019","title":"ie-configure-and-run-automl-bp","name":"Configure and run AutoML BP","fullPath":"iac/automl/mle-manual/ie-configure-and-run-automl-bp","content":" In the previous step you have activated a Cognitive Bot by configuring AutoML in the Manual Task. Once you run such Manual Task, Control Tower automatically launched instance of out of the box AutoML Business Process. This BP has been auto created with your original Manual Task inside of it for cases when manual processing will be still required. Now let's work on AutoML Business Process configuration. Configuring AutoML BP In the Business Processes list, select your newly created AutoML BP to perform the following steps: On the Data tab, click Upload Data, and then map input column( s) with answer types, if needed. Set up AutoML BP parameters (Due Date, Streaming, etc.). Set up model execution options. Navigate to the Workflow tab, and then select Manual Task > Configure AutoML > Execute Model. :::note By default, Machine Learning Model, Cognitive Bot Workflow, ID and Version are set according to the training configuration of the initial Manual Task. ID — Unique identifier of an experiment group of trained model (set on model training) in the following format: answer unique code runUUID. You can select other trained model ID from the list and Cognitive Bot will apply new selected trained model for execution. This list is not_ sorted by Machine Learning Model type. Version — Version of the selected Machine Learning Model. The list consists of available Machine Learning Models versions. Default value is version used for the selected Machine Learning Model during training. ::: Run your AutoML BP. :::caution Input column( s) mapping may not work in AutoML Business Process after activating the Cognitive Bot in the original Manual Task. To avoid this, provide input data files with column names that match answer codes. ::: You can optionally edit AutoML BP bot steps, such as, to enable or disable Auto Quality Control (AutoQC), or set data formats, etc. For more information, refer to Automation BP documentation. Monitoring AutoML BP You can monitor your AutoML BP progress on the BP diagram in the View Mode or on the Data tab. Have a look at the following example. On the process diagram, 24 documents were uploaded as the input data with the following results: 18 documents were successfully extracted by the Cognitive Bot, and then normalized. 6 documents were routed to human workers because the Cognitive Bot failed to extract information. Note that you still need to complete some number of Manual Tasks in these cases: Automatic Extraction failed or was partially successful. Data Normalization failed. AutoQC is enabled. In the previous diagram, 18 documents were successfully extracted and normalized by the Cognitive Bot, and then routed to the AutoQC sub process, of which: 5 documents were taken to inspection tasks for human workers to verify model results. 13 documents were put on hold until the inspection results. Human workers provided the same answers as the cognitive bot (Manual Classification step), and therefore the whole batch was approved and no additional inspection needed. Finally, you get the resulting data partially generated by AutoML, partially submitted by human workers. Training set update After human workers process tasks in WorkSpace, all processed documents supplement training set for the model. Documents processed by human placed to automatically created training set in the following cases: Model was trained on other task (or on other instance) than model execution initiated. Once human processed at least one document and task status is Completed, then training set will be created automatically with processed documents. Model was trained on the task and training set that used for model training was added manually, then model execution was initiated for the same task. Once human processed at least one document and task status is Completed, then training set will be created automatically with processed documents. Task is created by copying a task from one of two previous cases, without changing the task name. Automatically created training set has the same name as the manual task definition name with the following changes: uppercase is changed to lowercase, special characters are removed, spaces are substituted with underscores. Automatically created training set name can be found in the Train new model section of the Configure AutoML tab. The following example shows an automatically created Automation Training Set. What's Next Afterwards, to improve the results, you can apply Post Processing. Additional materials Analytics AutoQC "},{"version":"10.0","date":"Sep-20-2019","title":"ie-create-automation-manual-task","name":"Create automation manual task","fullPath":"iac/automl/mle-manual/ie-create-automation-manual-task","content":" WorkFusion AutoML provides automation by making ML models learn from human worker answers provided in Manual Tasks — web forms with data and input fields. Manual Tasks are designed in Control Tower, and then filled and submitted by human workers in WorkSpace. After a model is trained, WorkFusion creates a Cognitive Bot, and then routes work to it. Selecting an OOTB use case Let's start by creating and configuring a Manual Task in Control Tower. Click Manual Tasks > Create Manual Task. Select the type of a Task (in other words, your use case), and then click Select This Use Case. Your Manual Task for the use case has been created. Currently, the most wide spread use cases are: Information Extraction Classification :::note ALL fields that need to be automated by the model MUST be configured as required. Otherwise, the AutoQC process will fail to pick those fields. ::: Uploading Input Data Next, let's upload some data for a model to train on. This data is prepared in the Data Analyst Guide. Note, that your input file should contain at least 20 records (documents) or more. To upload your data, follow these steps: In your Manual Task, click the Data tab, and then Upload Data. Click Add, and then locate your CSV file, and then click Open. Wait till the file is processed, and then click Upload. Now that we have some data uploaded, let's configure the model. The Model Not Applied Option In case you don't want to automate your manual task, select Model not applied on the Configure AutoML tab. Configuring AutoML To train a model and eventually generate a Cognitive Bot, link your Manual Task to AutoML and configure it. On the Configure AutoML tab, select Train New Model. In the Machine Learning Pipelines section, select the Machine Learning Model that corresponds to your Task (for example, Information Extraction, Classification, Financial Reports Data Extraction, etc.). This model will learn from human answers and automate your Manual Task by creating a Cognitive Bot. Select your Cognitive Bot Workflow.The Cognitive Bot will be created from this Cognitive Bot Workflow when your Manual Task is completed. You can edit the Cognitive Bot Workflow by selecting next to the Cognitive Bot Workflow drop down menu. Select the Automation Training Set. If you have some historical data for training the ML models, you can upload this data to the Automation Training Sets, and then use it for Manual Task automation. When assigning an existing Training Set to a new Manual Task or doing a quick assessment of data for ML purposes, you can see the Training Set answer availability and its percentage using the Show Data Completeness button. If you do not have a Training Set, select the* Create new Training Set automatically* option. It is a default option, a new Training Set is created for storing Workers' answers and training ML models to automate your Task. Optionally, click the 3 dot button to perform the following actions: View Rename Copy Reset Data Status The latter sets the train _status column for all records to NEW. Used to include all records from a particular Training Set in an ML model training. Search Engine 2.0 training configuration Models with SE 2.0 are already optimized to produce best results, so you can adjust only the number of Fields Trained In Parallel. Maximum number of fields trained in parallel is calculated according to AutoML cluster capacity. Training Configuration tab also gives estimate of complete training time for the model. Search Engine 1.0 training configuration If the selected Machine Learning Model supports Search Engine 1.0 then Training Configuration section is enabled. By default, you can see preset options for Training Configuration and time needed to complete the training for all fields of the initial Manual Task. Move the slider to set up your training configuration according to the complexity of the task. Moving the slider instantly re calculates the estimated training time. To have more control over the training setup, click Show Details. In the following picture you can see an example of Training Configuration section for Information Extraction ML model. The following parameters are available: Max Training Time — Maximum amount of time AutoML will use to find the best subset of components required for a single field. Fields Trained in Parallel — Maximum number of fields (sub models) that can be trained in parallel on an AutoML cluster. By default, this slider is set to the optimal value based on the available cluster capacity. Metric — Metric used to compare the results of experiments and select the best one. Available metrics: F1, Precision, or Recall. :::note The Fields Trained in Parallel parameter is available only for Information Extraction models, and is not applicable to Classification models. ::: Values for Max Training Time and Metric parameters are predefined and listed in the table below, while values for Fields Trained In Parallel parameter are calculated according to AutoML cluster capacity. There are four possible preset values ranged from \"High Speed\" to \"High quality\". Slider Position (Preset) Max Training Time Metric 1 — Low Complexity 1 hour F1 2 2 hours F1 3 4 hours F1 4 — High Complexity 10 hours Precision You can select one of the preset options and it will be applied to all fields (sub models) while training. In case you cannot find an appropriate combination of parameters from the preset options, you can provide custom parameters by clicking Set Custom Goal link. Using this mode you can set up all parameters manually. The parameters have the following restrictions: Max Training Time ranges from 1 hour to 10 hours. Fields Trained In Parallel is limited by the AutoML cluster capacity. :::note While calculating Fields Trained In Parallel and Complete Training Time, currently running model trainings on an AutoML cluster are not taken into account. In case one or several models are trained on an AutoML cluster, values for Fields Trained In Parallel and Complete Training Time will remain the same and will not reflect the time in a queue. Before setting the Training Configuration parameters check other models with HPO trained on the same cluster. ::: Settings priority Since there are several places for training settings configuration, there are different priorities that each place have. Priorities are listed in descending order, i.e. at the top are the settings that will be applied primarily, in the bottom are settings that will be applied last. For Max Training Time field: Settings provided in Training Configuration section on the Configure AutoML tab. Settings provided in Zookeeper in section Search Engine limiting settings. Settings provided in Model configuration. For Fields Trained in Parallel: Settings provided in Training Configuration section on the Configure AutoML tab. Settings provided in Zookeeper for Search Engine limiting settings. For Metric: Settings provided in Training Configuration section on Configure AutoML tab. Settings provided in Model configuration. Note that managing Training Configuration section requires \"Manage Platform Monitor\" permissions for ML Engineer. It can be provided in System Settings > Role management > Select Role Permissions > check Platform Monitor."},{"version":"10.0","date":"Jul-31-2019","title":"ie-start-model-training","name":"Start model training","fullPath":"iac/automl/mle-manual/ie-start-model-training","content":" When you have set a Manual Task and automation options, you need to run this task to launch the model training process. Run manual task On the Run tab, click Run This Task. In a popup, click View Results. Afterwards, you have 2 options: If you've selected an existing Training Set on the previous step, stop the task by clicking Actions > Stop Task. Otherwise, if you have a new Training Set, you need to submit all Documents as a Worker in WorkSpace application. As a result, the task will be completed and the ML Model training should start. Triggers for AutoML to start ML model training Manual task is completed. Min docs limit reached. This limit depends on the ML Model selected and is about 20 documents, i.e. if Workers submit 20 tasks in WorkSpace the model training will start. Min delta limit reached – number of new documents to re train the model. Once Delta is reached (training set updated with required number of documents) → retrain will be started automatically. Force training (Evaluate Automation action with ignoring of limitations). How to check model training WorkFusion task list Go to the View All Tasks page: When the Automation training has started, you will see the Automation Evaluation icon next to your manual task. When the Automation training has completed, you will see the Automation Available icon next to your manual task. Troubleshooting Marathon Navigate to http: vds try master.workfusion.com:8080 ui apps. The URL may change, contact your administrators to get the exact Marathon URL. Click eval > vds folder. The models currently in training will show a number of Running Instances. To view error logs, click on a model folder (message ...), navigate to the bottom of the page and click the main link at the bottom of the list. Clicking on the stderr link will give you some log information, but for real time logs, click on the running instance link (in this case it is the 'eval vds message...' link). Click the link (Mesos details) at the bottom: You will be redirected to the Mesos application. Click the Sandbox link. The real time run log will be displayed click on stderr or stdout. S3 vds models bucket for hyper models. Hyper model content. vds resources bucket for trained models. output ... model.jar SSH to (Legacy) DUCC intallation VDS folder logs Next, logs can be checked during training: tail 100f opt workfusion apps vds gateway log gateway.log tail 100f opt workfusion apps vds ducc adapter log ducc adapter.log Next, logs can be checked during extraction: tail 100f opt workfusion apps vds gateway log gateway.log tail 100f opt workfusion apps vds emulator log emulator.log tail 100f opt workfusion apps vds scaling log scaling.log Once hyperModel and trained model will downloaded from S3 to ML server model log file will be created. For example: tail 100f opt workfusion apps vds emulator log generic invoice information extraction 1.0 message 6abcaa52 8f5f 47ae 9640 e139170b0976.log To restart vds services execute next command under wfuser user. service vds restart All hypermodels are downloading here: opt workfusion apps vds hypermodel work _lib All trained models are downloading here and have unique suffix: opt workfusion apps vds hypermodel work WFMLJobData data store experiment group job id start date status last updated training set size tags original campaign uuid model version automation root run uuid experiment group experiment id model code model version "},{"version":"10.0","date":"Jul-25-2019","title":"mle-introduction","name":"ML Engineer introduction","fullPath":"iac/automl/mle-manual/mle-introduction","content":" Machine Learning Engineer is one of most technically skilled (along with Automation Architect) roles in an SPA team. Background of MLE includes a number of completed end to end Java based projects. MLE has wide experience in creating business applications, rolling out to Production and dealing with a variety of environment challenges. What is expected from ML Engineer Enforce quality training set Perform and explain DA key functions: document tagging, analyze P R TP FP FN. Distinguish all problems with Training Set and have skills to solve them by manipulating document sets. Improve model Work on improving the model based on data analysis while following WorkFusion Machine Learning solution workflow. Analyze training results and understand what needs to be improved based on DA data analysis. Know which parameter influences most the overall model extraction quality and how to improve it. Know how to read extraction per field output statistics. Know what needs to be improved based on this analysis. Understand typical AutoML pipeline: OCR > Tagging > OOTB Model training > AutoML SDK. Practical skills in Search Engine settings for training (fields, experiments, time limit, max iterations). How to launch fixed model training without Search Engine (improved training set for previously trained model). Have practical skill to split dataset and test model on test data set. Have real use case implementation experience: delivered PoC use case as a final certification task during the Automation Academy learning path. Use WorkFusion AutoML components Can setup ML use case BP in Control Tower, fine tune BP, configure AutoML in Manual Task, launch training in Control Tower, create bot tasks. Know how to design sequential launch of training processes (know how to calculate and schedule). Can deploy model to make it available via Control Tower. Can transfer ML solution DEV > QA > UAT > PROD. Know environments difference and usage strategy (DEV vs. PROD). Solve issues on failed training process, have practical skills configuring Mesos Marathon ZooNavigator settings. Skill to get best result by tuning WF OCR capabilities. Know and use environment health checks. Apply investigation skills when training fails. Apply AutoML SDK Grasp theoretical meaning of Annotators and Feature Extractors, how they link together with model fields. Can logically describe AutoML SDK flow of how a model is built out of those components. Can distinguish OCR errors and can fix those using AutoML SDK. Role in RACI matrix Area Responsibility OCR (env BP) I Qualification task and training for customer (SME's) I Training Set preparation AC (technical help to DA, if required) Analyze TS How to improve TS AC Create TS from customer data R (with DA) Automation BP RA ML environment RA Train Model RA ML results evaluation, statistics R (with DA) FP, FN analysis, post processing description CR (with DA) Post processing implementation RA Automation report for customer C RACI stands for: R — Responsible C — Consult A — Accountable I — Informed Glossary For a detailed Glossary refer to AutoML Glossary. "},{"version":"10.0","date":"Jul-23-2019","title":"automl-dashboard","name":"AutoML Dashboard","fullPath":"iac/automl/da-manual/automl-dashboard","content":" Dashboard is out of the box tool in WorkFusion to provide all kinds of analytics including AutoML statistics. It is a convenient and user friendly way to present model statistics to the customer as well. However, it doesn't contain links to the original document and extracted model results so this tool is not effective when analyzing model results by the Data Analyst. AutoML Dashboard includes actual AutoML training data only if the Automation Use Case contains a \"Statistics calculation\" bot task. AutoML statistics chart In AutoML Statistics by Field chart, you can find results for each field. Chart shows model quality results per each field over all documents. Tooltips Tooltips contain the following information: Field Name Value for specific metric. Chart explainer Chart explainer has metrics description. Colors Black is Correct (True Positive); (66.67% for order _total) Orange is Mistake (False Positive); (16.67%) Grey is Not Learned (False Negative); (16.67%) Model metrics Accuracy: Ratio of correctly extracted fields to the number of extracted fields. Precision: How accurately the model extracts values Automation Rate: Ratio of correctly extracted fields to the total number of fields. Recall: Percent of existing values that can be extracted by the model. Not Learned: Ratio of fields that should be extracted but were not extracted by model. Low Confidence: Ratio of fields that should not be extracted but were weren't extracted, with model score lower than selected Accuracy Threshold. Mistake: Ratio of fields that should not be extracted but were weren't extracted, with model score equal or greater than selected Accuracy Threshold. Correct: Ratio of correctly extracted fields. Gold vs extracted statistics chart Gold vs Extracted Statistics chart can be filtered by process name and execution and field name. In addition, the Gold vs Extracted Statistics chart allows filtering by: values: all, incorrect only (FP FN), correct only (TP TN). By default this filter is set to \"Incorrect only\". result type: In the dashboard for the purpose of being easy to read by customer, FP is referred to as \"Extracted with Errors\"; FN as \"Failed to Extract\"; TP as \"Correctly Extracted\", TN as \"Should not be Extracted\". confidence: The confidence with which a field was extracted is reported by the model. Apart from field name, confidence and result type, the table contains: Gold Values: Values that were extracted by SMEs in the manual task and are supposed to be correct. Extracted Values: Values extracted by the model. Documents: Number of documents in which this combination of gold extracted values occurred. Result type: Correctly Extracted: Value should be extracted and was extracted correctly. Extracted with Errors: Value should not be extracted, but was extracted; or should be extracted, but was extracted with errors. Failed to Extract: Value should be extracted, but was not extracted by the model. Should not be Extracted: Value should not be extracted and was not extracted. Learn more information on AutoML Dashboard here. ⇪ Back to Data Analyst runbook "},{"version":"10.0","date":"Jul-25-2019","title":"da-job","name":"Data Analyst job","fullPath":"iac/automl/da-manual/da-job","content":" To understand a Data Analyst's expected contribution to a successful project delivery, let's first outline the context of a cognitive automation implementation. We will overview the project delivery from an entire team perspective. It is a joint effort to implement an ML Use Case, so effective collaboration while understanding one's own contribution is part of the Data Analyst job. Problem statement Operations businesses in many industries (banking, insurance, financial services, manufacturing, healthcare) have a current process of handling documents or information. This consists of many manual steps that, all together, build a workflow. A business goal is to reduce manual work and make processes more cost effective: This problem defines the goal of Operations automation. A manual workflow is transformed into a business process when automation is set up. Manual steps in the workflow are replaced by bot tasks and a machine learning (AutoML) business process. The transformation of manual workflows into business processes is performed for each use case to mimic all specific details, branches of logic and amounts of data to be processed. Documents in the workflow have to be processed in a certain way, defined by use case requirements. Usually data from the documents has to be taken out and then handled in other steps of the workflow. For example, purchase order details (quantity, product, price) have to be entered into internal applications like SAP, CRM systems, etc. When there are many document types, they can be processed in different ways. For example, data from purchase orders has to be entered into SAP, and data from invoices has to be sent by email in table format. So to best handle documents in this case, it's necessary to detect document type first. Use case lifecycle To understand the ML Data Analyst's contribution to a successful project delivery, first we need to understand and properly start practicing project process. The process, with phases described below, has been proven to work and practiced by WorkFusion PoC while delivering complex ML RPA implementations for customers. On a high level, each Use Case goes via five main phases. Each phase has its requirements, steps and roles involved. The Data Analyst contributes during the Implementation phase. 7 stages of implementation phase Let's logically structure a typical Use Case Implementation in stages defined by separate people doing specific work and handing off results to the next person: An implementation consists of ML and RPA parts. RPA deals with well structured and repeatable processes and develops automation that replaces manual tasks. ML development delivers a model that handles unstructured or semi structured data in the documents. The model is trained on a data set collected from original documents. A high quality data set is the Data Analyst's main responsibility. The better the quality of a data set is, the higher results of ML. Implementation can be delivered as PoC (Proof of Concept), Pilot, or Production, depending on the scope of the Use Case. In data set collection, the Data Analyst studies business logic of the Use Case and applicable documents, and defines the rules and corner cases. The Data Analyst works closely with the Subject Matter Experts (SMEs) from the customer partner side on the Data Set collection step. They have a deep understanding of documents' logic and tag the data set for model training. The Data Analyst trains SMEs to tag the documents and verifies tagging. The Machine Learning Engineer or Automation Engineer Specialist leads model training and the Data Analyst analyzes ML results and defines rules that help to improve the model. ML results are evaluated by comparing the results provided by people (manual extraction or classification that is considered to be correct) and calculating quality metrics statistics. The Data Analyst calculates interim and final statistics and submits these to the customer in the form of a report. At the analysis of results stage, the Data Analyst needs to review the model execution results and, if necessary, propose ways to improve. The model is trained on the test set for each training iteration. If there are several iterations, the Data Analyst calculates statistics of each iteration based on evaluation results. The Data Analyst needs to calculate statistics for each iteration and analyzes the delta for tagging iterations and model mistakes. ML improvements and retraining is handled by the Machine Learning Engineer or Machine Learning Engineer Data Scientist. This is the most complex stage, as it includes creation of custom model components. The (first) final report of automation results has to be shared with the customer, together with any reasons why some stats on some fields may fall below success criteria. Moreover, further steps and solutions should be proposed. In the integration of ML results into RPA stage, structured data made by machine learning is entered into organization's internal systems. Typically, RPA is used for that because corporate applications lack API support and the only way is to have bots simulate user interactions with UIs. During the final report, analytics helps DAs and business users to see and understand outcomes of automation in production. This may include aggregated numbers showing savings, amount of work done, SLAs, average ML measures, etc. "},{"version":"10.0","date":"Jul-25-2019","title":"da-introduction","name":"Data Analyst introduction","fullPath":"iac/automl/da-manual/da-introduction","content":" Data Analyst manual is an advanced guide developed to teach the Data Analyst role for projects which use WorkFusion Machine Learning capabilities. Its goal is to give an end to end understanding of the qualitative ML Training Set collection. This guide teaches best practices and WorkFusion Center of Excellence (CoE) approved delivery methods. In general, data analysis is a process of inspecting, cleansing, transforming and modeling data with the goal of discovering useful information, informing conclusions and supporting decision making. Data analysis has multiple facets and approaches, encompassing diverse techniques under a variety of names, while being used in different business, science, and social science domains. What does a Data Analyst do Data analysts are responsible for identifying and extracting valuable information from structured and unstructured data to explain business performance. Using this information they identify the best analytical models to present to business users and the best approaches to explain these models. Data analysts were among the most prominent sources of data understanding in the era prior to the rise of data science. They are still relevant today due to their distinct role, and in some companies have a role that has transformed due to a combination of data science know how and the need to integrate data scientists into the existing business data framework of that company. The primary role of the data analyst is to take questions or problems supplied by the business team and come up with solutions to them. Unlike data scientists, you will not be creating many predictive models, but instead you will be diving deep into data, finding new and innovative ways to turn it into something that is either directly actionable or can be used as the basis for the company or unit’s larger strategy. You will be required to become an expert in how either the business as a whole or your particular unit operate, depending on the scale of the company. Being effective in your position will require you to be able to take the data driven insights you acquire and turn them into something that is directly relevant to your business. As an effective data analyst you will end up understanding the nuts and bolts of how your company operates in a way that few other will be able to match. One thing to be aware of is that due to some level ambiguity during a time of rapid expansion in data science, some positions for data analysts may be referred to as data scientists and vice versa. Data Analyst for WorkFusion projects Is a dedicated role during the delivery of automation projects which include unstructured data processing. A person with DA skills will be in charge of appropriate documents selection, processing, structure and input for model training. Because the overall success of an ML implementation highly depends on quality of input, this role is crucial. From a technical perspective, a DA is not expected to possess coding skills, but instead must have deep knowledge and practical skills of work with data. As for prerequisite education, we've found that the following subjects are beneficial toward developing a career in data analysis: Mathematics Computer science Statistics Economics There are also a number of qualities to expect from a Data Analyst role: Experience in data models and reporting packages Ability to analyze large data sets Ability to write comprehensive reports Strong verbal and written communication skills An analytical mind and inclination for problem solving Attention to detail and patience What you need to know about becoming a Data Analyst Data analysts typically require a bachelor’s degree, though some positions at larger companies or those with more rigorous expectations may demand a master’s degree. Degree programs are typically statistics, information technology, or a particularly statistically rigorous business degree. Current data jobs landscape Career development There is lot of potential for growth for people who have the level of business and statistical knowledge that a data analyst possesses. There are two main paths for this growth, both of which will eventually require a master’s degree. If you are more interested in a management position, an MBA will allow you to combine your data know how with management techniques and position you to gain a leadership position. If you want to go in an even more data focused position, getting a master’s degree in computer science or statistics will put you in a good position to become a data scientist. Either way, becoming a data analyst is worthwhile and rewarding, particularly if you have the knowledge required to be excel. "},{"version":"10.0","date":"Jul-25-2019","title":"da-responsibilities","name":"Data Analyst responsibilities","fullPath":"iac/automl/da-manual/da-responsibilities","content":" The goal of this page is to clearly outline what is expected from a person in the Data Analyst role. Provide understanding of workflow, artifacts to produce, and cooperation with other roles. Regardless of the type of Use Case, all Data Analyst responsibilities can be divided into three main stages: Data set collection. The most considerable and important stage makes up more than half of all DA work. Mistakes made and not corrected here may entail serious risks in terms of timelines and achievement of success criteria. As a prerequisite to this stage, DAs should get familiar with different aspects of the project, clearly understand the business logic of the use case, what type is it (Information Extraction Classification), what fields classes will be used, document types and formats etc. Model Training and analysis of results. This stage is implemented by close cooperation of DA and ML Engineer. Preparation of the final report. At the end of the project, final results should be presented to the customer. To identify steps described below with a use case workflow, see the workflow diagram here: Stage 1. Data set collection Know documents' specific and business logic enough for further data processing. Guarantee an acceptable quality of the data set after OCR conversion. Split documents into batches by some criteria (if not done by the customer). Define tagging logic for each template. Train and qualify SMEs: Hold training session for SMEs Provide clear and specific instructions for labeling Hold Qualification Prepare Manual Tasks for tagging and assign batches to SMEs. Validate labeling results. Split data set on training and test. Stage 2. Model training and analysis of results Model training Launch model training. Launch IE Classification on test set (with assistance of MLE). Run statistics aggregation business process. Analysis of results Analyze type of mistakes and possible reasons for them. Define rules for post processing. ML improvements & retrain This step is mostly conducted by ML Engineer and Data Scientist. However, Data Analyst can support in analysis of interim results. Stage 3. Final report preparation Analyze final model results and prepare the report for the customer. More information about report preparation can be found here. "},{"version":"10.0","date":"Jul-25-2019","title":"customer-data-protection","name":"Customer data protection","fullPath":"iac/automl/da-manual/customer-data-protection","content":" The goal of this article is to provide insight on how to deal with a customer’s information without risk of sensitive data disclosure to third parties not involved in the project. Risk definition WorkFusion customer organizations hold terabytes of data sets about their customers or their activities. Those organizations have to release data files containing private information to WorkFusion for data analysis, model training or support. The challenge of data privacy is to utilize data while protecting individual privacy preferences and personally identifiable information. Information Privacy Law prohibits the disclosure or misuse of information about private individuals. Information privacy rules must be strictly followed by Data Analysts, as ignorance can easily lead to customer WorkFusion contract termination, make both parties liable as violations of law, and even lead to companies being shut down. There are two possible scenarios of working on a project, and consequently two ways of accessing data. Scenario Security level : : On site High Remote via VDI* Medium * VDI: Virtual desktop infrastructure is virtualization technology that hosts a desktop operating system on a centralized server in a data center. Let’s review each scenario in detail. Information privacy rules Work at customer site When working directly from the client’s office, as usual, you are provided by the company’s laptop which will have all the necessary programs already pre installed. You should prepare a list of software you may need on the project for approval before the start of the project. All additional tools programs you may need in the course of work MUST be agreed upon with the customer. Don'ts: Use any memory sticks, discs or any other devices to copy information to from the customer’s computer. Send any piece of a customer’s information (even if it doesn’t contain any sensitive data) outside, where it can be accessed by non engaged parties (even to yourself — to your WorkFusion email, your private email, your colleague DM) without mentioning it to somebody from the customer’s side. Take any photos of customer’s laptop screen. Bring customer’s computer out of the customer's premises without explicit approval by customer. Work remote via VDI Just as when working on site it’s not permissible to install any software on a customer's VDI without approval. Don'ts: Share your credentials — when first installing VDI, you’re provided with different logins, security keys, passwords etc. that shouldn't be shared with anybody. Send receive any attachments with a customer's information from to your WorkFusion account. To share information with the team engaged in the project, create a mailbox on VDI or a ShareFolder where all the necessary information will be stored. In the very beginning you should notify a customer that no sensitive data should be sent to your WorkFusion email. Everything related to the project can be provided ONLY by sending to the email on VDI or by putting to the ShareFolder. Copy any piece of a customer's information (including screenshots of VDI) on your local machine. "},{"version":"10.0","date":"Jul-25-2019","title":"da-workflow","name":"Data Analyst workflow","fullPath":"iac/automl/da-manual/da-workflow","content":" Data Analyst acts based on number of factors originated from the customer's data analysis. In general, we applied certain similar steps across projects. So now we can share a typical workflow with new Data Analysts. Preparatory work DA initial data: Information to work with: fields number, documents type and format DA goal: Plan possible answer types and their format, to work up manual task (MT) DA output: MT draft The DA workflow starts from the moment a work description is signed. The preparation period for automation development begins, and RPA and ML engineers clarify technical details while the DA works up HT design. DA receives the initial information to work with: number of fields and requirements, document types and formats, and possible answer types and their format. DA may receive information concerning number of SMEs, their workload, and then plans how to organize the Data set collection. This is the initial step of work for the UC and information available for DA may be limited. What the DA should do here is to draw up a list of checkpoints to inquire about with the customer ASAP, and to think through possible scenarios of work so as to minimize time required for planning and consulting at the customer’s site. Main development DA initial data: Manual task draft, a batch of documents DA goal: Data set (DS) collection, model training DA output: Labelled data set, trained model, report on statistics The main goal of the Data Analyst is data set collection. The better quality the data set, the higher results of ML. Data set should respond to several requirements. DA should know as much as possible about logic, workflow of the documents, document layouts and production distribution. When the production distribution is known, it should be followed in the data set. Unknown distribution is a more common case and requires more effort, because DA must define distribution manually. DA studies layouts distribution in order to split documents into batches. The aim is to increase tagging efficiency, decrease mistakes, and contribute to consistency. Each batch of layouts should be labelled by subject matter experts (SMEs). A possible solution to split documents into layouts can be to sort them by key words. A rational balance should be followed when splitting. The number of layouts should be limited and the quantity of documents per layout should be substantial (for example, 50 or more). The Data Analyst should check there is sufficient information about all fields or whether there is a need to increase the number of documents. Format of the document (PDF or picture) may require transformation by OCR into HTML XML format to be suitable for labeling. To collect the data set, the Data Analyst works with SMEs. Data Analyst provides training for SMEs, where DA explains rules of tagging and how to work with WorkSpace. As soon as documents are available and the logic in them is clarified, DA is to create a manual task, produce gold data and start qualification to teach SMEs what to tag. Then they start working in parallel: SMEs supply labelled documents, while DA checks quality and corrects mistakes, if there are any. Note that the main output after labeling is a batch of labelled documents provided, but to improve the statistics and demonstrate the results to the customer, compare original documents with labelled. The batch of original documents is usually added to S3 storage before the OCR step, and then referred by link during the next steps. After the OCR step, documents don’t keep their original names — that’s why it’s practical to establish a corresponding connection between the original document and electronic. Such a link can be set by the key, which is the original document hash and accompanies the document at all steps. That can be introduced by extra code added before the OCR step. When data set is collected and checked, DA splits it into two parts: training set and test set. The Machine Learning Engineer starts model training and together with the Data Analyst they monitor model performance and results. If statistics fall below the required criteria, it’s necessary to investigate the reasons and create improvements. The reasons may be incorrect tagging, insufficient data set, or OCR mistakes. Possible solutions may be retagging documents and then model retraining and or post processing application. This step can be prolonged and repeated until the statistics figures match the required goal. The Data Analyst prepares a final statistical report to provide the customer with results of model training, and final figures, which show its performance. This step closes the period of development, and the ML is ready for production. "},{"version":"10.0","date":"Jul-25-2019","title":"input-docs-characteristics","name":"Input document characteristics","fullPath":"iac/automl/da-manual/input-docs-characteristics","content":" Typically, a Data Analyst deals with an unspecified variety of documents during the start of Use Case analysis and implementation. To perform a proper analysis of the documents, a Data Analyst has to deeply understand and drive work using following characteristics: Document Types Document Distribution Document Formats Field Class Corner Cases Document types Document types processed in the Use Case vary depending on the industry. For example: statement of value, invoice, email, purchase order, loan booking, reconciliation, annual reports, etc. Usually in the Information Extraction (IE) use case, there is one document type, and in the Classification use case, there are several. A DA needs to consider the quantity of document types, their logic, and representativeness. A document is usually defined not only by the industry it circulates in, but also by the issuer. Documents of one issuer comprise one template. To collect a high quality representative data set, it's necessary to consider templates, so it's a common practice to split documents by supplier when collecting a data set. A group of documents which have a similar appearance is called a template. After OCR we speak about layout. Template examples The following images are examples of different templates. For some cases, there is only one template across one supplier (in this case, it's one State (Kentucky, Idaho, Mississippi), and for some cases there are two templates across the State (California, Georgia). Expand to see template examples Kentucky Idaho Mississippi California Georgia Document distribution To train the model effectively, a data set should contain enough documents distributed well. That means a data set has to well represent each document type, including different templates. We recommend 300–500 documents of each template of each type in a data set. Rare templates can cause low model statistics because there won't be enough examples for the model to train on. Document formats Format Approach : : HTML XML Can be directly processed with AutoML. Plain text Can be directly processed with AutoML. Image (.jpeg, .tiff, .png, .gif) Send to OCR. PDF Send to OCR. Excel (.xls, .xlsx) Convert to HTML. Word Convert to HTML plain text or convert to PDF and send to OCR. Email (.msg) Work with email body, as with plain text. AutoML can proceed directly with HTML XML formats and plain text. For the rest, format converters need to be applied to receive documents in the required format. The most typical converter used in use cases is OCR (Optical Character Recognition). After OCR of an image or PDF, we receive output in two formats: HTML and XML. For ML it's important to use XML, as it contains more information about the original document. Field Field — Information that should be extracted from the document in Information Extraction use cases. In ideal cases, each document contains all the fields that need to be extracted. But it's more common, due to business logic, that documents contain only some fields. Some fields will be present only in several templates or in some of the documents. In cases of these rare fields, it's necessary to make sure there are enough examples to train the model well. A field is considered rare when it's present in fewer than 50% of documents from the data set. Normally, to train the model it's advised to have 500 values of one field. Conflicting context The necessary field in the document is always surrounded by context. Context helps the model to train as well because it serves as notification for value confines. Special attention should be paid to cases with conflicting context: Supplier 1 9 Supplier 10 QTY ORDERED100,000 QTY ORDERED QTY DUE100,000 300,000 Documents of most suppliers contain value for field \"quantity\" in the line under \"QTY ORDERED\". For supplier 10, the value for \"QTY ORDERED\" always remains unchanged, but the data needed from a business perspective is placed under \"QTY DUE\" If documents of both suppliers are trained on one model, that can cause a low score for this field and result in the wrong values being extracted for supplier 10, or not extracting values for suppliers 1–9, depending on the number of examples provided. Conflicting field meaning Sometimes the value for the field can belong to different parameters. For example, \"extra smooth\" can refer to finish or surface parameter depending on supplier of other fields value: (product, ID, etc.). Such cases have to be investigated, and if necessary, a separate model has to be provided for them. Class Classification is the process of predicting the class of given data points. So in classification, it's necessary to predict what class the document belongs to. Often, document types are referred to as classes. Corner cases Sometimes there are some complicated dependencies in the documents. They have to be revealed and considered when documents are studied by a Data Analyst. DA considers the number and complexity of the following cases. A separate model might be needed for the following cases: Different languages Different document types Different set of fields for different document types IE and Classification in one Use Case Single field becomes multiple in some templates Conflicting fields meaning context "},{"version":"10.0","date":"Jul-23-2019","title":"labeling-instructions-provision","name":"Labeling instructions provision","fullPath":"iac/automl/da-manual/labeling-instructions-provision","content":" Goal of the step: To enable SMEs tag documents unambiguously by providing tagging instructions. Input: Data set split into batches. Output: Precise, brief and demonstrative instructions for SMEs. Tagging instructions should include only specific logic for fields in the particular batch — for example, how to tag the documents of supplier \"Taco Inc.\" or secretary of state \"Georgia\". Specific instructions should be precise, brief, unambiguous and clear. Special logic for tagging the fields must be provided there. It is strongly recommended to instruct SMEs separately on each batch and to assign a batch of one template to one person, for example, all the batches of \"Columbia\" template should be tagged only by SME A, and all the documents of \"Nevada\" template, by SME B. If each person can concentrate on one set of fields and similar layouts, that will result in fewer mistakes and higher speed of tagging. First, provide the screenshot of correctly tagged document with comments, like this: Also some important information should be specified, for example: entity_type field should be tagged from the row \"1. Exact name of the\". Federal Employer Identification Number row title indicates corporate_number field, don't confuse with \"Filing Number\". Address information (street, city, state, zip, country) should be tagged from the block \"5. Street address of the corporation's principal office:\". Anything else you think an SME is going to have some doubts around. If it's not clear from the business logic how the field must be tagged, or there are some corner cases, which require special logic, such things should be described as well: It is not clear where the boundaries of the field are, for example: It is not clear how to correct the value. There is some special logic when something must be selected from the list, etc. More information about tagging requirements and high quality data set here. ⇪ Back to Data Analyst runbook "},{"version":"10.0","date":"Jul-25-2019","title":"input-docs-formats","name":"Formats of input documents","fullPath":"iac/automl/da-manual/input-docs-formats","content":" Formats The format of input file on which the model will be trained is important both from the perspective of pre processing (e.g. digitizing it in case of PDF), and from the perspective of how successful the training will be. Let's consider some possible input formats and how to deal with them. HTML Action: none Post action: Can be directly processed with AutoML. IMAGE Action: Send to OCR. Post action: After OCR of an image or PDF, we receive output in two formats: XML and HTML. For ML, it's important to use XML, as it contains more information about the original document. It is also advisable to use ABBYY OCR (its engine is used by default in WorkFusion) where possible, because it produces XML that is most useful for WorkFusion ML models (e.g. contains tags that are directly used by some of the ML features). PDF Action: Digital PDF can be processed with Java code. PDF doesn't have fixed format and new documents will require an additional effort to process. Document formatting will be lost in most of the cases. Send to OCR. Post action: same as IMAGE. EXCEL Action: Rule based approach using Java, and convert to HTML before processing with AutoML. Post action: Excel input is complex: Excel files can be huge in size and after conversion to HTML, it might be an issue to display them in a manual task in WorkSpace. An approach in such case would be to split Excel files by 50–100 rows before displaying these parts in a manual task, and then combine the output back to what is required. WORD Action: Get plain text using Java. Convert to HTML using Java. Convert to PDF or Image and send to OCR. Post action: none EMAIL Action: Extract email body as text. Post action: Work with email body like with the plain text. Plain Text Action: none Post action: Use directly with AutoML. ML vs rule based approach HTML (general approach: rule based) For HTML files, we also can go with both approaches. Please be aware that if a customer has a well structured HTML format, probably the best solution is to use XPath to extract data from documents. IMAGE (general approach: ML) This type of document usually comes as scans. Typical workflow for images is the following: Make sure an image has sufficient resolution (at least 300 dpi) and convert it if needed (using ImageMagic library) OCR step MT or ML step to extract information from the document PDF (general approach: ML) PDF documents can be categorized as three different types, depending on the way the file originated. How it was originally created also defines whether the content of the PDF (text, images, tables) can be accessed or whether it is “locked” in an image of the page. “True” or Digitally Created PDFs Digitally created PDFs, also known as “true“ PDFs, are created using software such as Microsoft Word, Excel or via the “print” function within a software application (virtual printer). They consist of text and images. Both the characters in the text and the meta information have an electronic character designation. With ABBYY FineReader 14 you can easily search through these PDFs and select, edit or delete text similarly to how you would do that in other editable formats, such as Microsoft Word. The images in digitally created documents can be resized, moved or deleted. “Image only” or Scanned PDFs When scanning hard copy documents on MFPs and office scanners, or when converting a camera image, .jpg, .tiff or screenshot into a PDF, the content is “locked” in a snapshot like image. Such image only PDF documents contain just the scanned photographed images of pages, without an underlying text layer. Consequently, image only PDF files are not searchable, and their text usually cannot be modified or marked up. An “image only” PDF can be made searchable by applying OCR, with which a text layer is added, normally under the page image. Searchable PDFs Searchable PDFs usually result through the application of OCR to scanned PDFs or other image based documents. During the text recognition process, characters and the document structure are analyzed and “read.\" A text layer is added to the image layer, usually placed underneath. Such PDF files are almost indistinguishable from the original documents and are fully searchable. Text in searchable PDF documents can be selected, copied and marked up. Processing PDF workflow depends on type of PDF. In case of searchable or \"true\" PDF, we can get content of the files using PDFBox Apache library. \"Image only\" PDFs should go through an OCR step at first (the same workflow as for images). EXCEL (general approach: rule based) Both approaches can be applied for this type of document, but generally it is much easier to implement a rules based approach because Excel is a structured document. An ML approach can be used if the customer has many different templates of Excel documents. Please note there may be some issues while viewing and extracting data in the manual task step for a document with more than 50 rows. In addition to this, keep in mind that it's best to convert Excel to HTML before sending the document to Manual Task. PLAIN TEXT (general approach: ML) An ML approach is preferred for this format, but rules based also can be applied (for example, if 100% sure invoice number is the first word in the document). Note that plain text is the worth case for the ML approach, because this format does not have any additional information (like HTML tags). OTHER FORMATS You may encounter other types of documents. Review the structure of the documents to make the right decision on the use of the approach. "},{"version":"10.0","date":"Sep-20-2019","title":"input-docs-analysis","name":"Input documents analysis","fullPath":"iac/automl/da-manual/input-docs-analysis","content":" Goal: To understand the business logic of the Use Case and documents. Input: Statement of Work (project contract with client where final success criteria must be defined), list and description of fields classes, original documents, any other description documentation provided by client. Output: Logic defined for documents: formats, layouts, special business logic, fields classes and their meaning, corner cases. Steps: Understand Use Case business logic from the documents and conversations. Study documents intended to form ML training set, analyze quality and representativeness of different document structures and distribution. Study fields logic, consider answer types and rare fields. Finalize fields output format. Consider manual task design and logic. Most of the projects are delivered on the customer's premises, so DA needs to go to the customer. Steps at this stage can be divided into two parts: What you can do before you arrive. What should be done once on site. Before you begin Make sure all the credentials are received: Access to folder with original documents. S3 with full access at least on one bucket to upload documents into as well as standard AutoML model buckets to access results and configuration. Tested Control Tower WorkSpace instances with full access. Make sure the required software is installed: S3 Browser LibreOffice, OpenOffice, Ron's Editor, or any other viewer of large .CSV files Chrome 63 Notepad Microsoft Outlook Webex GTM Skype for Business or any other communication tool Before arrival First, the Data Analyst should study the SOW carefully and make sure the whole business process is clear. It's necessary to figure out the project scope and documents' workflow. Find out whether there any specific details depending on the processing period during the year (e.g., in the fourth quarter, does the flow contain the documents of some specific issuers with unique structure, or are the document structures different from period to period). Here is the list of questions to be answered: What type of Use Case is it — Information Extraction or Classification What are the types of documents how many are there What are the formats of documents How many pages are there in one document what pages are needed to be processed How many fields are to be extracted what are they How many classes are to be formed what are they How many models are required to process the document flow Consider additional questions that may arise and ask them in advance. On arrival If you have some documents' samples provided by customer, try to figure out from them as much as possible about documents' logic, structures, diversity and appearance, categories, fields' shapes and context before arrival. In most cases, the Data Analyst is able to investigate documents' logic when he or she is fully on boarded to the project and has been granted access to the documents, which takes time and is usually done on arrival. You may ask the DM to start this process in advance to save time. :::note Note that if the customer's documents include sensitive data, you need to observe security rules. Check the rules here. ::: Here are questions to be answered when studying documents: What is the quality of the original documents is it suitable for OCR can it be improved (for more details: OCR). What is the meaning of each class and field to be extracted from the documents Where is each field given in the documents Which fields are required and which are optional Which fields are rarely given in the documents Is there any special logic, complicated dependencies or exceptional cases Work with SMEs To organize work with SMEs in the most effective way, investigate the following questions: How many SMEs are available for tagging the documents Are they available full time or part time What are their working hours What are their names emails Who is the manager of the team and what are his her contacts You may ask SMEs to fill in the following document, or fill it out on your own after discussion with SMEs. Write an e mail for the whole team with your analysis of the documents and SMEs availability. Example of the email: > Hello team, > > After the initial analysis of the data I would like to raise some issues that need to be investigated in advance to prevent their becoming pain spots: > > 1. Complex document type > > There are purchase orders of supplier N that have table format and multiple pages. Here we can see that single value fields (date, purchase order number and product) become multi value fields, so we need a separate manual task for all the documents of that type. Most probably we will need to train a separate model for that or at least develop an extended model. For now there are 137 documents of that supplier, the info on volume of those documents and if there are similar cases has been requested from SMEs, waiting for response. > > 2. Rare field > > The field \"Oiling\" seems to appear only in the documents of supplier X. There are only 17 documents of this supplier and not more can be provided, because it's a new supplier. The data for this field for all the other suppliers is being downloaded from Data Base. The value for this field is not constant, so there can be no guarantee that if the model trained on 17 documents will process this field in production with the required accuracy level (95%). The situation is to be talked over with the client. > > 3. SMEs availability > > SMEs' team is not steady. Their manager told me that they are going to assign people each week depending on their business. That is an additional risk, because it will take extra time and effort to establish contact with them and train. We need to ask for a stable team of at least 3 SMEs with scheduled working hours at the project in order to tag all the required Data Set of 1480 documents in time. > >Best regards Potential risks not raised Cases with documents of some specific structures, where one or more fields are given in the way not very simple for extraction, conflicting data) are there enough examples of them Cases that require training a separate model. Cases of rare fields. Small number of specific document structures (rare document types). Such complicated cases should be additionally investigated and raised to Delivery Manager as potential risks, if confirmed. Extra documents resources may need to be requested from the customer, a special approach developed, or some complicated cases may be negotiated to be out of scope. When not reported during the early stages, there won't be enough time and resources to deal with them during later stages of the project. Logic is badly investigated When some dependencies or documents' layouts' logic is not studied well, the wrong solution may be developed — for example, in tagging logic or tagging instructions or corner cases revealed during later stages of the project. Additional Materials SMEs onsite training: best practices and materials "},{"version":"10.0","date":"Jul-23-2019","title":"labeling-logic-definition","name":"Labeling logic definition","fullPath":"iac/automl/da-manual/labeling-logic-definition","content":" Goal of the step: Define correct tagging patterns (value shape, value boundaries, position in the document, context and acceptable unacceptable cases due to structural changes after OCR) for each field in each layout. Output of the step: Precise unambiguous instructions for every field in each batch. In order to guarantee high quality of the data set and lower the number of tagging mistakes, it's necessary beforehand to define precise and unambiguous logic for each field in each batch so that the SME can clearly, without doubts, understand where and how to tag a field. Steps Identification of such logic requires thorough step by step analysis of documents from each batch: Take about 20 samples from each batch. Define for each field where and how to tag it. Take into account the factors described below. Check that the defined rules correspond with tagging requirements. Describe the logic with rules and screenshots in instructions. Factors None of the factors listed below should be ignored, as all of them together help identify the best place for each field to be tagged: Number of field's occurrences Field's occurrence means the number of times the same value of one field appears in the document. Usually the number of occurrences is considered for single value fields. See the following example. secretaryofstate field is presented in the document only once, so there will be no confusion in tagging this field and no specific instructions are required. Address information (street, city, zip, etc.) is presented in two different blocks, so instruction should specify only one place to be used for this field. Documents' structure Structure means that after OCR, in the XML document, the information can be represented in the following ways: As a free text inside of one or several strings. As a table with different number of cells. With values divided into several parts. Examples of different structures: Value and surrounding context For choosing specific occurrences which are going to be tagged, WorkFusion recommends considering the value itself and the surrounding context. Note: OCR quality is very important for choosing the specific occurrence of a field for tagging. OCR quality Both the value and surrounding context should be the most stable after OCR, this means: always recognized not corrupted no mistakes at all or only minor mistakes (which are systematic and can be fixed by applying a common rule) has the same structure Content Let us unambiguously identify the value: WorkFusion always means \"WorkFusion\". Workfusion always means \"WorkFusion\". But WF may not always mean \"WorkFusion\". WF sometimes means \"World Freight Group\", sometimes \"WorkFusion\". CC sometimes means \"Coca Cola\", sometimes means \"Chemours Company\". UBS can be interpreted as: \"UBS acting through its London branch\" \"UBS Russia\" \"UBS Group AG\" etc. Content should be informative, uniform, unambiguous across all documents. It is also better when there are some key words helping to identify the correct value, like: \"number\" \"ID\" \"name\" \"client\" etc. Example of tagging logic development We are not going to consider 20 documents now — let’s take just look at four to show the principle. Let's define specific occurrence for tagging business_id field for a batch (New Hampshire). Document A Original OCRed Document B Original OCRed Document C Original OCRed Document D Original OCRed We have two occurrences for business_id. Content factors have the same characteristics in both cases: Value is unambiguous in both cases, i.e. it always means specific business_id and it can not be interpreted in any other way. The content of surrounding context is good in both cases. For every occurrence, it goes after the words “Business ID” so it is good. OCR quality factor. To analyze it quickly, let’s create a table: Document 1st occurrence OCR quality 2st occurrence OCR A (table) B (not a table) C (not recognized) D (context structure is different) So, in this case it’s clear that it is better to choose the first occurrence, as it is more stable from the OCR quality point of view, while the other characteristics are the same. As a result, the most convenient way to deliver this information is to provide a screenshot to the user, like this: ⇪ Back to Data Analyst runbook "},{"version":"10.0","date":"Sep-20-2019","title":"labeling-requirements","name":"Labeling requirements","fullPath":"iac/automl/da-manual/labeling-requirements","content":" In order to ensure a data set is of high quality, result of labeling (tagging) must be: Consistent Complete Normalized Diverse Have Correct Data Values Comprehensive Consistency :::tip Each field in the data set should be tagged consistently for ML training, i.e. fields must be tagged in the same place across layout. The occurrence of the field in the document has to be chosen and adhered to across all documents in the batch. ::: Expand to see the example There are three possible ways to extract the field business_id, its value is C 133395, and there are 3 occurrences of this field. Let’s try to describe the value itself and its context for every occurrence: Case 1 Letter and digits after “Annual report for” in the first string before the table Case 2 Letter and digits after “No.” in the left upper corner cell of the table Case 3 Letter and digits in a cell of the table after: “5. Organized under the Laws of: ID”. Case 1. Correct If we tag this field in the same place all the time (let’s choose the second case, for example) we will get the same description for every document in our training set. So, after the training, model will be 100% confident that for the field business_id needs to be extracted. That is a letter and digits after “No.” in the left upper corner cell of the table. Case 2. Incorrect If we tag this field in different places all the time (let’s suppose we tag in the same proportion), after the training, model will be: 33% confident that for the field business_id needs to be extracted. Letter and digits after “Annual report for” in the first string before the table. 33% confident that for the field business_id needs to be extracted. Letter and digits after “No.” in the left upper corner cell of the table. 33% confident that for the field business_id needs to be extracted. Letter and digits after “5. Organized under the Laws of: ID” in a cell of the table. :::note The model does not treat text same way people do. While people can perceive text entirety, the model perceives text as strings of symbols organized in an HTML or XML tree. The goal is to identify the value and context surrounding the value. ::: Expand to see read more Context is basically everything that surrounds the tagged piece of text. It is the most important source of features and is used to determine where value should be searched for. The set of features is defined on the whole data set (all the documents). In each document, for each gold value feature, values will be calculated and weighed during the training process and the model (the function) will be built. When the model is trained and you run it on some data, it will extract the strings that can be classified as \"the value should be extracted\" by their features. So Feature represents a simple question such as \"Is the current cell located in the first column \" with a clear non ambiguous answer \"Yes No\" or \"1 0\". Learn more about features here. Values themselves are the source of features as well. For example, CUSIP number that is used for classifying financial instruments worldwide has a definite shape and check sum, so any extracted value for CUSIP can be validated by these criteria. Wrong values or wrong context are the source of incorrect features and weights, which affects the model's quality. If the value is not extracted in the context the model expects it to be, it may learn that the values in this context may be incorrect and should not be extracted at all. So It will generate additional FNs in extraction results. Another bad effect may take place if the model defined the features for context in not an optimal way. For example, in one type, invoice _date is given in the document after \"Invoice date:\" and after \"Date:\" and is tagged in both contexts. Documents also contain \"Payment date: {date in the same format}\". In this case, the model may find \"the context like required\" and extract payment date instead of invoice date and has incorrect feature \"date: {value that should be extracted}\". So it will generate additional FPs in extraction results. Completion Completeness has two different aspects: If a correct value in the correct form and with the correct context is given in the document, it must be tagged. If a field value contains multiple words, we should always tag the whole value with all the words belonging to the field, not only a part of it. Correct Incorrect Tampa Consultancy Solutions Ltd. Tampa Consultancy Solutions Ltd. Issue amount: 10 000 000.00 Issue amount: 10 000 000.00 :::note In case, it's necessary to exclude some part from the values (for example, there is a requirement that supplier _name shouldn't contain legal endings), it's recommended to tag the full value, correct the data value and apply post processing to the model results, so that you have full control of what is changed. ::: Normalization Data value can be given in many different formats across documents. It should be normalized to the same one format in all cases for several reasons: For some fields (for example dates or amounts), the values are normalized in a manual task. They should be normalized accordingly after ML extraction. Otherwise, it will be difficult to count the statistics correctly. Another reason for normalization is that the customer may need to upload this data in SAP or some data base and all the values should be of some particular format. Field In documents Normalized Date 01 18 2017, January 18, 2017, Jan 18 2017 01 18 17 Amount 10,000,000.00 or 10,000,000 or 10 000 000.00 or 10 000 000 10000000.00 Company Name WorkFusion or WorkFusion, LTD. or WF WorkFusion Diversity Data set should have good representation of all layouts and fields so as to train ML well. Objects from bad represented or not represented layouts may have feature values which make the model treat them like outliers. The required number of documents to train a layout field is not fewer than 20–50 examples. Also note that to launch the training you need to use not fewer than 20 documents, even if there is only one template in the data set. Correct data values Correctness of data values means that in each document, for every configured field, should present correct data value. In other words, the values should be exactly the same as we want the ML model to extract from the original document. Usually, mistakes are caused by: OCR, like iOO8.4O instead of 1008.40 (data value should be corrected manually in the task), By user (if the value is wrongly corrected manually or the wrong value is selected from the list). Data value is the information which is given in the answer box. It can differ from the tagged piece of information when formatting mask is applied. Comprehensiveness The value and especially context should be comprehensive and accurate so as to provide correct features and weights for extraction by model. So in this regard, attention should be paid to OCR quality of each field and its context. The following cases should not be tagged and provided for model training. OCR issues OCR has not recognized the field at all. Change OCR parameters. If this doesn't help, exclude document from training set. OCR has corrupted the value. If the value itself and its context are totally corrupted and without checking the original document you cannot understand what should be in this part of a document, it should not be tagged. The document should be excluded from the training set, because it won't let the model learn anything useful and may be the source of wrong features and weights. If such documents are regular cases in the common documents flow and are not caused by the original documents quality, it's recommended to improve OCR quality. OCR has not recognized the context at all. If the context of the field is missing or corrupted completely, it will be difficult for the model to recognize a required value and extract it with high confidence, so such values should not be tagged, \"N A\" option should be chosen. Write a comment about bad OCR to be able to find such records later. OCR has corrupted the context. Solution is the same as above. OCR has corrupted the structure, only the part of string is field's value. Original document After OCR Text may be converted into a tables with random structure that won't be reproduced in other documents. If the value to be extracted is in the table, it's recommended to exclude such documents from the training set as they may be a source of wrong features and weights. If such documents are rather regular cases in the common documents flow and are not caused by the original documents quality, it's recommended to improve OCR quality. If doesn't help, exclude document from training set. The reason for not tagging values with broken structure with the help of \"Append selected\" is that model reads entire string: \"FULL SPECTRUM GARDEN 35 ROSEMARY LANE, 001182906\". From a human perspective, we can define the borders and ending of the value, but we can't teach a model well where to stop extracting if we have the value in different strings surrounded by extra data. The model might learn well if the value is constant (for example, there are 30 more documents of this supplier and in all of them, the value is split in that way). But the most likely case is we'll receive a FN or FP FN — the model won't extract anything or will extract only part of the value. But we can also receive the following cases in other batches: Western 250 Broadway Frnt D Union Company New York, NY 10007 2516212 571 4511 Supplier: Financial Street Holdings Delivery date: 12 05 2018 "},{"version":"10.0","date":"Sep-20-2019","title":"labeling-validation","name":"Labeling and validation","fullPath":"iac/automl/da-manual/labeling-validation","content":" Goal: Get tagged data set of high quality Input: Manual Task, batches of documents Output: Reviewed data categorized by groups based on the type of inconsistency found. When there are at least some batches tagged, a Data Analyst can proceed with data validation. There are two common ways to validate data. Validate in browser mode. Validate via creating a new Manual Task. Let's see each approach in detail. Validating in browser mode As an output of labeling stage, the Data Analyst gets a CSV in which tagged text is presented as XML content. To use this method of validation, it's necessary first to convert tagged XML content into links to HTML files. For this purpose, a business process — \"Save tagged content to s3\" — can be used. Generating links Input: A snapshot from a Manual task with a column with tagged text (XML, HTML). The column name should be tagged_text. Output: CSV with column docpagexml_link that contains links to tagged documents on S3. Download a business process to import to Control Tower. As a preliminary step to validation by opening links, it's possible to look through data values. In an Excel file, create a filter for all the required fields and check whether there are empty cells or suspicious values that cannot be the value of this particular field. We can check it because the format and expected content for every field is known, so if anything is incorrect, it is easy to identify and mark them as incorrect without even opening the tagged document itself. Most common examples: The name of the field is company_name but the data value is California, which is the characteristic of state and not the name of the company. Some odd information was tagged: for example, the field company_name data value is “Corporate Name: Fitness And You, Inc.”. required fields include blank cells (no data values); inspect separately to define the reason for information missing: bad OCR or inattentiveness during tagging (should be re tagged). However, to validate consistency and completeness of tagging, and to guarantee the right text chunk was tagged, open each and every document to avoid cases like this. For example, the data value for the field invoice_number is \"2574635\", but if we open a document we'll see that \"2574635\" belongs to Reg.No and not to Invoice Number. The same problem can be with dates when there are some of them in the document (order date, delivery date, etc.) and only context allows us to check it's the right tagging. :::tip The following macros can be used to speed up the process of opening links. Select a range of links and run it. ::: Sub links_opening() On Error Resume Next For Each cell In Selection.SpecialCells(xlCellTypeVisible) ThisWorkbook.FollowHyperlink cell Next cell End Sub This method of validation can be used when the speed of tagging is high or you have to deal with already tagged large amount of data; it allows easier and faster processing of large volume data sets and defines incorrect records that will be corrected or removed if the size of the data set allows. :::tip Combine all the batches that are ready for validation in one file to generate links to the tagged content not to run this BP for each batch. ::: Mistakes categorization As it's impossible to correct tagging mistakes during validation in browser mode, WorkFusion recommends assigning each record a particular category based on the inconsistencies found: \"Correct\", \"Bad OCR\" and \"Re tag\". Look through the points below to understand how to categorize different types of mistakes. Such categorization will allow us to be aware of the quantity and quality of documents which can be used in the training and test set. Good Absolutely correct and consistently tagged documents, in which no values are missed and everything tagged correctly; OCR is good. For test only Corner cases (Bad OCR, Rare Template) documents, which are not eligible for training, but which exist in real life documents' flow and can be processed (at least partially). These are documents for which data values for ALL THE FIELDS presented in the original document (except handwritten) are correct, but: they belong to rare template, there are less than 10 such documents and they significantly differ from the others (so the Model will not be able to be trained well on such a small number of examples). or after OCR there are some issues: values with OCR mistakes, broken structure (so field can be tagged only with \"Append selected to\" option), context of fields has BAD OCR, etc. For training only Positions of presented tags are correct and tags are complete, but some tags are missed (usually due to OCR issue). Also documents with wrong data values or tag missed due to tagging mistake can be used for training, but only if there is no time or capacity for re tagging. Normally it is better to re tag such documents. Re tag Document contains some issues with both data values and tags, but it still can become \"Good\", \"For training\" or \"For test\" after re tagging. Good document but some values weren't tagged though presented in the document. Complete and correct tagging with good OCR around the tagged values, but field is tagged in wrong place in different documents (inconstancy) Value tagged from different locations across one template. Documents which are tagged incorrectly, i.e. at least one of the following. Not completely Tampa Consultancy Solutions Ltd. Tampa Consultancy Solutions Ltd. Wrong value was tagged Reg.No. 27638217 tagged as client_id. Wrong tag assigned to the value angelina.bell@csn.com tagged as requester_soeid. Not all values of one field were tagged (for group fields) Item Price Total LenovoA350 456.90 456.90 HP5389F 345.67 345.67 Dell 351 244.88 244.88 Invoice Total 1 047.54 Bad OCR was tagged Bank Account SWIFT Please remit your paymentto \"H1rA~@Lt1.\" B~40)ndg\"! Reos Sth.89OA~ c Changed data value Tagged string for product is \"ALUMINIZED\" so the data value should also be \"ALUMINIZED\", not \"AL\" or \"Aluminium\". Exclude None of the above mentioned. Document has some considerable issue, so it is inappropriate neither for training or test sets. Usually it is documents with really bad original quality or with considerable OCR issue. For example: There is some serious OCR problems for all fields, nothing is recognized correctly. Totally corrupted structure of document. There are only handwritten fields in the document. Enabling category dropdown So as not to require typing a name every time, a drop down function is recommended. Create an Excel copy of your CSV to keep all the formatting. Create a new spreadsheet, and insert categories. Select the range for which you want to set a drop down list, click the Data tab, and then Data Validation. Select List option from the Allow: field, and then specify your range of possible values. Validate via new manual task This method can be used when the speed of tagging is rather low or the customer's document flow doesn't allow processing large amount of records at a time. It allows the following: Correcting mistakes during validation. Checking tagging consistency, completeness and data values. Not generating links to the tagged content. However, when working on site, a customer's limited server resources may considerably slow down Control Tower and WorkSpace. Creating a new manual task Generate snapshot from the completed task. Create a new task by making an independent copy of the tagging task. As input data, upload the snapshot with the tagged data. On the Design tab, change the settings of the task. Click Show Advanced Options. In the Original Document field, provide the name of the column where the links to original data are stored. In Default Value, provide the name of the column in a snapshot where tagged data is stored. It should always be the same as Unique Code plus \" _tagged\" suffix. Click Save. Run the task. Go to Workspace and accept this task. If the record is correct, click Submit. If there are some inconsistencies, correct and submit the task. Additional information Despite the method of validation, the following issues must be observed: All the required fields are tagged. Here you need to make sure the \"n a\" option is soundly chosen and the value is not missed due to inattentiveness — except for the cases when it's totally corrupted by OCR. If the value is presented in the document, it was tagged. Otherwise, missed fields will be badly represented and such documents should be re tagged or the data set should be increased. Fields are tagged from the same location across the whole data set. If more than one variant of tagging the same field is found in one template, see how often this happens. If three times out of 200 documents, the supplier _name was tagged in some other place it’s better to exclude such documents. Pay attention to documents’ layouts as well. Note that due to OCR, the layouts of one vendor can be multiplied — for example, because of the table structure broken, etc. Such layouts, as well as badly represented layouts (fewer than 10 for a vendor), should be increased, if possible. Complete value is tagged. Data value was not changed manually — e.g., the tagged string for a product is \"ALUMINIZED\", then the data value should also be \"ALUMINIZED\", not \"AL\" or \"Aluminium\". There is no tagging of corrupted values which cannot be restored. Option “Append selected to” was used correctly. If you are not sure, just try to select a desired value. If no extra information (except for the required text chunk) gets into your selection, there is no appended parts. If the final number of good documents is not enough, it is highly recommended to correct a category \"Re Tag\". "},{"version":"10.0","date":"Jul-23-2019","title":"ml-model-improvements","name":"ML model improvements","fullPath":"iac/automl/da-manual/ml-model-improvements","content":" Once the first results are received, there is low probability that they will meet all the success criteria without any changes and tuning. More often, some enhancements should be done. Let’s review the most proven ways to achieve high ML model results. Model tuning is usually performed by an ML Engineer and (in some complicated cases) a Data Scientist. Below, find brief overviews of the most common practices in ML model improvements. More detailed information is here. Clean your data Perhaps there are missing or corrupt values (both in the training and test sets) that can be fixed or removed to increase the quality of the data. However, if the data set was prepared properly, in compliance with all tagging rules and instructions, issues over mistakes in gold data should not arise. Look for more data If the model’s accuracy on the validation set is low, or fluctuates between low and high each time, the model is insufficiently trained and more data is needed. Apply post processing Post processing is the final stage of the AutoML SDK pipeline, performed after model training. Post processing modifies the trained model output to fit the requirements of a customer by applying pre defined rules. In rare cases, it's based on a separate ML model. Generally, a rather high boost in precision can be achieved with the help of post processors just by modifying fields’ values (adding, removing, changing (80% of cases), and grouping). Post processing and ML customization must be aimed at improving ML quality in production, not at achieving good results on this particular test set. It means only frequent mistakes in valid cases should be corrected. The impact of exceptions should be counted separately and communicated to the Delivery Manager. Types of post processing Normalization Normalization is the process of changing the extracted values to a certain format. The extracted values may include: Numbers (1k → 1000) Dates (11.07.2018 → 07 11 2018) Prices (100 → 100.00) Currencies ( → USD) Addresses (NY Brooklyn 02356 Ralph Ave. → 02356 Ralph Ave. Brooklyn NY) Organizations (WF or WorkFusion Systems → WorkFusion) OCR errors correction During document processing, OCR may introduce errors due to damaged documents and low quality scans. In this situation, post processing can be used either to correct damaged values or remove them. Fixing value OCR output may contain the same recognition error in all documents. Post processing can fix incorrect values by introducing a rule (usually a regular expression replacing one character with another). For example, if values are corrupted in one and the same way across all the data set, the following rules can be applied: replace G with 6, B with 8, O with 0 in Zip Codes. Removing value Sometimes OCR may introduce a recognition error in 50% of cases while the other 50% are correct. For example, model training results may show that character B is replaced with 8 in 55% of cases and with 6 in 45% of cases. This is quite a low probability level and should not be used in a post processor. Depending on the use case requirements, it is generally recommended that values with low to medium level of positive correction probability should be skipped or removed by the post processor. Validation When extracting Bank Card Numbers, IBANs, Zip Codes, CUSIPs, etc. it’s crucial to check them to make sure the extracted value is valid. In cases where the extracted value is not valid, it’s usually removed. Mapping to Reference Data A post processor can extract additional data not contained in the original document using the reference data. For example, Employer’s Name and Surname can help extract a unique company ID. Having extracted only zip code, you can map it to the city, and then extract this information even though it’s not initially present in the document. Reference data can be taken from a database, an API request, a local dictionary, etc. Transformation Transformation represents simple procedures like trimming, lower upper case, capitalization, removing punctuation, special characters, etc. For example, NOT CHEMICALLY TREATED can be transformed into Not chemically treated. Grouping A model can extract multiple field values of the same type (for example, Currency, Quantity, and Price) that should be joined into a group according to their logical connection. For example, you may need to group Product Name, Amount and Price. Field values can be grouped based on their position in a document or words in a sentence, based on a table line, or based on some underlying custom logic. For example, table cells. Field values grouped based on a line in a table. For post processing code examples, visit post processing examples. Create additional components: feature extractors and annotators Features are unique signatures of the given value or unique properties that define a value. Creating new feature extractors depending on the particular data set can help identify a field more unmistakably and as a result, improve model accuracy on unseen data. For more details, see feature engineering. Select accurate features Not least important is feature selection. Feature selection is a process of finding out the best subset of attributes which better explains the relationship of independent variables with target variable. Feature selection methods help to create an accurate predictive model and can be used to identify and remove unneeded, irrelevant and redundant attributes from data that do not contribute to the accuracy of a predictive model or may in fact decrease the accuracy of the model. Fewer attributes is desirable because it reduces the complexity of the model, and a simpler model is simpler to understand and explain. ⇪ Back to Data Analyst runbook "},{"version":"10.0","date":"Sep-20-2019","title":"manual-task-design","name":"Manual task design","fullPath":"iac/automl/da-manual/manual-task-design","content":" Goal of the step: To design a manual task with all business logic of all the documents and fields applied Output of the step: Good manual task which is copied for qualification task and used for tagging The manual task can be created for one of the following most common use cases: Information Extraction (IE) Classification For model training, we need to find and tag entities (fields) in an XML document. They will become values for the model to train on. To create a good manual task for each case, it's necessary to consider business logic of all the documents, types of the documents and their layouts. Special attention should be paid to fields' logic and appearance in IE use cases. The data analyst reviews the set of fields and defines what fields have only one value in the document and what have several values or, in other words, is it a single value or multi value field. Single value field — field for which the result value will be the only one in each document. Typical examples are supplier_name, total, date. Multi value field — field for which the result value will be plural. Typical examples are productname, productprice, product_quantity, etc. :::note It's necessary to distinguish clearly in advance what fields are single value and what are multi value and all the cases for each, because if this is missed and revealed in later stages, extra effort will be required to redesign the manual task and retrain the model. ::: Answer types Answer Type is a parameter which defines an input field type and format. It's essential to select a correct answer type for each extraction field, as it affects model training. The decision should be considered in terms of business logic of the Use Case. It's advisable to select corresponding answer types, for example, number for corporatenumber field (if this field is always numeric), zip for zipcode field, etc. The most commonly used answer type is Free Text, but it shouldn't be misused and applied for all the cases. Learn more about Answer Types here. Most common Answer Types Answer Type Description Field example Free Text Allows entering a single line text string entity type, company name Number Allows entering integers only quantity, bank account number Price A combined input field used to enter monetary value total, price Date Allows entering a date in mm dd yyyy format date Country A drop down list of countries to select.(To speed up the selection, you can start typing the country name.) country Select one A drop down list that allows selection of a single option Classification use case Check one A radio button group that allows selection of a single option Classification use case Group of Answers Allows tagging multiple similar combinations of values that are supposed to be grouped together product name, price In a group of answers, multi value fields make up only one sub answer. So SMEs should tag repeatable information blocks — for example, information about products: name, price, quantity, etc. The answer is displayed as a multi tab panel containing its sub answers. Required or optional Required field — field which is presented in all the documents in all the batches. For example, invoice number, invoice date, etc. Optional field — field which is presented only in some documents or in some batches. These fields usually have special additional logic. Required and optional fields need to be considered together with SMEs. However, if the number of fields is 10 or higher, it's strongly recommended to make it impossible to submit the task without making sure that all the necessary values have been tagged. Best practices Apply hot keys Hot keys are really helpful for quick tagging. To increase speed, assign each field a hotkey and teach SMEs how to use them. There are two approaches to assigning convenient hot keys: Assign intuitively understandable and memorable letters — for example, first letters of fields: d = date, p = price, n = invoice number, etc. Assign letters and figures that are close to each other on the keyboard: 1, 2, 3 = first three fields; q, w, e = second three fields; a, s, d = next three fields; etc. Use \"n a\" It's better to mark optional fields as required and make an n a selection available in order to minimize SMEs' mistakes due to inattentiveness. Split long entities Check whether this value can be split into some independent parts — for example, the whole address line can be very difficult to extract, as address formats usually differ from country to country, or even within different documents' templates. It's recommended to split the value into shorter entities that are situated in one XML tree element (street address, city, state, postal zip code, etc.). :::caution Be careful with value splitting. Values like company _name cannot be split, as their parts cannot be considered independent and the value only makes sense with the whole name. ::: Always provide link to the original document There are different ways to provide an original document: as a separate link, or in a pop up view. Make sure that at least one is provided and require SMEs to compare it with the XML document when they have questionable values. Enable additional fields more informative task output (if necessary) comments — Comments can be provided to select documents quicker. While tagging, SMEs will provide notes on what is wrong with the document and that can be further picked out. Provide an optional field for comments in the manual task where SMEs can mention information about a missing field, document structure, etc. drop down to classify documents — If you have, for example, a combined data set in five different languages, it will be useful to add a special field with the list of all five languages as a drop down. This will simplify further analysis of the data set and its grouping. checkbox to mark bad OCR — Even after OCR tuning, there still can be documents with bad OCR. To avoid tagging such documents and easily find them later, it's recommended to include a special checkbox for OCR. Use reference data, when applicable Reference data let us use drop down lists for some fields. It means that during the tagging, a specific value from the list corresponds to a tagged piece of information. There are two main reasons to include reference data in the Manual Task: Normalize values For example, we want all the company names being extracted, and have a one format in which they will be entered into SAP: WorkFusion, Inc. However, in our data set we can encounter different formats for the same company: WORKFUSION, WF, Workfusion Incorporated. In order not to change the data value manually every time, all we need is to tag the value as it is presented in the document and than choose a full version of the same value from a drop down. Match information about some entity in the document with some additional data from the database which is not presented (or corrupted) in the document. Some fields may be connected with values of other fields, so, for example if you know the approverid, we can figure out the approver'sname as well. In some cases when you have trouble with OCR quality, that option might be useful. :::warning Reference data can be used when we have a limited number of values in a database, which should be up to date and match the corresponding values from the documents. Otherwise, information from the document will not match the reference data and the field's value will be empty or 'n a'. ::: How to enable reference data Create a data store for the field in which you’d like to use reference data. Example, for states: To be used as reference data, the data store should have two obligatory columns: ID and name. In Manual Task, open settings of the field, and then choose Answer Type > Select One. Select Input from data store and select the relevant data store from the drop down. In Manual Task, it will look like this. Don't make your manual task too big We recommend including 7–10 fields in one manual task. If the Use Case implies extraction of a larger number of fields, it's better to create several tasks. So, if, for example, you need to extract 30 fields, create three separate tasks with three different sets of fields in each. This way, your data set will be processed three times through three different manual tasks. Otherwise, too many fields in one task may lead to lower concentration and, consequently, lower quality of the data set. ⇪ Back to Data Analyst runbook "},{"version":"10.0","date":"Sep-20-2019","title":"ocr-analysis-tuning","name":"OCR analysis and tuning","fullPath":"iac/automl/da-manual/ocr-analysis-tuning","content":" The goal of this stage is to achieve acceptable OCR configuration and get understanding of final OCR quality. Input: Original documents Output: Effective configuration of OCR and all the documents pass through OCR process with acceptable final quality. Final report to Delivery Manager on OCR quality and all the risks identified (if any). OCR overview OCR (Optical Character Recognition) is the electronic conversion of images of typed, handwritten or printed text into machine encoded text, whether from a scanned document, a photo of a document or from subtitle text superimposed on an image (for example, from a television broadcast). WorkFusion uses ABBYY OCR, which proposes an OOTB (out of the box) solution: there is no necessity to write or maintain any custom code. However, it's a chargeable service and each page is paid for. Today WorkFusion mostly works with ABBYY OCR, which is integrated into separate business processes. The result of an OCR business process may look like this: However, you may encounter the following result as well: OCR quality factors Such a discrepancy in the OCR output is justified because the quality of the final document is affected by factors both internal (OCR built in parameters) and external (peculiarities of original documents): Internal — OCR parameters Export Format — usually XML Document language (language parameter must be supported by the license) Brightness Scanning resolution (DPI) Contrast Convert to black and white Correct orientation Allowed region type Invert image (true, false) Discard color image (true, false) etc. External — documents' initial quality See the examples of the most affecting factors. Resolution (low, high) Font size Print quality (fax, newspaper) Position on the page If a page is too skewed, it may severely impact the quality of the OCR Noise in the document Noise like shadows, lines, dots left from previous copying scanning. Handwritten text Logos, stamps, bar codes, watermarks Crumpled sheets, stains Recommendations We recommend to use documents with high readability without visible defects and noise with initial quality of ~300 dpi. If these requirements are met, the documents after OCR look adequate and can be processed by ML. The image above has good resolution, optimal font size and style, no noise and other defects, and thus will be well recognized. :::note It's necessary to understand that the result of the whole project directly depends on the OCR quality. Unacceptable OCR quality (i.e. missed and corrupted fields; corrupted structures, etc.) can be the most serious reason for bad ML model results from both training and testing aspects. If the success criteria stated in the SOW is 90% accuracy and current OCR quality is about 90%, there is high possibility that final result will not meet the project requirements. OCR as one of the initial stages sets up the maximum which can be achieved on the project. All the rest of the stages may bring additional mistakes, which will only decrease the overall quality of ML results. ::: Running OCR business process To launch the OCR business process, you need a file containing links to original documents. To get the links, follow the steps below. Getting links Open S3 browser. Choose a bucket (or create a new one) and create a separate folder where original documents will be stored. Click the Upload button, and choose Upload Folder( s). Select a folder on your computer with original documents and wait for all the records to be uploaded. Set the permission to let all users read the files: Select all files. Click the Permissions tab. Select Read next to All Users. Click Apply changes. Select all records in the folder (Ctrl A), right click the selected files, and then click Generate Web URL(s)..\". In the pop up window, click Copy to clipboard, and verify that all the links were copied. Open a new Excel file and paste the links. Assessing OCR quality Before sending all the available data set to the OCR business process, it’s necessary to make sure that current OCR settings are sufficient to ensure the required quality of the data set after OCR. To assess the quality of OCR, it's recommended to prepare a sample set. Here, two variants are possible: If a customer provided you with a data set already sorted by some criteria (supplier issuer secretary of state etc.), choose 10% from each group and OCR them. If documents were provided without any sorting, randomly choose 10% of the data set and OCR them. Be prepared that after OCR, a document will differ from the original, especially documents with complicated structures (tables), font styles, stamp, logos, etc. These are examples of input (original documents) and output (OCR documents): Original document OCR document Template : : Original doc. link OCR doc. link New Hampshire Original doc. link OCR doc. link Massachusetts Original doc. link OCR doc. link Georgia Analysis of initial OCR results To know what level of OCR quality we have, it's necessary to look through each document from the sample group (10 %) and fill in the report. The report should have links to the document (original and OCR) and an assessment of OCR quality: Acceptable Unacceptable. Acceptable: Each field and the context around it is recognized, mistakes can be corrected by post processing and don't create any difficulties for extraction due to missing and or incorrect and or strange context or unrecognizable values. Unacceptable: After OCR, at least one field can not be restored with post processing, structure is corrupted, or the context around the field is not recognizable. More examples of unacceptable OCR quality can be found here. Count the percentage of Acceptable quality assessments. If it is higher than success criteria stated in the SOW, no tuning is needed. If not, additional analysis and adjustments should be done. During analysis of OCR output, you may notice that not only does the OCR document differ from the original, but also documents which have the same original template may have different layouts after conversion. :::tip The group of documents after OCR which have a similar structure are called a layout. Before OCR, they're considered original documents. ::: Example of layouts Below are two documents which belong to the same template, Georgia, and have the same fields and rows. However, after OCR they have different layouts. Layout 1 Layout 2 :::important These differences in layouts within one template should also be taken into account when preparing batches for tagging. ::: Look through original documents which have \"Unacceptable\" quality after OCR. Try to define problems which may cause low character recognition. Tuning OCR You may try different configurations of OCR parameters to get the best possible output quality. For example: Select the correct language De skew images The program will automatically detect skewed pages and correct skew if necessary. Straighten text lines The program will automatically detect uneven text lines on images and straighten them without correcting trapezoid distortions. Correct image resolution The program will automatically determine the best resolution for images and will change the resolution of images when necessary. Fix inverted colors on image When appropriate, the program will invert the colors of an image so that dark text is printed on a light background. Convert to black and white The program will convert color images to black and white. This will greatly reduce the size of the resulting OCR project and speed up OCR. However, black and white conversion is not recommended for photos, magazine pages, and text in Chinese, Japanese or Korean. Remove color marks The program will detect and remove any color stamps and marks made in ink to facilitate the recognition of the text obscured by such marks. This option is designed for scanned documents with dark text printed on a white background. Do not select this option for digital photos and documents with color backgrounds. Adjust contrast Adjust brightness ABBYY OCR suggestions For various OCR problems ABBYY suggest the following solutions. Characters are too thin and sketchy Lower the brightness to make the image darker. Use the grayscale scanning mode (brightness is adjusted automatically in this mode). Characters are thick and stuck together Increase the brightness to make the image lighter. Use the grayscale scanning mode (brightness is adjusted automatically in this mode). Scanning resolution For best recognition results, vertical and horizontal resolutions must be the same. Use the following resolution for scanning: 300 dpi for typical text (printed in fonts of size 10pt or larger). 400–600 dpi for texts (fonts size 9pt or smaller). Setting the resolution too high (over 600 dpi) slows down OCR. Increasing the resolution beyond this point does not yield substantially improved OCR results. It should be adjusted. Setting an extremely low resolution (less than 150 dpi) adversely affects OCR quality. It should be adjusted. If the image has a non standard resolution, it should be adjusted. Some faxes, for example, have a resolution of 204 by 96 dpi. It's suggested to use color documents within OCR process. A color image, since it has different colors present, adds ability to remove RGB color channels prior to the processing, decreasing the document noise and providing a way to stabilize the image before applying grayscale conversion. After OCR, tuning quality should be assessed again. Same as during the initial assessment, take another sample group of 10% of the data set (different from the first one) and validate each record by filling in the report. Evaluate the volume of good quality documents. Try different combinations of OCR parameters to get the best possible result. :::warning If the initial documents' quality doesn't achieve a quality level sufficient for the model to meet SOW requirements despite any OCR configurations, this should be communicated to the customer as a potential risk. Such documents should be replaced with better ones or they should be processed separately with other (lower) success criteria. ::: "},{"version":"10.0","date":"Jul-23-2019","title":"prepare-for-labeling","name":"Prepare for labeling","fullPath":"iac/automl/da-manual/prepare-for-labeling","content":" Goal: Train SMEs to tag the documents properly to ensure high quality data set and to prepare documents for tagging Input: Original documents in XML HTML format, SMEs team Output: Documents prepared for tagging: split into batches, with tagging logic predefined. SMEs trained to tag documents qualitatively. This stage is critically important, because here the Data Analyst lays the foundation for the whole data set collection process. If there are any issues at this stage not revealed and addressed properly, they can result in major problems during later stages. The main aim of this stage is to prepare everything for tagging to start: to train and qualify SMEs, and to prepare documents for tagging. SMEs' training and qualifying means that they are able to tag documents and meet the requirements of a high quality data set and documents preparation — including splitting into batches, establishing tagging logic and describing it in tagging instructions. DA needs to perform the following steps in this stage: Manual task design Train SMEs Split into batches Tagging logic definition Tagging instructions provision In the end of this stage, the Data Analyst assigns a specific manual task for each SME, provides a batch and instructions for it, and creates a data tagging tracker to track SMEs' speed of tagging. In the result of this stage, the batches and manual tasks are prepared and assigned to SMEs, a report on the whole stage is provided, and all the necessary preparation for the first tagging iteration is conducted. The following template can be downloaded and used as a report for this stage: Download report example Risks at the stage: Wrongly configured Manual Task The Manual Task should be configured according to business logic of all the documents. It should have correct Answer Types and Grouping, because if a mistake is revealed in the process of tagging, it will take extra time and effort to reconfigure the manual task, re tag documents, and even retrain the model (if training iterations start in parallel with tagging). Low tagging quality There is a risk to involve under skilled SMEs for tagging, which could result in inconsistent and incorrect tagging — which would result in bad quality for the data set or extra time to re tag and verify documents. SMEs must be trained well or at least pass through the Qualification task after additional training. "},{"version":"10.0","date":"Jul-23-2019","title":"report","name":"Report","fullPath":"iac/automl/da-manual/report","content":" Goal: Report ML quality to the customer and communicate poor results, if any, in an accurate yet positive way, and propose further solutions. Output: Excel file with report. Ways to report ML quality There are several ways to demonstrate ML quality to the customer, some of which can be combined: Show the average document level statistics in a deck. Provide table with field level statistics. Launch the documents from the test set (tagged by the model) for SMEs to review in a manual task. Send statistics of extracted data to the customer so they can also calculate the statistics from their side. AutoML Dashboard. WorkFusion recommends using a report that includes most of these methods. Report Metrics Some metrics will be common to and some will be different for Information Extraction (IE) and Classification. TP (True Positive) TN (True Negative) FP (False Positive) FN (False Negative) FP FN (False Positive False Negative) Extracted (TP FP FP FN) The number of objects instances of a specific field that were extracted by the model. The number of extracted fields can be fewer than the number of documents, if the field does not appear in all documents and because the model may miss some values. If the field is multi value, then the number of extracted fields can be more than the number of documents. Gold (TP FN FP FN) The number of objects that are available in the test set. \"Gold\" in this case means the fields that were tagged by SMEs and thus are considered 100% correct. Correct (TP) The number of objects that were correctly extracted by the model. This number doesn't equal the number of extracted fields, because not all of the extracted fields might be correct. Accuracy (Correct Extracted) A metric that shows how exact the model is in its decisions. It is calculated by dividing the number of the values extracted correctly by the number of all values extracted. Automation Rate (Correct Gold) A metric that shows how complete the model is in its decisions. It is calculated by dividing the number of correct values extracted by machine by the number of all values available extracted by SMEs. This metric shows how much of work the model is able to automate, while precision shows how accurately it is done. F1 (2 (Accuracy Automation Rate)) (Accuracy Automation Rate) Score is the harmonic average of the Automation Rate and Accuracy. Delta of Automation Rate and Accuracy per iterationThese metrics are the most commonly used, but extra metrics can be added as well. Structure The Report is an Excel file that includes three main information blocks: Deliverables should describe the following: Model info: name, type, version (extraction or classification, to distinguish between trained models when some changes were applied). Fields' names: names of the fields for which the model was trained. Training and test set size. ML statistics: document identifier (original document link) gold value (document link tagged) extracted result by model (document link extracted) gold vs. extracted value per field and per document decision for gold vs. extracted comparison: (true positive (TP), false positive (FP), true negative (TN), false negative (FN), false positive false negative (FP FN)) Improvements: model mistakes that were not processed, with the reasons and solutions offered Additional: pivot table with documents per vendor in test and training sets and their OCR quality number of documents per class in test and training sets and their OCR quality extra metrics communicated with customer Note that the model is focused on tagged strings and the statistics are calculated and report is provided for data values (as the customer will see data values). Results and explanation Statistics should be calculated after each change in automatic data processing implementation (new model version, next iteration of training, post processing application, etc.). The final presentation of automation results should be shared with the customer together with the reasons why some stats on some fields fall behind success criteria, if there are any. Moreover, further steps and solutions should be proposed. For example, out of scope cases: handwritten text bad original document quality, which resulted in bad OCRed document quality bad fields representation The issues mentioned above should NOT be reported at the last stage for the FIRST time. They should be raised to the Delivery Manager and discussed with the customer during previous steps. Actions should be proposed for the next phase or future project to build on the current achievements and improve to desired level. For example, an agreement can be made with the customer that badly represented fields can be excluded from model stats calculation until their number in the data set reaches a desired level. Download Report example ⇪ Back to Data Analyst runbook "},{"version":"10.0","date":"Jul-23-2019","title":"sample-analyze-and-improve-model","name":"6. Analyze and improve model","fullPath":"iac/automl/da-manual/sample-analyze-and-improve-model","content":" Input: Generated CSV file with results of IE process. Output: Trained model properly tuned to meet the required success criteria. After splitting the data set into training and test sets, the Data Analyst together with the ML Engineer launches model training, and after that runs Information Extraction Classification on a test set. Make sure the input file provided for extraction includes the following information: column with links to the original documents column with links to the OCRed documents column with tagged text (as XML content) This is important since this information will be used in the next step: statistics calculation. When uploading a file for extraction, you may see a notification that names of the columns in your file don't match what was pre defined in the Manual Task or the Business Process. There will be an option to map columns. Click on \"Match columns\" and map columns from your input file with columns in this Task Process. After Model Execution on the test set (123 documents), as output we get a file containing all the information from the input file and also a column with extracted text (as XML content). Now, when we have a consolidated file with gold and extracted values, it's easy to run a business process Statistics aggregation v2.0 to get detailed information as well as accurate statistics per field. Running a statistics aggregation BP Now you need to run a statistics aggregation BP, for instructions see this document. Note, that instead of going to the Results tab in step 5 you need to go to S3, find the corresponding bucket, folder and download the Excel file with statistics. Analysis of Results After running the Statistics Aggregation business process on 123 documents, we get the following file with the results. Raw Data Deliverable Download a full file with statistics to get more details In order to understand what kind of improvements should be applied to the model to meet the required success criteria, it's necessary to thoroughly analyze extraction results for every field, specifically: reasons for FP reasons for FP FN reasons for FN As the result, the following report can be generated: Download the Excel version of this file to get more details In this report, you should specify name of the field, most common mistakes for each field, and propose a solution (some rules) for their correction. Note that your rules for post processing should not be confusing and should capture only frequent mistakes. For example: Gold Extracted 1 JOHN C. HAMPE JOHN C. HAMPE, 2 MANAGER OF THE EUROPEAN EQUIPMENT FUND MANAGER OF THE EUROPEAN EOUIPMENT LEASING FUND In the first line we see that during extraction \",\" (comma) was also selected, so the rule can be: Delete all commas dots after data value for the field authorized _person. In the second line we see that in the word \"EQUIPMENT,\" the letter Q was replaced with O, but we cannot write a rule to replace Os with Qs as this is a very specific case and replacement will affect other words. Same concern regarding records marked orange in the above report: They describe problems that cannot be fixed by simple post processing. Possible solutions for such cases can increasing the data set and creation of additional components. After major problems were analyzed and solutions defined, the next step is to apply post processing. It is usually performed by ML Engineer. After post processing is done it's necessary to analyze results one more time in order to define whether they meet success criteria and what additional model improvements need to be done (if any). Below are the results after implementation of all the steps defined in the \"post processing solutions\" file. As seen in the table above, we met success criteria for the Automation Rate (recall), which is 60%, and for Accuracy (Precision) for almost all the fields except business type and street address. Here more complicated logic and techniques should be applied. Nevertheless, these cases should be explained to the customer and a solution should be proposed. Some possible explanations and improvements are given on additional sheet of this Excel file: Statistics with PP and possible improvements Case 2. Data set with mistakes The results demonstrated above were produced on the high quality data set. But let's assume the mistakes were not found and corrected during the validation step. So the model will be trained on a data set with mistakes: Download data set with mistakes The statistics that we receive on this data set are the following: Statistics on Data set with mistakes Here we can see the statistics received on the bad data set are lower, because the confidence of the model is lower due to inconsistent tagging (Automation Rate is lower) and the values extracted are not corrected due to incomplete and wrong tagging (Accuracy is lower). So the data set is cleaned: All the documents with labels WRONG TAG, MISSING TAG, INCOMPLETE and INCONSISTENT TAG are re tagged and documents marked as Bad OCR are excluded. Then we'll arrive at the results mentioned above. See the delta of statistics changing on documents correction: Download Statistics delta The best statistics are generated on the correct data to the maximum possible extended data set with post processing applied. "},{"version":"10.0","date":"Jul-23-2019","title":"sample-documents-labeling-and-validation","name":"4. Documents labeling and validation","fullPath":"iac/automl/da-manual/sample-documents-labeling-and-validation","content":" Goal: Get tagged data set of high quality. Input: Manual Task, batches of documents. Output: Reviewed data categorized by groups based on the type of inconsistency found. Labeling Once qualification is finished, it’s time to go ahead with document tagging. To start tagging, each SME should be provided with: a link to the task on WorkSpace a file with general information about tagging and WorkSpace tools specific features tagging logic of this particular task. Here is an example of an email a DA can send to the team of SMEs and Delivery Manager. > Hi (SME's_name), > > Manual task is ready and available for data tagging. > > The name of the task is “TaggingGeorgiabatch_1” with 64 documents in it. Here is the link to the task: https: wf app.workfusion.com workfusion login > > Attached you’ll find a file with instructions. Please carefully review the sheet “Specific instructions” with tagging logic description and, if necessary, look through “General instructions” to stay aligned with basic tagging rules. > > In case of any questions, feel free to ask. > > Best regards, When an SME has finished tagging, the DA should provide him or her with another batch. If a new batch includes documents of another layout, a new file with instructions should also be sent. Labeling Report By the end of the day, a DA should provide a report about tagging progress to all the interested parties, but mainly to the Delivery Manager, the SMEs’ manager and to SMEs. The following template can be used as a report: Tagging tracker You may create your own tagging tracker, but it should include the following information: Name of SME Number of tasks performed for each day Average speed per task Comments on most common mistakes Overall documents tagged Number of documents left Data Set Validation When at least one batch is finished, DA can start validation. For demo purposes, we used a method of validation via browser: Download snapshot from the completed Manual Task Upload it into the business process \" Save tagged content to S3.\" Output of this process will be the same CSV file but with links to the tagged data (HTML) instead of XML content. Insert an additional column to enable categorization. For more information about how to enable category drop downs, see labeling and validation. Create an Excel copy of CSV file to keep all formatting. Open each document in browser and make sure: All the required fields are tagged. Here you need to make sure the \"n a\" option is soundly chosen and the value is not missed due to inattentiveness, except for the cases where it's totally corrupted by OCR. If the value is presented in the document, it was tagged. Otherwise missed fields will be badly represented and such documents should be re tagged or the data set should be increased. Fields are tagged from the same location across the whole data set. If more than one variant of tagging of the same field is found in one template, see how often this happens. If two out of 50 documents in registered agent name was tagged in some other place, it’s better to exclude such documents. Pay attention to documents’ layouts as well. Note that due to OCR, the layouts of one vendor can be multiplied — for example, because of a broken table structure, etc. Such layouts as well as badly represented layouts (less than 10 for one vendor) should be increased, if possible. Complete value was tagged. Data value wasn't changed manually — e.g., the tagged string for entity _type is \"Limited Liability Company,\" the data value should also be \"Limited Liability Company, \" not \"LLC.\" There is no tagging of corrupted values which cannot be restored. Option “Append selected to” was used correctly. If you are not sure, just try to select a desired value. If no extra information (except for the required text chunk) gets into your selection, there are no appended parts. Depending on the type of mistake found (if any), assign one of the categories to each record: Good, Bad OCR or Re Tag (for more information about categories, click here ). In the end, your report should look like this. Download the Excel version of this file to get more details If we take a \"Georgia\" layout as an example, we'll see that out of 68 documents of the first batch, 23 have different types of inconsistencies and can be included neither in training nor in test set without correction, and four records within category \"Bad OCR.\" The same validation logic should be applied to the rest of the batches for all templates. So the number of revealed inconsistencies for the rest of the states is the following: New Hampshire, 36 documents (out of which five had bad OCR); Massachusetts, 29; California, 35 (8 with bad OCR); and the other two batches of Georgia, 11. Total number of records to be corrected: 121 (23 31 29 27 11). All the incorrect documents — in category \"Re Tag\" — collected from all the batches will be an input for separate Manual Task for re tagging. As the result, we end up with 593 absolutely correct documents. Additional information Final data set review "},{"version":"10.0","date":"Sep-20-2019","title":"sample-final-data-set-review","name":"Final data set review","fullPath":"iac/automl/da-manual/sample-final-data-set-review","content":" Goal: To preprocess gold data (normalize), if necessary, for model training and check tags distribution. Input: CSV file with validated and corrected documents. Output: Number of values per field, gold data without extra spaces new lines for model training. Before splitting documents into test and training sets and running model training, it's necessary to double check gold data. All the data values of the tagged documents must be final and, if necessary, pre processed. A Data Analyst has to make sure that values are: well represented normalized Tags distribution It's recommended to сheck the gold tags representation each time when the validation of a batch is fulfilled. Before splitting into batches, the final check is to be conducted to ensure the same level of representativeness in training and test sets. That can be done with the help of a bot step. Refer to tags distribution check for more information. In the result, one can notice that there is sufficient tag distribution, but only half of all the documents contain tags , and , but still this number will be enough to train the model. :::note If the number of tags is few (less than 100) the data analyst has to escalate this, and request more documents, containing rare fields, and explain risks to the customer if there are not any more documents. ::: Example of email: Gold data normalization If it is necessary, gold data should be normalized. Refer to gold data normalization for more information. Format: CSV file Structure: Has column that contains tagged XML HTML for normalization. Column name by default is tagged_text, it can be changed in BP on params step. Learn more about this bot here. If there is extra logic that should be applied — for example, data values converting, mapping, removing additional symbols, etc. — extra rules should be designed by the Data Analyst and Machine Learning Engineer. ⇪ Back to sample use case solution "},{"version":"10.0","date":"Jul-23-2019","title":"results-analysis","name":"Results analysis","fullPath":"iac/automl/da-manual/results-analysis","content":" Goal: To analyze the results of the ML model and propose ways to improve them. Input: Test set, trained model. Output: Report with statistics on test extraction, proposals on ML model improvements. Results analysis steps This stage of the Use Case implies proceeding with the following steps: Run ML model training. Run Information Extraction Classification on a test set. Get ML model results. Run Business Process to get detailed statistics. Analyze results: Find reasons for mistakes and provide solutions. 1 2. Run model training & run IE Classification on a test set After splitting the data set into training and test sets, the Data Analyst and ML Engineer launch model training, then runs IE Classification on a test set. Make sure that the input file provided for extraction includes the following information: column with links to the original documents column with links to the OCRed documents column with tagged text (as XML content). This is important because this information will be used in the next step: statistics calculation. 3. Get model results After Model Execution on the test set, as output we'll get a file containing all the information from the input file (from steps 1 2) and also a column with extracted text (as XML content). originaldocumentlink ocrdocumentlink tagged_text (gold data) taggedtexttagged (extracted values) 4. Run Business Process to get detailed statistics To get detailed information as well as accurate statistics per field, the following business process can be used: Statistics aggregation v2.0. This business process transforms results of model instruction into a file which is structured and prepared for analysis information, with detailed extraction results on each document per field and aggregated statistics per each field. The final result will look like this: Raw Data Deliverables Running statistics aggregation BP Input for BP: CSV file with results of Information Extraction. Output: Excel file with calculated statistics per field. To launch this BP, proceed with these steps: Upload IE results as Input data. Go to Params block in Design tab (double click). Fill in all the fields (or correct if you have copied somebody's BP). datastore_name: create datastore beforehand by yourself or it will be created automatically. gold: name of the column with manually tagged data. extracted: name of the column with extracted data. documents_count: number of records in the input file. S3bucket folder nameto_save: specify a bucket folder name of the file where results will be stored on S3. Run a Business Process. Go to the Results tab and download Statistics.xlsx. 5. Analyze results: find mistakes and provide solutions At this stage, the Data Analyst should thoroughly analyze the model results in order to understand possible reasons for each type of mistake. In case of Information Extraction, there are five possible types of results. Type of value Definition Gold Value Extracted TP (correct) Value should be extracted, and it is extracted by the model. WorkFusion10 000.00 WorkFusion10 000.00 TN (correct) Value should not be extracted and nothing has been extracted by the model. FP (mistake) No value should be extracted, but the model has extracted something. WorkFusion FN (mistake) Value should be extracted but the model has extracted nothing. WorkFusion FP FN (mistake) Value should be extracted, but the model has extracted this value with mistake. WorkFusionExxon Mobil10000.0001 01 2018 W0rkfusi0nExxon1000001st of January, 2018 Value should be extracted, but the model has extracted something different. WorkFusion10000.0001 01 2018 Company name:170008 10 2017 For more details about metrics and how to calculate them, go here. Types of mistakes, reasons, and solutions As TP and TN mean absolutely correct model answers, attention should be paid to FP, FP FN and FN. False positive (FP) FP mistakes are unacceptable at this stage. DA should prepare data set very attentively in accordance with defined tagging rules and instructions. Incorrect grouping for multi value fields Solution: Implement correct grouping in post processing. Missing values in the test set Solution: Correct gold data, and tag re tag all the existing values that were missed or wrongly tagged, or exclude such records from test set. Mistakes in the test set, totally incorrect values are tagged False positive, false negative (FP FN) Insufficient normalization (extra symbols, different data types) Solution: Normalize values in post processing. Inconsistent gold data Variations in the values boundaries (e.g., invoice _number field has gold value “xxxxx” and extracted value “xxxxx HAB” — both are correct from business point of view, but are not equal and cannot be compared to each other). Solution: Correct all inconsistencies in the gold data: pre process normalize gold values re tag incorrect documents or exclude them. If previous steps were applied, re train the model. If gold data wasn't corrected, try to normalize values in post processing. Cases of incomplete tagging in the training set Solution: same as above. OCR errors in extracted values Example Solution: Analyze whether there is any logic that allows correcting these mistakes without generating other mistakes on the whole data set and possible unseen data. If yes, implement corresponding post processing. If a rule can cover only part of cases without creating extra FPs, try to define the remaining part to remove these values so they will be handled manually. Specific or broken document structure OCR makes it impossible to tag the value completely. Solution: Check whether any logic can be applied to extract the value completely in post processing without creating additional FPs. If yes, such post processing should be applied. If post processing cannot help, additional components should be added to the model (AutoML SDK). False negative (FN) Not enough examples in the training set Small overall number of examples of some field(s) in the training set. Small number of examples of the field in specific document structures. Should be identified and communicated in advance. Solution: Increase the number of examples in the training set, and or retrain the model. Additional components can be added (AutoML SDK). Tagging inconsistency in the training set The field is tagged in different positions. Solution: Correct exclude inconsistencies for the field in the training set and retrain the model. This kind of mistake shows that data set wasn’t prepared properly, which is DA‘s responsibility. New document structure after OCR within known layout (a case of bad representation). In test set, a document of known layout, which due to different initial quality after OCR has completely different appearance from the majority of other documents of this layout, i.e., this \"new\" layout is badly represented in training test and consequently is badly recognized in test set. Solution: If the case is valid, raise in advance that it is badly represented If the case is invalid, estimate the impact of such documents and raise to Delivery Manager. It is important to have enough documents of some specific structures, where this field is given in a way that's not very simple for extraction. Complicated dependencies in extracting logic Dependencies that haven’t been found by the out of the box model. Solution: Additional components (AutoML SDK). Possible Reasons for Mistakes of Any Type Mistakes in gold data If the data set was collected thoroughly in compliance with defined tagging rules and instructions, this reason will be invalid. Long free text For example, it was decided to tag and, consequently, extract address as one element: ADDRESS OF PRINCIPAL OFFICE 120 ANGELS RD COLEBROOK, NH 03576 In such a case, the model considers an address line just as some free text without any specific shape and format. Moreover, there is high possibility that different parts of the text line will be situated in several XML elements. Solution: Check if this value can be split into some independent parts — for example, the whole address line can be very difficult to extract as address formats usually differ from country to country or even in different documents' templates. Model will be more confident about extracting city, street and zip separately. Be careful with value splitting. Values like company _name cannot be split, as their parts cannot be considered independent and make sense only with the whole name. Unusual broken document structure When required values surrounding were moved to another value. Solution: Try to define some rule common for all such records that makes it possible to extract these values correctly. If such a rule is found and is applicable for post processing, use it. One more option is to collect all such cases together and use a separate model which will be trained on such specific cases. This option should be communicated to the Delivery Manager. During the analysis of ML model results, the Data Analyst should always consider every mistake in terms of its frequency and impact on the business. Frequency Define whether a mistake is a single case or a regular one. Exception case: It can be easily removed from the data set without serious impact on statistics. Special attention should be paid to regular and frequent cases. Regular: frequent typical mistake, the correction of which can significantly improve the statistics (different spaces, commas, dots etc.) without affecting the value itself. Business impact Define how critical the mistake is, to define its priority for post processing extended ML implementation: Mistakes in critical fields that can affect business. For example, we need to extract the price of product items our customer will deliver to the requester. The price provided in the document is \" .5525\" which means 0.5525 US Dollars but the extracted price is 5525 which is 10,000 times bigger than the true value. Be very careful with such fields. Even if one symbol was replaced or damaged (e.g., during OCR) and therefore a field's value cannot be defined with 100% confidence, it's better to remove the extracted value completely to be handled manually. ⇪ Back to Data Analyst runbook "},{"version":"10.0","date":"Jul-23-2019","title":"sample-get-qualitative-ocr-result","name":"2. Get qualitative OCR result","fullPath":"iac/automl/da-manual/sample-get-qualitative-ocr-result","content":" Goal: Get a high quality data set ready for tagging. Understand layouts' representation (i.e., number of documents for each layout) in order to collect a good training set and representative test set in future. Input: CSV file with links to the original documents. All the documents are combined with no sorting by layouts. Output: OCRed data set in which at least 90% of documents have acceptable quality. Generate links to original documents Click here for detailed step by step instruction. Validate initial OCR quality Before sending all documents to OCR Business Process (609 in this example), it’s necessary to make sure that current OCR settings are sufficient to ensure 90% quality of the data set after OCR. To assess the quality of OCR, we prepare a sample set: randomly choose 20% of the data set (in this example, 122 documents) and OCR them. Analyze results When analysis of each record is completed, it’s necessary to calculate the percentage of documents with acceptable quality. In our case, out of 122 sample documents, only 10 have unacceptable quality after OCR and the remaining 112 (91.8% of the sample set) have no OCR problems, so overall quality of OCRed documents is even higher than the required minimum of 90%. Refer to OCR results analysis section. Although it's necessary to remove the 10 unacceptable documents with OCR issues from the data set, we can run the OCR business process for the whole data set without any additional OCR tuning. When removing documents with unacceptable quality, pay attention to what type of template they belong to. If unacceptable quality items are mostly documents of one template, their exclusion may affect the representation of this template in the data set and additional documents should be requested. In this situation, the Delivery Manager should be notified. "},{"version":"10.0","date":"Jul-23-2019","title":"sample-prepare-team-for-labeling","name":"3. Prepare team for labeling","fullPath":"iac/automl/da-manual/sample-prepare-team-for-labeling","content":" Goal: To prepare the team of SMEs to label documents. Input: 609 XML documents split into batches: New Hampshire, Georgia, Massachusetts, California. 15 samples of Georgia selected. Output: Manual Task, SMEs Qualified to label, Qualification Report. Design manual task The first task is to consider the list of fields and documents and to design a Manual Task (MT) with all the logic included. To do this, it's necessary to study the list of fields one more time, decide on the Answer Type for each field, assign a hot key to each field and design a manual task. List of fields Field name Data type Required Multivalue Answer Type Hot key 1 Secretary of State Alphabetic Yes No Free text s 2 Trademark Name Character Yes No Free text t 3 Corporate Number Alphanumeric Yes No ID i 4 Entity Type Alphabetic Yes No Free text e 5 Business Type Character No No Free text b 6 Filling Date Date Yes No Date (dd mm yyyy) d 7 Effective Date Date No No Date (dd mm yyyy) c 8 Authorized Person Character Yes No Free text p 9 Authorized Title Alphabetic Yes No Free text a 10 Agent Name Character Yes No Free text n 11 Street Address Alphanumeric Yes No Invoice address 1 12 City Alphabetic Yes No Invoice address 2 13 State Alphabetic Yes No Free text 3 14 Zip Code Numeric Yes No Zip code 4 15 Filling Fee Numeric No No Number f Here is the Manual Task designed for the business process. There is one extra field (\"Comment\") designed so SMEs can provide comments on documents' quality and missing tags. Download Manual Task Importing a package To see this task, it's necessary to download this package and import it to the Control Tower. Go to Business Processes (Tasks) and click Import Package in the top right corner. Upload the Package .zip archive and click Preview Package. Click Import Package in the bottom left corner. Click View the imported Business Process. Qualify SMEs for proper labeling For qualification, 15 samples of one layout should be chosen. In this example, Georgia was chosen because it doesn't contain complicated business rules for tagging, so it would be easy to follow instructions while tagging. For the input, the following data is provided. Input file for Qualification Task Then, send an email to the team of SMEs with instructions, link to Qualification Task, and a specific deadline for the task. Instructions for Qualification Task Qualification results Here is what the DA will see in the Results tab when at least one SME passes the qualification task: Here is what the SME can see in WorkSpace when the qualification is obtained: The Report on qualification stage is submitted. Download Qualification Report Second Qualification Let's assume one SME failed to pass Qualification. The granted score was 60 (75.7%), which is not enough. The DA must check the mistakes, send feedback to the SME and ask them to retake the Qualification for the second time. Go to Results > Analytics > Export to CSV. Here, the CSV file with the workers' answers will be downloaded. You can sort them by Worker ID. Download .csv file with SMEs answers To see Worker ID, go to Workers > find the worker by name > see the ID. Filter the CSV by Worker ID and leave only answers of the second SME. Then upload CSV to the manual task and check tagging. See here how to do enable it. The email with mistakes and explanation should be sent to the SME and they must proceed with the qualification task for the second time. Download feedback with mistakes explanation If the SME fails for the second time, a message like the following email should be sent to the Delivery Manager. Additional information Split into batches "},{"version":"10.0","date":"Jul-23-2019","title":"sample-split-into-batches","name":"Split documents into batches","fullPath":"iac/automl/da-manual/sample-split-into-batches","content":" Goal: Divide documents into groups by layouts to facilitate labeling. Input: Snapshot with 609 OCRed documents. Output: Batches ready for tagging split on the basis of \"secretary of state\" office in which they were filed. In order to decrease the number of tagging mistakes and increase speed, all documents should be split into batches by structure. To assign a document a particular layout name it's recommended to look the documents and try to define all unique names that will be used as unambiguous identifiers of a particular layout: for example, if we use issuer _name as an indicator for classification, it'll be necessary to define the full list of issuers in the data set Tampa Holdings, Workfusion Inc. etc. The list of layouts will be used then in classification task that will make the process of documents' sorting more convenient. The task also has one useful option if during classification you encounter a new layout (e.g. new issuer), which is not in your list, you'll be able to add it manually right in the task. In our example, we'll use \"secretary of state\" as structure indicator. During documents analysis we identified 4 layouts: New Hampshire, Massachusetts, Georgia and California. As the result of classification task we have the following layouts' representation: Georgia 211 New Hampshire 144 Massachusetts 138 and California 118. ⇪ Back to sample use case solution "},{"version":"10.0","date":"Jul-23-2019","title":"sample-understand-use-case","name":"1. Understand use case and input documents","fullPath":"iac/automl/da-manual/sample-understand-use-case","content":" Report This is a POC \"Report\" — It's necessary to study and process documents of different templates and extract some fields that contain key information which will be entered into inner systems. The timeline of the project is 2 months. Number of documents Number of templates Names of templates Number of fields Success criteria 619 4 New HampshireMassachusettsGeorgiaCalifornia 15 Accuracy: 90%Automation rate: 60% Business logic All entities conducting business are required to file annual reports to the Secretary of State. Reports must be signed, dated and accompanied by a filing fee. They confirm the principal and registered agent office. There are also names and addresses of the entity's officers, directors, members, managers; and necessary changes. Some annual reports are filled in online, and part is printed and submitted by mail. Such reports arrive as PDF attachments and have to be proceeded manually. All the necessary data from each report is defined and entered manually into a data store. The monthly distribution is not equal; there are approximately 500–600 reports submitted monthly from January to June, 0–100 documents monthly from July to September, and 200–300 documents from September to December. For this use case, only four states are chosen: California, Massachusetts, Georgia and New Hampshire. Team There are two SMEs (part time availability) which perform this task. On the project, they will be available four hours daily. List of fields Field name Values Data type Meaning Notes Required Multivalue 1 Secretary of State California, Massachusetts, Georgia, New Hampshire Alphabetic Secretary of State where the report is submitted Yes No 2 Trademark Name Any Character Company name, which submits the report Yes No 3 Corporate Number Usually there are 9 digits, but there can be letters as well Alphanumeric Company ID Yes No 4 Entity Type Corporation, Domestic Profit Corporation Alphabetic Type of ownership Yes No 5 Business Type Any Character Type of company's activity No No 6 Filing Date Any Date Date when the report was signed Yes No 7 Effective Date Any Date Date when the changes become valid No No 8 Authorized Person Any Character Person that signed the report Yes No 9 Authorized Title Authorized Person, Incorporator, CEO, Secretary, President Alphabetic Title of the authorized person Yes No 10 Agent Name Any Character Name of the company or person founder Yes No 11 Street Address Any Alphanumeric Street address of the principal office Yes No 12 City Any Alphabetic City of the principal office Yes No 13 State GA, CA, MA, NH Alphabetic State of the principal office Yes No 14 Zip Code Any Numeric Zip code of the principal office Yes No 15 Filing Fee It can take any value, but usually fee equals to 100 or 25 Numeric Filing fee, which is charged when reports are sent after the due date No No Documents To see the original documents download and unpack this archive. For better understanding of the workflow, here is the list of action items that need to be done on the use case at this point which are not shown in this demo. From the customer, request access to the folder with documents, working instance and S3. Look through the documents and study the business logic of the documents and use case. Organize the kickoff meeting with the team of SMEs, discuss business logic of the documents, and explain next steps. Write an email to the team with the results of document analysis."},{"version":"10.0","date":"Jul-25-2019","title":"sme-onsite-training","name":"SMEs onsite training: best practices and materials","fullPath":"iac/automl/da-manual/sme-onsite-training","content":" The best practices described are validated on onsite SMEs training with the timeline 1 week, group of 20 Subject Matter Experts and 2 Data Analysts. The Use Case was IE from 10 different document types with complicated cross document dependencies and integration with third party systems. 1. Investigate use case Study business logic of the documents, find out corner cases, prepare a list of questions to SMEs to discuss on calls or on onsite meeting (see the example below). 2. Prepare for training Prepare the training plan. See the example below. Make sure that the following topics are covered during the training: What is WorkFusion product, what is the benefit and impact of its usage for SMEs in their daily work. High level workflow and solution of the business process being implemented. Data Analyst role and SME role on the project. WorkSpace application and its tools. Tagging rules. Bad OCR. Business logic of the documents, document workflow and corner cases. Prepare manual (qualification) tasks for all document types, review and prepare input data. Plan and negotiate with the customer the number of SMEs that will take part in the training. 3. Work with the group It's essential to set up work in the group in a proper way to distribute roles and responsibilities, create communication channels and encourage team members to participate in the discussions and raise questions and issues that can influence business logic or solution design. Get acquainted with the group, try to distinguish the leaders who can help you teach others and establish contact with them. Create and announce daily plan to the team, set up the priority of the tasks. Set up scheduled Q&A sessions, provide feedback on mistakes. Provide the team with the tips and tricks at hand in the form of a printout. Avoid risks of team disconnection, change document types or activity when people are bored, encourage them interact and teach each other, raise and highlight the connection of their task with their daily work. Build communication channels, sources of truth and information and establish the order of team interaction. Create instructions based on business logic discussed and typical mistakes and provide the team with it. Create a tracker to follow and report the tagging speed and the number of task SMEs submitted. 4. Use the following materials Use this archive for learning materials. Learning Materials 5. Tips and best practices Remember that dress code and certain security requirements can be required on prem. Keep in mind and transmit indirectly the message that you regard SMEs as experts in their sphere. Make sure everything discussed and decided is taken down. Walk around and ask for questions and propose assistance while tagging exercises. Make sure the knowledge on business logic received is transmitted to the rest team working on project. Ask SMEs to write down all the suggestions feedback they have during the day and collect by the end of the day. Before asking any questions to the customer, or committing to a certain solution make sure you're aligned with your own team in this approach. Make sure all the changes decided are applied timely. Stick to the plan but keep in mind what and how can be rearranged due to internal or external reasons. Distribute responsibilities between Data Analysts If there is a Data Analyst team working onsite. "},{"version":"10.0","date":"Sep-20-2019","title":"split-into-batches","name":"Split into batches","fullPath":"iac/automl/da-manual/split-into-batches","content":" Goal of the step: Split documents into batches of every layout before starting any manual handling (tagging, reviewing, etc.). Input: OCRed data set without any sorting or grouping. Output of the step: Data set divided into separate batches by layout. Increased speed of tagging re tagging validation of the documents. Increased quality. To make the flow of any documents' processing quicker and the output higher quality, documents' sorting should be applied. This allows tagger DA (depending on the process) to concentrate on one layout at a time and not to jump from one to another. This method ensures increased speed (both of tagging and validation) and reduced mistakes. The process of splitting into batches consists of the following steps: Define key words phrases. Create data store with key words. Sort documents by key words. Prepare batches. :::tip A group of documents of similar appearance is called a template. After OCR, we speak about layout. See more here. ::: Step 1 The most obvious and useful way of data set splitting is to use key words. To define key words, look through 10 documents of the data set and try to define words or phrases that can be used as unique and unambiguous identifiers of a particular layout. For example, we have two documents' layouts combined: New Hampshire and Georgia. It's necessary to analyze each carefully and try to define words phrases that help differ one layout from the other. New Hampshire Possible key words, phrases: State of New Hampshire Citizenship NAICS CODE Georgia Possible key words phrases: STATE OF GEORGIA Corporations Division Martin Luther King (street address of the state authority, permanent) :::important Remember that key words should always be unambiguous, specific and don't identify more than one layout. For instance, words such as \"title\" or \"officer\" should not be used as key words, as they are too general and appear frequently. Also, it's not recommended to use too specific items as street address (1000 Market Street) or names (ELEANOR JORDAN MITCHEL), as they will cover only several documents (or perhaps only one). Using only the name of the state, e.g., \"Georgia,\" would probably not be very reliable, as there is an Address block in each document where the state may be mentioned. ::: Step 2 If the number of documents is not very big, sorting can be done manually, or an ML Engineer can assist in some automation of this process by writing some code or a bot with the following logic, as in the example below. Solution for automation Input: CSV file after OCR containing \"originaldocumentlink\" column (links to original documents) and “ocrdocumentlink” column (contains the links to OCR results). Link to data store with: \"layout_name\" column, containing names of layouts, corresponding to value from “key words” column, e.g.: New Hampshire Georgia “key words” column, where every cell contains the list of key words, e.g. for Georgia layout: \"Martin Luther King\" \"State of Georgia\" \"Corporations Division\" etc. Output: CSV file with 3 columns: \"originaldocumentlink\", “ocrdocumentlink” and \"layout_name\". Description: This bot tries to find “key words” in the OCR result document. In case of success (i.e., OCR has recognized one of the key words correctly and the bot has found it), it assigns the corresponding name of layout from “layoutname” column to record from “ocrdocument_link” column. As a result, you will get a single .CSV file containing columns named \"originaldocumentlink\", “ocrdocumentlink” and “layout_name” correspondingly, as in the example below: Business process ends when all the key words were processed, but no match found. For these records, in the column \"layout_name\" appears \"No match\" message. The main reason for this can be OCR mistakes. For example: Instead of State of New Hampshire, the OCRed document contains State of New Hamp h1re, where s was replaced with , and i with 1. In this case, two options are possible: If many documents are missed by the bot, key words should be changed and should not include those mostly damaged by OCR parts, e.g.: only \"State of New\". If there are a few documents left, they can be checked manually by opening links. Step 3 When sorting is finished, it's possible to form batches for tagging. The recommended size of a batch is 50 documents. If some layouts are represented by fewer than 50 documents, they can be gathered into one common batch. In such a case, it's advisable to enable the WorkSpace Preview function, so SMEs can consistently tag at first one type of layout, then the other. Enabling WorkSpace preview Go to Run Task > Advanced Options > WorkSpace Preview. Select a Field Scheme from a drop down or create a new one by clicking the New Scheme button. Provide name of field scheme. The Field Scheme defines the grid column names (with format) shown to Workers in WorkSpace. In Unique Code, provide name of the column from your .CSV file with documents for tagging. This column will be later shown in WorkSpace and can be used as filter by SMEs. For example, your input data can be classified by country and you may want to assign a particular country to one SME. In this case, you should provide two answers in \"Search options scheme\": Original Document Link and Country. Go to Manual Task > WorkSpace Preview, and find your scheme, map the columns and click Save. Save the task, and then Run it. Check the sandbox. You will be able to view columns and filter your tasks by \"Search\" option. ⇪ Back to Data Analyst runbook "},{"version":"10.0","date":"Jul-23-2019","title":"sample-split-training-and-test-sets","name":"5. Split training set and test set","fullPath":"iac/automl/da-manual/sample-split-training-and-test-sets","content":" Goal: Split verified documents into two parts for model training and testing. Input: CSV file of the each layout with validated and corrected documents. Output: Test and training sets for each batch. Split Once all the batches of one layout are validated, it's necessary to combine them in one full file for easier splitting into test and training sets. Here is the example of Georgia splitting, the rest of the layouts are processed by analogy. Input: All the records for one template, validated and corrected. Download Georgia full Output: Two files for test and training sets. Combine validated snapshots from all the batches of the Georgia layout to create one file instead of three. (Note that in the example, documents that were labeled as \"Bad OCR\" are already excluded at the previous step). All the documents that are labelled as \"Re Tag\" MUST be retagged before including them into test or training set. Take 20% of documents for test set and 80% for training. Store the CSV files separately in the corresponding folders: Note that Georgia layout is represented by two types of documents, which also should be included proportionally into training and test sets. Start creating a data store for the training and test set. Upload corresponding CSV files from each batch when they are ready, so as to form the entire test and training set. All the other batches are processed in the same way, see the overall result of splitting. Training set: Download training set Test set: Download test set "},{"version":"10.0","date":"Sep-20-2019","title":"train-smes","name":"Train SMEs","fullPath":"iac/automl/da-manual/train-smes","content":" Goal of the step: Train SMEs so as to provide a high level of tagging accuracy for a high quality data set collection. Output of the step: SMEs obtain qualification and learn how to tag documents correctly. Qualification is mainly to prove that SMEs correctly understand the requirements and can follow instructions well. Qualification task is used to measure the percentage of correct answers that SMEs submit. Subject Matter Expert (SMEs) are experts from the customer’s side who have a deep understanding of the whole process and business logic of the documents. Usually they perform all the tasks of the process manually before automation is applied and have the full knowledge of documents' business logic. First, the Data Analyst provides introduction into the tagging process for SMEs, then the Qualification task has to be undertaken. Qualification is mainly to prove that SMEs can provide a high quality data set. The qualification task is used to measure the percentage of correct answers that SMEs submit. There is no need to check SMEs' knowledge of different document types, but it's necessary to verify they can follow instructions well and tag exactly what is needed. The Qualification task can be taken twice by SMEs, so there will be a first and second qualification. When SMEs try to tag documents according to the instructions for the first time, that will be initial qualification and if they are successful, it will be the only one. Success means all the SMEs submitted all the tasks in the qualification task, achieved the required percentage of correct answers, and were able to obtain qualification the first time. If some SMEs failed to obtain qualification during initial qualification, they can retake the qualification task, but only once. The second qualification is final and if the SME fails, the situation should be reported to the DM and the person should be excluded from further tagging process. The aim of the whole process is to confirm that all the SMEs tag the same documents in the same way, accurately, according to the instructions and can fulfill all the tagging requirements. For reference: Qualification — Degree that shows SME's capability and skills to tag documents well. Gold Data — Correct answers, prepared by DA to measure SMEs' accuracy. Qualification Task — Task that SMEs need to pass to obtain qualification. Gold data First, take 15 documents of the same type and layout. That can be 15 invoices from one supplier, for example. Define the correct values for all fields. Generate a CSV file with the links to the original and OCR'ed documents and upload it into the manual task. Note that gold data must be 100% correct from the business logic side. Creating gold data There are two options to tag gold data: Upload Data in Qualification Task, and then Mark as Gold on Preview. Tag all the values as if on the WorkSpace and select Mark as Gold for each document. Save the task. Tag documents regularly in a manual task. Select Upload Data > Run > Run This Task. Find these tasks in Workspace and complete submit them. Generate Snapshot > download .CSV file > add to all the columns' headers \"gold_\" prefix > save and download the CSV file into the Qualification task as Gold Data. :::important Note, that .CSV files should be opened and edited in special editors, such as Ron's Editor or LibreOffice. Excel editor breaks the structure of the snapshot. ::: Qualification Qualification is created in Control Tower separately from the Qualification Task. The main thing here is to set the score threshold sufficient to achieve success criteria. For example, if you have 95% accuracy required, a 70% score threshold won't be enough for qualification. It's recommended to set qualification threshold at 90% or higher. Creating a qualification To create a qualification, follow these steps: Go to Workers > Qualifications. Click Create Qualification. In Qualification Details, provide Qualification Name, Description (not mandatory), and Keywords. In Qualification Settings, select the following parameters: Privacy: Public (private Qualifications won't be visible to SMEs in Workspace) Type: Accuracy based Score Threshold: 90% or higher Click Save. Qualification is used to limit access to the task as well. For example: Two teams of SMEs are in different locations and they are tagging documents on the same WorkSpace. You want each team to tag specific batches only and not to see other tasks, so it's necessary to create and assign different Qualifications to SMEs from each team. Creating a manual task with limited access To create a manual task with limited access, proceed with these steps: Create a Qualification. Go to Workers, select your SMEs, and then select Assign Qualification. Select Qualification from drop down and Save. In the Manual Task, go to the Run tab > Advanced Options. On the Qualification tab, select the appropriate Qualification, and then select Require All Qualifications to Preview the Task. Click Apply, and then Save. Qualification task The documents can remain the same as in the first qualification. Precise and sufficient instructions are to be provided to SMEs together with the Qualification task. For a second Qualification task, the Data Analyst should provide feedback on mistakes, check the instructions and improve them if necessary. Creating a qualification task To create a qualification task, follow these steps: Copy the manual task created at the first step, and then enable it as a qualification. Upload Gold Data or Mark as Gold on the Preview. See create gold data step. See the example of input data. Go to the Run tab, and then select Advanced Options. In Advanced Options, go to Qualification > click Add. Find the name of the qualification you provided, and then set Value the same as Score Threshold. Select Qualification Task, and then set Number of Qualification Tasks to 15. Click Apply. Click Save, and then click Run. :::note If you Enable Training Mode, SMEs will see their mistakes if any at once (that mode might be used during initial qualification, but is not necessary). ::: SMEs training First, provide basic information about WorkSpace, i.e.: how to log into WorkSpace how to use Preview (to look at the original document) how to tag quickly using hot keys how to select a value from the list how to correct data values, etc. Also provide some common rules and principles of tagging, such as: Tag the fields consistently, i.e. in the same places as it is given in the specific instructions. Tag all available fields, don't miss anything, except for values corrupted by OCR. Don't tag fields corrupted by OCR (examples are provided below). Always tag ALL the products when there multi product lines. Never tag the amount together with a currency sign. Tag fields completely, i.e. don't cut off the words from values, especially for company names, addresses, etc. Be careful with values in the answer boxes: Select appropriate values from the list, if there is a dropdown, or correct the value if needed. Don't submit mistaken answers. Use reference data correctly. No \"N A\" if the value must be chosen from the drop down list. Before submitting: Check the position and data value of every tag; is everything correct More information about tagging requirements and high quality data set is here. The presentation example on SMEs' training can be downloaded here. ⇪ Back to Data Analyst runbook "},{"version":"10.0","date":"Sep-20-2019","title":"split-training-test-sets","name":"Split training and test sets","fullPath":"iac/automl/da-manual/split-training-test-sets","content":" Goal: Provide appropriate documents for model training and estimation. Input: Validated tagged documents categorized by groups. Output: Test set, training set. Input for the stage As a result of a finished Data Set Check stage, there should be an Excel file which contains documents with their categories. Mistakes categorization Good Absolutely correct and consistently tagged documents, in which no values are missed and everything tagged correctly; OCR is good. For test only Corner cases (Bad OCR, Rare Template) documents, which are not eligible for training, but which exist in real life documents' flow and can be processed (at least partially). These are documents for which data values for ALL THE FIELDS presented in the original document (except handwritten) are correct, but: they belong to rare template, there are less than 10 such documents and they significantly differ from the others (so the Model will not be able to be trained well on such a small number of examples). or after OCR there are some issues: values with OCR mistakes, broken structure (so field can be tagged only with \"Append selected to\" option), context of fields has BAD OCR, etc. For training only Positions of presented tags are correct and tags are complete, but some tags are missed (usually due to OCR issue). Also documents with wrong data values or tag missed due to tagging mistake can be used for training, but only if there is no time or capacity for re tagging. Normally it is better to re tag such documents. Re tag Document contains some issues with both data values and tags, but it still can become \"Good\", \"For training\" or \"For test\" after re tagging. Exclude None of the above mentioned. Document has some considerable issue, so it is inappropriate neither for training or test sets. Usually it is documents with really bad original quality or with considerable OCR issue. For example: There is some serious OCR problems for all fields, nothing is recognized correctly. Totally corrupted structure of document. There are only handwritten fields in the document. Training set Training Set is a set of documents used for training the ML model. The training set composes 80% of the whole data set. Training set contains gold values for model training. For Information Extraction models, gold values is a set of fields that should be extracted; for Classification, documents are accompanied by target class. Training set should consist only of Good and For Training documents. The ratio is 80% to 20% correspondingly. :::note Why are missing tags and wrong data values are acceptable for training The model trains each field separately, one by one or in parallel depending on resources available — for example, all the documents are trained on tags , then , etc. It is acceptable to include into training set documents with missing tags because if the document doesn't contain some tagged field, it won't participate in training of this specific field. Each document participates in the training only for the fields it contains. So missing tags are acceptable in training set, but not recommended, because it decreases number of tags for model training, so if tags are missed due to inattentiveness and not retagged, that will affect tags' distribution. Training set should include typical cases Training sets should contain high quality documents that represent typical cases. That means there should be at least 10 similar tagging cases for each field with one specific context and structure to let the ML Model catch the dependency correctly. Ideally, there are similar documents across one layout, so all the fields have the same context and structure. Thus 10 documents means 10 similar tagging cases for all the fields. But sometimes, some fields or context have different structures due to OCR even though they are tagged correctly. If there are some rare cases (i.e., there are temp counting tag in TS. Test set Test set is a set of unseen documents used for evaluating an ML model. Test set composes 20% of the whole data set not used for model training. The aim will be to run extraction on documents that haven't participated in training, The second aim of the test set is to check the model performance for possible exception cases in production. Test set should consist of Good (20%) and For Training (same quantity as in production flow) documents. :::note Documents with OCR issues must be obligatory marked (keep marks in the \"Category\" column and if possible in \"Comments\" in the input file for test extraction), because you should be able to identify them in the final report on statistics and explain the effect of such documents in final results. * If it's not possible to count the number of bad documents in the production flow, the results of OCR testing can be used. ::: Gold values normalization If it necessary, gold data should be normalized. The following business process can be used to remove extra spaces, tabs and new lines. Bot: Gold data normalization Download Bot This business process normalizes gold data values: removes extra spaces removes tabs and newlines. Input: CSV file that has column containing tagged XML HTML for normalization. Column name by default is tagged _text, but it can be changed in BP on params step. Output: CSV file with normalized gold values. Learn more about this here. How to split Use Excel file from data set check Figure out required quantity of \"Good\" category documents for both training and test set based on 80% 20% splitting. Use pivot table for more convenience. Copy and paste to separate spreadsheets required number of documents for training and test sets, respectively. Use filtering for more convenient navigation. Save spreadsheets for test and for training sets separately in CSV format. Download the example Risks at the stage Bad model training results. Unequally distributed layouts in test and training sets or wrong categories of documents in the test or training sets will result in that model training on wrong data or not trained for some fields layouts at all. Unreliable gold data and statistics. The data values from the test set are called \"gold values\" and they are going to be compared with the results of the ML model extraction. It means that in order to get real and reliable estimation of an ML model performance for each field in every document of the test set, we must have 100% correct data values and all the fields represented. If anything is missed or incorrect, it will cause incorrect evaluation of the model. In other words, in a test set there should be exact values, which we expect the ML model to extract."},{"version":"10.0","date":"Jul-23-2019","title":"test-report-improvements","name":"Test report and improvements","fullPath":"iac/automl/da-manual/test-report-improvements","content":" Goal: Analyze results of ML training and propose ways to improve them. Input: Test set, trained model. Output: ML results, post processing rules, report with statistics. At this stage, the Data Analyst needs to estimate model results and, if necessary, propose rules to improve them. The model is evaluated on the test set for each training iteration, if there are several of them, and DA calculates the statistics of each iteration based on evaluation results. DA needs to calculate the statistics for each iteration, then analyze delta for tagging iterations and model mistakes. Data Analyst should identify reasons of model mistakes (consider all cases of FP, FP FN results). Then propose options to improve ML results. Improvement options include: rules for post processing and mapping rules defined by Data Analyst extra feature extractors developed by Machine Learning Engineer. Data Analyst analyzes the statistics of each training iteration and statistics with post processing. Then counts delta for ML results changing after training iterations and post processing to define if the model performance improved or not. When the defined rules are implemented by the ML Engineer, the Data Analyst calculates final statistics. In the end, Data Analyst's task is to provide the final statistics of model performance to the customer. All this data is compiled and submitted to the customer in the form of a report. So, there are two main steps to this stage: analysis of results and report submission. Results analysis ML model improvements Report AutoML dashboard "},{"version":"10.0","date":"Aug-06-2019","title":"glossary","name":"Glossary","fullPath":"iac/rpa/glossary","content":" Name Description Bot Unit RPA entity that runs a particular bot task. Contains the following Java applications: Bot Agent, RPA Worker, Bot Relay, and Bot RPA Server Windows Linux server containing several Bot Units (RDP in RDP approach) Bot Master Bot Master User that initiates a master RDP session on RPA Server Master Bot Agent Master Bot Agent application that triggers Nginx and data collection services, as well as starts Bot Agent in RDP in RDP approach Bot Agent Bot Agent application that starts RPA Worker, Bot Relay, and Bot Bot Relay Bot Relay application used to transport commands from RPA Worker to driver Bot Bot application used to transport commands from RPA Worker to driver together with Bot Relay application RPA Worker Worker application used to connect to assigned RPA Queue, retrieve tasks, and send results to Result Queue Bot Fleet RPA Queue or group of bots used to distribute bot tasks according to their business value or machines with specified software "},{"version":"10.0","date":"Aug-09-2019","title":"overview","name":"Robotics (RPA)","fullPath":"iac/rpa/overview","content":" title: Robotics (RPA) This section describes \"robots\" — WorkFusion Bot configs that execute actions of a human Worker interacting with a user interface of a computer system. \"Robots\" have the following capabilities: clicking through a browser web page or a desktop application on a remote server; reading and writing content from to user interface fields; iterating through a set of predefined steps (looping) and executing steps upon some conditions (if then else logic); executing scripts, opening applications and browser windows. The section contains the following materials for you to dig into RPA: Automation technology classification Glossary Developer guide Administrator guide :::tip To start developing RPA scripts, you need to: install WorkFusion Studio set up RPA environment and make necessary Windows, Java, and IE tuning learn how to create Bot Tasks and use Web Harvest plugins learn Robotics API guide and simplified Robotics API cheatsheet learn how to use desktop automation tools ::: "},{"version":"10.0","date":"Aug-06-2019","title":"automation-technology-classification","name":"Automation technology classification","fullPath":"iac/rpa/automation-technology-classification","content":" The following levels of robotic labor classification are measured by complexity and stability of the underlying technology. Stability of Technology: Level 1 is the most stable and Level 5 is the least stable Execution Speed: Level 1 is the fastest way to execute automation and Level 5 is the slowest Resource Utilization: Level 1 is the most efficient in utilizing computing power, Level 5 causes highest hardware overhead The above complexity stability resource utilization comparison iы applicable for RPA automation levels only (level 1 through 5). Level 1: API Automation (APIs, File Exchange, Database, ETL) Examples Collect information from Social Network API(s), web services, SOA architectures or legacy APIs REST, SOAP, SNMP Automate file exchanges FTP, SFTP, S3, SMB Connect to a database and build a report JDBC, ODBC Extract Transform Load and document conversion Read, write, convert, process PDF, XML, DOC, DOCX, XLS, XLSX, HTML, TXT, images, etc. Sending and retrieving emails programmatically Microsoft Exchange, SMTP, IMAP, POP3 Place retrieve a message to from queue AMQP, STOMP, JMS, ActiveMQ, RabbitMQ, etc. Traditional solution ESB tools, ETL tools, protocol specific libraries, custom script code WorkFusion Pre built connectors to most all of the common use interfaces and APIs Out of the box plugins to handle file operations and transition, e.g., file, s3, zip, http, soap, rest, excel, json, excel, etc. Pros It's native \"robot\" language, which means the most stable, robust and effective integration and function No need for separate hardware resources to run robot agents with 3rd party software installed Operating System agnostic (can run on linux, windows, etc.) APIs have strictly defined (and often standardized) protocol It also means APIs do not tend to change. And in case of change preserve backward compatibility for extended amount of time Most of the protocols have connectors to higher level for humans (e.g., .xls file can be opened with Excel) Cons IT would be involved to integrate systems on this level, or develop interface for humans Level 2: Web Scraping and Crawling Examples Fetch public data from static websites HTTP, HTML Web search crawlers and indexers, full text search Solr, HTTP Traditional solution Transport on HTTP, Web scraping crawling tools WorkFusion Pre built set of plugins to handle http data exchange and results, e.g. http, http extended, XPath, css selectors, etc. WorkFusion crawling infrastructure Pros Refer to pros from Level 1 Cons Sites tend to change. Automation scripts require re writing (not applicable for crawling) Level 3: Web Automation (dynamic web applications sites) Examples Fetch data from dynamic websites HTTP, HTML, AJAX, Javascript Take and compare screenshots to monitor websites Traditional solution Browser (headless or actual) WorkFusion Headless browsing PhantomJS Headless Chrome Headless Firefox Real browser ran off desktop Chrome Firefox Pros Hardware utilization (can run multiple instances of automation in parallel on a single machine session) Cons Prone to breaks due to increased number of variables Requires virtual desktop * * no hardware is required for headless browsing Level 4: Desktop Automation (legacy applications and Internet Explorer applications) Examples Windows applications, .Net applications Web applications that only work with internet Explorer Legacy applications Citrix application * Java Applets Terminal command line application Mainframe Microsoft Office applications * When Citrix allows access to clipboard and windows properties Traditional solution Give up Instrumentation APIs Java Access Bridge, Win32, Microsoft Active Accessibility, Microsoft UI Automation, etc. RPA software solution AutoIt, AA, BP, UIPath, etc. WorkFusion Desktop driver Robotics Plugins RPA Recorder Pros Mimics human interaction No extra IT required Cons Requires Windows environment for bots Single \"robot\" per user session at a time Level 5: Surface Automation (screen only tools, image recognition) Examples Websites and or systems which are trying to block prevent bots from access Legacy applications that cannot be inspected Citrix application * Exotic Operating Systems Flash application * Remote access to uncontrolled environment Traditional solution Give up Image based robotics Screen scraping WorkFusion Surface RPA Citrix Automation with WorkFusion SPA Desktop driver with Optical Character Recognition (OCR) capabilities Pros Mimics human interaction No extra IT required End to end automation for legacy uncontrolled environments Flexibility (same approach is applicable to almost any sort of applications) Cons More complex for headless unattended execution dependency on screen size, resolution, fonts Level 6: Cognitive Automation (unstructured data) Examples Unstructured documents PDFs, Doc, Email, Text, HTML, etc. Computer vision (OCR, image recognition, etc.) Scans, photos Captcha, re captcha Traditional solution Give up IT project(s) WorkFusion AutoML Data Extraction Classification Ranking and Scoring Computer Vision OCR Pros Automated Continuous model re training Built in quality controls Level 7: Conversational Automation Examples Conversation understanding Chat and Voice Fulfilling back office requests Question Answer systems Traditional solution Process manually WorkFusion Chatbots Pros Direct integration with RPA and Cognitive Automation Levels (1 6) Chatbots collect data, enter information, process transactions, and deliver results directly in the chat window, via text, or via email Chat UI integrations, including: Website Mobile 3rd party (slack, on premise messaging systems, etc.) Appendix What else Level H: Physical Examples Manufacture lines, etc. Press physical buttons on hardware token, etc. Paper document as an input Canon Epson. WorkFusion Human in the loop Level Z: Alien intelligence Some other examples Golden Robot spans along levels 2 6 (with human in the loop) Humans operate on levels 6, 7 and H BPM to connect levels. Dynamic workload balancing (server based orchestration) With dynamic workload balancing, the RPA platform can distribute the workload among the robots in real time. This improves productivity, as work is optimally distributed among available robots. This leads to more autonomous functioning of the automations, requiring less manual support to manage them. Enterprise level architecture Disaster Recovery, High Availability, Scaling, Monitoring, Docker, Security ( SSO, PAM), BI (Tableau) Packaging (artifactory continous delivery integration configuration management release management unit testing of BPs ) Cloud based or on premise or hybrid deployment Language agnostic ML Non technical Per process license. No volume limit, or on amount of hardware utilized. Included cloud based hosting 24 7 Support Industry adapted tools (eclipse, selenium, autoit, sikuli) Dev Community Knowledge Base Training (online and offline) "},{"version":"10.0","date":"Aug-06-2019","title":"bot-manager","name":"Bot Manager","fullPath":"iac/rpa/administrator-guide/bot-manager","content":" Overview Bot Manager is an application installed on Application Server used to: manage Bot Units maintain active RDP sessions contain cluster configuration collect data for Analytics Architecture Via RDP connection, Bot Manager maintains active RDP sessions enabling Bot Master to autostart and bots to click on active UI. That also allows Bot Manager to receive data from Bot Agent (e.g., Fleet info, Worker port, etc.). Bot Manager UI allows to send commands to Bot Unit components, view statistics, and track the number of bots servers processes. Communication is performed via secured Nginx connection. Bot Manager gets data on bot statuses and bot performance from Bot Agent and transfers it to MS SQL Server to be used by Analytics. Bot Manager communicates with Secrets Vault to provide credentials for Bot Master. :::tip Refer to RPA Deployment, to see the interaction between RPA components. ::: Setup To run RDP sessions and allow Bot Master autostart, it is necessary to provide RPA Windows Server hosts on Application Server. Go to bot manager config and specify the hosts in machines.yml. machines.hosts: test1.workfusion.com test2.workfusion.com test3.workfusion.com Restart Bot Manager to apply the changes. For more information, refer to RPA Windows Server Installation. User Interface To access Bot Manager UI, login to Control Tower as well as specific permissions to view and perform actions in Bot Manager are required. In Control Tower, go to Control Settings > Role Management. Select a user in the list or create a new profile via the Create Role option. Make sure the following role permissions are selected: View Platform Monitor and Manage Platform Monitor. To access Bot Manager, click on the dropdown in the upper left corner and select Bot Manager. There are the following sections in Bot Manager UI. Bots: bots statistics (Total Running Stopped) RPA Servers: each RPA server has a list of managed processes (Total Available) Process metrics: State Uptime ID URL Info Tags Process actions: Start Stop Restart Log out Screenshot Statistics Bot Manager collects statistics on bots' statuses and servers' performance and provides data storage for Analytics. Bots statistics Statistics Description Bots Total amount of of stopped and running Bot processes Bots Running up and running Bot processes (no matter whether a Bot is executing a task or not) Bots Stopped crashed or manually stopped Bot processes Servers statistics Statistics Description Instances Total all RPA Servers configured and available in Bot Manager Instances Available RPA Servers accessible through Bot Manager Commands There are three entities to manage within each Bot Unit. These are as follows: Bot Agent Bot Relay RPA Worker :::tip For more details, refer to Glossary ::: To run an action for a selected entity, click Actions and select a needed action. Action Description Start Launches a selected process Stop Stops a selected process Restart Stops and starts a selected process again Log out Executes the logout command (an RDP session will close and all processes will be stopped) Screenshot Takes a screenshot (works for active RDP sessions only) "},{"version":"10.0","date":"Aug-06-2019","title":"hardware-security-token-robotics","name":"Hardware security token Robotics","fullPath":"iac/rpa/administrator-guide/hardware-security-token-robotics","content":" Use case Some applications require possession of a hardware security token for authentication. These applications as a rule employ two factor authentication following the principle of \"Something you know (password) and something you have (physical token)\". Robots though have difficulties mimicking the humans behavior as the hardware token interactions happen on Physical Level, and the actions are not easily transferable on lower levels where robots are more comfortable to operate. Solution 1. Disable it Pretty straightforward, ha Solution 2. Software tokens Give robot a programmatic way of getting the access codes from token. Most of the Hardware Token vendors also provide Software tokens, for example: using Google Authenticator in RPA https: www.rsa.com en us products identity and access management securid software tokens Solution 3. Physical level Solve it on the physical level! Take screenshots with web camera: http: limid.sitadella.com pictures https: smallhacks.wordpress.com 2012 11 11 reading codes from rsa secureid token OCR it with specialized software (note bunch of useful references on the bottom): https: www.unix ag.uni kl.de ~auerswal ssocr Three factor authentication In particular cases, a third factor is added following the principle of \"Something you are\". This means something about person (or robot ) that cannot be changed, such as fingerprints, facial feature, eyes, which can be used as a factor of identity verification. "},{"version":"10.0","date":"Aug-06-2019","title":"memory-settings-for-rpa-processes","name":"Memory settings for RPA processes","fullPath":"iac/rpa/administrator-guide/memory-settings-for-rpa-processes","content":" Overview This page describes how to configure memory settings for RPA processes (Hub, Node, Platform Monitor). Hub Configuration { MemorySettingsforRPAProcesses HubConfiguration} To configure the Hub process memory settings, specify Xms and Xmx parameters in the following file: RPA installation directory rpa grid start hub.bat Add the following parameters after the java call: ... java bin java Xmx256m Xms256m Restart the Hub. Node Configuration { MemorySettingsforRPAProcesses NodeConfiguration} To configure the Node processes memory settings, specify Xms and* Xmx* parameters in the following files: RPA installation directory rpa grid start nodeX.bat where X is the Node number (starts from 0) Add the following parameters after the java call: ... java bin java Xmx256m Xms256m Restart the Node. Platform Monitor Configuration { MemorySettingsforRPAProcesses PlatformMonitorConfiguration} To configure the Platform Monitor process memory settings, specify Xms and* Xmx* parameters in the following file: RPA installation directory wfagent bin wfagent.cmd Add the following parameters to the java _opts variable: set java_opts= Xmx256m Djava.awt.headless=false Dtray.type=RPA Dfile.encoding=UTF 8 Restart the Platform Monitor. By default, it is recommended to specify Xmx256m Xms256m. But it should be configured according to your use case. For example, if your use case invokes a lot of applications on RPA Node, you should increase these parameters. Document generated by Confluence on Aug 06, 2019 09:08 Atlassian "},{"version":"10.0","date":"Aug-06-2019","title":"log-management","name":"Log management","fullPath":"iac/rpa/administrator-guide/log-management","content":" Customize RPA logrotate The section describes how to tweak the log rotation on RPA Server. This feature is needed to free disk space and improve security. Go to {rpa installation folder} rpa grid (by default it is C: RPA rpa grid). Change the following values in logback hub.xml and in all needed logback nodeX.xml files: To change the max size of RPA log, add the ` tag inside the ` tag, and set value of this tag (1024KB, 100MB, 1GB etc.): 100MB To change max days of history of logs, add the ` tag inside the ` tag, and set value of this tag as a number (10, 30, 60, etc.): 30 Expand to see the combined example {LOGDIR} {LOGFILE_NAME} %d{yyyy MM dd}.%i.log 100MB 30 UTF 8 %date{ISO8601} % 5level %thread %class.%M :%L %msg%n Customize RPA logs location RPA Nginx Open nginx.conf located in nginx conf directory and add the following lines before server section: access_log d: RPA nginx access.log; error_log d: RPA nginx error.log; server { ... Make sure that target directory exists before restarting nginx. :::tip If rotation scripts are used, update rotate nginx logs.bat script. ::: RPA grid Open logback *.xml located in the rpa grid directory and edit the LOG _DIR property: Required files: logback hub.xml logback node0.xml logback node1.xml Optional files depending on the number of configured RPA nodes: logback nodeX.xml "},{"version":"10.0","date":"Aug-06-2019","title":"number-of-bots-estimation","name":"Number of bots estimation","fullPath":"iac/rpa/administrator-guide/number-of-bots-estimation","content":" Bots to FTE ratio checklist Summary Number of bots per process can be estimated as range varying from 30%FTE to 300%FTE count. Where FTE is number of real full time persons busy doing same use case manually. It's a wide range of 30% 300% because of below mentioned long list of key reasons. :::tip After you calculate the number of bots using explanation below, compare with this simple range. If the formula result is NOT in range, check your calculation and revisit all influenced reasons. ::: Number of bots is a function of implementation approach. Record replay implementation often codifies the process that can be easily made more simple. \"Number of bots\" concept does not translate to cognitive automation. What is commonly understood as \"bot\" – desktop agent – is not used in cognitive automation. Reasons to have more bots in process Volume. More bots are needed if volume is high. SLAs. More bots are needed if process latency is under SLA. Varying workload. More bots are needed to process \"spiky\" workloads (both in terms of volume or duration). Often additional \"reserve\" bots are needed when automatic scale up and down is not available. Speed of applications. More bots are needed to process given workload if process applications are slow. Stability of applications. More bots are often needed for unstable process systems (use of bot retries and bot restarts as a work around). Nature of the applications involved. More bots are needed for applet based applications. Application limitations. More bots are needed when multiple instances of application cannot be run on same desktop because of technical limitations (very common for legacy apps). Application availability. More bots are needed if process application is only available during part of the day (because of maintenance, overnight processing, ETL, etc). Licensing limitations. More bots are needed when licensing of process applications or OS do not allow for multiple apps to be run on the same desktop. Access restrictions. More bots are needed if access is restricted (access time windows, access rights). Work pattern. More bots are needed when implementing maker checker or any sort of adjudicated work pattern. Business process re use. More bots are needed for same tasks if they are included in different parts of the same or re used between different processes. Process creep. More bots are typically created if process scope creeps over time (new inputs added – for example, onboarding new customers or regions). Data privacy. More bots are needed if location specific deployments is required for data privacy. BCP. More bots are often needed for DR BCP. Limitations of server environment. More bots are needed when the tool is not able to \"hot deploy\" multiple bots to same virtual machine desktop. Bot worker hand offs. Relevant for the attended automation only: more bots are often needed if human action is required as part of the process. Bots calculator The number of bots calculation from the number of transactions per day can be performed using the below formula. Parameter Restrictions Description P 0 0 no errors1 100% errors X 0 < X < INF Result: the number of RDP connections "},{"version":"10.0","date":"Aug-06-2019","title":"rpa-deployment","name":"RPA deployment","fullPath":"iac/rpa/administrator-guide/rpa-deployment","content":" General overview Task Distribution Control Tower sends bot tasks to RabbitMQ where they are distributed to queues. In case of the ` plugin, the tasks are submitted to RPA Queues according to their fleet attribute. For example, you can route SAP RPA bot tasks to SAP RPA Queue by defining SAP as fleet attribute. If no fleet attribute is provided, a task is sent to Shared RPA Queue. Tasks with no ` plugin are assigned to Common Queue to be transferred to the related server for execution. Depending on their fleet attributes, the tasks are distributed further to corresponding Bot Units. :::tip For more details on the distribution mechanism, see Task Distribution. ::: RPA infrastructure deployment There are two scenarios of RPA infrastructure deployment: multiple Bots Units per server, or RDP in RDP 1 Bot Unit per Windows machine, or VDI To trigger Bot Units, an active RDP session should be established. For RDP in RDP deployment, Bot Master User starts an RDP session allowing Master Bot Agent to initiate Bot Units. For that, Bot Manager retrieves Bot Master credentials from Secrets Vault. For VDI deployment, Bot Agent startup is scheduled in order to autostart an RDP session by Bot Manager. RDP in RDP The RDP in RDP approach allows you to run Bot Units in isolated sessions. Refer to RPA Server on the deployment scheme above. When the RDP in RDP deployment type is used, the following conditions are created: All the processes cannot be accessed remotely as they are executed on local interfaces. Thus, other processes cannot influence your Bot task execution. You can have a remote access from your workstation only to the necessary RDP session and check what task is in progress at that particular time. VDI In the VDI deployment approach, there are two possible scenarios. RPA VDI – refers to a Windows machine having all the RPA components in a single active console session. No additional RDP connections are created. All the services (Bot Relay Bot, RPA Worker, Bot Agent) are run by one user, e.g., Administrator. Refer to RPA VDI on the deployment scheme above. Master Session VDI – is responsible for opening multiple RDP sessions and keeping them active on several Windows machines. There are as many RDP connections as there are Windows machines. GUI active session support Bot Manager maintains active RDP sessions. As soon as the RDP session is started, Bot Units are triggered to perform task execution. Each Bot Unit contains four specific Java applications: Bot Agent, RPA Worker, Bot Relay and Bot. Bot Agent is used to start Bot Relay, Bot, and RPA Worker, and is also responsible for sending data to Zookeeper. RPA Worker connects to the assigned RPA Queue, retrieves tasks and, as soon as RPA bot tasks are successfully executed, sends results to Result Queue. Bot RelayandBot are service applications used to transport commands from RPA Worker to driver. Management of Bot Units is performed via the Bot Manager UI. Communication between Bot Units and Bot Manager is established via the secured Nginx connection. :::tip To learn more about Bot Manager, refer to Bot Manager. Data and analytics As soon as Bot Units are started, Filebeat and Metricbeat are initiated to send logs and metrics respectively to Logstash, that is a service used to ingest them, transform, and transfer to Elasticsearch. Mind other data types and their storage locations below: Bot Manager gets bot statuses and performance data from Bot Agent and sends it to MS SQL Server to store. Zookeper stores information on bot configuration and queue addresses transferred from Bot Agent. Secrets Vault is the secured storage intended for keeping sensitive data (i.e., Bot Master login and password). Communication with Secrets Vault is performed via Bot Manager. "},{"version":"10.0","date":"Aug-06-2019","title":"task-distribution","name":"Task distribution","fullPath":"iac/rpa/administrator-guide/task-distribution","content":" General overview There are various types of bot tasks, for example, for several lines of businesses to automate different types of applications. In this case, there is a need to divide Bot Units into a few separate queues with one type of tasks to be transferred to the first queue, the second type of tasks – to the second, etc. Reasons for segregation could be: regulatory or security requirements business processes with significantly varied nature (in terms of applications used) segregated development teams of business processes (no direct communication and collaboration path, e.g., the case when implementations are done by different service providers) To distribute bot tasks, one unified method of task routing via Fleets is used. Fleets stand for groups of bots used to segregate tasks according to their business value or machines with specified software. Thus, a specific fleet attribute is defined within the ` plugin. If no fleet attribute is provided, a task is sent to Shared Fleet. In case no ` plugin is defined, a task is assigned to Common Queue to be transferred further. Fleets setup Mind the following requirements to be fulfilled. Configuration is performed on RPA Windows Server while configuring multiple Bot Units. Bot tasks need to specify a fleet to utilize. Dynamic fleet attributes with variables cannot be used. For more information, see Robotics Plugins. Basic steps are as follows. Modify or copy file bot agent conf units unit{Unit id}.yml where {Unit id} refers to a Bot Unit identifier. Change unit_id and fleet. unit_id: 2 fleet: shared shared is a default fleet. If your unit_id is more than 9, remove the last 0 in each port – maximum 99 units for 1 machine. To define custom Fleets to submit tasks to, specify fleet for each Bot Unit. unit_id: 3 fleet: SAP :::tip Refer to RPA Windows Server Configuration for more details. ::: "},{"version":"10.0","date":"Aug-02-2019","title":"automate-command-line","name":"Automating command line","fullPath":"iac/rpa/developer-guide/automate-command-line","content":" Below is the example how it's possible to run commands on remote RPA Agent machine. Pay attention to the main trick of how we can execute Groovy code remotely. Example below runs ping n 3 google.com command and fetches all the console output. import java.io.BufferedReader import java.io.InputStreamReader String command = \"ping n 3 google.com\" StringBuffer output = new StringBuffer() Process p try { p = Runtime.getRuntime().exec(command) p.waitFor() BufferedReader reader = new BufferedReader(new InputStreamReader(p.getInputStream())) String line = \"\" while ((line = reader.readLine())!= null) { output.append(line \" n\") } } catch (Exception e) { e.printStackTrace() } return output; "},{"version":"10.0","date":"Aug-02-2019","title":"automate-drag-and-drop","name":"Automating drag and drop","fullPath":"iac/rpa/developer-guide/automate-drag-and-drop","content":" Intro Some web applications have a functionality allowing to drag web elements from one location and drop them on a defined area or even another web element. These kinds of complex actions are not available in basic element properties. Drag and drop actions can be automated using advanced user interactions. Implementation Action chain generator implements the Builder pattern to create a Composite Action containing a group of other actions. This should ease building actions by configuring an Actions chains generator instance and invoking its perform() method to get the complex action. actions().dragAndDrop( ('From Selector'), ('To Selector')) .perform() Example In this example, we will drag the Mystery & Thrillers folder from the left table on to the Horror folder of the right side table. div > table > tbody > tr:nth child(2) > td:nth child(2) > table > tbody > tr:nth child(4) > td:nth child(2) > table > tbody > tr:nth child(3) > td:nth child(2) > table > tbody > tr:nth child(1) > td.dhxTextCell.standartTreeRow > span')); get base element center coordinates int elementX = element.getLocation().getX() element.getSize().width 2; int elementY = element.getLocation().getY() element.getSize().height 2; get target element center coordinates int targetX = target.getLocation().getX() target.getSize().width 2; int targetY = target.getLocation().getY() target.getSize().height 2; move mouse to base element and click and hold actions().moveByOffset(elementX, elementY).clickAndHold().build().perform(); sleep(500); move mouse to target element and click a click context for make realese actions().moveByOffset(targetX elementX, targetY elementY).build().perform(); for IE need to use next work around if(driver.toString().equalsIgnoreCase(\"internet explorer\")) { actions().contextClick().build().perform(); } else { for all other drivers actions().release().build().perform(); } sleep(5000) > Now, if you look at the folder, you will notice that the Mystery & Thrillers folder has been moved to the Horror folder. The new folder structure looks like this. "},{"version":"10.0","date":"Aug-02-2019","title":"automate-basic-authentication","name":"Automating basic authentication","fullPath":"iac/rpa/developer-guide/automate-basic-authentication","content":" "},{"version":"10.0","date":"Aug-02-2019","title":"automate-dynamic-web-elements","name":"Automating dynamic web elements","fullPath":"iac/rpa/developer-guide/automate-dynamic-web-elements","content":" The most of sites now are dynamic, it means they change page content by js scripts while user interact with page. Sometimes it can be a problem, let's look at the following example. Changing page at Alibaba: elem = (byXpath(\" div contains(@class,'ui2 pagination pages') a @class='next' \")).click() } > Here we access alibaba.com site, enter product name, and try to swap page in the loop, if we run this bot task, it will be finished with the exception: unknown error: Element ... is not clickable at point (1320, 994). Other element would receive the click... It happens because robot produces action like a human user: it moves cursor to the element and then send signal, that mouse button was pressed. This variant works in most cases, but not for alibaba.com: while mouse cursor is moving to the Next Page button, site loads more elements to the page and button changes its coordinates, so 'click' is sending to another UI element. There are two main variants how to do so: by using hover() method and by using JS JavaScriptExecutor. Clicking by JS We can write js script, which will press the button. That method is preferable, since the button will be pressed without scrolling and moving cursor, so it will be faster. Changing page at Alibaba: element = (byXpath(\" div contains(@class,'ui2 pagination pages') a @class='next' \")); JavascriptExecutor js = (JavascriptExecutor)driver(); js.executeScript(\"arguments 0 .click();\", element); sleep(2000); isn't really necessary, but helps to you to check, that pages are changing, without sleeping they will change to fast for naked eye. } > Hover() method We can use hover() method for loading extra elements before we try to click the button. Move cursor to the element before clicking: elem = (byXpath(\" div contains(@class,'ui2 pagination pages') a @class='next' \")).hover() elem.click() Though, it doesn't help. It happens because robot needs more time for updating its cash, so we should add sleep or wait to code. One sleep: elem = (byXpath(\" div contains(@class,'ui2 pagination pages') a @class='next' \")).hover() sleep(1000) elem.click() But it still generates an exception: stale element reference: element is not attached to the page document This error occurs because the page was changed, but robot tries to access element presented on previous page, it needs some time to reevaluate XPath, so we have to add another sleep and the final code will be as follows. Final variant with hover: elem = (byXpath(\" div contains(@class,'ui2 pagination pages') a @class='next' \")).hover() sleep(1000) elem = (byXpath(\" div contains(@class,'ui2 pagination gotopage') input @class='ui2 pagination goto' \")).val(page) elem.click() sleep(3000) } > "},{"version":"10.0","date":"Aug-02-2019","title":"automate-file-upload","name":"Automating file upload","fullPath":"iac/rpa/developer-guide/automate-file-upload","content":" The same example implemented using the Universal Driver: Universal Driver: "},{"version":"10.0","date":"Aug-09-2019","title":"automate-swingset-app","name":"Automating SwingSet application","fullPath":"iac/rpa/developer-guide/automate-swingset-app","content":" The following examples automate the SwingSet2 app using 3 options: JAR file – SwingSet2.jar Applet opening inside a browser tab – https: rpa tutorial.s3.amazonaws.com demo jfc SwingSet2 SwingSet2.html Applet opening in a new window – https: rpa tutorial.s3.amazonaws.com demo jfc SwingSet2 custom SwingSet2.html :::tip Check this guide to enable applets in your browser – Configuring Java Applets for RPA See the Inspecting Tools for Desktop Automation topic to understand selectors for desktop applications. ::: Tables The example automates the SwingSet app as a JAR file using only one Desktop driver. JAR file: Combo boxes The example automates the SwingSet app as a Java applet in current browser tab using two drivers: Browser and Desktop. :::note The first driver is not closed on completion, it is closed only after the desktop. The applet window is located using the CLASS:SunAwtCanvas selector. ::: Applet in current browser tab: Tree The example automates the SwingSet app as a Java applet in a new window using one Universal Driver. :::note The Universal Driver is used (you can also use Browser Desktop combination as in the previous example) The applet window is located using the CLASS:SunAwtFrame; TITLE:SwingSet2 selector. You can use regexps in CSS selectors: CLASS:JTree; PATH: Music Clas. cal Br. ms Conc. *tos . For more details, see Inspecting Tools for Desktop Automation. ::: Tree keys = new ArrayList(rpaVariables.keySet()) {rpaVariables.get(rpaVar.toString())} "},{"version":"10.0","date":"Aug-06-2019","title":"automation-of-complex-grids","name":"Automation of complex grids","fullPath":"iac/rpa/developer-guide/automation-of-complex-grids","content":" In case automation of complex trees or tables with a large number of elements takes much time for both inspecting and executing scripts, use the following SAP UI controls attributes: native_id row column The native_id attribute immediately interacts with a selected element, e.g., a table cell of the Nth column in a complex grid, without waiting for information from the whole list of elements. Open SAP GUI desktop client and select the screen you need to automate. Open Inspector and click Capture. Hover over the needed element and press Escape. :::warning Do not click on the element! ::: Build up your selector manually. Copy nativeid{style=\"letter spacing: 0.0px;\"} in the native id* field and paste it after NATIVE_ID in the *Selector field. After the NATIVE_ID block, add CELL to your and specify the cell coordinates using the column and row values. Your selector will look as follows: NATIVE_ID:wnd 0 usr cntlGRID1 shellcont shell shellcont 1 shell; CELL:10,3 Using native_id, you can build selectors for the following UI elements: table cells: use nativeid cell: row, column, e.g., NATIVEID:wnd 0 usr tabsTAXITVERVIEW tblSAPMV45ATCTRLUERFAUFTRAG; CELL:1,38 tree items: use nativeid name (refer to the name attribute in the element data), e.g., NATIVEID:wnd 0 usr cntlTREE_CONTAINER shellcont shell; NAME: Root:0 SAP UI controls, e.g., buttons: use nativeid only, e.g., NATIVEID:wnd 0 usr btnNEW "},{"version":"10.0","date":"Aug-02-2019","title":"automate-mainframe-apps","name":"Automating mainframe applications","fullPath":"iac/rpa/developer-guide/automate-mainframe-apps","content":" Created by Andrei Harhots, last modified on Jun 06, 2019 Mainframe Applications Basics ( AutomatingMainframeApplications MainframeApplicationsBasics) RPA Goals Mainframe ( AutomatingMainframeApplications RPAGoalsMainframe) Example of SSH automation via Putty (Edit Text File Remotely) ( AutomatingMainframeApplications ExampleofSSHautomationviaPutty(EditTextFileRemotely)) Supported Console Actions ( AutomatingMainframeApplications SupportedConsoleActions) Logging into System and Working with Sessions ( AutomatingMainframeApplications LoggingintoSystemandWorkingwithSessions) Mainframe Applications Basics { AutomatingMainframeApplications MainframeApplicationsBasics} Typical application looks like the one on the screenshot below. RPA Goals Mainframe { AutomatingMainframeApplications RPAGoalsMainframe} Enter information into Mainframe app. Usually to navigate to application input fields or views there will be legend of keys used on the bottom of app. Simulating keystrokes by sendKeys(), we can put a cursor into the desired field. The same API is used to enter String data into field. Collect information from Mainframe app. WorkFusion RPA tool can only fetch whole current application state as String. It will be split into multiple lines. So its developer's task to come up with algorithms of parsing and fetching required portions of data from the whole content. Preconditions Download and set up Windows SSH Telnet client PuTTY. If the script doesn't work, make sure the path to putty.exe is added to Environment Variables. Example of SSH automation via Putty (Edit Text File Remotely) { AutomatingMainframeApplications ExampleofSSHautomationviaPutty(EditTextFileRemotely)} true Supported Console Actions { AutomatingMainframeApplications SupportedConsoleActions} copyPuttyWindowText() – selects and returns all console text from Putty selectAllTextAndCopy() – selects and returns all console text copySelectedText() – returns all currently selected console text clipboardText() – returns all clipboard text setClipboardText(String text) – Sets text to RPA agent clipboard Logging into System and Working with Sessions { AutomatingMainframeApplications LoggingintoSystemandWorkingwithSessions} true Attachments: { attachments .pageSectionTitle} LUA.gif (image gif) selectAllTextAndCopy.url (application octet stream) Document generated by Confluence on Aug 01, 2019 12:15 Atlassian "},{"version":"10.0","date":"Aug-02-2019","title":"automate-mouse-hover","name":"Automating mouse hover","fullPath":"iac/rpa/developer-guide/automate-mouse-hover","content":" RPA : Automating Mouse Hover { title heading .pagetitle} Created by Andrei Harhots, last modified by Alexander Zinchuk on Dec 14, 2017 There will be situations where it is required to click on the item of the drop down menu, for example: To accomplish this, you can use the hover() method. See the following code sample: Example of hover action item.hover() sleep(2000) } > Attachments: { attachments .pageSectionTitle} image2017 12 14 _17 51 25.png (image png) Document generated by Confluence on Aug 01, 2019 12:15 Atlassian "},{"version":"10.0","date":"Aug-02-2019","title":"calculator-automation","name":"Calculator automation","fullPath":"iac/rpa/developer-guide/calculator-automation","content":" :::tip If you have Windows 8 or higher, you need to download and install the old Win 7 calculator. It will be installed into C: Windows System32 calc1.exe ::: keys = new ArrayList(rpaVariables.keySet()) {rpaVariables.get(rpaVar.toString())} "},{"version":"10.0","date":"Aug-02-2019","title":"browser-navigation-commands","name":"Browser navigation commands","fullPath":"iac/rpa/developer-guide/browser-navigation-commands","content":" Navigation commands Type Method Description void back() Browser: move back a single \"item\" in the browser's history.Desktop: move back to preview window in desktop driver history. void forward() Browser: move a single \"item\" forward in the browser's history. Does nothing if we are on the latest page viewed.Desktop: move forward to next window in desktop driver history. void driver().navigate().to(java.lang.String url)driver().navigate().to(java.net.URL url) Browser: load a new web page in the current browser window. This is done using an HTTP GET operation, and the method will block until the load is complete. This will follow redirects issued either by the server or as a meta redirect from within the returned HTML. Should a meta redirect \"rest\" for any duration of time, it is best to wait until this timeout is over, since should the underlying page change whilst your test is executing the results of future calls against this interface will be against the freshly loaded page.Desktop: run program. void refresh() Browser: refresh the current page.Desktop: empty implementation. Example Opens the KB main page. Navigates to a new KB page by clicking a link. Navigates back to the KB main page. Navigates forward to the Release Notes page. Opens the Wikipedia website in the current tab. Refreshes the current page. "},{"version":"10.0","date":"Aug-02-2019","title":"checkbox-radiobutton-dropdown","name":"Checkbox - RadioButton - DropDown - MultipleSelect","fullPath":"iac/rpa/developer-guide/checkbox-radiobutton-dropdown","content":" CheckBox & Radio Button Operations are easy to perform and most of the times the simple ID attributes work fine for both of these. But selection and deselection is not the only thing we want with Check Boxes and Radio Buttons. We might want to check that a Check Box is already checked or that a Radio Button is selected by default, etc. Check Boxes and Radio Button deals exactly the same way and you can perform below mentioned operations on either of them. :::tip Sample web form https: rpa tutorial.s3.amazonaws.com trainings iframe form.html ::: Different selection methods ID If ID is given for the Radio Button CheckBox and you just want to click on it irrespective of its value, the command will be like this: open(\"https: rpa tutorial.s3.amazonaws.com trainings iframe form.html\") (byId('sex 1')).click() IsSelected If your choice is based on the pre selection of the Radio Button Check Box and you just need to select the deselected Radio Button Check Box. Assume there are two Radio Buttons Check Boxes, one is selected by default and you want to select the other one. With IsSelectedstatement, you can figure out that the element is selected or not. open(\"https: rpa tutorial.s3.amazonaws.com trainings iframe form.html\") Store all the elements of same category in the list of WebElements oRadioButton = (byName('sex')) Create a boolean variable which will hold the value (True False) def bValue = false This statement will return True, in case of first Radio button is selected bValue = oRadioButton.get(0).isSelected() This will check that if the bValue is True means if the first radio button is selected if(bValue){ This will select Second radio button, if the first radio button is selected by default oRadioButton.get(1).setSelected(true) } else { If the first radio button is not selected by default, the first will be selected oRadioButton.get(0).click() } sleep(2000) :::note The name is always the same for the same group of Radio Buttons Check Boxes but their Values are different. So if you find the element with the name attribute, then it means that it may contain more than one element, hence we need to use findElements ( ) method and store the list ofWebElements. ::: Value You can even select Radio Buttons Check Boxes with their Values. open(\"https: rpa tutorial.s3.amazonaws.com trainings iframe form.html\") Find the checkboxes def oCheckBox = (byXpath(\" div@class='control group' input\")) This will tell you the number of checkboxes are present int iSize = oCheckBox.size() Start the loop from first checkbox to last checkboxe for(int i=0; i DropDown & multiple select operations Just like CheckBox & Radio Buttons, DropDown & Multiple Select operations also work together and almost the same way. To perform any action, the first task is to identify the element group, as DropDown Multiple Select is not a single element. They always have a single name and they contain one or more elements (options). The only difference between these two is deselecting statement and multiple selections are not allowed on DropDown. Let’s look at various operations. It is just an ordinary operation like selecting any other type of element on a webpage. You can choose it by ID, Name, Css & XPath etc. Select class Models a `` tag, providing helper methods to select and deselect options. Select is a class which is provided to perform multiple operations on DropDown object and Multiple Select object. As Select is also an ordinary class, so its object is also created by a New keyword with regular class creation syntax. import org.openqa.selenium.support.ui.Select open('https: rpa tutorial.s3.amazonaws.com trainings iframe form.html') def continents = new Select( (byId('continents'))) continents.getOptions().each { println(it.getText()) } Select commands You can access the following Select class methods: Type Method Simplified API method Description void selectByIndex(int index) selectOption(int index) Select the option at the given index. void selectByValue(String value) selectOptionByValue(String value) Select all options that have a value matching the argument. void selectByVisibleText(String text) selectOption(String text) Select all options that display text matching the argument. List getAllSelectedOptions() Gets a list of all selected option elements. WebElement getFirstSelectedOption() getSelectedOption() Gets the first selected option. getSelectedText() Gets text of selected option in select field getSelectedValue() Gets value of selected option in select field. If the value attribute does not exist, it returns the option text. List getOptions() Gets a list of all option elements which are child elements of select tag. boolean isMultiple() Checks whether the control supports multiple selection. void deselectAll() Clears all selected entries. This method works only for Multi Select elements. void deselectByIndex(int index) Deselects the option at the given index. This method works only for Multi Select elements. void deselectByValue(String value) Deselects all options that have a value matching the argument. This method works only for Multi Select elements. void deselectByVisibleText(String text) Deselects all options that display text matching the argument. This method works only for Multi Select elements. selectByVisibleText selectByVisibleText(String text) : void – intended to choose or select an option given under any dropdowns and multiple selection boxes with the selectByVisibleText method. It takes a parameter of String which is one of the text of the select element and it returns nothing. def continents = new Select( (byId('continents'))) continents.selectByVisibleText('Africa') Simplified API method selectOption(String text) (byId('continents')).selectOption('Africa') selectByIndex selectByIndex(int arg0) : void – It is almost the same as selectByVisibleText but the only difference here is that we provide the index number of the option here rather the option text.It takes a parameter of int which is the index value of Select element and it returns nothing. def continents = new Select( (byId('continents'))) continents.selectByIndex(2) Simplified API method selectOption(int index) (byId('continents')).selectOption(2) Index starts from Zero, so the 3rd position value (Africa) will be at index 2. selectByValue selectByValue(String arg0) : void – It is again the same what we have discussed earlier, the only difference in this is that it asks for the value of the option rather than the option text or index. It takes a parameter of String which is on of the value of Select element and it returns nothing. def continents = new Select( (byId('continents'))) continents.selectByValue('AFR') Simplified API method selectOptionByValue(String value) (byId('continents')).selectOptionByValue('AFR') The value of an option and the text of the option may not be always same and there can be a possibility that the value is not assigned to Select webelement. getOptions getOptions( ) : List – This gets the all options belonging to the Select tag. It takes no parameter and returns List . Sometimes you may like to count the element in the dropdown and multiple select box, so that you can use the loop on Select element. def continents = new Select( (byId('continents'))) def optionsList = continents.getOptions() println('Options count:' optionsList.size()) optionsList.each { println(it.getText()) } Getting selected Option and its attributes You can get selected option, its text, and value using the Simplified API: def continents = (byId('continents')) continents.selectOption(4) println continents.getSelectedText() println continents.getSelectedValue() println continents.getSelectedOption().getText() isMultiple isMultiple( ) : boolean – This tells whether the SELECT element support multiple selecting options at the same time or not. This accepts nothing by returns boolean value (true false). This is done by checking the value of the “multiple” attribute. import org.openqa.selenium.support.ui.Select open('https: rpa tutorial.s3.amazonaws.com trainings iframe form.html') def continents = new Select( (byId('continents'))) def commands = new Select( (' robotics_commands')) println 'Is the 1st select multiple ' continents.isMultiple() println 'Is the 2nd select multiple ' commands.isMultiple() Example 2 (Multiple Selection Box List) Open https: rpa tutorial.s3.amazonaws.com trainings iframe form.html. Select the Robotic Commands multiple selection box (use Name locator to identify the element). Select the Browser Commands option and then deselect it (use selectByIndex and deselectByIndex). Select the Navigation Commands option and then deselect it (use selectByVisibleText and deselectByVisibleText). Print and select all the options for the selected multiple selection list. Deselect all options. "},{"version":"10.0","date":"Aug-06-2019","title":"citrix-automation","name":"Citrix automation with WorkFusion SPA","fullPath":"iac/rpa/developer-guide/citrix-automation","content":" The subject of the topic is applications, which are published via desktop virtualization software such as Citrix XenApp, Citrix XenDesktop, VMWare Horizon, Remote Desktop Services. When accessed by the end client, the remote server installed in a data center only sends screenshots of the application running remotely without providing a full access to the application and its components and controls. The nature of these virtualization approaches implies certain limitations and considerations when choosing an automation technique. WorkFusion offers several options to enable automation of such applications. Automation inside Citrix Installing WorkFusion RPA software at the same host as the application being automated. So both the application and WorkFusion RPA are installed \"inside\" Citrix. This approach allows applying object level automation techniques, enabling higher stability, faster execution speed, and more efficient hardware resources utilization. This approach may not be feasible in case the Citrix environment is controlled by a 3rd party, for example, if the remote software is provided as a service. :::note For more details about automation approaches, refer to Automation Technology Classification. ::: Surface automation Surface Based Automation Bots identify elements on the screen through their images and or coordinates. WorkFusion provides: Intuitive RPA Recorder Surface Automation Driver with a built in function library Optical Character Recognition (OCR) :::tip To speed up your automation, it is highly recommended to use keyboard commands (keyboard shortcuts, Tab key, arrow buttons, etc.). ::: "},{"version":"10.0","date":"Sep-03-2019","title":"configure-internet-explorer","name":"Configuring Internet Explorer","fullPath":"iac/rpa/developer-guide/configure-internet-explorer","content":" :::note Before making any changes to Internet Explorer settings, make sure no Java processes are running. ::: On the previous step, you should have already run RPA Installer delivering all the necessary changes to your Internet Explorer browser. Refer to the screenshots below and check that all the settings have been set correctly. Security settings Open Internet Explorer Browser and go to Tools > Internet options. IE options The Internet Options dialog appears. IE dialog Open the Security tab and check that the Enable Protected Mode option is disabled for each security zone. On the Security tab, select Trusted sites and click the Custom Level button. Custom Level option Disable the Use Pop up Blocker option. Scroll down and check that the Active scripting option is enabled. Scroll down and check that the Scripting of Java applets option is enabled. Scripting of Java applets Pop ups and extensions in private mode Close the Security Settings window. On the Internet Options window, switch to the Privacy tab. The Turn on Pop up Blocker and Disable toolbars and extensions when InPrivate Browsing start options should be disabled. Pop up blocker Switch to the General tab. Click the Tabs button. Check that Warn me when closing multiple tabs is disabled. Multiple tabs check Zoom level To set up zoom, do as follows. Switch to the Advanced tab and uncheck the options Reset text size to medium for new windows and tabs and Reset zoom level for new windows and tabs under Accessibility. Accessibility options Set browser Zoom to 100% and close Internet Explorer. Zoom options Java add ons Open the Internet Explorer browser, go to Tools, and click on Manage Add ons. Manage add ons option In the opened popup window, select the Toolbars and Extensions menu item (on the left side) and find Java add ons. If they are disabled, click Enable. Enable Java :::note If you don’t have these Java add ons, most likely you don’t have Java run time environment in your system, and you won’t be able to run Java applets inside the browser. In this case, refer to this guide and install Java on your machine. Make sure you install Java x86 64 the same as your Windows. After Java is installed, enable IE Java add ons as described above. ::: :::warning If you see that Java add ons are enabled but you are not able to start any Java applet inside Internet Explorer, try to restart your machine. ::: User Account Control settings Set User Access Control to the lowest level in Control Panel > User Accounts > Change user Account Control settings. Account Control settings Proxy usage in IE If your IE browser is configured to use proxy, see here. "},{"version":"10.0","date":"Aug-06-2019","title":"create-dispute-case","name":"Creating dispute case","fullPath":"iac/rpa/developer-guide/create-dispute-case","content":" :::note The use case shows how to create a dispute case from the email information. ::: Open SAP GUI and log in. Start the FBL5N transaction. Enter the transaction code FBL5N into the OK Code field and press Enter. On the selection screen, specify the required parameters. Select the document and click Display. Click Create Dispute Case. On the Create Dispute Case screen, fill in the required fields. Press the Save button. On saving, the message Dispute case created appears. To display created dispute cases, do as follows. Start the UDMDISPUTE transaction. Select UDMDISPUTE in RMS_ID if required. Within SAP Dispute Management, run the Find Dispute Case option. Enter the required selection criteria into the corresponding fields and press Search. To display details, double click on a specific case ID. As a result, you can check and process a dispute case. ⇪ Back to SAP automation examples "},{"version":"10.0","date":"Jul-29-2019","title":"configure-java-applets","name":"Configuring Java applets","fullPath":"iac/rpa/developer-guide/configure-java-applets","content":" Replace Java policy files C: RPA registry folder contains the java.policy file which provides additional permissions for jvm agent. This agent is needed for properly connection between RPA Grid and java applications (e.g. applets). Copy (and replace the existing) java.policy file to Java run time environment on your machine. It's important to copy it to system run time JRE which is used to run applets: JRE_HOME lib security. If you have installed a JDK bundle, copy java.policy to JDK bundle: JDK_HOME jre lib security. :::note Most likely, JRE JDK home is located under C: Program Files Java or C: Program Files (x86) Java folders. Check and update both x86 and x64 versions (if installed). ::: Fix Java security restrictions for applets When you are running Java applets in Internet Explorer, you can get an Application Blocked by Java Security popup. Application blocked by Java security It happens because Java blocks applications with self signed certificates by default. To get around this, you need to add the target site into Exception Site List. First of all, open Java Control Panel by pressing (Win R) and entering javaws viewer. Java control panel should be opened. Java Control Panel Switch to Security tab and find the Exception Site List section. Check Enable Java content in the browser. Click the Edit Site List button. Edit Site List button To automate Java applets used in example configs, add the following URLs to Exception Site List: https: rpa tutorial.s3.amazonaws.com https: rpa tutorial.s3.amazonaws.com table base page.html https: rpa tutorial.s3.amazonaws.com webid portal login.html https: rpa tutorial.s3.amazonaws.com demo jfc SwingSet2 custom SwingSet2.html To use your custom applets, add them to Exception Site List too. Add applets locations to IE trusted sites In Internet Explorer Options, perform the following actions. Switch to Security > Trusted sites. Click the Sites button. Enter the applet URL and click Add. IE trusted sites Test Java Applets Try to open a Java applet for the first time in Internet Explorer, and you will get the following prompt. Security warning Click Run, and the next time it won't be shown. "},{"version":"10.0","date":"Aug-02-2019","title":"decrease-web-page-loading-time","name":"Decreasing web page loading time","fullPath":"iac/rpa/developer-guide/decrease-web-page-loading-time","content":" What needs to be done if the site page loads for a very long time Before executing each command, RPA web driver checks the value of the document.readyState property and stops the command execution until the property gets the complete status. However, sometimes this strategy leads to failure. There are situations where the document.readyState property either fails to get into the complete status for a very long time, or never reaches that status at all. For example, there is a large picture on your website that is loaded with a very slow server. The entire page has been downloaded but you can't work with it because the browser continues to load this picture, and RPA web driver continues to wait for the browser to finish downloading. This is a real example that demonstrates this problem. inicialization WebDriver driver = new ChromeDriver(); WebDriverWait wait = new WebDriverWait(driver, 10); try to open site and wait when status stay 'complete' driver.get(\"http: www.sazehgostar.com SitePages HomePage.aspx\"); button click driver.findElement(By.id(\"en\")).click(); wait loading next page wait.until(visibilityOfElementLocated(By.id(\"menu\"))); The execution of this code fragment takes from 20 to 40 seconds (without taking into account the time to start the browser). The reason is that a large image is loaded onto the page ( ~ 7 megabytes). In this case, the desired button for switching to the English version of the site becomes available after a few seconds, but RPA web driver waits until the entire page loads. To prevent RPA web driver from waiting so long while page loading, use the following ways to solve the issue: set download wait timeout change download completion strategy Set download wait timeout If your set the page download timeout, you can get TimeoutException in case the page downloading takes more time than your timeout limit. After the exception occurs, the page load doesn't stop, but you can perform any actions with it. However, note that on such an \"underloaded\" page, the elements needed for further actions might not yet appear, therefore additional expectations for the appearance of elements are required. inicialization WebDriver driver = new ChromeDriver(); WebDriverWait wait = new WebDriverWait(driver, 10); set page load timeout driver.manage().timeouts().pageLoadTimeout(1, TimeUnit.SECONDS); try to open site and wait in the try catch because we have small timeout try { driver.get(\"http: www.sazehgostar.com SitePages HomePage.aspx\"); } catch (TimeoutException ignore) { } wait when button will be available on the page after exception WebElement button = wait.until(visibilityOfElementLocated(By.id(\"en\"))); click try { button.click(); } catch (TimeoutException ignore) { } wait loading next page wait.until(visibilityOfElementLocated(By.id(\"menu\"))); As you can see, this works faster, but fast work does not mean correct. You have the menu element on both web pages and at the moment you click En (change the site language to English), the element with the menu identifier exists on this page, and RPA web driver, instead of waiting for the second page to load after the click, immediately \"finds\" this element on the first page. The main reason for this problem is that the page load doesn't stop after we get TimeoutException and at the moment when you execute the click command too. This confuses RPA web driver, it doesn't understand that another page should appear, and looks for items on the current page instead. Since you decreased the wait timeout, you must take responsibility for the page load. Before waiting for appearance of the element that should be found on the next page, first wait for the element on the current page to disappear. For example, the button on which you clicked will disappear: inicialization WebDriver driver = new ChromeDriver(); WebDriverWait wait = new WebDriverWait(driver, 10); set page load timeout driver.manage().timeouts().pageLoadTimeout(1, TimeUnit.SECONDS); try to open site and wait in the try catch because we have small timeout try { driver.get(\"http: www.sazehgostar.com SitePages HomePage.aspx\"); } catch (TimeoutException ignore) { } wait when button will be available on the page after exception WebElement button = wait.until(visibilityOfElementLocated(By.id(\"en\"))); click try { button.click(); } catch (TimeoutException ignore) { } wait when button will not be available on the page after click wait.until(stalenessOf(button)); wait loading next page wait.until(visibilityOfElementLocated(By.id(\"menu\"))); The disadvantage of the above method is that you have to wrap all the code in the try catch block where you can get TimeoutException. Thus, the second method is recommended. Change download completion strategy Before executing each command, RPA web driver checks the value of the document.readyState property and stops the command execution until the property gets the complete status. During the processing of the site page, the browser changes this document.readyState reflecting information about the current loading stage: loading – the page is still loading interactive – the page main content has been loaded and rendered, the user can already interact with it, but additional resources are still loaded complete – all additional resources have been loaded You can change the RPA web driver settings, so that it does not wait for the complete value, the interactive value, or even does not expect anything at all. For this, set pageLoadStrategy to: normal (the default) – wait until the document.readyState property is set to complete eager – wait until the document.readyState property is set to interactive none – don't wait at all In this case, you must take responsibility for waiting for the \"unloading\" of the pages. Here is the same example without timeouts but with a modified wait strategy: initialization DesiredCapabilities capabilities = new DesiredCapabilities(); capabilities.setCapability(CapabilityType.PAGELOADSTRATEGY, \"eager\"); WebDriver driver = new ChromeDriver(capabilities); WebDriverWait wait = new WebDriverWait(driver, 10); try to open site driver.get(\"http: www.sazehgostar.com SitePages HomePage.aspx\"); waiting when button will be available WebElement button = wait.until(visibilityOfElementLocated(By.id(\"en\"))); click button.click(); waiting when button will be unavailable wait.until(stalenessOf(button)); waiting loading next page wait.until(visibilityOfElementLocated(By.id(\"menu\"))); "},{"version":"10.0","date":"Aug-02-2019","title":"css-selectors","name":"CSS selectors","fullPath":"iac/rpa/developer-guide/css-selectors","content":" :::tip Refer to Detailed JavaDoc ::: Selector syntax A selector is a chain of simple selectors, separated by combinators. Selectors are case insensitive (including against elements, attributes, and attribute values). The universal selector ( ) is implicit when no element selector is supplied (i.e., .header and .header is equivalent). Pattern Matches Example ` any element ` tag elements with the given tag name div id elements with attribute ID of \"id\" div wrap, logo .class elements with a class name of \"class\" div.left, .result attr elements with an attribute named \"attr\" (with any value) a href , title ` elements with an attribute name starting with \"attrPrefix\". Use to find elements with HTML5 datasets , div` attr=val elements with an attribute named \"attr\", and value equal to \"val\" img width=500 , a rel=nofollow attr=\"val\" elements with an attribute named \"attr\", and value equal to \"val\" span hello=\"Cleveland\", a rel=\"nofollow\" attr =valPrefix elements with an attribute named \"attr\", and value starting with \"valPrefix\" a href =http: attr =valSuffix elements with an attribute named \"attr\", and value ending with \"valSuffix\" img src =.png attr=valContaining elements with an attribute named \"attr\", and value containing \"valContaining\" a href= search attr~=regex elements with an attribute named \"attr\", and value matching the regular expression. img src~=( i) .(png jpe g) E elements of type E in any namespace ns name finds elements ns E elements of type E in the namespace ns fb name finds elements The above may be combined in any order div.header title a.red class.new class – link having two classes: red class AND new class Combinators Pattern Matches Example E F an F element descended from an E element div a, .logo h1 E > F an F direct child of E ol > li E F an F element immediately preceded by sibling E li li, div.head div E ~ F an F element preceded by sibling E h1 ~ p E, F, G all matching elements E, F, or G a href , div, h3 Pseudo selectors Pattern Matches Example :lt(n) elements whose sibling index is less than n td:lt(3) finds the first 3 cells of each row :gt(n) elements whose sibling index is greater than n td:gt(1) finds cells after skipping the first two :eq(n) elements whose sibling index is equal to n td:eq(0) finds the first cell of each row :has(selector) elements that contains at least one element matching the selector div:has(p) finds divs that contain p elements :not(selector) elements that do not match the selector. See also Elements.not(String) div:not(.logo) finds all divs that do not have the \"logo\" class. div:not(:has(div)) finds divs that do not contain divs. :contains(text) elements that contains the specified text. The search is case insensitive. The text may appear in the found element, or any of its descendants. p:contains(jsoup) finds p elements containing the text \"jsoup\". :matches(regex) elements whose text matches the specified regular expression. The text may appear in the found element, or any of its descendants. td:matches( d ) finds table cells containing digits. div:matches(( i)login) finds divs containing the text, case insensitively. :containsOwn(text) elements that directly contain the specified text. The search is case insensitive. The text must appear in the found element, not any of its descendants. p:containsOwn(jsoup) finds p elements with own text \"jsoup\". :matchesOwn(regex) elements whose own text matches the specified regular expression. The text must appear in the found element, not any of its descendants. td:matchesOwn( d ) finds table cells directly containing digits. div:matchesOwn(( i)login)finds divs containing the text, case insensitively. :containsData(data) elements that contains the specified data. The contents of script and style elements, and comment nodes (etc) are considered data nodes, not text nodes. The search is case insensitive. The data may appear in the found element, or any of its descendants. script:contains(jsoup) finds script elements containing the data \"jsoup\". The above may be combined in any order and with other selectors .light:contains(name):eq(0) :matchText treats text nodes as elements, and so allows you to match against and select text nodes. Note that using this selector will modify the DOM, so you may want to clone your document before using. p:matchText:firstChild with input One Two will return one PseudoTextElement with text \"One\". Structural pseudo selectors { CSSSelectors Structuralpseudoselectors .auto cursor target} Pattern Matches Example :root The element that is the root of the document. In HTML, this is the html element :root :nth child(an b) elements that have an b 1 siblings before it in the document tree, for any positive integer or zero value of n, and has a parent element. For values of a and b greater than zero, this effectively divides the element's children into groups of a elements (the last group taking the remainder), and selecting the bth element of each group. For example, this allows the selectors to address every other row in a table, and could be used to alternate the color of paragraph text in a cycle of four. The a and b values must be integers (positive, negative, or zero). The index of the first child of an element is 1. In addition to this, :nth child() can take odd and even as arguments instead. odd has the same signification as 2n 1, and even has the same signification as 2n. tr:nth child(2n 1) finds every odd row of a table. :nth child(10n 1) the 9th, 19th, 29th, etc, element. li:nth child(5) the 5th li :nth last child(an b) elements that have an b 1 siblings after it in the document tree. Otherwise like :nth child() tr:nth last child( n 2) the last two rows of a table :nth of type(an b) pseudo class notation represents an element that has an b 1 siblings with the same expanded element name before it in the document tree, for any zero or positive integer value of n, and has a parent element img:nth of type(2n 1) :nth last of type(an b) pseudo class notation represents an element that has an b 1 siblings with the same expanded element name after it in the document tree, for any zero or positive integer value of n, and has a parent element img:nth last of type(2n 1) :first child elements that are the first child of some other element. div > p:first child :last child elements that are the last child of some other element. ol > li:last child :first of type elements that are the first sibling of its type in the list of children of its parent element dl dt:first of type :last of type elements that are the last sibling of its type in the list of children of its parent element tr > td:last of type :only child elements that have a parent element and whose parent element hasve no other element children :only of type an element that has a parent element and whose parent element has no other element children with the same expanded element name :empty elements that have no children at all XPath CSS Equivalents { CSSSelectors XPath CSSEquivalents} XPath CSS * @disabled :disabled * @checked :checked * @selected :selected * @type=\"text\" :text * contains(text(),\"you\") :contains(\"you\") p contains(@me,\"you\") p me *=\"you\" p starts with(@me,\"you\") p me =\"you\" p starts with(@me,concat(\"you\",' ')) p me =\"you\" p substring(@me,string length(@me) 2)=\"you\" p me =\"you\" p contains(concat(\" \",@me, \" \"),\" you \") p me ~=\"you\" p @me!=\"you\" p me!=\"you\" p @id=\"me\" p me p not(@id=\"me\") p:not( me) p contains(concat(\" \", @class, \" \"), \" me \") p.me div p div p div p div > p h1 following sibling::div h1 div h1 following sibling:: * count(div) h1 ~ div * 1 :root descendant:: * 1 :first child * last() :last child count( )=1 :only child count( ) = 0 :empty * position() mod n = 1 :nth child(n) * (position() mod 2)=1 :nth child(odd) * (position() mod 2)=0 :nth child(even) * (count() position()) mod n = 1 :nth last child(n) p n :nth of type(n) p (count() position()) mod n = 1 :nth last of type(n) descendant::p 1 :first of type p last() :last of type p count( *)=1 :only of type Document generated by Confluence on Jul 30, 2019 12:45 Atlassian "},{"version":"10.0","date":"Aug-02-2019","title":"dynamic-webtables","name":"Dynamic WebTables","fullPath":"iac/rpa/developer-guide/dynamic-webtables","content":" HTML table representation A Table is a kind of HTML data which is displayed with the help of the tag in conjunction with the and tags. Although there are other tags for creating tables, these are the basics for creating a table in HTML. Here, tag defines a row and tag defines a column cell of a table. Excel sheet is a simple example of table structures. Whenever we put some data in to excel we give them some heading as well. In HTML we use tag for headings which defines heading of the table. Each cell in the Excel sheet can be represented as in the HTML table. The elements are the data containers and these can contain all sorts of HTML elements like text, images, lists, other tables, etc. Simple table without heading: Automation Tool Licensing Notes RPA Express Free Can be extended with multiple bots WorkFusion SPA Commercial Contains Cognitive component Additionally, there is one more element – the element. This tag, which stands for table cell heading, is to be used instead of the element when the content of the cell is a heading instead of actual cell data. This means that it is the obvious choice inside the element (which should contain e.g. the first row of your table) but you can also use it for the very first column to indicate the headings table row headers. There can be a table footer section as well, which is always displayed under the even if its code is before the table body. A table can have a name which is given using the tag. Let’s look at an example – it is very similar to the previous but I have used meaningful content to make clear to you how to use the element. Table with heading row: Sample Table Automation Tool Licensing Notes Public Info RPA Express Free Can be extended with multiple bots WorkFusion SPA Commercial Contains Cognitive component Handle dynamic WebTables There is no rocket science in handling of tables. All you need to do is to inspect the table cell and get the HTML location of it. In most cases, tables contain text data and you might simply like to extract the data given in the each row or column of the table. But sometimes tables have link or images as well, and you can perform any action on those elements if you can find the HTML location of the containing cell. :::note Sample web page for table automation https: rpa tutorial.s3.amazonaws.com trainings table automation.html ::: Example 1 Let’s take an example of above table and choose Row 2 Column 3 cell which is Dubai in above case: * @id=\"content\" table tbody tr 1 td 2 If we divide this XPath into three different parts it will be like this: location of the table in the webpage: * @id=\"content\" table body (data) starts from here: table tbody it says table row 1 and table column 2 (because the first visible row is in the , and the first visible column is a ): tr 1 td 2 If you use this XPath you would be able to get the cell of the table. And how to get the text ‘Selenium’ from the table cell You need to use the getText() method of the WebDriver element: open('https: rpa tutorial.s3.amazonaws.com trainings table automation.html') def cellText = (byXpath(\" * @id='content' table tbody tr 1 td 2 \")).getText() Example 2 Table operations are not always so simple like above because tables can contain a large amount of data and you may need to pass rows and columns dynamically. In that case you need to build your xpath with using variables and you will pass rows and columns to your xpath in the form of variables. open('https: rpa tutorial.s3.amazonaws.com trainings table automation.html') def sRow = 1 def sCol = 2 def cellText = (byXpath(\" * @id='content' table tbody tr {sRow} td {sCol} \")).getText() Example 3 The above example is still easy as at least you know the row number and the column number to be fetched from the table and you are able to provide it in the xpath from external excel sheet or any source of data sheet. But what would you do when the row and columns are itself dynamic and all you know is the Text value of any cell only and you need to take out the correspondent values of that particular cell. For example, all you know is the \"Licensing\" column name in the above example and you like to record all possible values in that column (Free, Commercial). open('https: rpa tutorial.s3.amazonaws.com trainings table automation.html') def colHeading = 'Licensing' def columnValues = def tBodyXpath = \" * @id='main' div 2 div div 2 table tbody\" def totalRows = (byXpath(\" {tBodyXpath} tr\")).size() for(int i=1;i keys = new ArrayList(rpaVariables.keySet()) {rpaVariables.get(rpaVar.toString())} "},{"version":"10.0","date":"Aug-06-2019","title":"element-selectors","name":"Element selectors","fullPath":"iac/rpa/developer-guide/element-selectors","content":" Selectors (or Locators) are used to find and match the elements of a web page or desktop app that a robot needs to interact with. Using the right selector ensures the bots are faster, more reliable or has lower maintenance over releases. If you’re fortunate enough to be working with unique IDs and Classes, then you’re usually all set. It can be a real challenge to verify that you have the right selectors to accomplish what you want. This tutorial explains different selectors, how, when and ideal strategies to use these selectors. Web selectors ID selector WEB IDs are the most preferred way to locate elements on a page, as each ID is supposed to be unique which makes IDs a very fast and reliable way to locate elements. With this strategy, the first element with the ID attribute value matching the selector will be returned. If no element has a matching ID attribute value, NoSuchElementException is raised. Example: If an element is given like this: Login Username: Password: You can easily choose the element with the help of ID selector from the above example: id = username id = password def elementUser = (byId('username')) elementUser.val('my_login') (byId('password')).sendKeys('secure_pass').pressEnter() Even though this is a great selector, obviously it is not realistic for all objects on a page to have IDs. In some cases, developers make it having non unique IDs on a page or auto generate the iIDs, in both cases it should be avoided. Name selector WEB This is also an efficient way to locate an element with name attribute, after IDs give it your second preference but likewise IDs, name attributes don’t have to be unique. With this strategy, the first element with the name attribute value matching the selector will be returned. If no element has a matching name attribute, NoSuchElementException is raised. def elementUser = (byName('login')) elementUser.setValue('my_login') (byName('pass')).sendKeys('secure_pass').pressEnter() Text selector WEB You can find elements by their inner text using the following selectors. byText – returns all elements with given text (exact match) withText – returns all elements containing given text (substring) byLinkText – returns all anchor elements with given text (exact match) byPartialLinkText – returns all anchor elements containing given text (substring) Sample html element Name of the Link Order now! To click this hyperlink or button using the tag’s text, you can use the following text selectors: def spanElement1 = (byText('Order now!')) def spanElement2 = (withText('Order')) def linkElement1 = (byLinkText('Name of the Link')) def linkElement2 = (byPartialLinkText('Name of')) Tag and attribute selectors WEB Let's automate the following form using tag and attribute selectors: WHO was founded in 1948. Login Username: Password: You can use the following selectors: byTitle byValue byTagName byClassName byAttribute by('attribute name', 'attribute value') (byTitle('World Health Organization')).getText() (byValue('SignIn')).click() (byTagName('p')).getText() (byClassName('login')).sendKeys('username') (byAttribute('type','text')).sendKeys('username') (by('value','SignIn')).click() CSS selector WEB and DESKTOP :::tip See also: CSS Selectors CSS guide ::: Let's automate the following form using CSS selectors: Login Username: Password: You can use both () and (byCssSelector) selectors which are equivalent. (' password').sendKeys('secure_pass').pressEnter() (byCssSelector('form input:first child')).sendKeys('my_login') XPath selector WEB and DESKTOP :::tip See also: Automating SwingSet App Effective XPath ::: While DOM is the recognized standard for navigation through an HTML element tree, XPath is the standard navigation tool for XML; and an HTML document is also an XML document (xHTML). Example: To select the username from the above example you can use the following ways: (byXpath(\" * @id='username' \")) (byXpath(\" input @id='username' \")) (byXpath(\" form @name='loginForm' input 1 \")) (byXpath(\" * @name='loginForm' input 1 \")) Desktop selectors The following selectors are available for Desktop driver: or byCssSelector Example – CSS Selectors or Object selector Example – Object Selectors byXpath Example – Automating SwingSet App byImage Example – Surface based Robotics driver FindElement ( ) and FindElements ( ) Commands The difference between findElement() and findElements() method is the first returns an uiElement object otherwise it throws an exception and the latter returns a List of uiElements, it can return an empty list if no elements match the query. findElement() – () On Zero match: throws NoSuchElementException On One match: returns uiElement On One match: returns the first uiElement mathing the specified selector findElements() – () On Zero match: returns an empty list On One match: returns a list of one uiElement only On One match: returns a list with all matching instances Expand to see the example with findElements "},{"version":"10.0","date":"Aug-02-2019","title":"effective-xpath","name":"Effective XPath","fullPath":"iac/rpa/developer-guide/effective-xpath","content":" XPath in JavaScript: introduction What is XPath XPath is used to identify different parts of XML documents by indicating nodes by position, relative position, type, content, etc. Similar to the DOM, XPath allows to pick nodes and sets of nodes out of an XML tree. There are seven different node types XPath has access to (for most JavaScript purposes the first four node types will most likely be sufficient): Root Node Element Nodes Text Nodes Attribute Nodes Comment Nodes Processing Instruction Nodes Namespace Nodes How XPath traverses the tree XPath can use location paths, attribute location steps, and compound location paths to very quickly and efficiently retrieve nodes from our document. You can use simple location paths to quickly retrieve nodes you want to work with. There are two basic simple location paths the root location path ( ) and child element location paths. The forward slash ( ) servers as the root location path…it selects the root node of the document. It is important to realize this is not going to retrieve the root element, but the entire document itself. The root location path is an absolute location path, no matter what the context node is, the root location path will always refer to the root node. Child element location steps are simply using a single element name. For example, the XPath p refers to all p children of our context node. One of the really handy things with XPath is we have quick access to all attributes as well by using the at sign @ followed by the attribute name we want to retrieve. So we can quickly retrieve all title attributes by using @title. XPath in JavaScript You can execute the following JavaScript snippets on this page https: www.w3.org TR WCAG20 TECHS H33.html To use XPath in IE 11, you need to include the following library https: github.com google wicked good xpath The document.evaluate method looks like this: var theResult = document.evaluate(expression, contextNode, namespaceResolver, resultType, result); The expression argument is simply a string containing the XPath expression we want evaluate. The contextNode is the node we want the expression evaluated against. The namespaceResolver can safely be set to null in most HTML applications. The resultType is a constant telling what type of result to return. Again, for most purposes, we can just use the XPathResult.ANY _TYPE constant which will return whatever the most natural result would be. Finally, the result argument is where we could pass in an existing XPathResult to use to store the results in. If we don’t have an XPathResult to pass in, we just set this value to null and a new XPathResult will be created. Here’s a very simple XPath expression that will return all elements in our document with a title attribute. var titles = document.evaluate(\" * @title \", document, null, XPathResult.ANY_TYPE, null); If you take a look at the XPath expression we passed in `“ * @title ”`, you will notice that we used the attribute location step followed by the attribute we want to find, ‘title’: The two forward slashes preceding the at sign is how we tell the browser to select from all descendants of the root node (the document). The asterisk sign says to grab any nodes regardless of the type. Then we use the square brackets in combination with our attribute selector to limit our results only to nodes with a title attribute. The evaluate method in this case returns an UNORDERED NODE ITERATOR _TYPE, which we can now move through by using the iterateNext() method like so: var titles = document.evaluate(\" * @title \", document, null, XPathResult.ANY_TYPE, null); var theTitle = titles.iterateNext(); while (theTitle){ alert(theTitle.textContent); theTitle = titles.iterateNext(); } Since each item in the results is a node, we need to reference the text inside of it by using thetextContent property (line 3). You can only iterate to a node once, so if you want to use your results later, you could save each node off into an array with something like below: var titles = document.evaluate(\" * @title \", document, null, XPathResult.ANY_TYPE, null); var arrTitles = ; var theTitle = titles.iterateNext(); while (theTitle){ arrTitles.push(theTitle.textContent); theTitle = titles.iterateNext(); } Now arrTitles is filled with your results and you can use them however often you wish. This is just the beginnin, as we continue to look at XPath expressions and introduce predicates and XPath functions, you will start to see just how truly robust XPath expressions are. snapshotItem(int i) With the resultType property, the type of the result can be retrieved. If the value of the resultType property is UNORDERED NODE SNAPSHOT TYPE or ORDERED NODE SNAPSHOT TYPE, then the result contains snapshots for all nodes that match the expression. In this case, the snapshotItem(i) method can be used to retrieve the matching nodes from the snapshots collection, by position. Use the snapshotLength property to get the length of the snapshots collection. The snapshotItem method is similar to the iterateNext method. Both provide access to the matching nodes. The main difference is that the modification of the document invalidates the iteration, but does not invalidate the snapshots collection. document.evaluate(\" div @class='spacer js gps track' a\", document, null, XPathResult.ORDEREDNODESNAPSHOT_TYPE, null).snapshotItem(1).innerText Different ways of choosing XPaths Let’s take an example of the Automation Tester checkbox on the https: rpa tutorial.s3.amazonaws.com trainings iframe form.html form. Absolute XPath The easiest way of finding the xpath is to use the Browser Inspector tool to locate an element and get the XPath of it. html body form fieldset div 20 input 2 Relative XPath At times XPath generated by Firebug or XPath Helper plugins are too lengthy and there is a possibility of getting a shorter XPath. Above XPath will technically work, but each of those nested relationships will need to be present 100% of the time, or the locator will not function. Above choosen XPath is known as Absolute XPath. It is always better to choose a Relative XPath, as it helps us to reduce the chance of element not found exception. To choose the relative XPath, it is advisable to look for the recent Id attribute. Look below at the HTML code of the above screenshot. Relative XPaths: input @id='profession 1' div@class='control group' input 2 Difference between single ‘ ’ or double ‘ ’ A single slash at the start of Xpath instructs XPath engine to look for element starting from root node. A double slash at the start of Xpath instructs XPath engine to search look for matching element anywhere in the XML document. Relative XPath with FirePath There is an alternate way to get the relative XPath with help of the FirePath tool. Click on the drop down menu on the FirePath button and unselect Generate absolute XPath. Now, click on the same element with the Inspector, the new XPath will look like this: If something gets changed above the ID social media, your XPath will still work. Single ‘ ’ and double ‘ ’ in XPath A single slash ‘ ’ anywhere in Xpath signifies to look for the element immediately inside its parent element. A double slash ‘ ’ signifies to look for any child or any grand child element inside the parent element. Relative XPath with form input@name='profession' Partial XPath Contains Keyword Most of the times users face issues when the locator’s properties are dynamically generating. The only thing we are sure here is that the text ‘test excel’ will always be included in the href of this link, so we can utilize this hint in our XPath like this: a contains(@href,'test excel') Partial XPath starts with keyword Now let’s take another example and assume that the \"value\" attribute is dynamically generating. input starts with(@value,'Q') Partial XPath text keyword You can also select an element by its inner text: * text()='some text' Combination of contains and text(): a contains(text(),'Hybrid') "},{"version":"10.0","date":"Aug-02-2019","title":"explore-excel-window","name":"Exploring Excel window using Inspector","fullPath":"iac/rpa/developer-guide/explore-excel-window","content":" Use Inspector to get Excel UI and document controls In this example, we will practice the usage of Inspector, that is a built in tool to explore properties and values of objects located in the application, providing functionality for quick element selector search. Here, we will use it to automate Windows Desktop application on the example of MS Excel. Let's suppose we have the following table in the Excel file. Having Euro as the base currency, scrap five currencies. Open the file with 1,000 rows and filter by date = today and currency code. Set exchange rate to euro. Take the following currencies: USD, AUD, INR, GBP, SGD. Set the date to 28.11.2018. Using Inspector, we can get appropriate fields, like the ones shown on figures 1 and 2. Fig. 1 – Get the selector of A1 cell Fig. 2 – Navigation through cells in Excel using Inspector attributes. Use identified selectors in Bot Task code Bot Task source code can be designed in the following way. true The result file looks as follows. "},{"version":"10.0","date":"Aug-06-2019","title":"get-all-stock-movements","name":"Getting all stock movements for specified period","fullPath":"iac/rpa/developer-guide/get-all-stock-movements","content":" :::note The use case shows how to get all the stock movements for a specified material and period, and send the data to a user, along with the stock on start and end dates of the selected period. ::: Manual scenario Open SAP GUI and log in. Start the MB5B transaction. Enter the transaction code MB5B into the OK Code field and press Enter. Specify the material number and storage location in the Material number and Storage Location fields respectively. Set dates within Selection Date. Get all the information from the screen and send to the user via email. Automated scenario :::tip To be able to run the script, provide your own credentials to enter SAP, defined as ` and `. Also, use appropriate email addresses, passwords, etc. in the mail section of the script. ::: Expand to see the code sample of how to get all stock movements for specified period pw= language=EN system=ERD client=100 command=MB5B trace=0' open(\"C: Program Files (x86) SAP FrontEnd SAPgui {command}\") sleep(5000) * CONTINUE WITHOUT ENDING ANY OTHER LOGONS * if (Window.windowHandles(\" CLASS: 32770; TITLE:License Information for Multiple Logons \").isEmpty()) { log.info(\"Window License Information for Multiple Logons not found\") } else { window(\" CLASS: 32770; TITLE:License Information for Multiple Logons \") (\" CLASS:GuiRadioButton; NAME:MULTILOGONOPT2 \").click() (' CLASS:GuiButton; NAME:btn 0 ; ').click() log.info(\"Window License Information for Multiple Logons closed\") } * QUERY * window(\" CLASS:SAPFRONTENDSESSION; TITLE:Stock on Posting Date \") (\" CLASS:GuiCTextField; NAME:MATNR LOW \").setText(\"4\") (\" CLASS:GuiCTextField; NAME:LGORT LOW \").setText(\"0001\") (\" CLASS:GuiCTextField; NAME:DATUM LOW \").setText(\"28.11.2018\") (\" CLASS:GuiCTextField; NAME:DATUM HIGH \").setText(\"29.11.2018\") sendKeys(Keys.F8) * PARSE SAP GUI REPORT * window(\" CLASS:SAPFRONTENDSESSION; REGEXPTITLE:Material Stocks Between .* \") sys.defineVariable(\"emailSubject\", (\" CLASS:GuiTitlebar \").getText()) guiLabels = (\" CLASS:GuiLabel \") log.info(\"GuiLabels found: {guiLabels.size()}\") List mainInfoLines = new ArrayList(); rowsWithInfoCount = 8 for (i = 0; i > table = new ArrayList() List row = new ArrayList() for (i = 8; i () } } log.info(table.toString()) * PREPARE HTML REPORT * textRowTemplate = \"%s\" tableTemplate = \"%s\" tableRowTemplate = \"%s\" tableCellTemplate = \"%s\" StringBuilder reportBuilder = new StringBuilder(); mainInfoLines.each { reportBuilder.append(String.format(textRowTemplate, it)) } StringBuilder tableBuilder = new StringBuilder(); table.each { StringBuilder rowBuilder = new StringBuilder(); it.each { rowBuilder.append(String.format(tableCellTemplate, it)) } tableBuilder.append(String.format(tableRowTemplate, rowBuilder.toString())) } reportBuilder.append(String.format(tableTemplate, tableBuilder.toString())) sys.defineVariable(\"emailContent\", reportBuilder.toString()) > \" smtp host=\"\" security=\"ssl\" smtp port=\"\" to=\"\" username=\"\" password=\"\" type=\"html\" subject='SAP {emailSubject}'> ⇪ Back to SAP automation examples "},{"version":"10.0","date":"Aug-02-2019","title":"fail-without-windows-active-session","name":"Fail without Windows active session","fullPath":"iac/rpa/developer-guide/fail-without-windows-active-session","content":" Should be NPE if Node does not have active window session. (getRect) Should be NPE if Node does not have active window session. (click) Screenshot should be black screen if Node does not have active window session. Screenshot should be black screen if Node does not have active window session. Should be No Such Element Exception if Node does not have active window session. "},{"version":"10.0","date":"Aug-02-2019","title":"execute-groovy-script","name":"Executing GroovyScript","fullPath":"iac/rpa/developer-guide/execute-groovy-script","content":" GroovyScript The feature allows to execute GroovyScript on the Bot machine (Windows RPA Server), not in Control Tower. See the JavaDoc link for full reference. You can optionally pass timeout and parameters for your GroovyScript. Below, you can find all the possible variants of the executeGroovyScript() method: executeGroovyScript(script) executeGroovyScript(script, timeout) executeGroovyScript(script, scriptParams) executeGroovyScript(script, timeout, scriptParams) The scriptParams object can be created using the following syntax: import com.workfusion.rpa.helpers.ScriptParams initializing the object def scriptParams = new ScriptParams( 'fileSuffix':'.txt', 'fileData':fileData ) adding one property scriptParams.add('filePrefix','temp ') adding multiple properties scriptParams.addAll( 'key1':'val1', 'key2':'val2' ) Usage example local files creation on Bot machine: AutoIt scripts NOT RECOMMENDED You can also create AutoIt scripts using the following methods: executeAutoitScript(script) executeAutoitScript(script, timeout) executeAutoitScript(script, timeout, array) "},{"version":"10.0","date":"Aug-06-2019","title":"get-warehouse-stock","name":"Getting warehouse stock for specified material","fullPath":"iac/rpa/developer-guide/get-warehouse-stock","content":" :::note The use case shows how to get the stock for specific material and send the data to a user. ::: Manual scenario Open SAP GUI and log in. Start the MB52 transaction. Enter the transaction code MB52 into the OK Code field and press Enter. Specify the material number and storage location in the Material and Storage Location fields respectively. Click Execute or press F8. In the ALV List, see the line corresponding to the specified storage location and get the quantity and total cost. Send the data to the user via email. Automated scenario :::tip To be able to run the script, provide your own credentials to enter SAP, defined as ` and `. Also, use appropriate email addresses, passwords, etc. in the mail section of the script. ::: Expand to see the code sample of how to get warehouse stock for specific material pw= language=EN system=ERD client=100 command=MB52 trace=0' open(\"C: Program Files (x86) SAP FrontEnd SAPgui {command}\") sleep(5000) * CONTINUE WITHOUT ENDING ANY OTHER LOGONS * if (Window.windowHandles(\" CLASS: 32770; TITLE:License Information for Multiple Logons \").isEmpty()) { log.info(\"Window License Information for Multiple Logons not found\") } else { window(\" CLASS: 32770; TITLE:License Information for Multiple Logons \") (\" CLASS:GuiRadioButton; NAME:MULTILOGONOPT2 \").click() (' CLASS:GuiButton; NAME:btn 0 ; ').click() log.info(\"Window License Information for Multiple Logons closed\") } * QUERY * window(\" CLASS:SAPFRONTENDSESSION; TITLE:Display Warehouse Stocks of Material \") (\" CLASS:GuiCTextField; NAME:MATNR LOW \").setText(\"4\") (\" CLASS:GuiCTextField; NAME:LGORT LOW \").setText(\"0001\") sendKeys(Keys.F8) window(\" CLASS:SAPFRONTENDSESSION; TITLE:Display Warehouse Stocks of Material \") sys.defineVariable(\"emailSubject\", (\" CLASS:GuiLabel; INSTANCE: 43 \").getText().toString()) tableTemplate = \"%s\" tableRowTemplate = \"%s\" tableCellTemplate = \"%s\" StringBuilder firstRowBuilder = new StringBuilder() StringBuilder secondRowBuilder = new StringBuilder() StringBuilder tableBuilder = new StringBuilder() firstRowBuilder.append(String.format(tableCellTemplate, (\" CLASS:GuiLabel; INSTANCE: 53 \").getText())) firstRowBuilder.append(String.format(tableCellTemplate, (\" CLASS:GuiLabel; INSTANCE: 55 \").getText())) secondRowBuilder.append(String.format(tableCellTemplate, (\" CLASS:GuiLabel; INSTANCE:68 \").getText())) secondRowBuilder.append(String.format(tableCellTemplate, (\" CLASS:GuiLabel; INSTANCE:70 \").getText())) tableBuilder.append(String.format(tableRowTemplate, firstRowBuilder.toString())) tableBuilder.append(String.format(tableRowTemplate, secondRowBuilder.toString())) sys.defineVariable(\"emailContent\", String.format(tableTemplate, tableBuilder.toString())) > \" smtp host=\"\" security=\"ssl\" smtp port=\"\" to=\"\" username=\"\" password=\"\" type=\"html\" subject='SAP Warehouse Stocks of Material: {emailSubject}'> ⇪ Back to SAP automation examples "},{"version":"10.0","date":"Aug-02-2019","title":"handle-file-download","name":"Handling file download","fullPath":"iac/rpa/developer-guide/handle-file-download","content":" The example shows how to automate the Save File As system dialog. The file is being saved by the Desktop driver. :::tip It is easier to use the downloadFileOnAgent(String path) method to save a file on RPA Node. To pass a file from a RPA Node to the Control Tower server, you can use the downloadFileFromAgent(String path) method. ::: The file is saved into RPA Agent machine Temp folder. To upload it onto S3, the S3 Plugins are used. You need to configure S3 bucket first. To simulate the use of the S3 bucket, refer to File Storage. :::note Preconditions: Local disk D temp.bucket as S3 bucket name or any other name defined by the user. In this case, the corresponding changes should be made in the script. ::: Switching to Save As in IE11 and using desktop driver ARROWDOWN > ARROWDOWN > ENTER pull up 'Save As' and click context menu item pressTab() sendKeys(Keys.ARROW_DOWN) sendKeys(Keys.ARROW_DOWN) pressEnter() Switching to 'Save As' window switchTo().window(' CLASS: 32770 ') Entering full path of file to be saved def tempPath = 'D: _temp ' uniqueId = \" {UUID.randomUUID()}.json\" def savePath = tempPath uniqueId (' CLASS:Edit;INSTANCE:1 ').sendKeys(savePath).pressEnter() saving file content to a byte array file_content = downloadFileFromAgent(savePath) > File Download Example in Chrome The implementation does not guarantee correct parallel execution of the script on a single node. return (String) aDriver.executeScript( \" File dir = new File( \"\" dirPath.replace(' ', ' ') \" \"); n\" \" File files = dir.listFiles(); n\" \" if (files == null files.length == 0) { n\" \" return null; n\" \" } n\" \" n\" \" File lastModifiedFile = files 0 ; n\" \" for (int i = 1; i Object res = aDriver.executeScript( \"def bytes = new File('\" absoluteFilePath.replace(' ', ' ') \"').bytes; n\" \"return Base64.getEncoder().encodeToString(bytes);\", \"GROOVY\"); byte content = Base64.getDecoder().decode(res.toString()); return content; } getDownloadedFile = {RemoteWebDriver desktopDriver, String downloadDir, String latestFilePath > String downloadFilePath = getLastModifiedFileName(desktopDriver, downloadDir); Wait until new file appears. int maxCounter = 1000; boolean downloadStarted = downloadFilePath != null && !downloadFilePath.equals(latestFilePath); while (!downloadStarted && maxCounter > 0) { Thread.sleep(10); downloadFilePath = getLastModifiedFileName(desktopDriver, downloadDir); downloadStarted = downloadFilePath != null && !downloadFilePath.equals(latestFilePath); maxCounter ; } Lets wait until latest file is not tmp or crdownload. if (downloadStarted) { maxCounter = 300; downloadFilePath = getLastModifiedFileName(desktopDriver, downloadDir); String fileExtension = org.apache.commons.io.FilenameUtils.getExtension(downloadFilePath); boolean isDownloading = \"crdownload\".equals(fileExtension) \"tmp\".equals(fileExtension); while (isDownloading && maxCounter > 0) { Thread.sleep(100); downloadFilePath = getLastModifiedFileName(desktopDriver, downloadDir); fileExtension = org.apache.commons.io.FilenameUtils.getExtension(downloadFilePath); isDownloading = \"crdownload\".equals(fileExtension) \"tmp\".equals(fileExtension); maxCounter ; } if (!isDownloading) { return getFileContent(desktopDriver, downloadFilePath); } } throw new Exception(\"Some issues during file download\"); } > "},{"version":"10.0","date":"Aug-02-2019","title":"handle-javascript-alerts","name":"Handling JavaScript alerts","fullPath":"iac/rpa/developer-guide/handle-javascript-alerts","content":" Handle alerts, JavaScript alerts and popup boxes in WebDriver Alert is a pop up window that comes up on screen. There are many user actions that can result in an alert on screen. For example, user clicked on a button that displayed a message or may be when you entered a form, HTML page asked you for some extra information. In this article, we will learn handling of alerts, JavaScript alerts and popup boxes. :::tip Test page for alerts – https: rpa tutorial.s3.amazonaws.com trainings alert.html ::: Alerts are different from regular windows. The main difference is that alerts are blocking in nature. They will not allow any action on the underlying webpage if they are present. So if an alert is present on the webpage and you try to access any of the element in the underlying page, you will get following exception: UnhandledAlertException: Modal dialog present UnhandledAlertException: unexpected alert open To reproduce this exception, you can use this code: Types of alerts JavaScript provides mainly following three types of alerts: Simple alert document.alert(\"This is a simple alert\"); or alert(\"This is a simple alert\"); Confirmation alert var popuResult = confirm(\"Confirm pop up with OK and Cancel button\"); Prompt alert var person = prompt(\"Do you like RPA \", \"Yes No\"); Handle alerts using WebDriver RPA driver provides us with an interface called Alert: switchTo().alert(). Alert interface gives us the following methods to deal with the alert: accept() – accepts the alert dismiss() – dismisses the alert getText() – gets the text of the alert sendKeys() – writes some text to the alert Simplified API methods, which do not require switching to alert: confirm() – accepts (clicks \"Yes\" or \"Ok\") in the existing confirmation dialog (javascript 'alert' or 'confirm') confirm(String expectedDialogText) – the same as previous. expectedDialogText if not null, checks that confirmation dialog displays this message (case sensitive) dismiss() – dismisses (clicks \"No\" or \"Cancel\") in the existing confirmation dialog (javascript 'alert' or 'confirm'). dismiss(String expectedDialogText) – the same as previous. expectedDialogText if not null, checks that confirmation dialog displays this message (case sensitive) Handling JS alerts: button').click() sleep(2000) switchTo().alert().sendKeys('sure') confirm() > "},{"version":"10.0","date":"Aug-06-2019","title":"handle-iframes","name":"Handling iFrames","fullPath":"iac/rpa/developer-guide/handle-iframes","content":" How to handle IFrame IFrames with Robotics WebDriver Iframe is an HTML document embedded inside an HTML document. Iframe is defined by an tag in HTML. With this tag, you can identify an iFrame while inspecting the HTML tree. Here is a sample HTML code of a HTML page which contains two iFrames: :::tip Sample Iframe test page https: rpa tutorial.s3.amazonaws.com trainings iframe iframe test.html ::: We will use this to learn iFrame handling logic. Before starting, we have to understand that to work with different iFrames on a page we need to switch between these iFrames. To Switch between iFrames we have to use the driver’s switchTo().frame command. You can switch to an iframe in the following ways: switchTo.frame(int frameNumber) – pass the frame index and driver will switch to that frame switchTo.frame(string frameNameOrId) – pass the frame element Name or ID and driver will switch to that frame switchTo.frame(WebElement frameElement) – pass the frame web element and driver will switch to that frame switchTo().parentFrame() – driver will switch to it parent frame if it exists driver.switchTo().defaultContent() – driver will switch to the main page Let's see how each of these work but before that we have to know answers to the following questions: – What is an frame index – How to get total number of frames on a webpage How to find total number of iFrames on a webpage There are two ways to find total number of iFrames in a web page. First by executing a JavaScript and second is by finding total number of web elements with a tag name of iFrame. Here is the code using both these methods: This code sample will find only iframes that are on the top level. Nested iframes will be ignored. To find nested iframes, you need to switch to their child iframes. See the example below: Recursive iframe search int size = elements.size() elements.each { switchTo().frame(it) frames = (byTagName('iframe')) size = countFrames(frames) switchTo().parentFrame() } return size; } def iframeElements = (byTagName('iframe')) int totalFrames = countFrames(iframeElements) println(\" Number of iframes on the page (top level): {numberOfFrames}\") println(\" The total number of iframes (all levels): {totalFrames}\") > Switch to Frames by Index Index of an iFrame is the position at which it occurs in the HTML page. In the above example we have found total number of iFrames. In the sample page we have two IFrames on top level, and one nested iframe, index of iFrame starts from 0. Refer to the image below. To switch to 0th iframe we can simple write driver.switchTo().frame(0). Here is the sample code: open('https: rpa tutorial.s3.amazonaws.com trainings iframe iframe test.html') switchTo().frame(0) Switch to frames by name Now, if you take a look at the HTMLcode of iFrame you will find that it has a Name attribute. Name attribute has a value iframe0. We can switch to the iFrame using the name by using the switchTo().frame(“iframe0″) command. Here is the sample code: open('https: rpa tutorial.s3.amazonaws.com trainings iframe iframe test.html') switchTo().frame('frame0') Switch to frame by ID Similar to the name attribute in the iFrame tag we also have the ID attribute. We can use that also to switch to the frame. All we have to do is pass the id to the switchTo command like this SwitchTo().frame(“IF1″). Here is the sample code open('https: rpa tutorial.s3.amazonaws.com trainings iframe iframe test.html') switchTo().frame('IF0') Switch to frame by WebElement Now we can switch to an iFrame by simply passing the iFrame WebElement to the switchTo().frame() command. First, find the iFrame element using any of the locator strategies and then passing it to* switchTo* command. Here is the sample code: open('https: rpa tutorial.s3.amazonaws.com trainings iframe iframe test.html') iframeElement = (byId('IF0')) switchTo().frame(iframeElement) Switching back to main page from Frame There is one very important command that will help us to get back to the main page. Main page is the page in which two iFrames are embedded. Once you are done with all the task in a particular iFrame you can switch back to the main page using switchTo().defaultContent(). Here is the sample code which switches the driver back to main page. open('https: rpa tutorial.s3.amazonaws.com trainings iframe iframe test.html') iframeElement = (byId('IF0')) switchTo().frame(iframeElement) switching to a nested iframe switchTo().frame(0) switching to the main page switchTo().defaultContent() How to interact with elements inside an Iframe Now let's learn how to interact with elements inside an iFrame. Once we have switched to a particular iFrame everything else after that can be done using regular WebDriver command. Lets first create a hypothetical test case, here are the steps that we would take in our test. Switch to the first frame. See the image above. Find the First Name and Last name elements. Fill some value in the First name and Last name fields. Switch to the nested frame, get its text. Switch to the second frame. Find the search field and perform search. Try to click elements in another iframe. Once you switch to the frame, you can access the html elements that are inside the frame. Any attempt to access the elements which are inside iFrame without switching to that ifFrame will result in WebDriver exception. At the end of this script, we will reproduce the exception for learning purpose. Also, notice that once you have switch to the frame, the find element commands are exactly same as what we would use normally. Interacting with elements inside iframes "},{"version":"10.0","date":"Aug-06-2019","title":"hashicorp-vault","name":"Work with Hashicorp Vault using Vault token","fullPath":"iac/rpa/developer-guide/hashicorp-vault","content":" Get Vault token To work with Hashicorp Vault (Vault below) after SPA installation, you should go to the server where you deploy your vault and get the vault token, that is {DEPLOY DIR} vault keys vault keys.json Vault token structure: { \"keys\": \"fde831972c68746da5caf235a512ea06a4d1ea9bc7131d89445aecb5ea28725f52\", \"df4b0c5e3a9d1461be917c54eda4dd8470b91e518d7c95ca3978fce519b40dff02\", \"b39886cc2bc40fd18c9de87e99efd47e0b1e8911401c16876b6fc4557ec8e88693\", \"449c2afc42ad974721b158995a806f3673bb7f5a77057334539fd4aedeaf3f93ff\", \"825de25e569d9a95703c9718deb48a409ed1b505acf8520493a36a88075d6fe111\" , \"keys_base64\": \" egxlyxodG2lyvI1pRLqBqTR6pvHEx2JRFrsteoocl9S\", \"30sMXjqdFGG kXxU7aTdhHC5HlGNfJXKOXj85Rm0Df8C\", \"s5iGzCvED9GMneh me UfgseiRFAHBaHa2 EVX7I6IaT\", \"RJwq EKtl0chsViZWoBvNnO7f1p3BXM0U5 Urt6vP5P \", \"gl3iXladmpVwPJcY3rSKQJ7RtQWs FIEk6NqiAddb ER\" , \"root_token\": \"s.JJ3zcUOsv4bSmgu6Xnb4Fa5v\" } You should take root _token and use it for work with the vault. Work with Vault from Bot сonfiguration The next example shows how to work with the vault API from Bot configuration, and put secrets into vault or get secrets from vault. For more documentation about Vault API, see this article. When working with Vault API, fill data in the .JSON format when trying to POST data to vault. When trying to GET data from Vault, get a response in the .JSON format. Expand to see the example configuration SAP {\"status\": \"successful\"} Work with Vault in PowerShell PUT Secrets into Vault by ALIAS Create header object for set Vault token headers = New Object \"System.Collections.Generic.Dictionary String , String \" headers.Add('X Vault Token',{vault_token}) Set security settings for HTTPS request Net.ServicePointManager ::SecurityProtocol = \"tls12, tls11, tls\" Make secret data for fill to the Vault body = '{\"usr2\":\"pwdzxcgfef875 \",\"usr3\":\"pwdKLFJNemkfe%9\"}' make POST request with header where we set Vault token and body where we set data to fill into the Vault Response = Invoke WebRequest 'https: {host}:{vaultport} v1 secret {secretalias}' Method 'POST' Headers headers Body body print response into console echo Response GET Secrets from Vault by ALIAS Create header object for set Vault token headers = New Object \"System.Collections.Generic.Dictionary String , String \" headers.Add('X Vault Token',{vault_token}) Set security settings for HTTPS request Net.ServicePointManager ::SecurityProtocol = \"tls12, tls11, tls\" make GET request with header where we set Vault token Response = Invoke WebRequest 'https: {host}:{vaultport} v1 secret {secretalias}' Method 'GET' Headers headers print response into console echo Response convert Response content to JSON object aliasobj = Response.Content ConvertFrom Json AsHashtable; print passwords for users by user name echo aliasobj.data.usr1; echo aliasobj.data.usr2; echo aliasobj.data.usr3; Start RDP with Vault credentials from command line Start RDP in Powershell To start RDP in PowerShell with Vault credentials (rdp _connection.ps1), see the code below. param( string vaulttoken, string username, string secret_alias) Create header object for set Vault token headers = New Object \"System.Collections.Generic.Dictionary String , String \" headers.Add('X Vault Token',{vault_token}) Set security settings for HTTPS request Net.ServicePointManager ::SecurityProtocol = \"tls12, tls11, tls\" make GET request with header where we set Vault token Response = Invoke WebRequest 'https: {host}:{vaultport} v1 secret {secretalias}' Method 'GET' Headers headers convert Response content to JSON object aliasobj = Response.Content ConvertFrom Json AsHashtable; get password for users by user name password = aliasobj.data. user_name; cache credentials in windows security storage for server cmdkey add: serveraddress user: username pass: password start RDP session mstsc v: server_address Use RDP in bot agent To use RDP with Vault credentials in bot agent, do as follows. Edit the file {INSTALL _DIR} wfagent conf wfagent hub.yml Code to edit is as follows: id: rdp0 expression: \"cmd c start wait node0.RDP\" directory: \"rdp\" enabled: \" {environment.node0_enabled:true}\" Change each active RDP block in processes. The edited code is as follows: id: rdp0 expression: \"cmd c start wait powershell rdpconnection.ps1 vaulttoken {vaulttoken} username {username} secretalias {secret_alias}\" enabled: \" {environment.node0_enabled:true}\" tag: \"rdp\" "},{"version":"10.0","date":"Aug-06-2019","title":"inspecting-tools-for-desktop-automation","name":"Inspecting tools for desktop automation","fullPath":"iac/rpa/developer-guide/inspecting-tools-for-desktop-automation","content":" Web applications do not always confine themselves to working entirely on the web. Sometimes they need to interact with the desktop to do things like downloads & uploads. Automating these sorts of workflow is tricky in using Web Driver. Web driver is confined to automating browsers, so desktop windows are out of scope. The article describes Inspector as the WorkFusion tool used to inspect desktop applications and leverage desktop automation by providing information from a specified window (title, class, instance) and all its controls (edit boxes, check boxes, list boxes, combos, buttons, status bars). Inspector is a built in tool to explore properties and values of objects located in the application which you automate with your recording, providing functionality for quick element selector search. Selectors can be extracted only with the help of specific tools, one of which is Inspector as a part of WorkFusion Studio. :::tip The usual workflow contains the following steps: inspect UI controls check selectors apply selectors ::: General Overview To perform actions in any desktop application, a bot has to find particular elements to interact with (type, click, focus, etc.). For that purpose, a selector, or a unique link to a window or UI control, is used. :::note A selector is a unique property or the combination of several unique properties related to windows or UI controls. It is captured as a property name and a property value. The result is inserted into the Selector field to let the bot know which object to click. The most commonly used selector properties are enumerated in the table below. ::: Property Description CSS selector Object selector XPath selector CLASS class .TextBlock CLASS:TextBlock * @class='TextBlock' TITLE title .ApplicationWindowtitle=\"Calendar\" CLASS:ApplicationWindow; TITLE:Calendar * @class='ApplicationWindow' NAME instance name .TextBlockname=\"calendarLabel\" CLASS:TextBlock; NAME:calendarLabel * @class='TextBlock' TEXT text attribute .TextBlocktext=\"2019\" CLASS:TextBlock; TEXT:2019 * @class='TextBlock' INSTANCE instance number .TextBlockinstance=\"48\" CLASS:TextBlock; INSTANCE:48 * @class='TextBlock' The class and title properties are used to define both applications and UI controls, while others are applied for UI controls only. Launch Inspector To open Inspector, do as follows. Press the Start Inspector button () on the RPA Recording toolbar, or Use Ctrl Shift I, or Go to Window > Inspector > Start Inspector. Once you invoke Inspector, its window appears on the screen. ​(1) Browser contains either all the currently running applications, or the list of UI controls when exploring the selected window. ​(2) Result List displays the list of all properties related to the selected window or UI control. ​(3) Selector is the field to hold a unique selector. ​(4) Type allows to switch between selector types. On selecting an item in the Type filter, the selector string is rebuilt to match the given notation. There are three selector types you can choose from: CSS, e.g., .Button name=\"Maximize\" Object, e.g., CLASS:Button; NAME:Maximize XPath, e.g., *@class='Button' Inspect UI Controls You can inspect UI controls by different ways: capture (inspect) controls by hovering over them select controls in the Browser tree as in developer tools check how UI controls work in applications by setting commands test if controls match given selectors by using the search function To start exploring application components, select the application you want to inspect in Browser. :::tip In case you launched an application after launching Inspector, click the Refresh Browser button to update the list. ::: If a process is run by the system or other user, it cannot be inspected. Then, you are notified about a necessity either to run Inspector as Administrator or disable the user account control. See more details :::tip Inspection of SAP GUI desktop client differs from the usual inspecting workflow. To see more information on the topic, refer to SAP GUI Desktop Automation. ::: Capture Inspector provides the Capture option enabling you to enter the inspection mode and explore properties of application components on hovering over them. Click Capture. Hover the cursor over a window control. It gets highlighted directly in the application. Click the desired window control to select it. The target control opens in the Inspector window automatically, with its properties displayed in Result List and a selector value available in the Selector field. :::note There are cases when you need to click a control without selecting it, e.g., to explore other application options or switch to another application. Use the black menu as a prompt. To click the control without selecting, use the left mouse click Ctrl. To cancel capturing and exit the inspection mode, press Esc. ::: Explore in Browser In case it is challenging to get the right selector by capturing, e.g., it is hidden in the application UI, or you do not want to click through the application, you can switch between controls directly in the Browser area. Double click the application or process in Browser, or click Show UI Components in Result List. Navigate in Browser using the mouse or keyboard arrows. The selected component gets highlighted in the application window. Its properties are available in Result List together with the corresponding selector in the Selector field. To exit the inspection mode, click **Back to Applications and Windows* in Browser*. Test windows and UI controls Apart from highlighting, you can send different actions to any desired application or window control in order to emulate bot's behavior and make sure it functions as expected. To check that the bot is able to actually interact with the application and its components (e.g., Notepad), do as follows. To test interaction with the application, select the application in Browser, right click, and select one of the commands, e.g., Type (\"hello\"). The command is performed in the application window. To test if the application components respond, double click the application in Browser, or press Show UI Componentsin Result List. Select any of the components in Browser, right click, and select one of the commands, e.g., Click. The command is performed for the selected component. :::tip To view hide applications and elements within them, press Expand Collapse in front of the application or process you are interested in. You can also click the Expand All and Collapse All buttons in the Browser options. ::: The set of actions applied to applications is different from those applied to UI controls. Window commands UI control commands ActivateTypePress keysMaximize minimizeClose kill FocusMove mouseClick double click right clickType set textPress keys Inspect windows You can also create custom window selectors and assess them against all open windows. On the Applications and Windows screen, click on a needed window to get its selector. :::tip The inspecting procedure is the same as for UI controls. ::: Test selectors Before putting a selector to bot task configuration, it is recommended to test if it works correctly. To do that, search for the UI control matching the given selector. Copy your selector into the Selector field and press Enter. If the selector exists, Inspector finds the corresponding window control and selects it in Browser with detailed information in Result List. The target control is highlighted in the application. Inspector performs a check for selector uniqueness and supports collections of selectors corresponding to the search criteria. If there is more than one selector with the same attributes in the search results, switch between selectors by clicking the Back and Forward arrows until you find the needed selector. :::tip Mind to open the application the selector belongs to, as the search is performed among UI controls of the selected application. Otherwise, the selector will not match any components. If you still cannot find your selector, make sure you copied it correctly. Also, the Instance and Text attributes may facilitate the search. ::: Apply selectors As soon as the target component is found and tested, you can copy it to WorkFusion Studio for further usage either in Record or Code perspectives. Copy to Record perspective (UI control) Press Ctrl A to select your unique object selector. Press Ctrl C to copy. Open your recording in Record perspective, select an action from Actions Library and add it to Actions Flow. In the action properties window, select Target > Window control > Selector and press Ctrl V. The unique object selector value can also be used for Selector of the Click Mouse action to assign to a specific control in the application. Copy to Record perspective (window) Press Ctrl A to select your unique object selector. Press Ctrl C to copy. To use the window selector, add the Window action to your actions flow. Select Enter window selector (from Inspector) and paste (Ctrl V) the copied selector into the field. :::tip Mind that you can access Inspector directly on the Window action screen by clicking Open Inspector to get window selector. ::: Copy to Code perspective To copy a selector to bot task configuration, switch to Code perspective and paste your selector into the code. Get value from field in Windows application There are two ways of getting a value from a window control. Before you start to use selectors in the recording, keep in mind to switch to the target application with the help of the Window action. Way 1 Create a recorder variable of type String. Open Inspector and find a selector of the necessary field in the application. Apply the Click Mouse action > Window control. Use the Enter Keystrokes (Ctrl C) action to copy the value from the field (if needed, use Enter Keystrokes (Ctrl A) before). Use the Clipboard action to set the copied value into your variable. Way 2 Create a recorder variable of type String. Open Inspector and find a selector of the necessary field in the application. To get the text from the field, use the Script action with the code as in the example below. Special Cases Inspect Office 365 applications Special attention should be paid when retrieving selectors of UI controls related to Office 365 applications. After Microsoft software updates, attribute values may change, and selectors previously used in your scripts will not be unique or stop working at all. Thus, Excel cell selectors differ in various versions of Microsoft Office 365. Previous version of Microsoft Excel Newer version of Microsoft Excel CLASS:DataItem; NAME:A1 CLASS:DataItem; NAME:\"A\" 1 Be careful with the selectors related to UI controls of Office 365 applications, as Microsoft software updates can change selector values. To be on the safe side, it is recommended either to reinspect UI controls, or to use RPA Recorder for Excel actions and other Office 365 related actions. Focus on Hidden Elements If a UI control is collapsed and cannot be visible on the application screen, you can easily expand it to get a selector. It is especially useful when inspecting complex trees or tabs. To open a collapsible UI control, do as follows. Right click on any hidden element within the tree and select Focus. On expanding the element, copy its selector to use in your actions flow. You can also activate a hidden element using the focus() method in your script: ('treehiddenelement_selector').focus(). Export to XML You can use XML export features to build selectors. Either all windows or specific component trees can be exported. Export windows To export all the windows, click Export Windows to XML in Browser. View the XML code on the Source tab in WorkFusion Studio. Export component tree You can get the whole UI component tree and manipulate it in your text editor or IDE. Select the application or process in Browser. Double click it, or click Show UI Components. Press Export XML. View the XML code on the Source tab in WorkFusion Studio. Troubleshooting Inspect.exe If for some reason Inspector does not provide information about an UI control, you can use Inspect for that. Inspect (Inspect.exe) is a Windows based tool that enables you to select any UI element and view the element's accessibility data. You can view Microsoft UI Automation properties and control patterns, as well as Microsoft Active Accessibility properties. Inspect also enables you to test the navigational structure of the automation elements in the UI Automation tree, and the accessible objects in the Microsoft Active Accessibility hierarchy. Inspect is installed with the Windows Software Development Kit (SDK). (It is also available in previous versions of Windows SDK.) It is located in the bin folder of the SDK installation path (Inspect.exe). Follow the links for more details about this tool: https: msdn.microsoft.com en us library windows desktop dd318521(v=vs.85).aspx using inspect.aspx usinginspect) https: stackoverflow.com questions 34760513 how to install the inspect tool on windows 10 Wrong Java version In case you are unable to setup RPA Package Java as default Java for IE (jdk jre.exe installer can do that anyway), you'll need to load Java with the build version not higher than the RPA Package one. Having RPA Package Java version 8u92 and IE pointed to 8u101, Inspector is unable to connect to 8u101 JVM and display elements selectors. Even when a selector has been found through different Machine, RPA Agent is unable to find it. Installing 8u91 and pointing to IE fixes that issue. Thus, Inspector starts working and selectors are successfully executed. No Element Found Sometimes after Windows security updates have been installed, your RPA agent stops finding elements and Inspector doesn't detect any jJva processes for inspection. In the command window, instead of ...inspector.controller.JvmInspectorControllerImpl.updateVirtualMachineTree :73 Updating JVM Tree... ...common.utils.VirtualMachineConnector VMConnection.connect :140 Using jvmAgentPath = c: rpa rpa grid dependency rpa jvmagent.jar ...inspector.controller.JvmInspectorControllerImpl.connectToJvmAndUpdateVmTree :236 Successfully connected to: 23652 there are only updating JVM tree attempts. bad ...inspector.controller.JvmInspectorControllerImpl.updateVirtualMachineTree :73 Updating JVM Tree... ...inspector.controller.JvmInspectorControllerImpl.updateVirtualMachineTree :73 Updating JVM Tree... ...inspector.controller.JvmInspectorControllerImpl.updateVirtualMachineTree :73 Updating JVM Tree... You may start troubleshooting by looking at c: rpa java bin jconsole.exe. When you see that JConsole detects external processes but can't get any information, the warning message is as follows: The management agent is not enabled on this process. That means that JConsole can't access PID files created by a java process. To resolve the issue, follow the instruction. Close all java programs. Check where %TEMP% and %TMP% environment variables are pointed. Clean your TEMP folder (most likely c: Users {username} AppData Local Temp). Run settings rdp.reg from the RPA setup folder. Re login into your PC. Run all the java programs, test with JConsole, and then use Inspector. Check if c: Users {username} AppData Local Temp hsperfdata _{username} has files corresponding to Java process IDs. "},{"version":"10.0","date":"Aug-02-2019","title":"notepad-with-save-as-dialog","name":"Notepad with save as dialog","fullPath":"iac/rpa/developer-guide/notepad-with-save-as-dialog","content":" "},{"version":"10.0","date":"Aug-02-2019","title":"javascript-executor","name":"JavaScriptExecutor","fullPath":"iac/rpa/developer-guide/javascript-executor","content":" JavaScript is the preferred language inside the browser to interact with HTML document object model (DOM). This means that a Browser has a JavaScript implementation in it and understands the JavaScript commands. You can disable it using browser options in your browser. JavaScript is still used by web driver to perform some actions. For example, the XPath element search is implemented in JavaScript for Internet Explorer, to overcome the lack of XPath engine in this browser. Understanding JavaScript is fun and it can enable you to do lot of cool things that otherwise you may find tricky. Let's understand how WebDriver gives you a method called executeScript() which executes the JavaScript in context of the loaded browser page. executeJavaScript(script, arguments) First thing to know that the JavaScriptExecutor comes separately and also comes under the WebDriver but both do the same thing. Expand to see the usage example td > span').click();\" \"document.querySelector('body > table > tbody > tr:nth child(1) > td:nth child(2) > a:nth child(9)').click();\" ) sleep(2000) invoking alert in JS and passing parameters to script def messages = 'Hello ', 'from ' executeJavaScript(\"alert(arguments0 arguments0 arguments 1 )\", messages, 'JS') sleep(2000) dismiss() getting a node value using JS element = executeJavaScript(\"return document.querySelector('body > p').textContent;\") > Evaluate XPaths You can use javascripts to find an element by Xpath. This is a brilliant way to skip internal Xpath engines. Here is what you have to do. value = executeScript(\"return document.evaluate( ' body div iframe' ,document, null, XPathResult.FIRSTORDEREDNODE_TYPE, null ).singleNodeValue;\") Note the document.evaluate() command. This is the Xpath evaluator in JavaScript. Signature of the function is: document.evaluate( xpathExpression, contextNode, namespaceResolver, resultType, result ); Explanation of parameters: xpathExpression: A string containing the XPath expression to be evaluated contextNode: A node in the document against which the xpathExpression should be evaluated, including any and all of its child nodes. The document node is the most commonly used. NamespaceResolver: A function that will be passed any namespace prefixes contained within xpathExpression which returns a string representing the namespace URI associated with that prefix. resultType: A constant that specifies the desired result type to be returned as a result of the evaluation. The most commonly passed constant is XPathResult.ANY _TYPE which will return the results of the XPath expression as the most natural type. result: If an existing XPathResult object is specified, it will be reused to return the results. Specifying null will create a new XPathResult object Find element You can find any element on page using something similar to: element = (byId('some id')) You can do the same thing using JavaScript: element = executeScript(\"return document.getElementById('gsc i id1');\") You will get the element in the element variable. Change element attribute style You can change the style property of elements to change the rendered view of the element. For example, this is how you can create a border around the about me element on a web page. executeScript(\"document.getElementById('text 4').style.borderColor = 'Red'\"); Coloring elements can also help you take screenshots with visual markers to identify problematic elements. Get element attributes You can get any value of valid attributes of the element. (byId('some id')).getAttribute('Class') This code will get the value of class attribute of an element with id = gsc i id1. In Javascript same thing can be executed using: className = executeScript(\"return document.getElementById('gsc i id1').getAttribute('class');\") Total frames in browser Let's say you want to know total number of frames inside a webpage, including the Iframes. You can't do it directly, you may need to create your own logic to parse and find frames. However in JavaScript is simple and is done like this. numberOfIframes = executeScript(\"return document.frames.length;\") :::tip See more info here Handling iFrames ::: Add element into DOM If you want to add an element into the DOM, you can use the following syntax: executeScript(\"var btn = document.createElement('BUTTON'); document.body.appendChild(btn);\") Window size The size of inner browser window is the size of the window in which you see the web page. height = executeScript(\"return window.innerHeight;\") width = executeScript(\"return window.innerWidth;\") Navigate to different page executeScript(\"window.location = 'https: wikipedia.org'\") Generate alert pop window executeScript(\"alert('hello world');\") Click action executeScript(\"arguments 0 .click();\", element) Refresh browser executeScript(\"history.go(0)\") Get web page InnerText sText = executeScript(\"return document.documentElement.innerText;\").toString() Get web page title sText = executeScript(\"return document.title;\").toString() Scroll page executeScript(\"window.scrollBy(0,150)\") Similarly you can execute practically any JavaScript command. executeAsyncScript(script, arguments) It allows to execute JavaScript code asynchrously. Execute script asynchronously and return result "},{"version":"10.0","date":"Aug-06-2019","title":"object-selectors","name":"Object selectors","fullPath":"iac/rpa/developer-guide/object-selectors","content":" DESKTOP DRIVER ONLY When automating desktop applications, you can provide a unique window or element locator using Object selectors. For example: (\" CLASS: JToggleButton \").click() If you set multiple attribute:value pairs separated by semicolon in square brackets, the target element (or window) should meet all these descriptors (AND logic): (\" CLASS: JToggleButton; TOOLTIP: JTable demo; TEXT: Start \").pressEnter() If an attribute value contains the semicolon sign, you need to escape it using another semicolon. ( TEXT: Some text ;; with semicolons; CLASS: JTable ).click() Attributes for desktop apps No. Criteria Description 1 NAME element instance name: NAME: Toolbar 2 REGEXPNAME regular expression for element instance name: REGEXPNAME: d{3} 3 TITLE element title: TITLE: Notepad 4 REGEXPTITLE regular expression for element title: REGEXPTITLE: a zA Z g 5 CLASS element class: CLASS: JToggleButton 6 REGEXPCLASS regular expression for element class: REGEXPCLASS: J. 7 TEXT element text property: TEXT: Submit 8 REGEXPTEXT regular expression for element text property: TEXT: S a z 9 INSTANCE UI control instance number: INSTANCE: 42 10 CSS jQuery like CSS styles search in desktop UI tree controls: CSS: .JPanel.JButton See more in CSS Selectors. 11 XPATH XPath search in desktop UI tree controls: XPATH: element @class=Child3 element @class=JButton See Effective XPath for more details. 12 TOOLTIP element tooltip text for desktop apps: TOOLTIP: Open Java specific attributes No. Criteria Description 1 PATH path to a tree control element, descendant texts are separated by pipe: PATH: Music Classical Beethoven Quartets 2 LEVEL search in a specific tree level by its sequential number starting from 0 (JTree, DTree, etc.): LEVEL:3 3 TEXT _LEVEL search in a specific tree level by its text: LEVEL:Quartets 4 CELL search Java table cell by its coordinates (row, column): CELL:2,1 5 CELLTEXT search Java table cell by its text: CELLTEXT: Invoice Amount 6 CELLREGEXP search Java table cell by its text regular expression: CELLREGEXP: Invoice.* 7 TABINDEX search a tab in a tabbed panel by its sequential number starting from 0: TABINDEX: 6 "},{"version":"10.0","date":"Aug-02-2019","title":"launch-applet-and-insert-extract-data","name":"Launch applet and insert-extract-data","fullPath":"iac/rpa/developer-guide/launch-applet-and-insert-extract-data","content":" The example shows how to automate a simple applet. make sure that you've configured IE for applets Configuring Java Applets for RPA sendKeys('{TAB 3}') the syntax is described here SendKeys Keystrokes Guide "},{"version":"10.0","date":"Aug-02-2019","title":"page-objects-design","name":"Page objects design","fullPath":"iac/rpa/developer-guide/page-objects-design","content":" When you automate web page, you need to refer to elements within that web page in order to click links and determine what's displayed. However, if you manipulate the HTML elements directly your code will be brittle to changes in the UI. A page object wraps an HTML page, or fragment, with an application specific API, allowing you to manipulate page elements without digging around in the HTML. The basic rule of thumb for a page object is that it should allow a software client to do anything and see anything that a human can. It should also provide an interface that's easy to program to and hides the underlying widgetry in the window. So to access a text field you should have accessor methods that take and return a string, check boxes should use booleans, and buttons should be represented by action oriented method names. The page object should encapsulate the mechanics required to find and manipulate the data in the gui control itself. A good rule of thumb is to imagine changing the concrete control in which case the page object interface shouldn't change. Despite the term \"page\" object, these objects shouldn't usually be built for each page, but rather for the significant elements on a page. So a page showing multiple albums would have an album list page object containing several album page objects. There would probably also be a header page object and a footer page object. That said, some of the hierarchy of a complex UI is only there in order to structure the UI such composite structures shouldn't be revealed by the page objects. The rule of thumb is to model the structure in the page that makes sense to the user of the application. Similarly if you navigate to another page, the initial page object should return another page object for the new page. In general page object operations should return fundamental types (strings, dates) or other page objects. There are differences of opinion on whether page objects should include assertions themselves. Advocates of including assertions in page objects say that makes it easier to provide better error messages, and supports a more TellDontAsk style API. Advocates of assertion free page objects say that including assertions mixes the responsibilities of providing access to page data with assertion logic, and leads to a bloated page object. I've described this pattern in terms of HTML, but the same pattern applies equally well to any UI technology. I've seen this pattern used effectively to hide the details of a Java swing UI and I've no doubt it's been widely used with just about every other UI framework out there too. Concurrency issues are another topic that a page object can encapsulate. This may involve hiding the asynchrony in async operations that don't appear to the user as async. It may also involve encapsulating threading issues in UI frameworks where you have to worry about allocating behavior between UI and worker threads. Page objects can also be used to provide a scripting interface on top of an application. Usually it's best to put a scripting interface underneath the UI, that's usually less complicated and faster. However with an application that's put too much behavior into the UI then using page objects may make the best of a bad job. (But look to move that logic if you can, it will be better both for scripting and the long term health of the UI.) Patterns that aim to move logic out of UI elements (such as Presentation Model, Supervising Controller, and Passive View) make it less useful to automate through the UI and thus reduce the need for page objects. Page objects are a classic example of encapsulation they hide the details of the UI structure and widgetry from other components. It's a good design principle to look for situations like this as you develop ask yourself \"how can I hide some details from the rest of the software \" As with any encapsulation this yields two benefits. I've already stressed that by confining logic that manipulates the UI to a single place you can modify it there without affecting other components in the system. A consequential benefit is that it makes the client code easier to understand because the logic there is about the intention and not cluttered by UI details. Implementation Notes PageObjects can be thought of as facing in two directions simultaneously. Facing towards the developer of a automation they represent the services offered by a particular page. Facing away from the developer, they should be the only thing that has a deep knowledge of the structure of the HTML of a page (or part of a page) It's simplest to think of the methods on a Page Object as offering the \"services\" that a page offers rather than exposing the details and mechanics of the page. As an example, think of the inbox of any web based email system. Amongst the services that it offers are typically the ability to compose a new email, to choose to read a single email, and to list the subject lines of the emails in the inbox. How these are implemented shouldn't matter to the automation. Because we're encouraging the developer of a automation to try and think about the services that they're interacting with rather than the implementation, PageObjects should seldom expose the underlying WebDriver instance. To facilitate this, methods on the PageObject should return other PageObjects. This means that we can effectively model the user's journey through our application. It also means that should the way that pages relate to one another change (like when the login page asks the user to change their password the first time they log into a service, when it previously didn't do that) simply changing the appropriate method's signature will cause the automations to fail to compile. Put another way, we can tell which automation would fail without needing to run them when we change the relationship between pages and reflect this in the PageObjects. One consequence of this approach is that it may be necessary to model (for example) both a successful and unsuccessful login, or a click could have a different result depending on the state of the app. When this happens, it is common to have multiple methods on the PageObject: public class LoginPage { public HomePage loginAs(String username, String password) { ... clever magic happens here } public LoginPage loginAsExpectingError(String username, String password) { ... failed login here, maybe because one or both of the username and password are wrong } public String getErrorMessage() { So we can verify that the correct error is shown } } The code presented above shows an important point: the automations, not the PageObjects, should be responsible for making assertions about the state of a page. For example: public void messagesAreReadOrUnread() { Inbox inbox = new Inbox(driver); inbox.assertMessageWithSubjectIsUnread(\"I like cheese\"); inbox.assertMessageWithSubjectIsNotUnread(\"I'm not fond of tofu\"); } The code could be re written as: public void messagesAreReadOrUnread() { Inbox inbox = new Inbox(driver); assertTrue(inbox.isMessageWithSubjectIsUnread(\"I like cheese\")); assertFalse(inbox.isMessageWithSubjectIsUnread(\"I'm not fond of tofu\")); } Of course, as with every guideline there are exceptions, and one that is commonly seen with PageObjects is to check that the WebDriver is on the correct page when we instantiate the PageObject. This is done in the example below. Finally, a PageObject need not represent an entire page. It may represent a section that appears many times within a site or page, such as site navigation. The essential principle is that there is only one place in your automation with knowledge of the structure of the HTML of a particular (part of a) page. Summary The public methods represent the services that the page offers Try not to expose the internals of the page Generally don't make assertions Methods return other PageObjects Need not represent an entire page Different results for the same action are modelled as different methods Expand to see the example public class LoginPage { private final WebDriver driver; public LoginPage(WebDriver driver) { this.driver = driver; Check that we're on the right page. if (!\"Login\".equals(driver.getTitle())) { Alternatively, we could navigate to the login page, perhaps logging out first throw new IllegalStateException(\"This is not the login page\"); } } The login page contains several HTML elements that will be represented as WebElements. The locators for these elements should only be defined once. By usernameLocator = By.id(\"username\"); By passwordLocator = By.id(\"passwd\"); By loginButtonLocator = By.id(\"login\"); The login page allows the user to type their username into the username field public LoginPage typeUsername(String username) { This is the only place that \"knows\" how to enter a username driver.findElement(usernameLocator).sendKeys(username); Return the current page object as this action doesn't navigate to a page represented by another PageObject return this; } The login page allows the user to type their password into the password field public LoginPage typePassword(String password) { This is the only place that \"knows\" how to enter a password driver.findElement(passwordLocator).sendKeys(password); Return the current page object as this action doesn't navigate to a page represented by another PageObject return this; } The login page allows the user to submit the login form public HomePage submitLogin() { This is the only place that submits the login form and expects the destination to be the home page. A seperate method should be created for the instance of clicking login whilst expecting a login failure. driver.findElement(loginButtonLocator).submit(); Return a new page object representing the destination. Should the login page ever go somewhere else (for example, a legal disclaimer) then changing the method signature for this method will mean that all automations that rely on this behaviour won't compile. return new HomePage(driver); } The login page allows the user to submit the login form knowing that an invalid username and or password were entered public LoginPage submitLoginExpectingFailure() { This is the only place that submits the login form and expects the destination to be the login page due to login failure. driver.findElement(loginButtonLocator).submit(); Return a new page object representing the destination. Should the user ever be navigated to the home page after submiting a login with credentials expected to fail login, the script will fail when it attempts to instantiate the LoginPage PageObject. return new LoginPage(driver); } Conceptually, the login page offers the user the service of being able to \"log into\" the application using a user name and password. public HomePage loginAs(String username, String password) { The PageObject methods that enter username, password & submit login have already defined and should not be repeated here. typeUsername(username); typePassword(password); return submitLogin(); } } :::note See the example of page objects framework usage – http: toolsqa.wpengine.com selenium webdriver selenium automation hybrid framework ::: "},{"version":"10.0","date":"Aug-06-2019","title":"post-incoming-invoice","name":"Posting incoming invoice into SAP","fullPath":"iac/rpa/developer-guide/post-incoming-invoice","content":" :::note The use case shows how to create and post an incoming invoice into the SAP system. ::: Business case Daimler purchased Brake Pads from Bosch. Materials have been delivered. Bosch sent an Invoice for those Brake Pads to Daimler. Daimler is saving the invoice in the system. This step is to be processed by the Bot. Manual scenario Open SAP GUI and log in. Start the MIROtransaction. Enter the transaction code MIRO into the OK Code* field and press *Enter. Select Invoice and specify the basic data. Set the current date in the Invoice date field. Set a purchase order number into the Purchase Order field (e.g., 4500000005 4500000012). Set the amount for Amount. If not the full purchase order amount is invoiced (e.g., not 600, but 300 for 2 pieces), change the amount in both header and items. The total items amount should be equal to the header amount. Click on the Simulate button. When a financial document appears, click on Post. :::tip Alternatively, a Bot may press Save instead of clicking on Simulate and Post in a sequence. ::: Create purchase order To create a purchase order as a copy of the existing one, do as follows. Start the ME21N transaction. Select Document Overview On. Select Purchase Orders within selection variants. On the next selection screen, select the number of Purchasing document (=4500000002) and click Execute F8. Drag the purchase order and drop into the basket. Save the purchase order by pressing Ctrl S and take the purchase order number from the status bar. Create good receipt On delivering the purchased goods to the storage, a good receipt for the purchase order is created. Start the MIGO transaction. Select Goods Receipt and Purchase order as a reference document. Type the purchase order number (the one created above) and press Enter. Specify the Storage location (= 0001) and check Item OK in the bottom left corner. Click on the Check button and ensure that the message \"Document is O.K.\" is displayed in the status bar. Click on the Post button. The created document is displayed in the status bar as a part of the message. :::tip Alternatively, you may press Save instead of clicking on Check and Post in a sequence. ::: Automated scenario :::tip To be able to run the script, provide your own credentials to enter SAP, defined as ` and `. ::: Expand to see the code sample of how to post incoming invoice into SAP pw= language=EN system=ERD client=100 trace=0' open(\"C: Program Files (x86) SAP FrontEnd SAPgui {command}\") sleep(5000) * CONTINUE WITHOUT ENDING ANY OTHER LOGONS * if (Window.windowHandles(\" CLASS: 32770; TITLE:License Information for Multiple Logons \").isEmpty()) { log.info(\"Window License Information for Multiple Logons not found\") } else { window(\" CLASS: 32770; TITLE:License Information for Multiple Logons \") (\" CLASS:GuiRadioButton; NAME:MULTILOGONOPT2 \").click() (' CLASS:GuiButton; NAME:btn 0 ; ').click() log.info(\"Window License Information for Multiple Logons closed\") } * CREATE PRODUCT ORDER * window(\" CLASS:SAPFRONTENDSESSION; TITLE:SAP Easy Access \") (\" CLASS:GuiOkCodeField \").setText(\"ME21N\").pressEnter() Document Overview enable window(\" CLASS:SAPFRONTENDSESSION; TITLE:Create Purchase Order \") documentOverviewSelector = \" CLASS:GuiButton; INSTANCE: 18 \" if ( (documentOverviewSelector).getText() == \"Document Overview On\") { log.info(\"Overview panel is disable\") (documentOverviewSelector).click() } else { log.info(\"Overview panel is enable\") } (\" CLASS:GuiShell; NAME:shell 0 ; \").click(10, 10) sleep(5000) sendKeys(Keys.DOWN) pressEnter() window(\" CLASS:SAPFRONTENDSESSION; TITLE:Purchasing Documents \") (\" CLASS:GuiCTextField; NAME:SP 00014 LOW \").setText(\"4500000002\") sendKeys(Keys.F8) Drag and drop purchase doc window(\" CLASS:SAPFRONTENDSESSION; TITLE:Create Purchase Order \") (\" CLASS:TreeNode \").click(50, 10) (\" CLASS:GuiShell; NAME:shell 0 ; \").click(50, 5) (\" CLASS:GuiButton; NAME:btn 11 \").click() Close system message window(\" CLASS: 32770; TITLE:Save Document \") (\" CLASS:GuiButton; NAME:SPOP VAROPTION1 \").click() Get PO number window(\" CLASS:SAPFRONTENDSESSION; TITLE:Create Purchase Order \") status = (' CLASS:GuiStatusPane; NAME:pane 0 ; ').getText() log.debug(status) poNumber = (status =~ ( d ) )0 log.debug(\"PO number: {poNumber}\") * GO BACK TO EASY ACCESS * (\" CLASS:GuiButton; NAME:btn 15 \").click() * CREATE Goods Receipt * window(\" CLASS:SAPFRONTENDSESSION; TITLE:SAP Easy Access \") (\" CLASS:GuiOkCodeField \").setText(\"MIGO\").pressEnter() window(\" CLASS:SAPFRONTENDSESSION; REGEXPTITLE:Goods Receipt Purchase Order(.*) \") (\" CLASS:GuiCTextField; NAME:GODYNPRO PO_NUMBER \").setText(poNumber).pressEnter() (\" CLASS:GuiTab; NAME:OKGOITEMDESTINAT. \").click() (\" CLASS:GuiCTextField; NAME:GOITEM LGOBE \").setText(\"0001\") (\" CLASS:GuiCheckBox; NAME:GODYNPRO DETAIL_TAKE \").setSelected(true) (\" CLASS:GuiButton; NAME:btn 7 \").click() assert (' CLASS:GuiStatusPane; NAME:pane 0 ; ').getText() == \"Document is O.K.\" (\" CLASS:GuiButton; NAME:btn 23 \").click() log.info( (' CLASS:GuiStatusPane; NAME:pane 0 ; ').getText()) * GO BACK TO EASY ACCESS * (\" CLASS:GuiButton; NAME:btn 15 \").click() * GO TO MIRO transaction * window(\" CLASS:SAPFRONTENDSESSION; TITLE:SAP Easy Access \") (\" CLASS:GuiOkCodeField \").setText(\"MIRO\") (\" CLASS:GuiButton; NAME:btn 0 \").click() * SET INVOICE DATE (\" CLASS:GuiCTextField; NAME:INVFO BLDAT \").setText(currentDate) * SET COMPANY CODE sendKeys(Keys.F7) window('class=\" 32770\"') (\" CLASS:GuiCTextField \").setText(companyCode) sendKeys(Keys.ENTER) * SET PURCHASE ORDER window(\" CLASS:SAPFRONTENDSESSION; TITLE:Enter Incoming Invoice: Company Code {companyCode} \") (\" CLASS:GuiCTextField; NAME:RM08M EBELN \").setText(poNumber) sendKeys(Keys.ENTER) * SET AMOUNT * (\" CLASS:GuiCheckBox \").setSelected(true) amount = (\" CLASS:GuiTextField; NAME:RM08M DIFFERENZ \").getText().replace(\" \", \"\") log.info(amount) sys.defineVariable(\"amount\", amount) (\" CLASS:GuiTextField; NAME:INVFO WRBTR \").setText(amount) * POST ORDER * (\" CLASS:GuiButton; NAME:btn 11 ; \").click() status = (' CLASS:GuiStatusPane; NAME:pane 0 ; ').getText() log.info(status) documentNumber = (status =~ ( d ) )0 log.info(\"Document {documentNumber} created.\") > ⇪ Back to SAP automation examples "},{"version":"10.0","date":"Aug-09-2019","title":"project-configuration","name":"Project configuration","fullPath":"iac/rpa/developer-guide/project-configuration","content":" Use command line mvn archetype:generate DinteractiveMode=false DarchetypeGroupId=com.workfusion.intakequickstart DarchetypeArtifactId=intake quickstart archetype DarchetypeVersion=0.3 DgroupId= DartifactId= Dversion= Select and copy the above mentioned command. Replace ` with your project group ID (for example, com.microsoft.automation`). Replace ` with your artifact ID (for example, msft suite automation`). Replace ` with your artifact version (for example, 1.0 dev`). Run your command. For example, the archetype command can look like this: mvn archetype:generate DinteractiveMode=false DarchetypeGroupId=com.workfusion.intakequickstart DarchetypeArtifactId=intake quickstart archetype DarchetypeVersion=0.3 DgroupId=com.microsoft.automation DartifactId=msft suite automation Dversion=1.0 dev Import project into WorkFusion Studio Go to File > Import. Choose Maven > Existing Maven Projects. Specify a location to you project director and click Finish. Run the Maven install command at project root. As soon as Maven downloads all the required dependencies, a successful build message should be printed. Configure project classpath Right click on the rpa bcb project and choose Properties. Navigate to Java Build Path and choose Order and Export. Select WorkFusion Libraries and move them on Top. Make sure the checkbox is checked. Open Transaction Supplier Example.xml and execute **Run As > Bot Task**. As soon as you see the EXECUTION SUCCESS message in the console, you are ready to create your own components. Project structure In the generated stub, you can find the following classes: AppExample — example of Intake Engine created for this example. It comes with AppBuilderExample which is a class providing nice building API. Also two modules ModuleExample and CipherModule are coming together, which are used in AppBuilderExample class. TransactionEncryptionProcessor — example of TransactionProcessor interface, which downloads documents, encodes them, and uploads back to S3. TransactionEncryptionProcessorTest — example of Unit Testing class for TransactionEncryptionProcessor class. TransactionSupplierExample — example TransactionSupplier class which generates N transactions. InMemoryBase64Encoder — example of Cipher service. Encrypt Documents Example.xml — Bot config which shows how to invoke TransactionEncryptionProcessor from XML. Transaction Supplier Example.xml — Bot Config which shows how to invoice TransactionSupplierExample from XML. Display XML files for BCB project You may find that XML files are not displayed for the rpa bcb project after cloning to WorkFusion Studio from Git. To fix the issue, do as follows. Open the .project file located at the head of the project. Check the similarity of the `` lines and add the following line: com.workfusion.studio.mcb.mcbProjectNature Your file contents will look like follows. .project file: configs org.eclipse.jdt.core.javabuilder org.eclipse.m2e.core.maven2Builder com.workfusion.studio.mcb.mcbProjectNature org.eclipse.jdt.core.javanature org.eclipse.m2e.core.maven2Nature Restart WorkFusion Studio. In case, nothing changed, add the settings folder to the project. "},{"version":"10.0","date":"Aug-06-2019","title":"robotics-plugins","name":"Robotics plugins","fullPath":"iac/rpa/developer-guide/robotics-plugins","content":" Robotics plugins provide possibilities to click through desktop or web applications using commands in Web Harvest scripts. Configuration of where the script is executed is done on the RPA Windows server side. robotics flow ` plugin might contain tags as well as tags. For each ` tag, new roboticsDriver will be created. Robotics plugins structure looks as follows. Attributes Name Default Description fleet shared used for distribution of tasks and their routing. For more information, refer to Task Distribution. :::warning Mind that dynamic fleet attributes are not supported yet. Thus, the code with a dynamic fleet attribute will not work. ::: Example with dynamic fleet attribute – not supported yet: SAP robot ` plugin manipulates with the roboticsDriver object (defined in the name attribute) and should have a ` child section. :::note All the ` sections inside one ` are executed on the same RPA Bot Unit one after another. ::: Attributes Name Required Default Description driver yes Defines the driver that will be used for executing robotics instructions: universaldesktopchromeinternet explorerMicrosoftEdge (BETA VERSION)firefox (BETA VERSION) name no Driver name – can be any string; driver is accessible in the `` section by this name. close on completion no true Specifies whether to close the driver window on plugin completion. If you set this attribute to false, a robot session is not closed, and RPA Bot Unit remains occupied. start in private no false If true, the browser will start in private mode. block images no false Blocks images to speed up the page load. maximize on startup no true Maximizes the browser window on startup. Robotics Object Creation: sDriver = roboticsDriver.getWrappedObject(); this object is provided in the context of robot plugin defined in the \"name\" attribute You can include one or several ` sections (Groovy language) inside the ` plugin and manipulate with the roboticsDriver object. Robot Plugin Example capability With the help of ` plugin, you can define requirements or parameters for the machine where your bot task is executed. For example, you have defined `. When this config is grabbed from a pool, the following actions are performed: Bot Manager tries to find all available RPA Bot Relays that support the \"browserName\" capability. If RPA Bot Relay(s) is found, Bot Manager tries to find all available RPA Bots that have the \"browserName\":\"firefox\" capability in configuration JSON. If RPA Bot Relay(s) is found, Bot Manager routes your config to be executed on this particular Bot supporting the \"browserName\":\"firefox\" capability. If RPA Bot Relay or Bot is not found, the config execution fails with an exception. Capability is a child plugin of ` or ` plugins; you can create several capabilities. Attributes Name Required Default Description name yes Capability name value no Capability value To learn more, see capability reference. :::note Capability inside ` plugin overrides the capability inside ` plugin (see the code example). ::: Capability usage: ... Configuring keyboard, mouse, and window modes There are cases when you need to automate some legacy applications or software which responds to user interactions with significant delays. Such applications do not support fast typing and can render their window content after some pause. To handle such situations, you can tweak the delays for robot mouse clicks, key press events, and window activations. Robot keyboard ability to set speed and delay for typing Example: Setting delay between key presses and duration of the key press: Robot mouse To prevent undesired hover events while bot mouse movements, you can disable the default Smart Mouse Move mode and switch to the Teleport mode. You can also tweak mouse click delays in case your application has a big response time. **Example: Setting mouse click delays and disabling Smart Mouse Move (enabling Teleport)**: Windows wait delay You can also alter for how long a bot should pause after a successful window related operation. Default value is 300 milliseconds. Example: Wait 500 milliseconds after each window focusing: All options are enumerated in the table below. Option name Description Default value Example WinWaitDelay Alters how long a script should briefly pause after a successful window related operation. Time in milliseconds to pause. 300 optionsMap = 'WinWaitDelay':500 SendKeyDelay Alters the delay between typing consequent keystrokes. Time in milliseconds to pause. 55 optionsMap = 'SendKeyDelay':20 SendKeyDownDelay Alters the length of time a key is held down before released during a keystroke. For applications that take a while to register key presses (and many games) you may need to raise this value from the default. Time in milliseconds to pause. 20 optionsMap = 'SendKeyDownDelay':55 MouseSmartMove Specifies whetherrobot moves mouse smoothly (true),or just \"teleports\" cursor (false). true optionsMap = 'MouseSmartMove':false MouseClickDelay Alters the length of the brief pause in between mouse clicks. Time in milliseconds to pause. 300 optionsMap = 'MouseClickDelay':10 MouseClickDownDelay Alters the length a click is held down before release. Time in milliseconds to pause. 30 optionsMap = 'MouseClickDownDelay':20 SendCapslockMode Specifies if the library should store the state of capslock before a Send() function and restore it afterwards.IGNORE (0) = don't store restore.IGNORE AND RESTORE (1) = (default) store and restore.CONSIDER (2) = don't store restore, but consider capslock state when sending capital noncapital keystrokes by adding shift key. 1 optionsMap = 'SendCapslockMode':1 DumpsDirectory Specifies the directory to store dump files. . (dot means current application directory) optionsMap = 'DumpsDirectory':'C: dumps' ClipboardRetryDelay Specifies delay in milliseconds between consequent attempts to access clipboard. 10 optionsMap = 'ClipboardRetryDelay':30 AcceptableSizeDeviation Specifies the maximum control size and position deviation (expected vs actual) in pixels while actual control geometry is still considered as expected. 3 optionsMap = 'AcceptableSizeDeviation':4 PreTypingDelay Ability to set delay before typing starts. Some applications require time to become ready to accept keyboard events after their window is activated. If keyboard events are sent immediately after window activation, then a few first events might be lost. That would cause skipping first characters during typing. To handle such situations, you can configure the delay between window activation and typing start. 200 optionsMap = 'PreTypingDelay':200 Desktop driver specific capabilities SEARCHALLWINDOWS Default value is true By default, the desktop driver can only switch to windows opened in the driver session. To be able to switch to any existing window or window launched outside the driver session, the SEARCHALLWINDOWS capability must be set to true. CLOSEALLWINDOWS Default value is false By default, the desktop driver closes all the windows to which it switched (including external windows that were opened outside the driver session). To close external windows, the CLOSEALLWINDOWS must be set to true. imageSimilarityThreshold Default value is 0.7 Can take double values from 0.0 to 1.0. For surface based automation, it is possible to set the imageSimilarityThreshold capability, which can help solve complicated cases where images should strictly match. See more details here. Capabilities for proxy usage If your IE browser is configured to use proxy, you need to add the following code into your bot tasks. See more details here. :::warning You can add only one of the `` blocks from the code block above. ::: "},{"version":"10.0","date":"Aug-06-2019","title":"receive-info-about-sap-table","name":"Receiving information about SAP tables","fullPath":"iac/rpa/developer-guide/receive-info-about-sap-table","content":" :::note The use case shows how to get information about SAP tables. There are two scenarios each of them performing almost the same actions but with different tools. ::: Scenario A Open SAP GUI and log in. Start the SE16 transaction. Enter the transaction code SE16 into the OK Code field and press Enter. In the menu, select Settings > User Parameters. Go to the Data Browser tab, check ALV Grid Display, and press Enter the OK button. Enter DD02L into the Table Name field. Press Number of Entries. The number you see on the screen is the number of all entries that can be copied and used in your workflow. Thus, you can view the number of tables that meet your selection criteria. Enter a table name into TABNAME (e.g., TRANSP or T005 T006) and press Number of Entries. Click on the Execute button to view the selected entries. Press Ctrl Y for multiple selection and select all the entries in the TABNAME column. You can copy the entries and paste them, for example, into an Excel document. The same can be done for any column in the table. Scenario B Open SE16 via the SAP menu. Go to SAP Menu > Tools > ABAP Workbench > Overview. In the menu, select Settings > User Parameters. Go to the Data Browser tab, check SE16 Standard List, and press Enter the OK button. Press the button next to Table name to open the input help for the field. Press Information System. Type DD into the Table name field and press OK*. Find DD02L in the search results, focus on it, and press OK. Table name should be filled with DD02L. Click on the Table Contents button to go to the selection screen. Repeat step 6 from scenario A. Ensure that number of entries is the same. Press the search help button next to TABCLASS LOW. Double click the TRANSP entry and repeat step 7 from scenario A. Make sure the number of entries is the same as in scenario A. Press Multiple Selection for the TABNAME field. Go to the Select Ranges tab and enter T005 and T006 into lower and upper limits respectively. Press Copy and repeat step 7 from scenario A. Make sure the number of entries is the same as in scenario A. Press Execute and repeat steps 8 9 from scenario A. Make sure the data is the same. ⇪ Back to SAP automation examples "},{"version":"10.0","date":"Aug-06-2019","title":"robotics-scripts-examples","name":"RPA scripts examples","fullPath":"iac/rpa/developer-guide/robotics-scripts-examples","content":" Get available and busy RPA nodes number We can call RPA Grid Console page and parse required totals out: Expand to see the code sample 0 && (tmp.contains(\"explorer\") tmp.contains(\"executed\"))) allIENum ; } List busyAutoit = ieDriver.findElements(By.xpath(\" div @type='browsers' p a\")); int busyAutoitNum = 0; for (WebElement element : busyAutoit) { tmp = element.getAttribute(\"class\"); if (tmp.length()>0) busyAutoitNum ; } List busyIE = ieDriver.findElements(By.xpath(\" div @type='browsers' p img\")); int busyIENum = 0; for (WebElement element : busyIE) { tmp = element.getAttribute(\"class\"); if (tmp.length()>0 && tmp.equals(\"busy\")) busyIENum ; } > {allIENum.toString()} {busyIENum.toString()} {allAutoitNum.toString()} {busyAutoitNum.toString()} :::important Constant string HUB CONSOLE URL = \"http: 127.0.0.1:4444 grid console\" ; should be changed to pull page from URL where RPA Bot Relay is installed in your environment. ::: Escape special symbols for sending to RPA Node When we need to send a password to RPA, we should escape special symbols. See this sample function in the example below. Expand to see the code sample Web scrape using RPA, XML transformation, XPath Sample below demonstrates an approach to HTML page parsing and scrapping using RPA. Expand to see the code sample {searchcasenumber} {first_name} {last_name} {dob} false false Duval County Clerk of Courts https: core.duvalclerk.com CoreCms.aspx mode=PublicAccess false {profilePage} {xmlValue} {profilePageCase} true Catch RPA processing issues This sample shows how to catch RPA processing issue and use it inside a business process like a true false flag site _chaged – flag is true by default. Expand to see the error flag example {searchcasenumber} {first_name} {last_name} {dob} false false Duval County Clerk of Courts https: core.duvalclerk.com CoreCms.aspx mode=PublicAccess true Improve switchTo action in case of window opened by another driver This sample shows how to improve the switchTo action, when you try to switch to a window opened by other driver or opened before. :::note The switchTo action is executing for a long time. ::: To improve this, before running the switchTo action, you should set the page load time to a small value. :::tip Precondition: Notepad should be opened. ::: Expand to see the switchTo() improved sample Set timeout for switchTo action The switchTo().window(\"windowName\", \"timeoutInMilis\") method is added to minimize waiting time in case window was not opened previously. This method does not use the pageLoadTimeout option, instead it uses the timeoutInMilis parameter. In each case, the value of this timeout is unique, i.e., when using it one should understand how long it takes to wait for a specific window. You can use one value pageLoadTimeout (which is large) for all driver.swithTo().window(), but in this case there will be big time delays when the requested window was not opened. :::tip Precondition: Notepad should be opened. ::: Expand to see the set timeout for switchTo action example "},{"version":"10.0","date":"Jul-29-2019","title":"rpa-compatibility-matrix","name":"RPA compatibility matrix","fullPath":"iac/rpa/developer-guide/rpa-compatibility-matrix","content":" RPA Server should be compatible with the following operating systems and software. RPA Windows IE Chrome Firefox Desktop 10.0 Windows Server: 2012, 2012 R2, 2016 Windows: 7, 8, 8.1, 10 11 55 46.0.1 JVM app, Win32, WPF, UWP, SAP 9.3 Windows Server: 2012, 2012 R2, 2016 Windows: 7, 8, 8.1, 10 11 55 46.0.1 JVM app, Win32, WPF, UWP, SAP 9.0 Windows Server: 2012, 2012 R2, 2016 Windows: 7, 8, 8.1, 10 11 55 46.0.1 JVM app, Win32, WPF, UWP 8.4 Windows Server: 2012 Windows: 7, 8, 10 10, 11 51 55 JVM app 8.2 Windows: 7, 8, 10 10, 11 51 55 JVM app 8.0 Windows: 7, 8, 10 10, 11 51 53 46.0.1 JVM app 7.6 Windows: 7, 8, 10 10, 11 51 53 46.0.1 JVM app "},{"version":"10.0","date":"Aug-09-2019","title":"rpa-environment-setup","name":"RPA dev environment setup","fullPath":"iac/rpa/developer-guide/rpa-environment-setup","content":" Requirements Windows OS (32 bit or 64 bit version) with Administrator access Internet Explorer 11 At least 1 GB of free space :::note To perform full RPA Windows Server installation in case of several Windows machines, see RPA Windows Server Installation. ::: Download and run RPA installer In order to set up development environment, download the RPA installer zip archive. A link to the package for manual installation is provided as well, though this type of installation is not recommended to use. Installer: RPAInstaller 10.0.0.15.zip Package: RPA_10.0.0.15.zip Unzip the RPAInstaller {version}.zip to your local temp folder and run the RPAInstaller.exe file. RPA installer file Follow the RPA Installer steps and allow the program to make changes to your device. RPA Setup Wizard Allow to make changes Accept the License Agreement and proceed. Welcome to RPA Setup Wizard License Agreement Set the Destination Folder for RPA installation and wait until the installation process is finished. RPA installer destination folder RPA setup complete :::warning Do not select existing folders with sensitive content (C: Windows) or root folders (C: D: ) because their content will be deleted when you choose to uninstall. ::: Check Internet Explorer settings To use your IE browser for automation, go through the checklist in Configuring Internet Explorer for RPA. Configure Java applet settings If you need to automate Java applets, go through the checklist in Configuring Java Applets for RPA. Start Bot Relay Make sure the QuickEditMode is disabled for all users. To do that, right click on Command Prompt and select Properties > Options. QuickEditMode disabled Open C: RPA rpa grid and run the start unit hub1.bat. RPA grid folder In the opened console screen, check that there are no exceptions, and it says \"Selenium Grid hub is up and running\". Selenium Grid hub is up and running In the same folder (C: RPA rpa grid), run start unit node1.bat in order to start Bot and register it to Bot Relay. In the opened console screen, check that there are no exceptions and it says \"The node is registered to the hub and ready to use\". The node is ready to use To check that everything works as expected, open the following link in your browser: The Grid Console screen with configured WebDrivers looks as follows. Grid Console Configure WorkFusion Studio Before you start WorkFusion Studio, link it to PRA package. Go to WorkFusion Studio installation directory and open the configuration folder. Open config.ini and add the workfusion.rpa.package.path property that defines the path to the RPA package, e.g.: workfusion.rpa.package.path=c: RPA :::note The default path to the RPA package is C: RPA. If you selected another directory, make sure to specify it correctly. ::: Check installation To check that the environment is set up as expected, either run RPA configs or samples from Examples Library in WorkFusion Studio. If configuration is done properly, a browser or a desktop application opens, and the script works as expected. Run RPA configs from WorkFusion Studio As soon as you configure RPA environment, you can test it by executing RPA scripts from our bot samples.zip package in WorkFusion Studio. Start WorkFusion Studio and create a new WorkFusion Project. Create a new bot task configuration and paste the code from one of the example package files. Open the Run Configurations menu and check that the Bot Relay URL option is http: localhost:15444 wd hub. Click Run and make sure the bot task config execution is succeeded. Run samples from Examples Library You can also check WorkFusion Studio performance by running samples from Examples Library. Start WorkFusion Studio. Go to Help > Welcome to WorkFusion Studio and select any example to run, e.g., Dates translation. WorkFusion Studio samples Click the Play recording button. Check Bot Logs As soon as your RPA script is executed, view the Bot Agent and Bot Unit consoles where successful actions and errors are displayed. Errors could be caused by: incorrect RPA script (window selector or timeout) incorrect RPA Bot Unit setup desktop application failure, etc. :::note In most cases, you need to restart Bot Unit or Bot Agent (to do that, close the console and open the appropriate batch file again), modify the script code, or modify the Bot Agent Bot Unit setup. ::: "},{"version":"10.0","date":"Aug-06-2019","title":"rpa-dev-best-practices","name":"RPA development best practices","fullPath":"iac/rpa/developer-guide/rpa-dev-best-practices","content":" Wait of condition Bad practice: use Thread.sleep Reason: stop executing this thread during this time (2000, 3000, etc.). Best practice: use WebDriverWait Reason: executing thread is not stopped, flexible tool for control of waiting, ignoring exception, control of polling interval, etc. Close driver manually If script with the robot plugin fails, all drivers are closed. Processing of errors is done if needed. Bad practice: close driver manually Best practice: throw the error further, and plugin closes all drivers "},{"version":"10.0","date":"Aug-09-2019","title":"rpa-training-plan","name":"RPA training plan","fullPath":"iac/rpa/developer-guide/rpa-training-plan","content":" Set up RPA dev environment DAY 1 Set up RPA Dev Environment Set up WorkFusion Studio Common RPA dev environment problems observed in training facilities Missed installation steps. Make sure you followed each step from setup guide. Proxy. Check corporate PROXY settings and use it in WorkFusion Studio. Make sure you have PROXY exceptions for localhost 127.0.0.1 so IDE can communicate to local RPA Bot Relay. IE Settings. Check that \"Private Mode\" checkbox is unchecked for ALL zones. Also IE should start from Blank page, not Previously Opened Pages. RPA Grid. Make sure Hub and at least 1 Node are running. Check Console to make sure you have available Drivers of required type. Architecture DAY 1 Robotics – RPA Automation Technology Classification :::tip Assignment: Review Examples. Download actual RPA Bot Task examples – bot samples.zip Review examples in your local Eclipse IDE. Run them. Ask questions. ::: Web applications RPA Tools to find web application controls guide DAY 2 Take the XPath Tutorial and see Practical Examples. Learn Effective XPath. Use browser inspectors Use special tools ElementLocator, XPath Helper Learn element selectors :::note Assignment: browser dev tools locators Setup ElementLocator to Firefox. Setup XPath Helper to Chrome. Open cnn.com and using XPath get all Top Stories links all Top Stories titles Open http: toolsqa.wpengine.com automation practice form and using XPath get: all radio buttons Years of Experience Automation Tester checkbox from Profession all options from Continent drop down element of button ::: Learn Robotics API guide DAY 2 WebElement commands Browser navigation commands Work with CheckBox RadioButton DropDown MultipleSelect Get content of WebTables Robotics API Guide :::note Assignment: web table scraping Start from https: invoiceplane.workfusion.com. Log in as wf robot@mail.com freedom4ROBOTS. Navigate to Products > View Products. Parse Products table: Family SKU Product name Product description Price. Provide Products in Results Data. Put Products into Excel (use the list to excel plugin). Save Excel into S3 (save to temp.bucket bucket, create your _name folder in this bucket). ::: Advanced WebDriver Topics DAY 3 Execute JavaScript Drag and drop Mouse mover Handle JavaScript alerts Switch between browser windows and tabs Handle browser iFrames Webdriver wait commands Take screenshots ::: Assignment: RPA DataStores S3 Excel Apache POI Start from https: invoiceplane.workfusion.com. Log in as wf robot@mail.com freedom4ROBOTS. Get invoiceplane account from Secure Storage. Create Data Store manually by uploading your .CSV file for DS names use Invoices YOUR _NAME . Create 2 Invoices. Invoices should contain 3 and 5 Products come up with single table. Now it's OK to duplicate Products in Invoices table. In BPs Machine Task read data from Data Store you've created using plugin. Use this data in RPA script to populate into controls while automating Invoice creation. Create 2 Invoices. Notice that each Invoice creation requires two steps: General Invoice form. Client Invoice Date. Extended Invoice form: Payment Type: Paypal Add all Products you've linked in DB to your Invoice. Fields of Product Item Quantity Price Description Each invoice should have different combination of items Set total Discount to 10% Save your new Invoice Take screenshot of each invoice entered. Upload on S3. Put S3 link into Result Data. Open the View Invoices page. Scrap the Invoices data from page. Take screenshot of Invoices page. Upload on S3. Put S3 link into Result Data. All Invoices should be exported to .XLSX file using Apache POI library. Fields to export Status ID Created Due Date Client Name Amount Balance Save Excel into S3 (save to temp.bucket bucket, use your _name folder in this bucket). Put S3 link into Result Data. Simplified RPA API DAY 3 Simplified Robotics API Simplified API JavaDocs Desktop Automation Using AutoIt DAY 4 SendKeys keystrokes guide Examples Notepad with Save As Dialog Handle file download Automate file upload Automate basic authentication Calculator automation :::note Assignment: PRA Windows System Dialog S3 Start from https: invoiceplane.workfusion.com. Log in as wf robot@mail.com freedom4ROBOTS. Generate Reports > Invoice Aging report and save it to RPA Agent file system. Upload this file into S3 and delete from RPA Agent. Put S3 link into BP Result Data. ::: Java applets automation using Inspector DAY 4 Applets with Inspector Examples Launch applet and insert extract data Automate SwingSet App :::note Assignment: Automate Java applet application SwingSet application https: rpa tutorial.s3.amazonaws.com demo jfc SwingSet2 custom SwingSet2.html Parse textual table data from the JTable demo tab. Convert data into CSV. Save CSV into S3. Have link on CSV in result data of you BP run. For each value from column 'Favorite movie' search for movie and scrap data: Poster picture Year Actors Director Description Rating Budget. Prepare movies description as HTML table and send it with robotic script in email body from tuk.tuk.rpa@gmail.com work4WorkFusion into aharhots@gmail.com. ::: Mainframe automation using AutoIt DAY 5 Automate mainframe applications Automate command line :::note Assignment: Automate Mainframe Application Download and setup Windows SSH Telnet client PUTTY http: www.chiark.greenend.org.uk ~sgtatham putty download.html. Your RPA code should launch PUTTY and connect to WF playground Linux machine as putty.exe ssh traning@traning ssh.workfusion.com pw dEFgEmzL. Your task is to remotely generate SSH key with command: ssh keygen t rsa C \"your _email@example.com\". After you launch command, you will be asked questions your script should answer all of them (you can easily find guides of keys generation, for example, http: www.cyberciti.biz faq linux unix generating ssh keys). As a result, you will get PRIVATE and PUBLIC keys in files. Read contents from these files and put it into result data of your BP. ::: Large RPA Project Design DAY 5 Page objects design :::note Assignment: Design with Page Objects MCB Refactor your Create Invoices RPA script. Split it into multiple files containing corresponding Page Objects. Use Page Objects in main Controller. Configure MCB on your Eclipse. Start from Kickstart project Follow the MCB project structure to refactor your project with Page Objects. Deploy your package into your WF instance. Pull Bot Tasks into new BP. Test that all works. Send the URL of BP. ::: "},{"version":"10.0","date":"Aug-06-2019","title":"rpa-flow-details","name":"RPA flow details","fullPath":"iac/rpa/developer-guide/rpa-flow-details","content":" Start RPA Worker Establish RDP connection with Bot Manager Execute tasks "},{"version":"10.0","date":"Aug-06-2019","title":"sap-automation-examples","name":"SAP automation examples","fullPath":"iac/rpa/developer-guide/sap-automation-examples","content":" Here, you will find SAP automation examples that cover possible business scenarios and show some client UI functionality. The use cases give a general overview of scenarios. Your steps can vary depending on the SAP GUI version you are using. Posting incoming invoice into SAP Getting warehouse stock for specific material Getting all stock movements for specified period Creating dispute case Testing SAP ALV Receiving information about SAP tables Testing SAP controls "},{"version":"10.0","date":"Oct-14-2019","title":"rpa-windows-server-installation","name":"RPA Windows Server installation","fullPath":"iac/rpa/developer-guide/rpa-windows-server-installation","content":" :::note To install RPA Windows Server, we recommend to use an easier installation scenario described in this guide. ::: General information RPA deployment diagram RPA deployment diagram To learn more about deployment, see RPA Deployment. RPA folder structure As soon as you have downloaded the installer file, extract all the files from the RPAInstaller {version}.zip to any folder on your PC. The packages inside the folder are as follows. Package name Package usage autoit executes autoit scripts bot agent contains Bot Agent together with its configuration and logs filebeat contains Filebeat configuration and logs java executes Java processes (for 64 bit Windows) java _x86 executes Java processes (for 32 bit Windows) libs contains dll files used for native work of the system logs contains logs of Bot Relays, Bots, and Workers metricbeat contains Metricbeat together with its configuration and logs nginx contains Nginx configuration and logs registry contains registry files used while installation installer runs machine settings and user settings rpa grid performs configuration and runs Bot Relay and Bot files scripts collects logs from the package, contains scripts for both logs and configuration worker contains executable code Prerequisites Internet Explorer 11 The CA certificate must be imported into Trusted Root Certificates Authorities of Local Machine's Certificate TrustStore. See here for more information. Before configuring users, make sure your Internet Explorer browser is turned off and no Java processes are running. MS SQL, Integration and Application servers should be installed. RPA Windows Server installation process 1. Install RPA package :::important Install RPA as the Administrator user on a Windows machine. ::: Download the installer file. Extract all files from the RPAInstaller {version}.zip to any folder on your PC. Launch RPAInstaller.exe and follow the RPA Setup Wizard steps. Allow the program to make changes to your device. Accept the License Agreement and proceed. Set the Destination Folder for RPA installation and wait until the installation process is finished. :::note By default, the destination directory for RPA installation is C: RPA that will be referred to as `` below. ::: 2. Import and configure certificates The list of your certificates needed at this step is as follows. Certificate Usage Description ca.crt Filebeat Metricbeat Nginx Java Root CA certificate in the .pem format (the file may have .crt, .cer, .pem, or .cert extension). All the other certificates in the table must be signed by this Root CA that is imported into Java Truststore on all servers. The certificate file must include only Root cert, without any intermediate certs. rpa.crt > Rename to server.pem while configuration rpa.key > Rename to server.key while configuration. Nginx Certificate and its private key files in the .pem format. Certificate's CommonName or SANs must match all servers in the list of rpa_hostnames. The certificate file must include end certificate plus intermediate cert(s) if exist(s). Private key should not be password protected. mtls client.crt mtls client.key Nginx Certificates used to implement Mutual TLS authentication between services. vault _workfusion.p12 > Rename to workfusion.p12 while configuration. Secrets Vault Specific Secret Storage certificate used for authentication with Secrets Vault. :::tip For more information on certificates, see Prepare TLS certificates for installation. ::: Your trusted certificates are located in the certificates directory on Integration Server. To find them, follow the path opt workfusion wf _installer certificates . Import your root certificate file ca.crt into Local Machine's Certificate TrustStore > Trusted Root Certificates Authorities. Proceed to configuration of certificates for: Filebeat Metricbeat Secrets Vault Nginx Java 2.1 Filebeat Filebeat is a service responsible for sending content of specific log files. To get logs, Filebeat configuration should be done. Create the ssl folder in your filebeat folder and copy ca.crt into filebeat ssl. 2.2 Metricbeat Metricbeat is a service responsible for sending metrics data to the Logstash service. Create the ssl folder in the metricbeat folder and copy ca.crt into metricbeat ssl. 2.3 Secrets Vault Create the ssl folder in the bot agent folder and upload the vault_workfusion.p12 file to bot agent ssl. Rename it to workfusion.p12. Create the ssl folder in the worker folder and upload vault _workfusion.p12 to the worker ssl. Rename it to workfusion.p12. 2.4 Nginx Create the ssl folder in your nginx folder and copy ca.crt into nginx ssl. Upload SSL Certificate and SSL key files to the nginx ssl directory. Rename rpa.crt to server.pem and rpa.key to server.key. Upload the TLS certificate mtls client.crt and key file mtls client.key to the same directory. 2.5 Update Bot Agent keystore :::note RPA Bot Agent does not support SSL while Application Server does. In case of Server, RPA Bot Agent keystore needs to be updated. ::: Go to the Java bin directory java bin. Import the downloaded ca.crt to the RPA Java keystore. Run the command with your arguments from the java bin directory. keytool import alias localCA file \"path to ca.crt\" keystore \"path to cacerts\" storepass changeit where Arguments are as follows: Argument Description Example path to ca.crt the absolute path to downloaded ca.crt nginx ssl ca.crt path to cacerts absolute path to default RPA Java keystore java jre lib security cacerts changeit default password for RPA Java keystore 3. Configure users Before RPA Windows Server installation, users should have local profiles on RPA machine. :::note Run the following commands in PowerShell as Bot Master on a Windows machine. Bot Master is the user that triggers Bot Units. ::: Register Bot Master in Secrets Vault in Control Tower. To do that, add two secret entries with Alias for Bot Master user and Alias for Bot Master password. The Key field should be the same as Alias, while Value stands for either username or password. Bot Master User secret entry Bot Master Password secret entry Grant RPA full folder permissions to all created users – Bot Master, Bot Unit X. The RPA full folder is the folder where your RPA is installed. Make sure that Bot Master is a member of the Administrator group. Follow the link (https: www.ibm.com support knowledgecenter en SSZJPZ11.7.0 com.ibm.swg.im.iis.found.admin.common.doc topics wsisinstinstallprmssnsgrps_win5plus.html) to see more on configuring groups and permissions in Windows. 4. Enable RDS to use multiple bots on one server :::note Multiple RDP Sessions are configured on RPA Windows Server. ::: Configure as many RDS sessions as the number of multiple bots. Navigate to Server Manager > Manage > **Add Roles and Features**. The Add Roles and Features Wizard opens. Go to Server Selection > Server Roles > Remote Desktop Services > Next. Select Remote Desktop Session Host > Next > Add Features. On finishing, reboot your machine. 5. Verify Internet Explorer settings :::note Verification is required in case Internet Explorer is used. The settings are verified for each added user. For more details, go to Configuring Internet Explorer for RPA. ::: 6. Configure Nginx Nginx configuration is done to provide secure communication between RPA machines. To provide access to Nginx, configure Windows Firewall to define a port for connection. Go to Control Panel > Windows Defender Firewall. Select Advanced settings > Inbound Rules and click New rule to allow inbound secure communication for applications to use the SSL connection. Select TCP and define the local port 443 to allow the connection. Specify the connection name and click Finish. Go to nginx conf nginx.conf. Replace all {RPA _HOSTNAME} with a valid DNS name of your RPA Windows Server. Replace {APP HOST IP} with IP address of Application Server. Replace {RPA HOST IP} with IP address of your RPA Windows Server. To set up communication with Control Tower, add the following settings into nginx.conf. Replace {APP DNS NAME} with Application Server FQDN. mTLS client config for communication with Control Tower. Used by worker. server { listen 127.0.0.1:7080; location workfusion internal api { proxysslcertificate .. ssl mtls client.crt; proxysslcertificate_key .. ssl mtls client.key; proxysslciphers HIGH:!aNULL:!MD5; proxysslprotocols TLSv1.1 TLSv1.2; proxysslsession_reuse on; proxysslverify on; proxyssltrusted_certificate .. ssl ca.crt; proxysslverify_depth 2; proxypass https: {APPDNS_NAME}:7083 workfusion internal api; } location workfusion { deny all; return 403; } } To set up communication with BEP, add the following settings into nginx.conf. Replace {BEP MASTER DNS _NAME} with BEP Master Server FQDN. mTLS client config for communication with automl gateway service server { listen 127.0.0.1:9070; location automl gateway service { proxysslcertificate .. ssl mtls client.crt; proxysslcertificate_key .. ssl mtls client.key; proxysslciphers HIGH:!aNULL:!MD5; proxysslprotocols TLSv1.1 TLSv1.2; proxysslsession_reuse on; proxysslverify on; proxyssltrusted_certificate .. ssl ca.crt; proxysslverify_depth 2; proxypass https: {BEPMASTERDNSNAME}:9073 automl gateway service; } } 7. Configure Bot Agent For correct performance of Bot Units, you should configure Bot Agent environment parameters as well as to set up Bot Master autostart and Bot Agent startup. 7.1 Configure Bot Agent environment parameters For correct RPA environment work, set all the needed environment parameters. Replace parameters with {} brackets (be careful not to replace parameters in {} with like {rpa.rabbitmq.worker.queue.template}) in bot agent conf environment.yml. You have to specify the following parameters: {ZOOKEEPER_HOSTNAME} – hostname of Integration Server where the Zookeeper service in installed {ZOOKEEPER_PORT} – by default, 2181 {BOT MANAGER HOSTNAME} – hostname of Application Server where Bot Manager is installed {RPA _HOSTNAME} – RPA Windows Server hostname {LOGSTASH _HOSTNAME} – hostname of Integration Server where the Logstash service is installed Default environment parameters file looks as follows. spring: cloud: zookeeper: connect string: {ZOOKEEPERHOSTNAME}:{ZOOKEEPERPORT} environment: rpanginxport: 443 botmanagerhostname: {BOTMANAGERHOSTNAME} botmanagerport: 443 botmanagerscheme: https botmanagerpath_context: \"bot manager\" botmanageruser: ' {bot.manager.security.user.name}' botmanagerpassword: ' {bot.manager.security.user.password}' agent.hostname: {RPA_HOSTNAME} agent.port: 443 agent.scheme: https logstashhost: {LOGSTASHHOSTNAME} logstash_port: 4567 metricbeat_port: 4569 rabbitmqqueuefull_name: ' {rpa.rabbitmq.worker.queue.template}. {fleet}' worker_gavp: ' {rpa.default.worker.gavp}' workerclientid: ' {rpa.default.worker.client.id}' agentzookeeperconnectstring: {ZOOKEEPERHOSTNAME}:{ZOOKEEPER_PORT} server_address: 127.0.0.1 :::tip Mind that Bot Manager credentials are not visible in bot agent conf environment.yml. The credentials are set up and stored in Secrets Vault. ::: 7.2 Schedule Bot Agent startup on logon Scheduling of tasks can be performed using the task scheduler or advanced group policies. Bot Agent startup is scheduled in order to autostart an RDP session (Bot Master, Bot Unit X) by Bot Manager. One Bot Unit per server (VDI) Open the Windows console and enter the following commands: SchTasks Create SC onlogon TN \"bot agent BotMaster\" TR \" bot agent bin bot agent nordp.bat\" RU BotMaster IT F Multiple Bot Units per server (RDP in RDP) Configure tasks – one for Bot Master user and one for each Bot Unit. To do that, enter the following commands: SchTasks Create SC onlogon TN \"bot agent BotMaster\" TR \" bot agent bin bot agent master.bat\" RU BotMaster IT F SchTasks Create SC onlogon TN \"bot agent BotUnit1\" TR \" bot agent bin bot agent unit1.bat\" RU BotUnit1 IT F SchTasks Create SC onlogon TN \"bot agent BotUnitX\" TR \" bot agent bin bot agent unitX.bat\" RU BotUnitX IT F 7.3 Configure Bot Master autostart Bot Master autostart is set up in order to run RDP sessions. To allow autostart, it is necessary to provide correct RPA Windows Server hosts. Switch to the bot manager config directory on Application Server. Modify machines.yml and provide correct RPA Windows Server hosts. machines.hosts: test1.workfusion.com test2.workfusion.com test3.workfusion.com Restart Bot Manager. wfmanager restart bot manager 8. Configure Bot Units The configuration step includes configuring multiple Bot Units as well as Master Bot Agent. 8.1 Configure multiple Bot Units Configure Nginx for multiple Bot Units: Go to the nginx conf bot units folder. Make as many templates as the number of Bot Units you have to set up, e.g., 5 for 5 Bot Units. Copy unit.template to unit{UNIT ID}.conf where UNIT ID is a digit between 1...99 (if your **unit id* more than *9, you should remove the last 0** in the proxypass port). Open the template and replace all the {UNIT ID} placeholders with the same UNITID number you used to name the file. Modify or copy file bot agent conf units unit{Unit _id}.yml(Unit1.yml base example). Change unit_id and fleet. :::note shared is a default fleet. If your unit_id is more than 9, remove the last 0 in each port – maximum 99 units for 1 machine. ::: unit_id: 2 fleet: shared server.port: 1000 {2} logging.folder: logs unit {2} node_port: 1510 {2} hub_port: 1500 {2} worker_port: 1520 {2} bot.agent.context.path: unit {2} environment.agent.ns: \" {environment.agent.hostname}: {server.port}\" In case you need to have custom fleets to submit tasks to, specify fleet for each Bot Unit. Fleets are used to distribute tasks between RPA Workers. To learn more, refer to Task Distribution. unit_id: 3 fleet: SAP Create the start bat file for each Unit: bot agent bin bot agent unit{Unit _ID}.bat. Then change unitid. @ECHO OFF set unitid=2 cd %~dp0 set config=conf bootstrap.yml,conf environment.yml,conf units unit%unitid%.yml,conf bot agent unit.yml call bot agent.cmd run Unit%unitid% \"%config%\" 8.2 Configure Master Bot Agent Modify or copy bot agent conf bot agent master.yml. Add additional Unit processes for start. Default config part: processes: id: rdp1 expression: \"cmd c start wait mstsc unit.rdp v:127.0.0.1\" directory: \"rdp\" id: rpa.nginx expression: \"cmd c start wait start_nginx.bat\" directory: \".. nginx\" tag: \"nginx\" id: rpa.filebeat expression: \"cmd c start wait filebeat pipeline.bat {environment.agent.hostname} {environment.logstashhostname} {environment.logstashport}\" directory: \".. filebeat\" tag: \"filebeat\" The code example in case there are two Bot Units: processes: id: rdp1 expression: \"cmd c start wait mstsc unit.rdp v:127.0.0.1\" directory: \"rdp\" id: rdp2 expression: \"cmd c start wait mstsc unit.rdp v:127.0.0.2\" directory: \"rdp\" id: rpa.nginx expression: \"cmd c start wait start_nginx.bat\" directory: \".. nginx\" tag: \"nginx\" id: rpa.filebeat expression: \"cmd c start wait filebeat pipeline.bat {environment.agent.hostname} {environment.logstashhostname} {environment.logstashport}\" directory: \".. filebeat\" tag: \"filebeat\" 9. Start Bot Units Before starting Bot Units, make sure you made necessary configurations. There are several ways to run Bot Units – either with Bot Manager (that is automatic start from Application linux server call) or using Bot Agent. :::tip We recommend avoiding manual start. Use automatic start from Application linux server call instead. ::: :::note Select Deployment architecture before starting the services manually. ::: One Bot Unit per server (VDI setup) Run bot agent bin bot agent nordp.bat to start Nginx, Bot Master, and Bot Unit. Mind that your user session should be active for correct work. Multiple Bot Units per server (RDP in RDP) Run bot agent bin bot agent master.bat to start Nginx, Unit RDP sessions in a separate RDP session. :::warning Make sure to change master agent configuration that depends on your sessions Unit count and create additional configs for Units. ::: :::note For correct work, Unit sessions should be active internally in the master session but the master session can have the disconnected status. ::: Run Unit agents in each Unit RDP session bot agent bin bot agent unit{Unit _ID}.bat. 10. Check RPA Windows Server installation There are several ways to check RPA Windows Server installation. Make sure that Bot Units are available, and WebDrivers exist on the connected Bots. To do that, open the following link under any user on RPA machine. On evoking the command, view the list of available Bot Units in your browser. http: localhost:1500{unit_id} grid console To check RPA Worker, open the link http: localhost:1500{unit_id} grid console in your browser under any user on RPA machine. You should get the following response. { \"status\": \"UP\", \"details\": { \"rabbit\": { \"status\": \"UP\", \"details\": { \"version\": \"3.7.14\" } }, \"diskSpace\": { \"status\": \"UP\", \"details\": { \"total\": 53317988352, \"free\": 25653489664, \"threshold\": 10485760 } }, \"refreshScope\": { \"status\": \"UP\" }, \"zookeeper\": { \"status\": \"UP\", \"details\": { \"connectionString\": \"hostname.workfusion.com:port\", \"state\": \"STARTED\" } } } } Open Bot Manager and make sure your RPA Windows Server hostname is available and you are able to send commands to Bot Unit components. Run any RPA task and view logs in Kibana. "},{"version":"10.0","date":"Aug-06-2019","title":"sap-controls-list","name":"SAP controls list","fullPath":"iac/rpa/developer-guide/sap-controls-list","content":" The SAP GUI desktop client internal structure is seen as any other Windows application. SAP GUI is automated using the Inspector tool, which is available in WorkFusion Studio. The section contains the list of SAP UI controls that a WorkFusion Bot can interact with. :::tip Mind some restrictions applied to using certain UI controls. Also, refer to SAP Tips where we collected some tips and tricks for you to automate faster. ::: UI control Selector Notes Tool bar Tool bar – CLASS:GuiToolbar;NAME:tbar 1 Tool bar button – CLASS:GuiButton;NAME:btn 17 Title bar CLASS:GuiTitlebar Status bar Status bar – CLASS:GuiStatusbar Status bar message – CLASS:GuiStatusPane;NAME:pane 0 Checkbox button CLASS:GuiCheckBox;NAME:P_LAY04 Push button CLASS:GuiButton;NAME:btn 17 Radio button CLASS:GuiRadioButton;NAME:RS38M FUNC_EDIT Label CLASS:GuiLabel;TEXT:Vessel ID is;INSTANCE:3 Text field CLASS:GuiTextField;NAME:SOURCE_ID Text field with search help CLASS:GuiCTextField The Search Help button within the text field can be clicked on using clicks with offset. Ok code field CLASS:GuiOkCodeField Combobox CLASS:GuiComboBox;NAME:BUSLOCASRCH01 SEARCH_ID ALV grid (fullscreen) CLASS:Normal;NAME:PRICE:1 The ALV grid control (ALV = SAP List Viewer) is a flexible tool for displaying lists. ALV grids contain all rows and columns, regardless of whether they are displayed or not. To view a cell that is not on the screen, right click on it in Inspector and select the Focus option. ALV grid (embedded) Grid cell– CLASS:Normal;NAME:PRICE:3 Grid button – CLASS:Button;INSTANCE:30 ALV grids contain all rows and columns, regardless of whether they are displayed or not. To view a cell that is not on the screen, right click on it in Inspector and select the Focus option. ALV list CLASS:GuiLabel;CELL:2,3 ALV lists generate their content dynamically for visible area only with SAP API not being able to get all the children outside it. Thus, for columns and other content outside the visible area, you need to scroll the content using your mouse or keyboard. ALV hierarchical list CLASS:GuiLabel;CELL:5,12 ALV hierarchical lists generate their content dynamically for visible area only, with SAP API not being able to get all the children outside it. Thus, columns and other content outside the visible area cannot be inspected. Table control CLASS:GuiTextField;NAME:Column 1:1 To activate a cell that is not on the screen, use the focus () method. Easy tree CLASS:TreeItem;NAME:New1:2 List tree CLASS:TreeNode;NAME:Child1 Column tree CLASS:TreeItem;NAME:Child1:3 Tabstrip control CLASS:GuiTab;NAME:BUSLOCATORTAB_01 To activate an invisible tab, use the focus() method. "},{"version":"10.0","date":"Aug-06-2019","title":"sap-setup","name":"SAP setup","fullPath":"iac/rpa/developer-guide/sap-setup","content":" :::note Before you start working with the SAP GUI client, you have to turn on scripting both in SAP GUI desktop client and on the server side. ::: Client scripting setup Scripting is enabled in the SAP GUI desktop client either in registry or manually. Enable scripting in registry To turn on scripting, add changes to the registry under each Bot or user. For all users by default (can be overridden by current user settings) – HKLM WOW6432Node SAP.reg Windows Registry Editor Version 5.00 HKEYLOCALMACHINE SOFTWARE WOW6432Node SAP SAPGUI Front SAP Frontend Server Scripting \"ShowNativeWinDlgs\"=dword:00000000 HKEYLOCALMACHINE SOFTWARE WOW6432Node SAP SAPGUI Front SAP Frontend Server Security \"DefaultAction\"=dword:00000000 \"SecurityLevel\"=dword:00000001 \"UserScripting\"=dword:00000001 \"WarnOnAttach\"=dword:00000000 \"WarnOnConnection\"=dword:00000000 For local users – HKCU SAP SETTINGS.reg Windows Registry Editor Version 5.00 HKEYCURRENTUSER SOFTWARE SAP SAPGUI Front SAP Frontend Server Scripting \"ShowNativeWinDlgs\"=dword:00000000 HKEYCURRENTUSER SOFTWARE SAP SAPGUI Front SAP Frontend Server Security \"DefaultAction\"=dword:00000000 \"SecurityLevel\"=dword:00000001 \"UserScripting\"=dword:00000001 \"WarnOnAttach\"=dword:00000000 \"WarnOnConnection\"=dword:00000000 Enable scripting manually Edit scripting settings By default, when you run a recorded script, the following pop up appears, that blocks script execution and must be switched off. To switch off the pop up and proceed with script execution, do as follows. Open SAP GUI Settings > Options. Select Accessibility & Scripting > Scripting. If the warning message is thrown, the option Notify when a script attaches to SAP GUI is checked. Uncheck the following options: Notify when a script attaches to SAP GUI Notify when a script opens a connection. Click Apply followed by OK. Enable connection by command line To allow connection by command line, do as follows. Open SAP GUI Settings > Options. Select Security and click on the Open Security Configuration button. Specify Allow for Default Action. Click Apply to save the changes. Application server scripting setup Open SAP GUI and check if scripting is turned on. If the appropriate menu item is grayed out, perform the following steps. Start the rz11 transaction. Enter the transaction code rz11 into the OK Code field and press Enter. Open the sapgui user _scripting parameter. Enter the parameter name into the related field and either click Display or press Enter. To edit the current value, click on the Change Value button. In the window that follows, enter TRUE in the New Value field and press Save. Re log into the system to view the changes. "},{"version":"10.0","date":"Aug-06-2019","title":"sap-restrictions","name":"SAP restrictions","fullPath":"iac/rpa/developer-guide/sap-restrictions","content":" Object recording Object recording is performed in the beta mode and not supported for all of the SAP GUI controls. To report an unsupported control, reach out to our support team or post on the forum. UI Controls Search help windows Search help windows are recognized only in the Dialog (modal) mode. To enable usage of search help windows, do as follows. Go to Help > Settings. Select the* F4 Help* tab and check Dialog (modal). To save the changes, press Enter or the OK button. Scrollbars and scrollbar buttons Scrollbars and scrollbar buttons within tables are not supported. The workarounds are as follows: mouse scroll arrow keys (for example, PgUp and PgDn) the focus() method object click with offset (top right, bottom right corner) Expand to see the clicking with offset code sample pw= language=EN system=ERD client=100 trace=0 command=ZTABLE01 ' open(\"C: Program Files (x86) SAP FrontEnd SAPgui {command}\") sleep(5000) log.info(\"* CONTINUE WITHOUT ENDING ANY OTHER LOGONS *\") if (Window.windowHandles(\" CLASS: 32770; TITLE:License Information for Multiple Logons \").isEmpty()) { log.info(\"Window License Information for Multiple Logons not found\") } else { window(\" CLASS: 32770; TITLE:License Information for Multiple Logons \") (\" CLASS:GuiRadioButton; NAME:MULTILOGONOPT2 \").click() (' CLASS:GuiButton; NAME:btn 0 ; ').click() log.info(\"Window License Information for Multiple Logons closed\") } log.info(\"* CLICK TO DOWN SCROLLBAR WITH OFFSET *\") window(\" CLASS:SAPFRONTENDSESSION;TITLE:SAP \") (\" CLASS:GuiTextField;NAME:Column 2:1 \").click(120, 175) (\" CLASS:GuiTextField;NAME:Column 2:1 \").click(120, 175) (\" CLASS:GuiTextField;NAME:Column 2:1 \").click(120, 175) (\" CLASS:GuiTextField;NAME:Column 2:1 \").click(120, 175) (\" CLASS:GuiTextField;NAME:Column 2:1 \").click(120, 175) def actualResult_1 = (\" CLASS:GuiTextField;NAME:Column 1:8 \").getAttribute(\"Visible\") assert \"true\" == actualResult_1 log.info(\"* CLICK TO RIGHT SCROLLBAR WITH OFFSET *\") window(\" CLASS:SAPFRONTENDSESSION;TITLE:SAP \") (\" CLASS:GuiTextField;NAME:Column 1:8 \").click(180, 50) (\" CLASS:GuiTextField;NAME:Column 1:8 \").click(180, 50) (\" CLASS:GuiTextField;NAME:Column 1:8 \").click(180, 50) def actualResult_2 = (\" CLASS:GuiTextField;NAME:Column 5:8 \").getAttribute(\"Visible\") assert \"true\" == actualResult_2 log.info(\"* KILL ALL SAPLOGON PROCESESS *\") open(\"TASKKILL F IM saplogon.exe T\") sleep(3*1000) > Menu bar items The top menu bar items are not supported. To automate, use shortcuts. Click on each of the top menu bar buttons to see available variants. Context menu To automate the context menu, refer to one of the following workarounds: the Enter Keystrokes action to apply function keys (e.g., F1, F3, F4, etc.) arrow keys, for example, the order of actions in the script will look like this: Mouse Click (right click) Enter Keystrokes (Down) Enter Keystrokes (Down) Enter Keystrokes (Enter) Calendar picker The calendar picker option is not supported. You can input the date in the user specified format instead, e.g., ('element_selector').sendKeys(\"29.03.2019\") "},{"version":"10.0","date":"Aug-06-2019","title":"sap-tips","name":"SAP tips","fullPath":"iac/rpa/developer-guide/sap-tips","content":" Navigate to hidden elements If a UI control is not visible on the SAP GUI screen, you can easily activate it using the focus() method. To grab a selector of a hidden element, use Inspector Focus on tree elements Right click on any hidden element within the tree and select Focus. On expanding the element, copy its selector to use in your script, for example, ('treehiddenelement_selector').focus(). To be able to run the scripts available on this page, provide your own credentials to enter SAP, defined as ` and `. Focusing on tree hidden element code sample pw= language=EN system=ERD client=100 trace=0 command=DWDM' open(\"C: Program Files (x86) SAP FrontEnd SAPgui {command}\") sleep(10000) log.info(\"* CONTINUE WITHOUT ENDING ANY OTHER LOGONS *\") if (Window.windowHandles(\" CLASS: 32770; TITLE:License Information for Multiple Logons \").isEmpty()) { log.info(\"Window License Information for Multiple Logons not found\") } else { window(\" CLASS: 32770; TITLE:License Information for Multiple Logons \") (\" CLASS:GuiRadioButton; NAME:MULTILOGONOPT2 \").click() (' CLASS:GuiButton; NAME:btn 0 ; ').click() log.info(\"Window License Information for Multiple Logons closed\") } log.info(\"Go to the list tree element\") window(\" CLASS:SAPFRONTENDSESSION; TITLE:Enjoy Demo Center: Display \") (\" CLASS:TreeItem; TEXT:SAP List Tree \").doubleClick() log.info(\"Try to focus on hidden element of the tree\") window(\" CLASS:SAPFRONTENDSESSION; TITLE:SAP \") (\" CLASS:TreeNode;NAME:Child1 \").focus() assert \"true\" == (\" CLASS:TreeNode;NAME:Child1 \").getAttribute(\"Visible\") log.info(\"* KILL ALL SAPLOGON PROCESESS *\") open(\"TASKKILL F IM saplogon.exe T\") > Focus on tabs Right click on any hidden tab and select Focus. On expanding the tab, copy its selector to use in your script, for example, ('hiddentabselector').focus(). Focusing on hidden tab code sample pw= language=EN system=ERD client=100 trace=0 command=SE38' open(\"C: Program Files (x86) SAP FrontEnd SAPgui {command}\") sleep(10000) log.info(\"* CONTINUE WITHOUT ENDING ANY OTHER LOGONS *\") if (Window.windowHandles(\" CLASS: 32770; TITLE:License Information for Multiple Logons \").isEmpty()) { log.info(\"Window License Information for Multiple Logons not found\") } else { window(\" CLASS: 32770; TITLE:License Information for Multiple Logons \") (\" CLASS:GuiRadioButton; NAME:MULTILOGONOPT2 \").click() (' CLASS:GuiButton; NAME:btn 0 ; ').click() log.info(\"Window License Information for Multiple Logons closed\") } log.info(\"Go to the component with Tabs\") window(\" CLASS:SAPFRONTENDSESSION;TITLE:ABAP Editor: Initial Screen \") (\" CLASS:GuiCTextField \").setText(\"DEMO_DYNPRO\") sendKeys(\"{F8}\") log.info(\"Navigating to Tabs\") window(\" CLASS:SAPFRONTENDSESSION;TITLE:Elements of a Screen \") (\" CLASS:GuiTab;NAME:T5 \").focus() assert \"true\" == (\" CLASS:GuiTab;NAME:T5 \").getAttribute(\"Visible\") log.info(\"* KILL ALL SAPLOGON PROCESESS *\") open(\"TASKKILL F IM saplogon.exe T\") > Click with offset Clicking on elements with offset facilitates automating complex controls. The preferred way is to click not in the center of an element, but somewhere else, for example, the top left corner or custom offset, as the offset is counted from the element itself, not the screen. :::tip Apply clicking with offset for any mouse click action (double, triple, wheel) with offset counted from top left. ::: Clicking with offset code sample pw= language=EN system=ERD client=100 trace=0' open(\"C: Program Files (x86) SAP FrontEnd SAPgui {command}\") sleep(10000) log.info(\"* CONTINUE WITHOUT ENDING ANY OTHER LOGONS *\") if (Window.windowHandles(\" CLASS: 32770; TITLE:License Information for Multiple Logons \").isEmpty()) { log.info(\"Window License Information for Multiple Logons not found\") } else { window(\" CLASS: 32770; TITLE:License Information for Multiple Logons \") (\" CLASS:GuiRadioButton; NAME:MULTILOGONOPT2 \").click() (' CLASS:GuiButton; NAME:btn 0 ; ').click() log.info(\"Window License Information for Multiple Logons closed\") } log.info(\"Clicking with Offset\") window(\" CLASS:SAPFRONTENDSESSION;TITLE:SAP Easy Access \") (\" CLASS:TreeItem;NAME:Root:2 \").click() assert \"true\" == (\" CLASS:TreeItem;NAME:Root:2 \").getAttribute(\"Selected\") (\" CLASS:TreeItem;NAME:Root:2 \").click(20,135) assert \"true\" == (\" CLASS:TreeItem;NAME:0000000052:2 \").getAttribute(\"Selected\") log.info(\"* KILL ALL SAPLOGON PROCESESS *\") open(\"TASKKILL F IM saplogon.exe T\") > Run SAP GUI from console You can use sapshcut.exe to connect to the SAP system via the command line, as it is a faster method. Default path: C: Program Files (x86) SAP FrontEnd SAPgui sapshcut Example: sapshcut user= pw= language=EN system=A4H client=001 command=se38 trace=0 Running SAP GUI from console code sample pw= language=EN system=A4H client=001 command=se38 trace=0' open(\"C: Program Files (x86) SAP FrontEnd SAPgui {command}\") sleep(5000) * CONTINUE WITHOUT ENDING ANY OTHER LOGONS * if (Window.windowHandles(\" CLASS: 32770; TITLE:License Information for Multiple Logons \").isEmpty()) { log.info(\"Window License Information for Multiple Logons not found\") } else { window(\" CLASS: 32770; TITLE:License Information for Multiple Logons \") (\" CLASS:GuiRadioButton; NAME:MULTILOGONOPT2 \").click() (' CLASS:GuiButton; NAME:btn 0 ; ').click() log.info(\"Window License Information for Multiple Logons closed\") } * WAIT FOR TRANSACTION * sleep(3000) > Close processes There are several ways to close your open processes in the SAP GUI desktop client. Use the taskkill option in Command Prompt and wait until all the windows are closed. To do that, add open(\"TASKKILL F IM saplogon.exe T\") to your script. :::tip If the taskkill command is executed at the end of the script, with the driver closing all the processes, add the waiting period right after that, e.g., sleep(3000). ::: Iterate over all open windows and close each of them. Go to Program > Exit within each open window. To close all the running sessions, go to System > Log Off. Use specific commands in the OK Code field, for example, nend or nex. See the table below for more information. SAP GUI commands The below mentioned commands are entered into the OK Code field, allowing to perform some actions quicker. Command Description n used together with a transaction name, navigates to a transaction from the existing transaction window o displays a list of open SAP GUI sessions; all your sessions will be displayed in a dialog box with the options to create and end sessions i used together with a transaction name, closes the specified transaction window nend logs out prompting to confirm that you want to close all the sessions nex closes all the sessions and exits the SAP GUI desktop client without prompting SAP GUI shortcuts Startup parameters Shortcut Description version Display version information edit Edit the shortcut via a dialog register Register the shortcut class to integrate into Windows maxgui Display SAP GUI window maximized Logon parameters – user identification Shortcut Description user=userid SAP system user identification pw=password Password for the SAP system user language=EN Language or logon to the SAP system Logon parameters – system identification Shortcut Description system=DEV SID of the SAP system client=032 Client of the SAP system to log on to sysname=\"DEV PUBLIC \" Connect via message server (load balancing) quiparam=\"sapserver 10\" Connect via single application server Logon parameters – function identification Shortcut Description command=\"se38\" Transaction or function to be executed type=Transaction Type of command used (transaction report systemcommand) title=\"ABAP 4 Editor\" Title displayed in shortcut logon dialog "},{"version":"10.0","date":"Aug-02-2019","title":"sendkeys-keystrokes-guide","name":"SendKeys keystrokes guide","fullPath":"iac/rpa/developer-guide/sendkeys-keystrokes-guide","content":" Examples Browser Keys sendKeys(Keys.chord(Keys.LEFT_ALT, 'n')) By TAB > ARROWDOWN > ARROWDOWN > ENTER pull up 'Save As' and click context menu item pressTab() sendKeys(Keys.ARROW_DOWN) sendKeys(Keys.ARROW_DOWN) pressEnter() Desktop keys sendKeys('vim the new file.txt{ENTER}') windowContent = copyPuttyWindowText() sendKeys('aaaaaaa') sendKeys('{TAB}{ENTER 2}') sendKeys('{UP}{UP}{DOWN}{DOWN}') sendKeys('i'); sendKeys('{UP}{DOWN}{RIGHT}{RIGHT}{TAB}{TAB}') sendKeys('Hello, Robot{!}') sendKeys('{CTRLDOWN}c{CTRLUP}') sendKeys(':q{!}') sendKeys('{ENTER}') sendKeys('{CTRLDOWN}d{CTRLUP}') Selenium keys enum :::tip Official guide http: seleniumhq.github.io selenium docs api java org openqa selenium Keys.html These constants can also be used for sending keys into desktop apps. ::: Expand to see org.openqa.selenium.Keys enum constants ADD ALT ARROW _DOWN ARROW _LEFT ARROW _RIGHT ARROW _UP BACK _SPACE CANCEL CLEAR COMMAND CONTROL DECIMAL DELETE DIVIDE DOWN END ENTER EQUALS ESCAPE F1 F10 F12 F2 F3 F4 F5 F6 F7 F8 F9 HELP HOME INSERT LEFT LEFT_ALT LEFT_CONTROL LEFT_SHIFT META MULTIPLY NULL NUMPAD0 NUMPAD1 NUMPAD2 NUMPAD3 NUMPAD4 NUMPAD5 NUMPAD6 NUMPAD7 NUMPAD8 NUMPAD9 PAGE_DOWN PAGE_UP PAUSE RETURN RIGHT SEMICOLON SEPARATOR SHIFT SPACE SUBTRACT TAB UP ZENKAKU_HANKAKU Simulating multiple keys press sendKeys(Keys.chord(Keys.LEFT_ALT, 'n')) sendKeys(Keys.chord(Keys.LEFT_CONTROL, 'c')) sendKeys(Keys.chord(Keys.LEFT_CONTROL, 'v')) Technical details Use org.openqa.selenium.chord public static java.lang.String chord(java.lang.CharSequence... value). Simulate pressing many keys at once in a \"chord\". Takes a sequence of Keys.XXXX or strings; appends each of the values to a string, and adds the chord termination key (Keys.NULL) and returns the resultant string. :::note When the low level webdriver key handlers see Keys.NULL, active modifier keys (CTRL ALT SHIFT etc) release via a keyup event. Parameters:value characters to sendReturns:String representation of the char sequence. ::: public static java.lang.String chord(java.lang.Iterable value). Parameters:value characters to sendReturns:String representation of the char sequenceSee Also:chord(CharSequence...) AutoIt keys guide for desktop automation :::tip Official guide https: www.autoitscript.com autoit3 docs appendix SendKeys.htm ::: ! – This tells AutoIt to send an ALT keystroke, therefore sendKeys(\"This is text!a\") would send the keys \"This is text\" and then press \"ALT a\". :::note Some programs are very choosy about capital letters and ALT keys, i.e., \"!A\" is different from \"!a\". The first says ALT SHIFT A, the second is ALT a. If in doubt, use lowercase. ::: – This tells AutoIt to send a SHIFT keystroke; therefore, sendKeys(\"Hell o\") would send the text \"HellO\". SendKeys(\"! a\") would send \"ALT SHIFT a\". – This tells AutoIt to send a CONTROL keystroke; therefore, sendKeys(\" !a\") would send \"CTRL ALT a\". :::note Some programs are very choosy about capital letters and CTRL keys, i.e., \" A\" is different from \" a\". The first says CTRL SHIFT A, the second is CTRL a. If in doubt, use lowercase. ::: – The hash now sends a Windows keystroke; therefore, sendKeys(\" r\") would send Win r which launches the Run() dialog box. If a user is holding down the Shift key when a SendKeys function begins, text may be sent in uppercase. One workaround is to sendKeys(\"{SHIFTDOWN}{SHIFTUP}\") before the other sendKeys operations. Certain keyboards as the Czech keyboard sends different characters when using the Shift Key or with CAPS LOCK enabled while sending a character. Due to the send AutoIt implementation the CAPS LOCKed character will be sent as a Shifted character so it will not work. Command (if zero flag) resulting Keypress {!} ! { } { } { } {{} { {}} } {SPACE} SPACE {ENTER} ENTER key on the main keyboard {ALT} ALT {BACKSPACE} or {BS} BACKSPACE {DELETE} or {DEL} DELETE {UP} Up arrow {DOWN} Down arrow {LEFT} Left arrow {RIGHT} Right arrow {HOME} HOME {END} END {ESCAPE} or {ESC} ESCAPE {INSERT} or {INS} INS {PGUP} PageUp {PGDN} PageDown {F1} {F12} Function keys {TAB} TAB {PRINTSCREEN} Print Screen key {LWIN} Left Windows key {RWIN} Right Windows key {NUMLOCK on} NUMLOCK (on off toggle) {CAPSLOCK off} CAPSLOCK (on off toggle) {SCROLLLOCK toggle} SCROLLLOCK (on off toggle) {BREAK} for Ctrl Break processing {PAUSE} PAUSE {NUMPAD0} {NUMPAD9} Numpad digits {NUMPADMULT} Numpad Multiply {NUMPADADD} Numpad Add {NUMPADSUB} Numpad Subtract {NUMPADDIV} Numpad Divide {NUMPADDOT} Numpad period {NUMPADENTER} Enter key on the numpad {APPSKEY} Windows App key {LALT} Left ALT key {RALT} Right ALT key {LCTRL} Left CTRL key {RCTRL} Right CTRL key {LSHIFT} Left Shift key {RSHIFT} Right Shift key {SLEEP} Computer SLEEP key {ALTDOWN} Holds the ALT key down until {ALTUP} is sent {SHIFTDOWN} Holds the SHIFT key down until {SHIFTUP} is sent {CTRLDOWN} Holds the CTRL key down until {CTRLUP} is sent {LWINDOWN} Holds the left Windows key down until {LWINUP} is sent {RWINDOWN} Holds the right Windows key down until {RWINUP} is sent {ASC nnnn} Send the ALT nnnn key combination {UTF} Type into a window (e.g. RemoteApp, RDP, Citrix, VirtualBox) regardless of its active language sendKeys('{UTF 怒helloनमस्ते}') {BROWSER _BACK} Select the browser \"back\" button {BROWSER _FORWARD} Select the browser \"forward\" button {BROWSER _REFRESH} Select the browser \"refresh\" button {BROWSER _STOP} Select the browser \"stop\" button {BROWSER _SEARCH} Select the browser \"search\" button {BROWSER _FAVORITES} Select the browser \"favorites\" button {BROWSER _HOME} Launch the browser and go to the home page {VOLUME _MUTE} Mute the volume {VOLUME _DOWN} Reduce the volume {VOLUME _UP} Increase the volume {MEDIA _NEXT} Select next track in media player {MEDIA _PREV} Select previous track in media player {MEDIA _STOP} Stop media player {MEDIA PLAY PAUSE} Play pause media player {LAUNCH _MAIL} Launch the email application {LAUNCH _MEDIA} Launch media player {LAUNCH _APP1} Launch user app1 {LAUNCH _APP2} Launch user app2 {OEM _102} Either the angle bracket key or the backslash key on the RT 102 key keyboard To send the ASCII value A (same as pressing ALT 065 on the numeric keypad): sendKeys(\"{ASC 065}\") When using 2 digit ASCII codes you must use a leading 0, otherwise an obsolete 437 code page is used. To send UNICODE characters, enter the character code (decimal or hex), for example, this sends a Chinese character. sendKeys(\"{ASC 2709}\") sendKeys(\"{ASC 0xA95}\") Single keys can also be sent repeated: sendKeys(\"{DEL 4}\") Presses the DEL key 4 times sendKeys(\"{S 30}\") Sends 30 'S' characters sendKeys(\" {TAB 4}\") Presses SHIFT TAB 4 times To hold a key down: sendKeys(\"{a down}\") Holds the A key down sendKeys(\"{a up}\") Releases the A key To set the state of the capslock, numlock and scrolllock keys: sendKeys(\"{NumLock on}\") Turns the NumLock key on sendKeys(\"{CapsLock off}\") Turns the CapsLock key off sendKeys(\"{ScrollLock toggle}\") Toggles the state of ScrollLock Most laptop computer keyboards have a special Fn key. This key cannot be simulated. :::note By setting the flag parameter to 1, the above \"special\" processing will be disabled. This is useful when you want to send some text copied from a variable and you want the text sent exactly as written. ::: For example, open Folder Options (in the control panel) and try the following: Command Description sendKeys(\"{TAB}\") Navigate to next control (button, checkbox, etc) sendKeys(\" {TAB}\") Navigate to previous control. sendKeys(\" {TAB}\") Navigate to next WindowTab (on a Tabbed dialog window) sendKeys(\" {TAB}\") Navigate to previous WindowTab. sendKeys(\"{SPACE}\") Can be used to toggle a checkbox or click a button. sendKeys(\"{ }\") Usually checks a checkbox (if it's a \"real\" checkbox.) sendKeys(\"{ }\") Usually unchecks a checkbox. sendKeys(\"{NumPadMult}\") Recursively expands folders in a SysTreeView32. Unicode mode for the sendKeys() Bot can type into a window (e.g., RemoteApp, RDP, Citrix, VirtualBox) regardless of its active language using the new Unicode typing mode: Example: Typing text in three languages sendKeys('{UTF 怒helloनमस्ते}') "},{"version":"10.0","date":"Sep-20-2019","title":"simplified-api-cheatsheet","name":"Simplified Robotics API cheatsheet","fullPath":"iac/rpa/developer-guide/simplified-api-cheatsheet","content":" :::tip This article contains a cheat sheet for using main Robotics Simplified API methods. ::: Selectors (locators) :::note Before interacting with a ui element, you need to locate it using () or () constructions. ::: (), () locating and manipulating element def uiElement = ('.css class') uiElement.doubleClick() (byLinkText('Purchase')).click() navigating to hidden element ('.TreeNode').focus() finding the Nth element matching given criteria (withText('Open', 5)).hover() locating focused element def focusedEl = getFocusedElement() if (focusedEl.getId() == 'tuk tuk') { focusedEl().click(2) } using element collections def fieldCollection = (byXpath(\" * @name='field product' \")) fieldCollection.each { println it.getText() } getting element from collection fieldCollection.last().val('butter') def firstField = fieldCollection.first() def sixthField = fieldCollection.get(5) filtering collection by condition def readonlyCollection = fieldCollection.filter(READONLY) def inactiveReadonlyCollection = readonlyCollection.exclude(cssClass('temp')) searching child elements (byTitle('Web Form')).find('div > table > tr:nth child(5) > td').click() def childRowCollection = (' multirowTable').findAll('tr.active') searching parent and ancestor elements def parentClass = ('td').parent().getAttribute('class') def ancestorText = (' exp 0').closest('div').text() alternatives to () and () getElement(byImage('https: site img1.png'), 200, 300).click() def abbreviations = getElements(byTagName('abbr')) Selector types Web: byCssSelector default, same as ('.selector') byXpath byId byName byClassName byTagName byTitle byValue by('attributename', 'attributename') byAttribute('name','value') withText byText byLinkText byPartialLinkText Desktop: CSS selectors ('.Button text='Click me' ') Object selectors (' CLASS:Button; INSTANCE:2 ') byXpath for Java apps only byImage('link', offsetX, offsetY) Opening apps and browsers open(), openAndFocus(), openIE() open a website open('https: www.wikipedia.org') open an application open('calc.exe') open an app and switch to the newly opened window openAndFocus('notepad.exe', 3000, 250) in Universal driver, you can open a specific browser openFirefox('https: www.wikipedia.org') openChrome('https: forum.workfusion.com') openIE('https: google.com chrome browser desktop index.html') opening new browser tabs openLinkInNewWindow(byCssSelector('.container a.create new')) openLinkInNewWindow('https: automationacademy.com') openNewWindow() Excel, files and folders openExcel(), Resource, downloadFileOnAgent() opening excel file for manipulating it def filePath = 'C: downloads report.xlsx' openExcel(filePath) switchSheet(filePath, 'Fact') def temp = getCell(filePath, 'B1') manipulating with local files and folders def filePath = downloadFileOnAgent(s3Path) Resource.append(filePath, '_ New Content _', 'utf 8') def content_utf8 = Resource.read(filePath, 'utf 8') Resource.createDirectoryOverwrite('D: temp new ') downloading and uploading files to from RPA Server def filePathOnAgent = downloadFileOnAgent('https: s3 path 1.csv') downloadFileOnAgent('https: s3 path 1.csv', 'C: Temp new file.csv') byte byteFileContent = downloadFileFromAgent('C: Temp 1.csv') def stringFileContent = downloadTextFileFromAgent('C: Temp 2.csv') deleteFileOnAgent('C: Temp 1.csv') def textFilePath = sendToAgent('new file text content') def binaryFilePath = sendToAgent(byteFileContent, 'D: new new file.csv') :::tip Excel API description Excel Class Files and folders API Files and Folders Resource Class ::: Mouse click(), hover(), dragAndDrop() clicking in different ways (' nav bar').click() (byTagName('p')).get(2).click(3) or tripleClick(), doubleClick() (' nav bar').contextClick() clicking by coordinates mouse().click(20, 20) mouse().wheelClick(100, 78) mouseDown(coordinates) mouseUp(int,int) moving mouse, hovering, scrolling mouseMove(100, 500) (' nav bar').hover() (' nav bar').scrollTo() (' nav bar').scrollDown(7) or scrollUp(5) submitting forms ('.btn__act').submit() clicking with offset ('.MyMenu').click(20,135) drag and drop by selectors or coordinates offset actions().dragAndDrop( (byImage(\" {imagePath} source.png\")), xOffset, yOffset) .perform() actions().clickAndHold( (byImage(\" {imagePath} image.png\"))) .moveToElement( (byImage(\" {imagePath} image.png\"))) .release( (byImage(\" {imagePath} image.png\"))) .perform() actions().dragAndDrop( (byImage(\" {imagePath} source folder.png\")), (byImage(\" {imagePath} target folder.png\"))) .perform() :::tip See more about advanced user interactions. ::: Keyboard sendKeys(), pressEnter(), Keys.chord() sending text (' input username').sendKeys('Jimmy') sending multiple keys sequentially sendKeys(Keys.DOWN, Keys.ENTER) sending key combinations sendKeys(Keys.chord(Keys.CONTROL, 'a')) holding keys to perform complex actions keyboard().pressKey(Keys.SHIFT) sendKeys(Keys.DOWN, Keys.DOWN) keyboard().releaseKey(Keys.SHIFT) pressing popular keys and combinations pressCtrlC() pressCtrlV() pressEnter() combining text and action keys sendKeys('username{Enter}') will type the \"username\" and press Enter key switching to raw keys mode switchSendRawKeysMode(true) sendKeys('username{Enter}') will type the \"username{Enter}\" string without pressing the Enter key typing into window regardless of its active language sendKeys('{UTF 怒helloनमस्ते}') typing DEL 4 times sendKeys(\"{DEL 4}\") :::tip To learn more about Web and Desktop keystrokes, refer to SendKeys Keystrokes Guide ::: Conditions and assertions should(), has(), is() (byXpath(' p 2 ')).shouldHave(text('Hello')) ('.loading_progress').should(DISAPPEAR) ('input').shouldNotHave(cssClass('active')) (' mydiv').shouldHave(attribute('fileId')) (withText('Your message has been sent.')).shouldBe(VISIBLE) ('h1').should(matchRegexp(\"Hello s*John\")) (' logoutLink').should(APPEAR) (' myInput').waitUntil(partialValue('John'), 5000) ('.errorMessage').first().shouldBe(VISIBLE, ENABLED) ('td').shouldHaveSize(5) (' errorMessage').should(APPEAR).shouldBe(ENABLED) Assertion Conditions Logical operators should()shouldNot()shouldHave()shouldNotHave()shouldBe()shouldNotBe()has()is()hasPartialValue()shouldHaveSize(n) APPEARDISABLEDDISAPPEAREMPTY _ELEMENTENABLEDEXISTFOCUSEDHIDDENPRESENTREADONLYSELECTEDVISIBLEcssClass()attribute()text()textCaseInSensitive()matchRegexp()value()partialValue()id()name()type() and(name, conditions)or(name, conditions)not(condition) :::tip UIConditions class ExpectedConditions class ::: Windows and frames switching to window by its attribute switchTo().window(' CLASS:Notepad ') window(' TITLE:Photoshop ') works faster than window() switchToExistingWindow(' CLASS:Notepad ', 100L) switching to window by its order switchToLastWindow() switchToRootWindow() switchToPrevWindow() switchToNextWindow() switching to iframe switchTo().frame('iframeResult') switchTo().parentFrame() switchTo().defaultContent() closing window(s) close() closeAllAnotherWindows() closeAllWindows() Changing window size minimizeWindow() maximizeWindow() zoom(2.5) navigation in history forward() back() refresh() only for web Handling popups and alerts confirm() confirm('Do you want to exit ') dismiss() :::tip Handling iFrames Switching to Window, Frame, Popup ::: Timeouts and waits setting timeouts for different actions (milliseconds) pageLoadTimeout(5000) for open(), openAndFocus(), switch() web or window implicitlyWaitTimeout(3000) used for finding element (); collections () do not use timeout scriptTimeout(30 * 1000) for JavaScript or Groovy scripts setting all timeouts in one command timeouts(40 * 1000) set timeout for explicit waits setFluentWaitTimeout(3 * 1000) setFluentWaitPollingInterval(200) check the existence of element if it might be missing fluentWait() .ignoring(org.openqa.selenium.NoSuchElementException.class) .until(ExpectedConditions.presenceOfElementLocated(byCssSelector(' new'))) explicit wait usage, only for elements which exist ('.demo el').waitUntil(VISIBLE) ('.demo el').waitWhile(SELECTED) Wait() .withMessage('mesage') .withTimeout(5, java.util.concurrent.TimeUnit.SECONDS) .pollingEvery(500, java.util.concurrent.TimeUnit.MILLISECONDS) .until(...) explicit wait with custom timeout and polling interval ('.demo el').waitWhile(READONLY, 3000, 200) pausing script execution not recommended in production sleep(3000) :::tip Webdriver Waits ::: Clipboard copying to clipboard selectAllTextAndCopy() pressCtrlC() ('p.news').tripleClick() copySelectedText() copyPuttyWindowText() getting clipboard text def clipboard = clipboardText() (byXpath(' input 3 ')).pressCtrlV() setting clipboard text setClipboardText('new_content') Screenshots desktop using org.sikuli.script or java.awt.Robot String path = executeGroovyScript(javaScreenshot.toString()) browser def screen = driver().getScreenshotAs(OutputType.BYTES) or BASE64, FILE def bytes = screenshotAsImage() :::tip Taking Screenshots ::: Get and set value getting value, text, attribute def emailValue = (' email').val() or getValue() def iNumber = (' invoice n').attr('name') or getAttribute(), name() def someText = (byXpath(' p 2 ')).text() or getText() def allPosts = ('p.content').getTexts() def tag = ('.btn').getTagName() setting value and text (' email').val('wf robot@mail.com') or setValue() (byXpath(' input 3 ')).text('new text') resetting field text ('.nav').get(5).clear() getting location and coordinates (byImage(' {imageLink}')).getRect().getWidth() (byImage(' {imageLink}')).getLocation() (withText('create')).getSize() radio buttons def oRadioButton = (byName('occupation')) if (oRadioButton.get(0).isSelected()) { oRadioButton.get(1).setSelected(true) } dropdowns def optionsList = (byId('continents')) optionsList.selectOption(2) optionsList.selectOptionByValue('AFR') println optionsList.getSelectedText() println optionsList.getSelectedValue() println optionsList.getSelectedOption().getText() :::tip CheckBox RadioButton DropDown MultipleSelect ::: Scripting executing a Groovy Script on RPA Node def scriptResult = executeGroovyScript(console_script.toString()) JavaScript or AutoIt def messages = 'Hello ', 'from ' executeScript(\"alert(arguments0 arguments0 arguments 1 )\", messages, 'JS') Output: \"Hello from JS\" :::tip JavaScriptExecutor ::: Explicitly set driver in Universal Script "},{"version":"10.0","date":"Jul-29-2019","title":"simplified-robotics-api","name":"Simplified Robotics API","fullPath":"iac/rpa/developer-guide/simplified-robotics-api","content":" Overview RPA developers can create lighter weight, clearly readable robotics scripts using WorkFusion Simplified API, and temporary files are automatically cleaned up after bot execution. The Robotics Simplified API provides a wrapper library with a lightweight and succinct jQuery like syntax, encapsulating best practices into the most frequent functions and hiding the complexity from the end user. As a result, the amount of code is significantly reduced and the code itself becomes clearly readable. Simplified API reference Javadocs Packages list Latest release com.workfusion.rpa.helperscom.workfusion.rpa.helpers.conditionscom.workfusion.rpa.helpers.conditions.collectioncom.workfusion.rpa.helpers.selectorscom.workfusion.rpa.helpers.utilscom.workfusion.rpa.helpers.resources Compare API samples Simplified API sample Original Selenium API sample The simplified API code is clear and can be understood without deep knowledge of Robotics API: open(\"https: workfusion.com\"); Open website in driver's browser (byName(\"username\")).val(\"johny\"); Enter text into field (\" submit\").click(); Click submit button (\".loading_progress\").should(DISAPPEAR); Wait until element disappears (\" username\").shouldHave(text(\"Welcome!\")); Wait until element gets text Examples Here you can find some examples of using the Robotics Simplified API: Automating webpage with AJAX and iframe Automating Invoiceplane web CRM Abbble Inc. 10 02 2016 {\"productsandprices\": {\"product\":\"Wooden Table\",\"quantity\":\"2\",\"price\":\"140\"},{\"product\":\"Printer Ink\",\"quantity\":\"1\",\"price\":\"30\"},{\"product\":\"Paper Rim\",\"quantity\":\"3\",\"price\":\"60\"} } Desktop automation: Notepad, Clipboard, Download file Simplified API has a lot of classes and methods. We suggest you to stop reading, open your WorkFusion Studio and start typing. Just type: or (selector).{.highlighter rouge} and IDE suggest you all the available options. Automatically imported classes com.workfusion.rpa.helpers refer to the RPA* class that represents a simplified API for RPA. Static methods and variables from this class and the following classes are automatically included into Bot configs: UiConditions UiCollectionConditions UiSelectors The following classes are also automatically imported into Bot configs. They are accessible by their short class name, e.g., RemoteWebDriver: RemoteWebDriver Action Actions Mouse Keyboard Wait FluentWait WebDriverWait ExpectedCondition ExpectedConditions Coordinates ApiUtils PointedCoordinates RobotsMouse UiElement UiElementCollection UiCondition UiCollectionCondition Classes from the following packages also are automatically imported into Bot configs: org.openqa.selenium. Important API methods :::tip See the Simplified API Cheat Sheet to learn robotics API on examples. ::: WorkFusion Studio supports autocomplete for all the simplified API methods. Main simplified methods: driver() – accessing current driver instance keyboard() – current driver keyboard mouse() – current driver mouse (By) – finds the first element matching given descriptor. With a UIElement instance, you can either do action with it (click{.highlighter rouge}, copySelectedText) or check a condition: shouldHave(text(\"abc\")){.highlighter rouge}. Both will trigger the search of the elements in DOM or desktop. (String) – finds all elements matching given descriptor. Other important simplified methods: sendKeys(CharSequence...) – sends a string to current window using keyboard open(String) – opens a specified URL or application window(String) – switches to a window with a specified title or descriptor timeouts(long) – sets the maximum amount of time to wait for condition, page load, script to execute, element search The following additional methods are implemented for desktopDriver: executeGroovyScript(String) clipboardText() copySelectedText() selectAllTextAndCopy() sendToAgent(String) downloadFileOnAgent(String downloadLink, String filePath) downloadTextFileFromAgent(String) deleteFileOnAgent(String) Methods to manipulate files folders and Excel: Finding window handles by criteria Identifying process ID when starting program Clicking on element with offset Excel class Files and folders Resource class Script class Working with S3 inside robot plugin Method chaining examples Note that practically all Simplified API methods support chaining because they return a driver, keyboard, mouse, window, UiElement, or UiElementCollection. Therefore you can use the following examples with chaining: Selecting elements, checking conditions (\".errorMessage\").first().shouldBe(visible, enabled) (\"td\").shouldHaveSize(5) (\".edit\").getTexts() (\" myInput\").waitUntil(hasPartialValue(\"John\"), 5000) Mouse driver, browser actions mouse().doubleClick(20, 40).contextClick(100, 78) openLinkInNewWindow(byCssSelector(\".container a.create new\")).switchTo() Windows handling, keyboard driver, typing switchTo().window(\" CLASS:Notepad \").maximize() keyboard().sendKeys(Keys.TAB) window().close() Console automation, typing and copying text window(\" CLASS:PuTTY \") sendKeys(\"vim 123.txt\", Keys.ENTER) copyPuttyWindowText() Finding window handles by criteria To improve automation performance and stability, window handles can be found by criteria, for example, a class, a title, etc. To narrow down the window search, the getWindowHandles method is used. API Set windowHandles = Window.windowHandles(\" CLASS:Notepad \"); The following window attributes are used for search: parentPID processName commandLine class Title Regexp title Examples Class handlesNotepad = Window.windowHandles(\" CLASS:Notepad \"); assert handlesNotepad.size() == 3 Set handlesPaint = Window.windowHandles(\" CLASS:MSPaintApp \"); assert handlesPaint.size() == 2 > Parent PID Performance metrics Previous implementation handles = driver().getWindowHandles() List handlesFiltered = new ArrayList() for(String handle : handles){ try { window(handle) if (driver().getTitle() == \"Untitled Notepad\") { handlesFiltered.add(handle) } } catch (Exception e) {} } } > Current implementation handlesByClassAndTitle = Window.windowHandles(\" CLASS:Notepad; TITLE:Untitled Notepad \"); } > Execution results Total windows Filtered windows Execution time previously (ms) Execution time now (ms) Speed growth 8 3 2,952 161 18 16 3 5,936 164 36 Identifying process ID when starting program The process identifier, normally referred to as the process ID or PID, is a unique decimal number used to uniquely identify an active process. This number may be treated as a parameter in various function calls to manipulate processes while automating desktop applications. That makes it much easier in complex use cases, when you need to know and save the ID of the process created by the open ('app.exe') command, for example, in order to detect all children of the opened application and perform some actions with them. API Integer pid = open(\"notepad.exe\") Integer pid = openAndFocus(\"notepad.exe\", 1000, 100) Description Works for desktop automation only. Returns null value for web drivers. Open method returns only parent process, child processes might exist in your case. Examples The feature can be used in combination with window filtering by process ID. Expand to see the example Clicking on element with offset When automating desktop applications, clicks are often done not only in the center of the selected element but somewhere else (the corner, or the corner with offset). Click offset is a click on the given element with the relative position (x, y) from the top left corner of that element. Clicking on elements with offset facilitates automating complex controls like trees (Outlook) or tables with checkboxes (SAP). API def elem = (By.cssSelector(\" TOOLTIP:ToolTip demo \")); elem.click(elem.right() 30, elem.bottom() 15); elem.click(elem.left() 10, elem.top() 15); elem.click( 10, 15); Description Works for desktop automation only. It throws exception. Valid for any mouse click (double, triple, wheel). Point 0,0 is in the top left corner of the element. Examples The feature can be used in combination with window filtering by process ID. Expand to see the example Excel class The Excel class is intended for automating Excel spreadsheet manipulations, such as getting setting cell values, switching between sheets, saving, etc. All the Excel Actions are executed in the background, so the application window does not appear on the screen. See the Excel action group in RPA Express Recorder for better understanding the Excel API. For a quick start, do as follows. Create a script with files folders manipulations in RPA Recorder. Export this script as a Bot task or export to Groovy code and analyze the auto generated code which uses the Resource API. :::tip To create advanced Excel automation, you can use the Apache POI library. See sample usage in Apache POI Working with MS Office Files ::: To start using Excel methods, it is needed to understand **cell, row, and column position** concept: Scheme with description: Excel Javadocs: Cell Position, Row Column Position When you get set a cell row column, the currently active cell is changed Examples Excel class usage This example performs the following actions: Downloads an excel file from S3 file storage Switches to the sheet by its name and sets an active cell Gets row and column values Searches for a cell with a particular value Copies a range between sheets Saves as a new file and closes the file Expand to see the example Copying range between 2 files This example performs the following actions: Downloads two excel files from S3 file storage Opens the first file and copies a range to a temp variable Opens the second file and sets a range using the temp variable value Saves the second excel as a new file and closes all files Expand to see the example Excel class methods The Excel class has the following methods. Type Method Description void closeExcel(String filePath) Closes excel file and removes it from script context void deleteCell(String filePath, ExcelCellPosition position) Clears cell value void deleteCell(String filePath, String coordinate) Clears cell value String getActiveCell(String filePath) Gets active cell String getCell(String filePath, ExcelCellPosition position) Gets cell value and returns it as string String getCell(String filePath, String coordinate) Gets cell value and returns it as string List getColumn(String filePath, ExcelColumnRowPosition position) Gets column values as List List getColumn(String filePath, ExcelColumnRowPosition position, Integer rowFrom, Integer rowTo) Gets column values as List List getColumn(String filePath, String columnLettes) Gets column values as List List getColumn(String filePath, String columnLettes, Integer rowFrom, Integer rowTo) Gets column values as List List > getRange(String filePath, String coordinateFrom) Gets range values List > getRange(String filePath, String coordinateFrom, String coordinateTo) Gets range values List getRow(String filePath, ExcelColumnRowPosition position) Gets row values as List List getRow(String filePath, ExcelColumnRowPosition position, String columnFrom, String columnTo) Gets row values as List List getRow(String filePath, int rowNum) Gets row values as List List getRow(String filePath, int rowNum, String columnFrom, String columnTo) Gets row values as List void openExcel(String filePath) Reads excel file by path, stores this in script context void saveExcel(String filePath) Saves excel file void saveExcel(String filePath, String newFilePath) Saves excel as new file void setActiveCell(String filePath, ExcelCellPosition position) Sets active cell void setActiveCell(String filePath, String coordinate) Sets active cell void setCell(String filePath, ExcelCellPosition position, String value) Sets cell value void setCell(String filePath, String coordinate, String value) Sets cell value void setCells(String filePath, String coordinate, List values, boolean isVertical) Sets cell value void setRange(String filePath, String coordinateFrom, List > values) Gets range values void setRange(String filePath, String coordinateFrom, String coordinateTo, List > values) Gets range values void switchSheet(String filePath, int index) Selects as active sheet by index void switchSheet(String filePath, String name) Selects as active sheet by index Files and folders Resource class The Resource class is intended for general manipulations with files and folders, for example, creating a folder under the path specified, or a file with a specific content in a defined location, or copy move a folder or a file to the location specified. See the Files and Folders action group in RPA Express Recorder for better understanding the Resource API. For a quick start, do as follows. Create a script with files folders manipulations in RPA Recorder. Export this script as a Bot task or export to Groovy code and analyze the auto generated code which uses the Resource API. Examples Resource class usage This example performs the following actions: Downloads a text file from S3 file storage. Checks that the new file was downloaded to a temp folder. The check result is saved in the is_existing variable. Appends the downloaded text file content with a new content. Reads the result into the content_utf8 variable. Creates a new directory with a new file (randomly generated name). Overwrites this file content with a new string containing the current timestamp. Expand to see the example Actions with files and folders This example performs the following actions: Getting all files and folders recursively and exporting as a list Getting all TXT files recursively and copying them to the destination folder Getting all folders changed in the last five days and moving them to the destination folder Expand to see the example Resource.copyOverwrite(filename, destinationFolder) }; Getting all folders changed in the last 5 days and moving them to destination folder def filterByDate = Filter.folders() .includeSubFolders() .modifiedInLast(Integer.valueOf(\"5\"), ChronoUnit.DAYS) .get() def onlyNewFolders = Resource.listFolder(sourceFolder, filterByDate) onlyNewFolders.each { foldername > Resource.moveSkip(foldername, destinationFolder) }; > Resource class methods The Resource class has the following options: You can choose what to do in case some conflicts occur while the actions are executed, for example, overwrite or skip (keep an existing) file or folder, or fail the process execution. You can create text files with the* encoding* that enables writing, saving, and displaying all characters properly. When using the listFolder() method, you might need to import the following classes: com.workfusion.rpa.helpers.resources.Filter java.time.temporal.ChronoUnit Type Method Description void append(String path, String value, String encoding) Opens a file using the path specified, adds a given string value to the end of the file boolean createDirectoryFail(String path) Creates a folder under a path specified in the input field. If such folder already exists, exception is thrown boolean createDirectoryOverwrite(String path) Creates a folder under a path specified in the input field. If such folder already exists, it will be overwritten boolean createDirectorySkip(String path) Creates a folder under a path specified in the input field. If such folder already exists, no action is taken boolean createFileFail(String path) Creates a file under a path specified in the input field. If such file already exists, exception is thrown boolean createFileOverwrite(String path) Creates a file under a path specified in the input field. If such file already exists, it will be overwritten boolean createFileSkip(String path) Creates a file under a path specified in the input field. If such file already exists, no action is taken boolean delete(String path) Deletes a file or a folder under a path specified with entire content boolean exist(String path) Checks if a file or a folder already exists and returns a Boolean result void overwrite(String path, String value, String encoding) Opens a file using the path specified, deletes file content, adds a given string value to the file String read(String path, String encoding) Reads content from a specified file and returns a string value. List listFolder(String path) Reads content of a defined folder including files and sub folders, and returns the result (full paths to the items) as a List List listFolder(String path, Filter filter) Reads content of a defined folder, filters the content (Filter object), and returns the result (full paths to the items) as a List void moveFail(String resourceFrom, String pathTo) Moves a file or a folder from one location to another. If such file or folder already exists, exception is thrown void moveOverwrite(String resourceFrom, String pathTo) Moves a file or a folder from one location to another. If such file or folder already exists, it will be overwritten void moveSkip(String resourceFrom, String pathTo) Moves a file or a folder from one location to another. If such file or folder already exists, no action is taken void copyFail(String resourceFrom, String pathTo) Copies file or folder from one location to another. If such file or folder already exists, exception is thrown. void copyOverwrite(String resourceFrom, String pathTo) Copies file or folder from one location to another. If such file or folder already exists, no action is taken. void copySkip(String resourceFrom, String pathTo) Copies file or folder from one location to another. If such file or folder already exists, no action is taken. Script class Executes Script on RPA agent side, not in Сontrol Tower. The same as: driver.executeScript(...) Examples GroovyScript This example performs the following actions: Executes GroovyScript (sum operation) and stores result in the result1 variable Executes GroovyScript with timeout. Action fails if execution time exceeds 31 seconds Expand to see the example JavaScript This example performs the following actions: Opens Selenium localhost page Executes JavaScript Stores script output in the result variable Expand to see the example AutoitScript This example executes AutoitScript with timeout. Action fails if execution time exceeds 10 seconds. Expand to see the example GroovyScript with params This example performs the following actions: Executes GroovyScript with timeout and additional parameters Returns script results to the result variable Expand to see the example Script class methods The Script class has the following methods. Type Method Description static T executeAutoitScript(String code) Executes AutoIt Script static T executeAutoitScript(String code, long timeout) Executes AutoIt Script with execution timeout static T executeAutoitScript(String code, long timeoutInMillis, Object... args) Executes AutoIt Script with execution timeout and additional arguments static T executeGroovyScript(String code) Executes Groovy Script static T executeGroovyScript(String code, long timeoutInMillis) Executes Groovy Script with execution timeout static T executeGroovyScript(String code, long timeoutInMillis, ScriptParams arguments) Executes Groovy Script with execution timeout and additional arguments static T executeGroovyScript(String code, ScriptParams arguments) Executes Groovy Script with additional arguments static T executeJavaScript(String code, Object... args) Executes JavaScript with additional arguments S3 inside robot plugin There is a special class for working with S3 on the RPA Bot side (Windows RPA Server). Call example Downloading file The method for downloading a file from S3 to the RPA Bot side returns absolute path to a local file. The attributes are enumerated in the table below. Parameter Required Description s3EndpointUrl yes url to S3 signerType yes S3 signature types (S3SignerType) accessKey yes access key to S3 bucket secretKey yes secret key to S3 bucket bucket yes bucket name s3Key yes path to file on bucket targetPath yes path to destination file timeout no timeout, i.e., how long to wait for response (in milliseconds) Expand to see the example Uploading file The method for uploading a file to S3 on the RPA Bot side returns a link to the uploaded file. The attributes are given in the table below. Parameter Required Description s3EndpointUrl yes url to S3 signerType yes S3 signature types (S3SignerType) accessKey yes access key to S3 bucket secretKey yes secret key to S3 bucket bucket yes bucket name s3Key yes path to file on bucket sourcePath yes path to source file strategy yes ENUM (S3OverwriteStrategy) for values OVERWRITE, SKIP, and FAIL timeout no timeout, i.e., how long to wait for response (in milliseconds) Expand to see the example Creating folder The method for creating a folder in S3 on the RPA Bot side returns a link to the created folder. The attributes are given in the table below. Parameter Required Description s3EndpointUrl yes url to S3 signerType yes S3 signature types (S3SignerType) accessKey yes access key to S3 bucket secretKey yes secret key to S3 bucket bucket yes bucket name s3Key yes path to folder on bucket Expand to see the example Deleting folder or file The method for deleting a folder or a file in S3 on the RPA Bot side has the following attributes. Parameter Required Description s3EndpointUrl yes url to S3 signerType yes S3 signature types (S3SignerType) accessKey yes access key to S3 bucket secretKey yes secret key to S3 bucket bucket yes bucket name s3Key yes path to folder or file on bucket Expand to see the example Getting directory list The method for getting a list of folders and files from S3 bucket returns a list of JSON objects with information about an S3 object (a file or a folder). The attributes are given in the table below. Parameter Required Description s3EndpointUrl yes url to S3 signerType yes S3 signature types (S3SignerType) accessKey yes access key to S3 bucket secretKey yes secret key to S3 bucket bucket yes bucket name s3Key yes path to root folder or file on bucket Expand to see the example 0) { columnSet = new ArrayList(directoryList.get(0).keySet()); } else { columnSet = new ArrayList(); } sys.defineVariable(\"directoryList\", directoryList); sys.defineVariable(\"columnSet\", columnSet) } > "},{"version":"10.0","date":"Aug-06-2019","title":"switch-to-window","name":"Switching to window, frame, popup","fullPath":"iac/rpa/developer-guide/switch-to-window","content":" Window selectors Before starting to automate a desktop or web application (bot clicking, typing, etc), you need to switch to a particular window by its unique parameters: Handle Title Name Class Instance :::note You can search windows by exact match or by a regular expression (REGEXPNAME, REGEXPTITLE, etc.). ::: Web browser windows The following window properties can be used to switch to a particular browser window (tab) using Web driver (Chrome, IE, Firefox, Edge): Text string Title: see how to switch to browser window by its title: window('Confluence Chrome') Handle: the window handle value is dynamic so you need to get it for each bot execution using driver().getWindowHandle() or driver().getWindowHandles() methods. See how to switch to browser window by its handle: window('CDwindow (EC7514DEC6898198C75C31A50145CAE7)') Desktop windows The following window properties can be used to switch to a particular desktop app window using Desktop driver: Text string Title: see how to switch to desktop app window by its title: window('Open file') Handle: see how to switch to desktop app window by its handle: window(' HANDLE:0x0000000000020340 ') CSS: see how to switch to desktop app window by its title, class, name, instance in CSS notation: window('.window class title =Submit ') window('* class =Total Commander ') window(' name*=Submit ') window('.window class instance=4 ') Object NAME, REGEXPNAME window(' NAME: New Connection ') window(' REGEXPNAME: .Connection ') CLASS, REGEXPCLASS window(' CLASS: V8Win ') window(' REGEXPCLASS: V. ') TITLE, REGEXPTITLE window(' TITLE: V8Win ') window(' REGEXPTITLE: V. ') INSTANCE window(' INSTANCE: 3 ') Switching to windows and browser tabs Some web applications have many frames or multiple windows. The Web Driver assigns an alphanumeric ID to each window as soon as the WebDriver object is instantiated. This unique alphanumeric ID is called window handle . RPA driver uses this unique ID to switch control among several windows. In simple terms, each unique window has a unique ID, so that robot can differentiate when it is switching controls from one window to the other. getWindowHandle Purpose: To get the window handle of the current window. Return a string of alphanumeric window handle: def handle = driver().getWindowHandle() output for chrome: CDwindow (EC7514DEC6898198C75C31A50145CAE7) getWindowHandles Purpose: To get the window handle of all the current windows. Return a set of window handles: def allHandles = driver().getWindowHandles() output for desktop driver: HANDLE:0x0000000000010768 , HANDLE:0x0000000000020340 , HANDLE:0x00000000000C075C , HANDLE:0x0000000000040758 , HANDLE:0x00000000000107B0 , HANDLE:0x0000000000010406 switchTo Window Purpose: WebDriver supports moving between named windows using the switchTo method. switchTo().window(\"windowName\") Alternatively, you can pass a window handle to the switchTo().window() method. Knowing this, it’s possible to iterate over every open window like so: be careful with the SEARCHALLWINDOWS capability as driver will close all windows when the script finishes execution! for (String handle : driver().getWindowHandles()) { switchTo().window(handle); } You can also switch between windows with Iterators: open('https: www.w3schools.com tags attatarget.asp') ('.w3 btn.w3 margin bottom').click() sleep(2000) Set handles = driver().getWindowHandles() firstWinHandle = driver().getWindowHandle() handles.remove(firstWinHandle) String winHandle = handles.iterator().next() if (winHandle!=firstWinHandle) { To retrieve the handle of second window, extracting the handle which does not match to first window handle secondWinHandle=winHandle Storing handle of second window handle Switch control to new window switchTo().window(secondWinHandle) } switchToExistingWindow Command Purpose: WebDriver supports switching to existing window (already opened). With default timeout 100 milliseconds: With custom timeout specified in milliseconds: With custom timeout specified in milliseconds (without Simplified API): Switching to frames and popups Purpose: WebDriver supports moving between named frames using the switchTo method. switchTo().frame(\"frameName or id\") :::tip See more information about iframes in Handling iFrames. ::: switchTo PopUp Command Purpose: WebDriver supports moving between named popUps using the switchTo method. After you’ve triggered an action that opens a popup, you can access the alert and it will return the currently open alert object. With this object you can now accept, dismiss, read its contents or even type into a prompt. This interface works equally well on alerts, confirms, and prompts. alert = switchTo().alert() :::tip See more information about alerts in Handling JavaScript Alerts. ::: Other switch commands switchToLastWindow() – switching focus to the last opened window switchToNextWindow() – switching focus to the next opened window after current window switchToPrevWindow() – switching focus to the previously opened window before current window switchToRootWindow() – switching focus to the first opened window Examples The example below shows how to manipulate window handles and switch between browser windows. Expand to see how to switch to windows by their handle :::note There is no cross browser solution for manipulating with tabs, so we show the example solution for Chrome. ::: Expand to see how to open a new tab using CTRL click "},{"version":"10.0","date":"Aug-02-2019","title":"surface-based-robotics-driver","name":"Surface-based Robotics driver","fullPath":"iac/rpa/developer-guide/surface-based-robotics-driver","content":" Overview To automate console and core applications where there is no way to get a window or element locator, you can use WorkFusion's image based (or surface based) driver for automating desktop and web applications. :::tip For faster image capturing and defining offsets, you can use the RPA Recorder for: making screenshots media files panel exporting code ::: Surface Selector byImage() The Robotics API has been extended with a byImage(String imageUrl, int offsetX, int offsetY) selector which enables you to locate interface elements by their screenshots. The bot will perform click, hover, or other actions directly into the geometrical center of the screenshot (with an offset in pixels if defined): offset X coordinate is positive from center to the right offset Y coordinate is positive from center to bottom Center click Click with offset Center click Click with offset Robotics API example (byImage(\"https: server name 1478701332260 click.png\")).doubleClick(); Images should be uploaded to a server and should be accessible through HTTP. :::note Mind that non Latin symbols are not allowed when providing the image file path in the byImage selector. ::: Surface Capability imageSimilarityThreshold For surface based automation, it is possible to set the image similarity threshold capability, which can help solve complicated cases where images should strictly match (or alternatively be alike by 60%). The imageSimilarityThreshold capability can take double values from 0.0 to 1.0. imageSimilarityThreshold syntax section, add the enableTypeOnScreen()` method to disable this behavior, use the disableTypeOnScreen() method Enabling this typing option is not stable because random popup windows can appear while bot execution. We recommend using the window() method for each new window. Typing on screen example Multiple image search (byImage) returns a collection of similar images. Multiple image search 0 } inDesktop() { def imageCollection = (byImage(image_1)).size() assert imageCollection == objectCollection def imageElements = (byImage(image_1)) imageElements.each { it.click() } } inChrome() { (byXpath(\" html body img\")).each { assert it.getAttribute(\"value\") == expectedResult } } inDesktop() { def imageCollection = (byImage(image_2)).size() assert imageCollection == objectCollection } > Examples Here, you can find several examples on how to use the surface based robotics driver. Clicking on Win10 calculator The example below shows clicking on Win10 calculator. The following images are used: https: rpa grid.s3.amazonaws.com integration test images win 10 calculator calculator2.png https: rpa grid.s3.amazonaws.com integration test images win 10 calculator menu.png Win 10 calculator sample Getting location and size There is also an ability to get the size of the rectangle found on screen by robot and its location: getSize() getLocation() getRect() getWidth() getHeight() getPoint() getX() getY() GetDimension() Getting location and size Using image based selectors in RPA API See some samples on how to use image based selectors in RPA API. Image based RPA examples def imagePath = \"https: your server some folder \"; (byImage(\" {imagePath} image1.png\")).click(); (byImage(\" {imagePath} image1.png\"), 40, 60).doubleClick(); (byImage(\" {imagePath} image1.png\")).tripleClick(); (byImage(\" {imagePath} image1.png\")).click(n); (byImage(\" {imagePath} image1.png\")).contextClick(); (byImage(\" {imagePath} image2.png\")).hover(); (byImage(\" {imagePath} image3.png\")).isExists(); (byImage(\" {imagePath} image4.png\"), 25, 77).getLocation(); (byImage(\" {imagePath} image5.png\"), 10, 77).getCoordinates(); actions() .dragAndDrop( (byImage(\" {imagePath} source folder.png\")), (byImage(\" {imagePath} target folder.png\"))) .build().perform(); actions().dragAndDrop( (byImage(\" {imagePath} source.png\")), xOffset, yOffset).build().perform(); actions().clickAndHold( (byImage(\" {imagePath} image.png\"))).build().perform(); actions().release( (byImage(\" {imagePath} image.png\"))).build().perform(); actions().moveToElement( (byImage(\" {imagePath} image.png\"))).build().perform(); "},{"version":"10.0","date":"Aug-06-2019","title":"test-sap-alv","name":"Testing SAP ALV","fullPath":"iac/rpa/developer-guide/test-sap-alv","content":" :::note ALV, or ABAP List Viewer, is one of the most useful and powerful SAP GUI Controls. When displayed as a list, it might not be identified as a \"table\" on the screen, rather as a set of text rows labels. ::: ALV list and grid Open SAP GUI and log in. Start the SE38 transaction. Enter the transaction code SE38 into the OK Code field and press Enter. Select the SALV DEMO TABLE LAYOUT or other SALV DEMO * report and execute (F8). On the selection screen, choose one of the options (all three are to be tested). Click on the Execute button or press F8. On the grid that appears, all the buttons columns headers rows are clickable. ALV hierarchical list Start the SE38 transaction. Select the SALV DEMO HIERSEQ COLUMN or other SALV DEMO _HIERSEQ * report and execute (F8). On the selection screen, click on the Execute button or press F8. All the buttons columns headers rows are clickable and can be used. ⇪ Back to SAP automation examples "},{"version":"10.0","date":"Aug-06-2019","title":"test-sap-controls","name":"Testing SAP controls","fullPath":"iac/rpa/developer-guide/test-sap-controls","content":" :::note The use case allows to view SAP UI controls. The easy way to test SAP UI controls is to use the transactions DWDM or SE83. ::: Open SAP GUI and log in. Start the SE83 transaction. Enter the transaction code SE83 into the OK Code field and press Enter. Select the respective control and its example. Double click the example, open the report, and press the Execute button F8. ⇪ Back to SAP automation examples "},{"version":"10.0","date":"Aug-02-2019","title":"take-screenshots","name":"Taking screenshots","fullPath":"iac/rpa/developer-guide/take-screenshots","content":" Screenshot of browser getScreenshotAs() See JavaDoc here, the following screenshot types are supported: BYTES, BASE64, FILE. Alternatively, you can use the screenshotAsImage() method. Web Harvest function That is a generic function which takes browser screen capture and saves it to S3. Screenshot of Java applet Active window standard Java library "},{"version":"10.0","date":"Aug-06-2019","title":"universal-rpa-driver","name":"Universal RPA driver","fullPath":"iac/rpa/developer-guide/universal-rpa-driver","content":" The new Universal driver provides a lot of benefits for rapid RPA scripts development: No need to switch between drivers and create multiple plugins. Universal driver can be used in 99% of use cases. Almost all robotics use cases don't need complex driver juggling (with transactions handling and support). To use the Universal Driver, complete the following conditions: in the plugin, set the driver=\"universal\" attribute add a capability inside the plugin: Expand to see the Universal driver example See more Universal driver examples here: Automating File Upload Automating SwingSet App Explicitly set driver type in Universal driver While using the Universal driver in complex use cases, the driver auto switching algorithm can fail to automatically detect which application (web or desktop) should accept the current action. For example: (byXpath(\" input @id='searchInput' \")).click() web switchTo().window(' CLASS:SunAwtFrame; TITLE:SwingSet2 ') (byXpath(\" @text='Eyes' following sibling::\")).click() desktop To handle such cases, you can use an explicit driver switching methods which accept a closure with all actions for this driver: inDesktop{} inChrome{} inFirefox{} inEdge{} inIE{} For instructions which are outside the closure, the default auto switching mechanism is applied. Explicit driver closures improve the stability and readability of your code. Using driver type closures Universal driver code generated in WorkFusion Studio To view an example of an RPA Bot task with Universal driver, you can: Create a new recording in RPA Recorder. Click Export code to Bot Task. As a result, a valid bot task with Universal Driver is generated: "},{"version":"10.0","date":"Aug-02-2019","title":"tools","name":"Tools - ElementLocator, XPath helper","fullPath":"iac/rpa/developer-guide/tools","content":" XPath Helper plugin for Chrome browser This plugin helps dynamically write and apply XPath to a current page. Very useful to test XPath which aimed to get multiple results i.e. required set of links from HTML Installation Go to https: chrome.google.com webstore detail xpath helper hgimnogjllphhhkhlmebbmlgjoejdpjl hl=en Click Add to Chrome. :::note Note: If you open the URL in any other browser, you will see the option Available in Chrome instead of Add to Chrome. Click on the Add button. Once it is installed, it will display a pop up message for successful installation. :::important After installing this extension, you must reload any existing tabs or restart Chrome for the extension to work. ::: Now the icon for XPath Helper will appear on the top right most side of the Chrome browser. Just click on it to open the helper. How to use XPath Helper Open a new tab and navigate to any webpage. Hit Ctrl Shift X (or Command Shift X on OS X), or click the XPath Helper button in the toolbar, to open the XPath Helper console. Hold down Shift as you mouse over elements on the page. The query box will continuously update to show the XPath query for the element below the mouse pointer, and the results box will show the results for the current query. If desired, edit the XPath query directly in the console. The results box will immediately reflect your changes. Repeat step (2) to close the console. :::note If the console gets in your way, hold down Shift and then move your mouse over it; it will move to the opposite side of the page. ::: When rendering HTML tables, Chrome inserts artificial tags into the DOM, which will consequently show up in queries extracted by this extension. Build your own XPath For Registration Link on the page, try your own XPath: li @id=’menu item 374′ :::note In case of invalid XPath, the result side will display error but if the XPath syntax is correct and no matches found then it will say NULL. ::: Try another XPath for the same element: li @id=’menu item 374′ WebDriver element locator WebDriver Element Locator is a decent add on for Firefox browser which lets you do just that and save a lot of your time. As this is a add on to Firebox, it is easy to use and to use this, just right click on the web element you wish to locate, select an appropriate locator string and it’ll be copied to your clipboard. It would show multiple options of element locators in your browser’s context menu. It displays the element locator with the complete robotic script in different languages like C , Java, Python, and Ruby. :::tip To disable any locator types you don’t need (e.g., Support, Ruby), just go to Tools > Add ons (Ctrl Shift A), and un check them in the add on options. ::: How to download Go to Tools > Add Ons. Search for WebDriver Element Locator. Click on Install. Restart the Firefox Browser. Add on features It also checka the locators for uniqueness, signified by red crosses and green ticks. If elements have long, fragile, auto generated attributes such as id=”ctl00 ElementContainer Inputs _txtForename” it will attempt to locate based on the final (and most significant) part of the value only. If locating via attributes is difficult, it also attempts to locate via text value. This extension will attempt to populate the context menu with usable webdriver XPATH based findElement commands for the dotnet, python and ruby bindings and Support Locator Library references for the focused web element. "},{"version":"10.0","date":"Jul-29-2019","title":"update-chrome-and-firefox-drivers","name":"Updating Chrome and Firefox drivers","fullPath":"iac/rpa/developer-guide/update-chrome-and-firefox-drivers","content":" If your RPA scripts stopped working after browser update, proceed as follows. Chrome driver Download the latest driver here. Replace the old one in RPA rpa grid drivers windows x32. Firefox driver Download the latest drivers (both x32 and x64) here. Replace old drivers in the following directories: geckodriver vx win32.zip > in RPA rpa grid drivers windows x32 geckodriver vx win64.zip > in RPA rpa grid drivers windows x64 "},{"version":"10.0","date":"Aug-02-2019","title":"webdriver-waits","name":"Webdriver waits","fullPath":"iac/rpa/developer-guide/webdriver-waits","content":" Why do we need waits in web automation Most web applications are developed using Ajax and JavaScript. When a page is loaded in our browser, the elements which we want to interact with may load at different time intervals. Not only does this make it difficult to identify an element, but if the element is not located it will also throw an ElementNotVisibleException exception. Using Waits, we can resolve this problem. Let's consider a scenario where we have to use both implicit and explicit waits. Assume that the implicit wait time is set to 20 seconds and the explicit wait time is set to 10 seconds. Suppose we are trying to find an element which has some ExpectedConditions (Explicit Wait), and the element is not located within the time frame defined by the Explicit wait (10 seconds). It will use the time frame defined by implicit wait (20 seconds) before throwing an ElementNotVisibleException. Implicit waits vs. explicit waits Parameter Explicit Wait Implicit Wait Side Client side Driver side Timing 500 ms (lower) 1000 ms (higher) Automatic No, it is applied only to those elements which are intended by user Yes, it is applied to all the elements in the script Need to specify ExpectedConditions on the element to be located Yes No Use Case This is recommended to use when elements are taking a long time to load and also for verifying the property of an element like visibilityOfElementLocated, elementToBeClickable, elementToBeSelected This is recommended to use when elements are located with the time frame specified in implicit wait Wait for what Do NOT use for element presence checkCan wait for anything Is used for the following methods: driver.findElement()driver.findelements()And waits until element appears in DOM. Exception type TimeoutException NoSuchElementException Network calls count Multiple Single :::note Conclusion: Implicit, Explicit and Fluent Wait are the different waits used in the web driver. It is always not recommended to use Thread.Sleep(). Usage of these waits are totally based on the elements which are loaded at different intervals of time. Set Default polling interval > 100 ms. ::: Implicit Waits The implicit wait will tell to the web driver to wait for certain amount of time before it throws a \"No Such Element Exception\". The default setting is 0. Once we set the time, web driver will wait for that time before throwing an exception. In the below example we have declared an implicit wait with the time frame of 10 seconds. It means that if the element is not located on the web page within that time frame, it will throw an exception. To declare implicit wait, do as follows. timeouts().implicitlyWait(TimeOut, TimeUnit.SECONDS); In the above example, Codeline 19: Implicit wait will accept 2 parameters, the first parameter will accept the time as an integer value and the second parameter will accept the time measurement in terms of SECONDS, MINUTES, MILLISECOND, MICROSECONDS, NANOSECONDS, DAYS, HOURS, etc. Explicit Waits The explicit wait is used to tell the Web Driver to wait for certain conditions (Expected Conditions) or the maximum time exceeded before throwing an ElementNotVisibleException exception. The explicit wait is an intelligent kind of wait, but it can be applied only for specified elements. Explicit wait gives better options than that of an implicit wait as it will wait for dynamically loaded Ajax elements. Once we declare explicit wait we have to use ExpectedCondtions or we can configure how frequently we want to check the condition using Fluent Wait. In the below example, we are creating reference wait for WebDriverWait class and instantiating using WebDriver reference, and we are giving a maximum time frame of 20 seconds. WebDriverWait wait = new WebDriverWait(WebDriverRefrence,TimeOut); Code line 39: In the above example, wait for the amount of time defined in the \"WebDriverWait\" class or the \"ExpectedConditions\" to occur whichever occurs first. The above Java code states that we are waiting for an element for the time frame of 20 seconds as defined in the \"WebDriverWait\" class on the webpage until the ExpectedConditions are met and the condition is visibilityofElementLocated. The following are the Expected Conditions that can be used in Explicit Wait: alertIsPresent() elementSelectionStateToBe() elementToBeClickable() elementToBeSelected() frameToBeAvaliableAndSwitchToIt() invisibilityOfTheElementLocated() invisibilityOfElementWithText() presenceOfAllElementsLocatedBy() presenceOfElementLocated() textToBePresentInElement() textToBePresentInElementLocated() textToBePresentInElementValue() titleIs() titleContains() visibilityOf() visibilityOfAllElements() visibilityOfAllElementsLocatedBy() visibilityOfElementLocated() :::tip See documentation of ExpectedCondition class http: s3.amazonaws.com pub _demo machine javadoc ExpectedConditions.html ::: Fluent waits The fluent wait is used to tell the web driver to wait for a condition, as well as the frequency with which we want to check the condition before throwing an ElementNotVisibleException exception. Frequency is setting up a repeat cycle with the time frame to verify check the condition at the regular interval of time Let's consider a scenario where an element is loaded at different intervals of time. The element might load within 10 seconds, 20 seconds or even more then that if we declare an explicit wait of 20 seconds. It will wait till the specified time before throwing an exception. In such scenarios, the fluent wait is the ideal wait to use as this will try to find the element at different frequency until it finds it or the final timer runs out. Wait wait = new FluentWait(WebDriver reference).withTimeout(timeout, SECONDS).pollingEvery(timeout, SECONDS).ignoring(Exception.class); Code line 39: In the above example, we are declaring a fluent wait with the timeout of 30 seconds and the frequency is set to 5 seconds by ignoring NoSuchElementException. Declaring fluent wait with time out of 30 seconds and frequency is set to 5 seconds: Wait wait = new FluentWait(driver) .withTimeout(30, TimeUnit.SECONDS) .pollingEvery(5, TimeUnit.SECONDS) .ignoring(NoSuchElementException.class); Code Line 46: We have created a new function to identify the Web Element on the page. (Ex: Here Web Element is nothing but the web link on the webpage). Frequency is set to 5 seconds and the maximum time is set to 30 seconds. Thus this means that it will check for the element on the web page at every 5 seconds for the maximum time of 30 seconds. If the element is located within this time frame it will perform the operations else it will throw an \"ElementNotVisibleException\" Identifying a web element on the page with frequency set to 5 seconds: WebElement clickweblink = wait.until(new Function() { public WebElement apply(WebDriver driver) { return driver.findElement(By.xpath(\" * @id='java_technologies' li 3 a\")); } }); click on the guru99 web link clickweblink.click(); Sleep command This is rarely used, as it always force the browser to wait for a specific time. Thread.Sleep is never a good idea and that’s why web driver provides wait primitives. If you use them you can specify much higher timeout value which makes tests more reliable without slowing them down as the condition can be evaluated as often as it’s required. Thread.sleep(3000); Detailed waits description How to handle Ajax call using Webdriver The biggest challenge in handling Ajax call is knowing the loading time for the web page. Since the loading of the web page will last only for a fraction of seconds, it is difficult for the tester to test such application through automation tool. For that, Webdriver has to use the wait method on this Ajax Call. So by executing this wait command, web driver will suspend the execution of current test case and wait for the expected or new value. When the new value or field appears, the suspended test cases will get executed by Webdriver. Following are the wait methods that Webdriver can use 1. Implicit Wait() This method tells webdriver to wait if the element is not available immediately, but this wait will be in place for the entire time the browser is open. So any search for the elements on the page could take the time the implicit wait is set for. 2. Explicit Wait() Explicit wait is used to freeze the test execution till the time a particular condition is met or maximum time lapses. 3. WebdriverWait It can be used for any conditions. This can be achieved with WebDriverWait in combination with ExpectedCondition The best way to wait for an element dynamically is checking for the condition every second and continuing to the next command in the script as soon as the condition is met. But the problem with all these waits is, you have to mention the time out unit. What if the element is still not present within the time So there is one more wait called Fluent wait. 4. Fluent Wait This is an implementation of the Wait interface having its timeout and polling interval. Each FluentWait instance determines the maximum amount of time to wait for a condition, as well as the frequency with which to check the condition. 5. Thread.Sleep() Thread.Sleep () is not a wise choice as it suspends the current thread for the specified amount of time. In AJAX, you can never be sure about the exact wait time. So, your test will fail if the element won't show up within the wait time. Moreover, it increases the overhead because calling Thread.sleep(t) makes the current thread to be moved from the running queue to the waiting queue. After the time 't' reached, the current thread will move from the waiting queue to the ready queue, and then it takes some time to be picked by the CPU and be running. Challenges in handling Ajax call in Webdriver Using \"pause\" command for handling Ajax call is not completely reliable. Long pause time makes the test unacceptably slow and increases the testing time. Instead, \"waitforcondition\" will be more helpful in testing Ajax applications. It is difficult to assess the risk associated with particular Ajax applications Given full freedom to developers to modify Ajax application makes the testing process challenging Creating automated test request may be difficult for testing tools as such AJAX application often use different encoding or serialization technique to submit POST data. Thus, the summary is as follows. AJAX allows the Web page to retrieve small amounts of data from the server without reloading the entire page. To test Ajax application, different wait methods should be applied ThreadSleep Implicit Wait Explicit Wait WebdriverWait Fluent Wait Creating automated test request may be difficult for testing tools as such AJAX application often use different encoding or serialization technique to submit POST data. Explicit waits in details Test automation scripts should synchronize with the web site every time they interact with website elements. The synchronization is done using explicit waits and expected conditions. Why do we need explicit waits Lets take the simplest WebDriver method: driver.findElement(locator). How does it work findElement() tries finding the element matched by the locator in the browser DOM. If the element is found in the browser DOM, findElement() returns it. Otherwise, findElement() fails. findElement() works well if the website is fast. But if the website is slow and elements are not in the browser DOM when findElement() is executed, findElement() will fail. We need a way of interacting with website elements that waits until the website elements are in the browser DOM. What are explicit waits An explicit wait object uses the WebDriverWait class. It gets two parameters: the driver object a timeout WebDriverWait wait = new WebDriverWait(driver, timeout); The explicit wait works by waiting until an expected condition is reached. The expected condition is created using the ExpectedConditions class: wait.until(ExpectedConditions.condition(parameters)); To see how an explicit wait works, refer to the following example: WebDriverWait wait; wait = new WebDriverWait(driver, 10); WebElement element; element=wait.until(ExpectedConditions. elementToBeClickable(locator)); ​1. The wait object is created using the driver object and a 10 seconds timeout as parameters. ​2. The until() method will start a timer. ​3. The until() method verifies if the expected condition is met: element matched by the locator is in the browser DOM and is clickable. ​4. If the condition is met, the until() method returns the found element; the explicit wait process finishes successfully. ​5. If the condition is not met and the timer did not reach yet the timeout value, the process continues from step 3. ​6. If the condition is not met and the time reached the timeout value, the explicit wait finished with an error. When can we use explicit waits Explicit waits can be used in the following cases: ​ find single web element find multiple web elements check web page title and URL check element’s status interact with frames (not included in this article) The following expected conditions can be used for finding a web element. It is recommended that they are used instead driver.findElement(). Code samples Expand to see details with code samples elementToBeClickable ExpectedCondition elementToBeClickable(By locator) ExpectedCondition elementToBeClickable(WebElement element) Defines an expectation for checking that an element is visible and enabled such that you can click it. Example: The next 2 lines of code search for the element matched by the locator. If the element can be found in the browser DOM and has the clickable status within 10 seconds, it is returned and saved in a WebElement variable. The locator can be by xpath, css, id or name locator. WebDriverWait wait = new WebDriverWait(driver, 10); WebElement element; element = wait.until(ExpectedConditions. elementToBeClickable(locator)); presenceOfElementLocated ExpectedCondition presenceOfElementLocated(By locator) Defines an expectation for checking that an element is present on the DOM of a page. Example: The next 2 lines of code search for the element matched by the locator. If the element can be found in the browser DOM within 10 seconds, it is returned and saved in a WebElement variable. The locator can be by XPath, CSS, ID or name locator. `WebDriverWait wait = new WebDriverWait(driver, 10); WebElement element; element = wait.until(ExpectedConditions. presenceOfElementLocated(locator)); ` visibilityOfElementLocated ExpectedCondition visibilityOfElementLocated(By locator) ExpectedCondition visibilityOf(WebElement element) Defines an expectation for checking that an element is present on the DOM of a page and visible. Example: The next 2 lines of code search for the element matched by the locator. If the element can be found in the browser DOM and is visible within 10 seconds, it is returned and saved in a WebElement variable. The locator can be by XPath, CSS, ID or name locator. WebDriverWait wait = new WebDriverWait(driver, 10); WebElement element; element = wait.until(ExpectedConditions. visibilityOfElementLocated(locator)); The following expected conditions can be used for finding multiple web elements. It is recommended to use them instead of driver.findElements(). visibilityOfAllElementsLocatedBy ExpectedCondition> visibilityOfAllElementsLocatedBy(By locator) ExpectedCondition> visibilityOfAllElements(List elements) Defines an expectation for checking that all elements present on the web page that match the locator are visible. Example: The next 2 lines of code search for all elements matched by the locator. If element(s) can be found in the browser DOM and is (are) visible within 10 seconds, they are returned and saved in a list of WebElement variables. The locator can be by XPath, CSS, ID or name locator. WebDriverWait wait = new WebDriverWait(driver, 10); List elements; elements =wait.until(ExpectedConditions. visibilityOfAllElementsLocatedBy(locator)); presenceOfAllElementsLocatedBy ExpectedCondition> presenceOfAllElementsLocatedBy(By locator) Defines an expectation for checking that there is at least one element present on a web page. Example: The next 2 lines of code search for all elements matched by the locator. If element(s) can be found in the browser DOM within 10 seconds, they are returned and saved in a list of WebElement variables. The locator can be by XPath, CSS, ID or name locator. `WebDriverWait wait = new WebDriverWait(driver, 10); List elements; elements = wait.until(ExpectedConditions. presenceOfAllElementsLocatedBy(locator)); ` The following expected conditions can be used for checking the web page title and url. It is recommended that they are used instead of driver.getTitle() and driver.getCurrentUrl(). titleContains ExpectedCondition titleContains(java.lang.String title) An expectation for checking that the title contains a case sensitive substring. Example: The next 2 lines of code verify if the page title contains a keyword. If the keyword is included in page title within 10 seconds, the explicit wait returns true. Otherwise, the explicit wait returns false. WebDriverWait wait = new WebDriverWait(driver, 10); assertTrue(wait.until(ExpectedConditions. titleContains(keyword))); titleIs `ExpectedCondition titleIs(java.lang.String title)` Defines an expectation for checking the title of a page. Example: The next 2 lines of code verify if the page title is equal to a specific value. If the page title is equal to a specific value within 10 seconds, the explicit wait returns true. Otherwise, the explicit wait returns false. `WebDriverWait wait = new WebDriverWait(driver, 10); assertTrue(wait.until(ExpectedConditions. titleIs(titleValue)));` urlContains `ExpectedCondition urlContains(java.lang.String fraction)` Defines an expectation for the URL of the current page to contain specific text. Example: The next 2 lines of code verify if the page url contains a keyword. If the keyword is included in the page url within 10 seconds, the explicit wait returns true. Otherwise, the explicit wait returns false. `WebDriverWait wait = new WebDriverWait(driver, 10); assertTrue(wait.until(ExpectedConditions. urlContains(keyword)));` urlToBe `ExpectedCondition urlToBe(java.lang.String url)` An expectation for the URL of the current page to be a specific url. Example: The next 2 lines of code verify if the page url is equal to a specific value. If the page url is equal to a specific value within 10 seconds, the explicit wait returns true. Otherwise, the explicit wait returns false. `WebDriverWait wait = new WebDriverWait(driver, 10); assertTrue(wait.until(ExpectedConditions. urlToBe(urlValue))); ` urlMatches `ExpectedCondition urlMatches(java.lang.String regex)` Expectation for the URL to match a specific regular expression Example: The next 2 lines of code verify if the page url matches a regular expression. If the page url matches the regular expression within 10 seconds, explicit wait returns true. Otherwise, the explicit wait returns false. `WebDriverWait wait = new WebDriverWait(driver, 10); assertTrue(wait.until(ExpectedConditions. urlMatches(regularExpression)));` These expected conditions can be used for verifying the element’s status. They are ideal for being used in assertions. elementSelectionStateToBe `ExpectedCondition elementSelectionStateToBe(By locator, boolean selected) ExpectedCondition elementSelectionStateToBe(WebElement element, boolean selected)` Defines an expectation for checking if the given element is selected. Example: The next 2 lines of code verify if the element is selected. If the element is selected within 10 seconds, the explicit wait returns true. Otherwise, the explicit wait returns false. `WebDriverWait wait = new WebDriverWait(driver, 10); assertTrue(wait.until(ExpectedConditions. elementSelectionStateToBe(locator, true)));` elementToBeSelected `ExpectedCondition elementToBeSelected(By locator) ExpectedCondition elementToBeSelected(WebElement element)` An expectation for checking if the given element is selected. Example: The next 2 lines of code verify if the element is selected. If the element is selected within 10 seconds, the explicit wait returns true. Otherwise, the explicit wait returns false. `WebDriverWait wait = new WebDriverWait(driver, 10); assertTrue(wait.until(ExpectedConditions. elementToBeSelected(locator)));` invisibilityOfElementLocated `ExpectedCondition invisibilityOfElementLocated(By locator)` An expectation for checking that an element is either invisible or not present on the DOM. Example: The next 2 lines of code verify if the element is invisible. If the element is invisible within 10 seconds, the explicit wait returns true. Otherwise, the explicit wait returns false. `WebDriverWait wait = new WebDriverWait(driver, 10); assertTrue(wait.until(ExpectedConditions. invisibilityOfElementLocated(locator)));` invisibilityOfElementWithText `ExpectedCondition invisibilityOfElementWithText(By locator, java.lang.String text)` Defines an expectation for checking that an element with text is either invisible or not present on the DOM. Example: The next 2 lines of code verify if the element is invisible. If the element is invisible within 10 seconds, the explicit wait returns true. Otherwise, the explicit wait returns false. `WebDriverWait wait = new WebDriverWait(driver, 10); assertTrue(wait.until(ExpectedConditions. invisibilityOfElementWithText(locator, text)));` stalenessOf `ExpectedCondition stalenessOf(WebElement element)` Wait until an element is no longer attached to the DOM. Example: The next 2 lines of code verify if the element is no longer included in the browser DOM. If the element is no longer included in the DOM within 10 seconds, the explicit wait returns true. Otherwise, the explicit wait returns false. `WebDriverWait wait = new WebDriverWait(driver, 10); assertTrue(wait.until(ExpectedConditions. stalenessOf(element)));` textToBePresentInElement `ExpectedCondition textToBePresentInElement(WebElement element, java.lang.String text) ExpectedCondition textToBePresentInElementLocated(By locator, java.lang.String text)` An expectation for checking if the given text is present in the element that matches the given locator. Example: The next 2 lines of code verify if a keyword is included in an element. If the keyword is included in the element within 10 seconds, the explicit wait returns true. Otherwise, the explicit wait returns false. `WebDriverWait wait = new WebDriverWait(driver, 10); assertTrue(wait.until(ExpectedConditions. textToBePresentInElementLocated(locator, keyword)));` textToBePresentInElementValue `ExpectedCondition textToBePresentInElementValue(By locator, java.lang.String text) ExpectedCondition textToBePresentInElementValue(WebElement element, java.lang.String text)` Defines an expectation for checking if the given text is present in the specified elements value attribute. Example: The next 2 lines of code verify if a keyword is included in the value attribute of an element. If the keyword is included in the value attribute of the element within 10 seconds, the explicit wait returns true. Otherwise, the explicit wait returns false. `WebDriverWait wait = new WebDriverWait(driver, 10); assertTrue(wait.until(ExpectedConditions. textToBePresentInElementValue(locator, keyword)));` isElementPresent: Below is the syntax to check for the element presence using WebDriverWait. Here we need to pass locator and wait time as the parameters to the below method. Here it is checking that an element is present on the DOM of a page or not. That does not necessarily mean that the element is visible. ExpectedConditions will return true once the element is found in the DOM. `WebDriverWait wait = new WebDriverWait(driver, waitTime); wait.until(ExpectedConditions.presenceOfElementLocated(locator));` We should use presenceOfElementLocated when we don't care about the element visible or not, we just need to know if it's on the page. We can also use the below syntax which is used to check if the element is present or not. We can return true ONLY when the size of elements is greater than Zero. That mean there exists atleast one element. `WebElement element = driver.findElements(By.cssSelector(\"\")); element.size()>0;` isElementClickable Below is the syntax for checking an element is visible and enabled such that we can click on the element. We need to pass wait time and the locator as parameters. `WebDriverWait wait = new WebDriverWait(driver, waitTime); wait.until(ExpectedConditions.elementToBeClickable(locator));` isElementVisible Below is the syntax to check if the element is present on the DOM of a page and visible. Visibility means that the element is not just displayed but also should also has a height and width that is greater than 0. `WebDriverWait wait = new WebDriverWait(driver, waitTime); wait.until(ExpectedConditions.visibilityOfElementLocated(locator));` We can also use the below to check the element to be visible by WebElement. `WebDriverWait wait = new WebDriverWait(driver, waitTime); wait..until(ExpectedConditions.visibilityOf(element));` We can also use the below to check all elements present on the web page are visible. We need to pass list of WebElements. `List linkElements = driver.findelements(By.cssSelector(' linkhello')); WebDriverWait wait = new WebDriverWait(driver, waitTime); wait..until(ExpectedConditions.visibilityOfAllElements(linkElements));` isElementInVisible Below is the syntax which is used for checking that an element is either invisible or not present on the DOM. `WebDriverWait wait = new WebDriverWait(driver, waitTime); wait.until(ExpectedConditions.invisibilityOfElementLocated(locator));` isElementEnabled Below is the syntax which is used to check if the element is enabled or not. `WebElement element = driver.findElement(By.id(\"\")); element.isEnabled();` isElementDisplayed Below is the syntax which is used to check if the element is displayed or not. It returns false when the element is not present in DOM. `WebElement element = driver.findElement(By.id(\"\")); element.isDisplayed();` Wait for invisibility of element Below is the syntax which is used to check for the Invisibility of element with text. `WebDriverWait wait = new WebDriverWait(driver, waitTime); wait.until(ExpectedConditions.invisibilityOfElementWithText(by));` Wait for invisibility of element with text Below is the syntax which is used for checking that an element with text is either invisible or not present on the DOM. `WebDriverWait wait = new WebDriverWait(driver, waitTime); wait.until(ExpectedConditions.invisibilityOfElementWithText(by, strText));` "},{"version":"10.0","date":"Jul-29-2019","title":"windows-registry-settings","name":"Windows registry settings","fullPath":"iac/rpa/developer-guide/windows-registry-settings","content":" user settings.reg Windows Registry Editor Version 5.00 (user settings.reg) Purpose HKEY CURRENT USER Software Microsoft Windows CurrentVersion Internet Settings Zones 1 @=\"\" ; Local intranet \"1400\"=dword:00000000 ; Internet Option Security Custom level Scripting: Active scripting \"2500\"=dword:00000003 ; Internet Option Privacy Turn on Protected Mode Vista only setting 1809\"=dword:00000003 ; Internet Option Security Custom level Miscellaneous: Don't use Pop up Blocker \"1402\"=dword:00000000 ; Internet Option Security Custom level Scripting: Scripting of Java applets \"1605\"=dword:00000000 ; Internet Option Security Custom level Run Java HKEY CURRENT USER Software Microsoft Windows CurrentVersion Internet Settings Zones 2 @=\"\" ; Trusted sites \"1400\"=dword:00000000 ; Internet Option Security Custom level Scripting: Active scripting \"1809\"=dword:00000003 ; Internet Option Security Custom level Miscellaneous: Don't use Pop up Blocker \"1402\"=dword:00000000 ; Internet Option Security Custom level Scripting: Scripting of Java applets \"1605\"=dword:00000000 ; Internet Option Security Custom level Run Java \"2500\"=dword:00000003 ; Internet Option Privacy Turn on Protected Mode Vista only setting \"1A10\"=dword:00000000 ; Internet Option Privacy Accept all Cookies HKEY CURRENT USER Software Microsoft Windows CurrentVersion Internet Settings Zones 3 @=\"\" ; Internet \"1400\"=dword:00000000 ; Internet Option Security Custom level Scripting: Active scripting \"2500\"=dword:00000003 ; Internet Option Privacy Turn on Protected Mode Vista only setting \"1809\"=dword:00000003 ; Internet Option Security Custom level Miscellaneous: Don't use Pop up Blocker \"1402\"=dword:00000000 ; Internet Option Security Custom level Scripting: Scripting of Java applets \"1605\"=dword:00000000 ; Internet Option Security Custom level Run Java \"1A10\"=dword:0000000 ; Accept all Cookies \"1601\"=dword:00000000 ; Disable modal dialog When you send information to the Internet, it might be possible for others to see that. HKEY CURRENT USER Software Microsoft Windows CurrentVersion Internet Settings Zones 4 @=\"\" ; Restricted sites \"1400\"=dword:00000000 ; Internet Option Security Custom level Scripting: Active scripting \"2500\"=dword:00000003 ; Internet Option Privacy Turn on Protected Mode Vista only setting \"1809\"=dword:00000003 ; Internet Option Security Custom level Miscellaneous: Don't use Pop up Blocker \"1402\"=dword:00000000 ; Internet Option Security Custom level Scripting: Scripting of Java applets \"1605\"=dword:00000000 ; Internet Option Security Custom level Run Java \"1A10\"=dword:00000000 ; Accept all Cookies HKEY CURRENT USER Software Microsoft Windows CurrentVersion Internet Settings \"WarnonZoneCrossing\"=dword:00000000 ; Internet Option Advanced Warn if changing between secure and not secure mode HKEY CURRENT USER Software Microsoft Internet Explorer LowRegistry DontShowMeThisDialogAgain \"DisplayTrustAlertDlg\"=dword:00000000 ; Enhanced security dialog box is not displayed. HKEY CURRENT USER Software Microsoft Internet Explorer Zoom \"ResetZoomOnStartup2\"=dword:00000001 ; Internet Option Advanced Reset zoom level for new windows and tabs Required by Selenium IE Driver https: github.com SeleniumHQ selenium wiki InternetExplorerDriver \"ResetTextSizeOnStartup\"=dword:00000001 ; Internet Option Advanced Reset text size for new windows and tabs Required by Selenium IE Driver https: github.com SeleniumHQ selenium wiki InternetExplorerDriver \"ZoomFactor\"=dword:000186a0 ; set zoom to 100% Required by Selenium IE Driver https: github.com SeleniumHQ selenium wiki InternetExplorerDriver HKEY CURRENT USER Software Microsoft Internet Explorer TabbedBrowsing \"WarnOnClose\"=dword:00000000 ; Internet Option General Tabs CloseAllTabs Required for consistent behavior of IE across all the sessions. \"PopupsUseNewWindow\"=dword:00000002 ; Internet Option General Tabs Always open pop ups in a new tab Required for consistent behavior of IE across all the sessions. HKEY CURRENT USER Software Microsoft Internet Explorer Main \"TabProcGrowth\"=\"0\" ; TabProcGrowth=0 : tabs and frames run within the same process Required by Selenium IE Driver https: github.com SeleniumHQ selenium wiki InternetExplorerDriver \"Enable Browser Extensions\"=\"yes\" ; Internet Option Advanced Enable 3rd party browser extensions \"NoProtectedModeBanner\"=dword:00000001 ; Don't show warning Required for consistent behavior of IE across all the sessions HKEY LOCAL MACHINE SOFTWARE Microsoft Internet Explorer Main FeatureControl FEATURE _BFCACHE \"iexplore.exe\"=dword:00000000 ; For IE 11 only, you will need to set a registry entry on the target computer so that the driver can maintain a connection to the instance of Internet Explorer it creates (x32). Required by Selenium IE Driver https: github.com SeleniumHQ selenium wiki InternetExplorerDriver HKEY CURRENT USER Software Microsoft Internet Explorer New Windows \"PopupMgr\"=dword:00000000 ; Disable pop up blocker HKEY CURRENT USER Software Microsoft Internet Explorer Safety PrivacIE \"DisableToolbars\"=dword:00000000 ; Internet Option Privacy InPrivate HKEY CURRENT USER Software Microsoft Internet Explorer Main \"DisableFirstRunCustomize\"=dword:00000002 ; Disable first run recommended setting Required for consistent behavior of IE across all the sessions. HKEY CURRENT USER SOFTWARE Wow6432Node Microsoft Terminal Server Client \"RemoteDesktop _SuppressWhenMinimized\"=dword:00000002 ; Minimizing the Remote Desktop window causes the Windows operating system to switch the remote session to GUI less mode. It is off. For user x64 Fixing default Remote Desktop Window behavior so that GUI mode remains after minimizing. HKEY CURRENT USER SOFTWARE Microsoft Terminal Server Client \"RemoteDesktop _SuppressWhenMinimized\"=dword:00000002 ; Minimizing the Remote Desktop window causes the Windows operating system to switch the remote session to GUI less mode. It is off. For user x32 Fixing default Remote Desktop Window behavior so that GUI mode remains after minimizing. HKEY CURRENT USER Console \"QuickEdit\"=dword:00000000 Disable Quick Edit Mode by default for command line Quick Edit Mode in console often results in paused RPA Bot Relay Bot processes. Should be disabled by default. HKEY CURRENT USER Console %SystemRoot% System32 cmd.exe \"QuickEdit\"=dword:00000000 Disable Quick Edit Mode by default for command line Quick Edit Mode in console often results in paused RPA Bot Relay Bot processes. Should be disabled by default. HKEY CURRENT USER Console %SystemRoot% SysWOW64 cmd.exe \"QuickEdit\"=dword:00000000 Disable Quick Edit Mode by default for command line Quick Edit Mode in console often results in paused RPA Bot Relay Bot processes. Should be disabled by default. machine settings.reg Windows Registry Editor Version 5.00 (machine settings.reg) Purpose HKEY LOCAL MACHINE SOFTWARE Microsoft Internet Explorer Main FeatureControl FEATURE _BFCACHE \"iexplore.exe\"=dword:00000000 ; For IE 11 only, you will need to set a registry entry on the target computer so that the driver can maintain a connection to the instance of Internet Explorer it creates (x64). Required by Selenium IE Driver https: github.com SeleniumHQ selenium wiki InternetExplorerDriver HKEY LOCAL MACHINE SOFTWARE Policies Microsoft Internet Explorer Main \"DisableFirstRunCustomize\"= ; Disable first run recommended settings Required for consistent behavior of IE across all the sessions. HKEY LOCAL MACHINE SYSTEM CurrentControlSet Control Terminal Server \"PerSessionTempDir\"=dword:00000000 ; Each user session receives its own temporary directory. It is off. ; https: support.microsoft.com en us kb 243215 Java application automation (JVM Agent, JVM Inspector) relies on information written by Java into temp folder (information about pids). By default information for user session is written in its own temp folder modification prevents that. HKEY LOCAL MACHINE SOFTWARE Wow6432Node Microsoft Terminal Server Client \"RemoteDesktop _SuppressWhenMinimized\"=dword:00000002 ; Minimizing the Remote Desktop window causes the Windows operating system to switch the remote session to GUI less mode. It is off. For all users x64 Fixing default Remote Desktop Window behavior so that GUI mode remains after minimizing. HKEY LOCAL MACHINE SOFTWARE Microsoft Terminal Server Client \"RemoteDesktop _SuppressWhenMinimized\"=dword:00000002 ; Minimizing the Remote Desktop window causes the Windows operating system to switch the remote session to GUI less mode. It is off. For all users x32 Fixing default Remote Desktop Window behavior so that GUI mode remains after minimizing. "},{"version":"10.0","date":"Aug-02-2019","title":"use-of-browser-inspectors","name":"Use of browser inspectors","fullPath":"iac/rpa/developer-guide/use-of-browser-inspectors","content":" All the modern browsers have a web inspector (F12), the built in tool for examining structure of webpages. Activate Web Inspector You can activate the web inspector in any browser by right clicking on any element in the page, such as a photo. A pop up menu should appear with the option to Inspect Element. Or you can use the short cut key for Inspector which is F12. F 12 will pop up a panel, usually in the bottom half of your browser, showing the HTML source. The red circle is pointing to the Element Inspector in Firefox. Select elements Once you click the Inspector Icon from the bottom panel your element inspector is active and as you move your mouse around the page, the element under your mouse is highlighted with a dotted border and an annotation displays its HTML tag. At the same time, its HTML definition is displayed, in context, in the Inspector’s left hand pane. ​a) In the screenshot below, we have chosen to inspect the My Account button. The inspector (in the bottom panel of the browser) highlights where in the HTML that the My Account button appears. With the help of above html code you can simply write the below code in your script: (byClassName('account_icon')).click() ​b) Let's take few more examples. In the next example I am looking for Username Element attributes. You can see the annotation for the element gets a button on the left and a button on the right. The button on the left unlocks the element, enabling you to select a new element in the page. The button on the right displays a popup menu for the element. Your code will be like this: (byId('log')).sendKeys('user_1') Alternatively, you can also write your code like this: (byName('log')).sendKeys('user_1') There’s not much more to it. The web inspector gives us a handy point and click interface for browsing the underlying structure of a webpage. "},{"version":"10.0","date":"Aug-02-2019","title":"webelement-commands","name":"WebElement commands","fullPath":"iac/rpa/developer-guide/webelement-commands","content":" This article is all about WebDriver WebElement Commands. But before moving on to finding different WebElements, it better to cover that what all operations we can perform on a WebElement. In this chapter we will learn What is WebElement and the List of Actions can be performed on various WebElements. :::tip Simplified API Java Docs https: workfusion docs.s3.amazonaws.com rpa simplified api latest index.html Java Docs https: seleniumhq.github.io selenium docs api java ::: What is WebElement WebElement represents an HTML element. HTML documents are made up by HTML elements. HTML elements are written with a start tag, with an end tag, with the content in between: content The HTML element is everything from the start tag to the end tag: My first HTML paragraph. HTML elements can be nested (elements can contain elements). All HTML documents consist of nested HTML elements. My First Heading My first paragraph. To get the WebElement object, write the below statement: def element = (byId('some_id')) Notice that WebElement can be of any type, like it can be a Text, Link, Radio Button, Drop Down, Table or any HTML element. But all the actions will always populate against any element irrespectively of whether the action is valid on the WebElement or not. For example, the clear() command, even if you have a link element still you get the option to choose clear() command on it, which if you choose may result in some error or may not do anything. WebElement сommands clear clear( ) : void – If this element is a text entry element, this will clear the value. This method accepts nothing as a parameter and returns nothing. This method has no effect on other elements. Text entry elements are INPUT and TEXTAREA elements. def element = (byId('some_id')) element.clear() Or can be written as (' some_id').clear() sendKeys sendKeys(CharSequence… keysToSend ) : void – This method simulates typing into an element and accepts CharSequence as a parameter; returns nothing. This method works fine with text entry elements like INPUT and TEXTAREA elements. See also SendKeys Keystrokes Guide def element = (byId('some_id')) element.sendKeys('Admin') ('.someclass').sendKeys(Keys.chord(Keys.LEFTALT, 'n')) click click( ) : void – This simulates the clicking of any element. Accepts nothing as a parameter and returns nothing. Clicking is perhaps the most common way of interacting with web elements like text elements, links, radio buttons, and many more. def element = (byLinkText('About')) element.click() ('.some_class').click(2) double clicking ('.some_class').wheelClick() clicking using mouse wheel Most of the time we click on the links and it causes a new page to load, this method will attempt to wait until the page has loaded properly before handing over the execution to next statement. But If click() causes a new page to be loaded via an event or is done by sending a native event for example through javascript, then the method will not wait for it to be loaded. There are some preconditions for an element to be clicked. The element must be Visible and it must have a Height and Width greater than 0. isDisplayed isDisplayed( ) : boolean – This method determines if an element is currently being displayed or not. This accepts nothing as a parameter but returns boolean value(true false). def element = (byLinkText('About')) boolean status = element.isDisplayed() :::note Do not confuse this method with element present on the page or not. This will return true if the element is present on the page and throw aNoSuchElementFound exception if the element is not present on the page. This refers the property of the element, sometimes the element is present on the page but the property of the element is set to hidden, in that case this will return false, as the element is present in the DOM but not visible to user. ::: isEnabled isEnabled( ) : boolean – This determines if the element currently is Enabled or not This accepts nothing as a parameter but returns boolean value(true false). def element = (byLinkText('About')) boolean status = element.isEnabled() if (status == true) { element.click() } else { log.warn('Element is not enabled') } isSelected isSelected( ) : boolean – Determine whether or not this element is selected or not. This accepts nothing as a parameter but returns boolean value(true false). This operation only applies to input elements such as Checkboxes, Select Options and Radio Buttons. This returns True if the element is currentlyselected or checked, false otherwise. def element = ('.class1 .class2') boolean status = element.isSelected() submit submit( ) : void – This method works well better than the click() if the current element is a form, or an element within a form. This accepts nothing as a parameter and returns nothing. If this causes the current page to change, then this method will wait until the new page is loaded. def element = ('.btn.btn__submit') element.submit() getText getText( ) : String – This method fetches the visible (i.e. not hidden by CSS) innerText of the element. This method accepts nothing as a parameter but returns a String value. This returns an innerText of the element, including sub elements, without any leading or trailing whitespace. def element = ('.btn.btn__submit') element.getText() getTagName getTagName( ) : String – This method gets the tag name of this element. This accepts nothing as a parameter and returns a String value. This does not return the value of the name attribute but return the tag for e.g. “input“ for the element ``. ('.btn.btn__submit').getTagName() getCssValue getCssvalue( ) : String – This method fetches CSS property value of the given element. This accepts nothing as a parameter and returns a String value. Color values should be returned as rgba strings, so, for example if the “background color” property is set as “green” in the HTML source, the returned value will be “rgba(0, 255, 0, 1)”. ('.btn.btn__submit').getCssvalue() getAttribute getAttribute(String Name) : String This method gets the value of the given attribute of the element, accepts the String as a parameter, and returns a String value. Attributes are IDs, Name, Class, etc. (byPartialLinkText('New Customer')).getAttribute('href') getSize getSize( ) : Dimension – This method fetches the width and height of the rendered element, accepts nothing as a parameter, and returns the Dimension object. This returns the size of the element on the page. def element = ('.btn.btn__submit') def dimensions = element.getSize() you can also use element.size() println \"Height : {dimensions.height} Width : {dimensions.width}\" getLocation getLocation( ) : Point – This method locates an element on a page. This accepts nothing as a parameter and returns a Point object, from which we can get X and Y coordinates of a specific element. def element = ('.btn.btn__submit') def point = element.getLocation() println \"X cordinate: {point.x}; Y cordinate: {point.y}\" "},{"version":"10.0","date":"Aug-06-2019","title":"changelog","name":"Changelog aka What's New","fullPath":"iac/core/changelog","content":" July 8th 2019 — WorkFusion IA Version 10.0 The following pages have new information regarding the release: Architecture: Bot Execution Platform and 10.0 Architecture Labeling Experience: Tagging Over Document Use Case Information Extraction: v10.0 Information Extraction Answer Types AutoML: Python Classifiers and Local training execution runner Bot Tasks: AutoML plugins WorkSpace: WorkSpace Secure Properties and WorkSpace Application Properties API: WorkFusion REST API and Data Store REST API Data: Data Purge Settings, Connection to MS SQL, Data Stores Purging "},{"version":"10.0","date":"Oct-04-2019","title":"control-tower","name":"Control Tower","fullPath":"iac/core/control-tower","content":" Navigation Analytics Business processes Manual tasks Bot tasks Advanced System settings Introduction Control Tower is one of the main components of Smart Process Automation. Use Control Tower to: design, run and schedule Business Processes create Manual Tasks create and manage Users Starting Control Tower Before you start working with Control Tower, make sure there are no Windows updates pending, as some of them may interfere with Control Tower. Control Tower in SPA is started by infrastructure administrator. To launch Control Tower: Open the link provided by your administrator in browser. Log in using your credentials (username and password provided by your administrator): Control Tower Overview After launching Control Tower, you will see a dashboard with the following links: Account Information Business Processes Manual Tasks Datastores Secrets Vault (Secure Storage) Schedules Account Information Clicking your user name in the top right corner of your screen will bring up your account menu. Here you can: View and update your User Settings. Backup all your data Logout User Settings In the User settings tab, you can customize your user settings, i.e. your name, email, and timezone. Business Processes Overview To view your business processes, go to Business Processes → View All. It will open the list of your business processes: You can use a filter with multiple parameters to sort your business processes. Info RPA Express comes with a set of sample business processes in Control Tower: Account Payable – collects invoices as images from a web application into an S3 bucket, processes them using OCR and extracts the required information in a Manual task. License Verification – finds the licenses online by their numbers and retrieves required information about them. Check criminal records – finds the criminal records using personal information and saves the results in HTML. Business Processes Details A business process can have the following statuses: Draft (hasn’t been run yet) Running (in the process of execution) Completed (executed successfully) Errors (started but wasn’t completed due to errors) When expanding the business process list, you can see the details about each instance: when it was executed how it was launched (manually or via scheduler) the number of processed records in the business process executed tasks in the business process (bot and manual), their status and the number of processed records in each task. Clinking on the BP instance will open the workflow (for drafts) or the results tab (completed processes). Main Operations with business process You can perform the following operation with a Business Process: Link to Workflow – view the workflow defined for the Business Process as a diagram, including all tasks (Manual and Bot) and rules for transitions between tasks. View events log – opens the events log of the Business Process with an option to export it to Excel. Open process definition in a new tab – opens the Business Process with all its instances in a separate tab. Actions – opens the list of actions you can perform with the Business Process. Note The items displayed in the menu depend on the status of the Business Process. 1. Copy – create a copy of the Business Process as an instance linked to the source Business Process, i.e. if the source Business Process structure is modified, all referenced Business Processes will inherit the changes respectively, but each copy can be run separately without affecting the others. Note You can copy a Business Process, if you need to re run a completed Business Process. The Business Process copy function has the following options: Include the input data – the input data from the source Business Process will be included to the new one, alternatively you can provide new input data for the copied Business Process. Create an independent process definition – a new Business Process is created without a link to the source Business Process. View data – view the input data provided in the Business Process; Pause – pause execution of the Business Process with the possibility to resume it later (for running Business Processes only); Starting from Version 10.0 with BEP orchestration: In local mode Pause operation works in the same way as Stop: when BP is paused, next steps are not executed. It can then be resumed. In cluster mode Pause continues to execute stateless steps until a non stateful step. Learn more about Stateless Execution of Bot Tasks. Resume – resume execution, if the Business Process has been paused (for paused Business Processes only); Stop – complete execution of the Business Process at the current stage (for running Business Processes only); Download Original Data – download the input data as a .csv file. Delete – remove the Business Process completely from Control Tower. "},{"version":"10.0","date":"Sep-18-2019","title":"user-guide","name":"User guide","fullPath":"iac/core/user-guide","content":" Answer types Work with tasks Work with business processes BP Manage workers and groups Create a use case Migrate task, BP, use case Schedule task and BP Sample tasks and BPs Best practices FAQ "},{"version":"10.0","date":"Aug-06-2019","title":"certificate-based-authentication","name":"Certificate-Based Authentication","fullPath":"iac/core/api/certificate-based-authentication","content":" Certificate generation (Client and Server) Tomcat configuration Nginx configuration Spring Security configuration Known issues Certificate generation (Client and Server) Example for windows: @echo off if \"%1\" == \"\" goto usage keytool genkeypair alias servercert keyalg RSA dname \"CN=Web Server,OU=Unit,O=Organization,L=City,S=State,C=US\" keypass password keystore server.jks storepass password keytool genkeypair alias %1 keystore %1.p12 storetype pkcs12 keyalg RSA dname \"CN=%1,OU=Unit,O=Organization,L=City,S=State,C=US\" keypass password storepass password keytool exportcert alias %1 file %1.cer keystore %1.p12 storetype pkcs12 storepass password keytool importcert keystore server.jks alias %1 file %1.cer v trustcacerts noprompt storepass password keytool list v keystore server.jks storepass password del %1.cer goto end :usage echo Need user id as first argument: generate_keystore username goto end :end pause Download file generate certificates.bat and execute it with parameter Put generated file server.jks to conf Use generated file .p12 on client side: import into browser or use in client API Tomcat configuration Add a new ssl connector into conf server.xml Add a new jndi variable into conf context.xml Nginx configuration Add new location to conf nginx.conf location { proxy_pass https: localhost:8443 mturk web; proxysetheader Host host; proxysetheader X Real IP remote_addr; proxysetheader X Forwarded For proxyaddxforwardedfor; clientmaxbody_size 4G; clientbodybuffer_size 128k; proxyreadtimeout 3000; proxysendtimeout 30; } Spring Security configuration Add certificate authentication for needed path \"> ..... .... HowTo Use Link to test (nginx >tomcat) Link to test (tomcat only) Download linked user certificate from CCS 5600 Double click on it and add as user cert authority Try to open links Known issues a ) open url:8443 > cert request appear. After it nginx url works fine b ) open WF dashboard using any existing creds. After it nginx url works fine c ) if no auth before then case nginx >tomcat will fail with 401 full auth required d ) authentication problem if additional certificate configured in nginx "},{"version":"10.0","date":"Aug-15-2019","title":"ocr-api","name":"OCR REST API","fullPath":"iac/core/api/ocr-api","content":" WorkFusion 10.0.0 General information Primary API's: api v1 cloud submitImage api v1 cloud processDocument api v1 cloud getTaskStatus ABBYY's User guide attached OCR API ABBYY's User guide with short examples: OCR API or OCR API Authentication As some other parts of WorkFusion, OCR uses JWT for authentication. To access most of OCR resources you need to retrieve JWT token. For that you can access any Workfusion instance, provide a users login password and retrieve short living JWT token: curl X POST H Content Type:application json d '{\"username\":\"'\",\"password\":\"\"}' http: workfusion api v1 jwt login That token you can use to access any OCR resource, e.g.: curl form \"file=@FILENAME\" H \"Authorization:Bearer \" \"http: api v1 cloud processImage\" Using command line Process image specify appropriate location of file (FILENAME) and OCR parameters to processImage API call curl s form \"file=@FILENAME\" \"http: ocr.hostname:8080 api v1 cloud processImage correctSkew=true&xml:writeRecognitionVariants=false&profile=documentConversion&exportFormat=txt&language=English&correctOrientation=true\" Output copy task id for next command Get status by task id: curl s \"http: ocr.hostname:8080 api v1 cloud getTaskStatus taskId=5693d8f77b78005fcbbfbe84\" Output if message=\"OK\" you can download results from links in resultUrl,resultUrl2,resultUrl3 (depends on export format) API Usage We implement API to be compatible with ABBYY Cloud OCR API sequence call Upload file with submitImage Copy taskId from response: Start image processing with processDocument Check task status with getTaskStatus (specify correct taskId) If task is complete (status=\"Completed\") open results from resultUrl, resultUrl2 or resultUrl3 fields (replace &amp; with &) Custom request parameters We implement custom additional request parameters: GET processDocument OCR API export format (supports html) html html page pdfSearchable text can be searched in such file xml file contains characters words along with their location in the original document (coordinates frames) xmlForCorrectedImage the same as xml, except location is taken from a processed adjusted document txt – plain text the default format For multiple export formats you can use combinations delimited by the comma: pdfSearchable,xmlForCorrectedImage,html Attention You can use maximum 3 types at once. customRegions (json variable) { \"type\":\"BT_Table\", \"page\":1, \"left\":0, \"top\":0, \"right\":2000, \"bottom\":2000 }, { \"type\":\"BT_Table\", \"page\":1, \"left\":0, \"top\":2000, \"right\":4000, \"bottom\":4000 }, { \"type\":\"BT_Table\", \"page\":2, \"left\":0, \"top\":0, \"right\":4000, \"bottom\":4000 } Example to select one table region for all pages { \"type\":\"BT_Table\", \"left\":0, \"top\":0, \"right\":2000, \"bottom\":2000 } correctSkew (default true) if true, the page skew will be detected and automatically corrected correctOrientation (default true) if true, the page orientation will be detected and if it differs from normal, will be automatically rotated skipPreprocessing (default false) skips preprocessing stage. It will increase performance up to 30% useOnlyCustomRegions (default false) skips original analyzing stage extract information from custom regions only alphabetExtension string with specials symbols to extend already defined alphabet useDefaultPattern (default false) if true, it requires to apply the default pattern (from the OCR application bundle) removeNoiseModels removes noise on the image (optinal, valid value comma separated values: CorrelatedNoise, WhiteNoise). Important! This method can be used for color and 8 bit gray images only. removeGarbageSize removes garbage (excess dots that are smaller than a certain size) from the image (optinal, valid value > 0 and 1 for automatically detect garbage size) allowedRegionTypes allowed region types for identified blocks classification. If allowedRegionTypes=Empty, all types will be processed. For example, to suppress classifying any block as picture (BT RasterPicture) specify the parameter value as BT Table,BT Text,BT Barcode,BT VectorPicture, BT Separator,BT SeparatorGroup,BT Checkmark,BT _CheckmarkGroup.NOTE narrowing down type of regions can break page layout. Do not use it if you're not sure you need it. discardColorImage If you work with black and white images or the color of images is not important, set the discardColorImage to true. enhanceLocalContrast Specifies whether the local contrast of the image should be increased. Such preprocessing may increase the quality of recognition. It is effective for: photos or scans of documents with texture or pictures in the background. photos or scans of documents with highly colorful background or text highlighting. language Specify predefined language ex. English You can also define multiple languages and use comma as a separator: English,German,Polish lowResolutionMode: true,false improves recognition of images with low resolution, e.g. faxes. Pre processing parameters(available from 9.1): invertImage(default false) inverts Image discardColorImage(default false) leaves only black and white plane in the prepared image removeColorObjects removes color objects from the image, colors values: Blue, Green, Red, Yellow removeColorObjectsType(default Background) removes color objects from the image, modes values: Background, Full, Stamp convertTo(value=tiff) auto detection of the file type and conversion to TIFF(convert before processing). Accept images: PDF, PNG, JPG, JPEG pages selection of pages from PDF files for recognition(example:pages=1,2,3,10 15) changeDPI contains the new value of dpi, changeDPI available values from 50 to 3200 (example:changeDPI=300) useAutoDetectedDPIFromRange({\"min\": 50,\"max\": 3200}) DPI detection from defined range to detect best resolution and apply it before further processing.Note: changeDPI is not applicable when useAutoDetectedDPIFromRange is defined.When engine cannot define best DPI then original DPI is used. priority: 0 10 defines priority in the image processing queue POST processImage file file in format PDF, PNG, JPG, JPEG OCR API export format: html html page pdfSearchable text can be searched in such file xml file contains characters words along with their location in the original document (coordinates frames) xmlForCorrectedImage the same as xml, except location is taken from a processed adjusted document txt – plain text the default format For multiple export formats you can use combinations delimited by the comma: pdfSearchable,xmlForCorrectedImage,html Attention You can use maximum 3 types at once. xml:writeRecognitionVariants makes xml xmlForCorrectedImage formats to contain all variants of character word OCR considered as a possible recognition customRegions (json variable) correctSkew (default true) if true, the page skew will be detected and automatically corrected correctOrientation (default true) if true, the page orientation will be detected and if it differs from normal, will be automatically rotated skipPreprocessing (default false) useOnlyCustomRegions (default false) skips original analyzing stage extract information from custom regions only alphabetExtension string with specials symbols to extend already defined alphabet pattern file upload for a pattern to be applied to recognize special symbols. Note: 1 . you should also add the symbol to alphabetExtension 2 . you can use either the explicit pattern uploaded with the field or useDefaultPattern=true. Both can not be used at a time. useDefaultPattern (default false) if true, it requires to apply the default pattern (from the OCR application bundle) dictionary file where each line contains a word or combination of characters which can be used to improve OCR recognition. The set of words extends, not limits, the default dictionary. removeNoiseModels removes noise on the image (optinal, valid value comma separated values: CorrelatedNoise, WhiteNoise). Important! This method can be used for color and 8 bit gray images only. removeGarbageSize removes garbage (excess dots that are smaller than a certain size) from the image (optinal, valid value > 0) allowedRegionTypes allowed region types for identified blocks classification. If allowedRegionTypes=Empty, all types will be processed. For example, to suppress classifying any block as picture (BT RasterPicture) specify the parameter value as BT Table,BT Text,BT Barcode,BT VectorPicture, BT Separator,BT SeparatorGroup,BT Checkmark,BT CheckmarkGroup. To narrow down type of regions even more you can use the following set: BT Table,BT Text,BT Separator,BT _SeparatorGroup.NOTE narrowing down type of regions can break page layout. Do not use it if you're not sure you need it. enhanceLocalContrast Specifies whether the local contrast of the image should be increased. Such preprocessing may increase the quality of recognition. It is effective for: photos or scans of documents with texture or pictures in the background. photos or scans of documents with highly colorful background or text highlighting. language Specify predefined language ex. English lowResolutionMode: true,false improves recognition of images with low resolution, e.g. faxes. Pre processing parameters(available from 9.1): invertImage(default false) inverts Image discardColorImage(default false) leaves only black and white plane in the prepared image removeColorObjects removes color objects from the image, colors values: Blue, Green, Red, Yellow removeColorObjectsType(default Background) removes color objects from the image, modes values: Background, Full, Stamp convertTo(value=tiff) auto detection of the file type and conversion to TIFF(convert before processing). Accept images: PDF, PNG, JPG, JPEG pages selection of pages from PDF files for recognition(example:pages=1,2,3,10 15) changeDPI contains the new value of dpi, changeDPI available values from 50 to 3200 (example:changeDPI=300) useAutoDetectedDPIFromRange({\"min\": 50,\"max\": 3200}) DPI detection from defined range to detect best resolution and apply it before further processing.Note: changeDPI is not applicable when useAutoDetectedDPIFromRange is defined.When engine cannot define best DPI then original DPI is used. priority: 0 10 defines priority in the image processing queue Custom API's GET summary return the total number of tasks with status QUEUED. Could produce json and xml GET cancelTasks changed status for all NEW, QUEUED and INPROGRESS tasks to CANCELLED. response example: Recommendation: will be good if we add the new parameter that will be define, what tasks we need to cancel. For example date of upload or task id. POST trainPattern trains user pattern for the symbol. file image file with the symbol baseLine contains the distance from the base line to the top edge of the cropped image of the character. The base line is the line on which the characters are located. The top edge of the image is determined by the character orientation. H1 on the picture smallSymbolHeight specifies the height of small characters in pixels on the source image. H2 on the picture symbol the symbol that is associated with picture(s) mergePattern – a pattern is uploaded as a file. If provided, the training output pattern will be combined with the uploaded. request example: Response is a downloadable pattern file in a proprietary binary format. This is different from version 8.0.0, where the pattern was referred by the patternName. Note: when using a custom pattern, you need to specify alphabetExtension with the trained symbol in processImage or processDocument. POST submitPattern The method attaches a pattern file for recognition of special symbols to an (existing) task created with submitImage. To be consequently processed with processDocument action. Parameters: pattern – the pattern file to upload taskId – mandatory GET api project info GET activeLicense Method returns current active license and all available information about this license. Result example: Expand source { \"availableTextTypes\": \"ATT_Normal\", \"ATT_Typewriter\", \"ATT_Matrix\", \"ATT_Index\", \"ATTOCRA\", \"ATTOCRB\", \"ATTMICRE13B\", \"ATTMICRCMC7\", \"ATT_Advanced\" , \"availableBarcodeModules\": \"ABM_1D\", \"ABM_PDF417\", \"ABM_Aztec\", \"ABM_QRCode\", \"ABM_MaxiCode\", \"ABM_DataMatrix\", \"ABM_Autolocation\" , \"availableEngineModules\": \"AEM_ProcessAsPlainText\", \"AEM_Process\", \"AEM_Analyze\", \"AEM_Recognize\", \"AEM_Synthesize\", \"AEM_ExtendedCharacterInfo\", \"AEM_OpenPDF\", \"AEM_UserPatterns\", \"AEM_BalancedMode\", \"AEM_FastMode\", \"AEM_BCR\", \"AEM_Classification\" , \"availableExportFormats\": \"AEF_RTF\", \"AEF_HTML\", \"AEF_XLS\", \"AEF_PDF\", \"AEF_Text\", \"AEF_PDFImageOnly\", \"AEF_XML\", \"AEF_PPT\", \"AEF_PDFA\", \"AEF_PDFMRC\", \"AEF_ALTO\", \"AEF_EPUB\", \"AEF_FB2\", \"AEF_ODT\", \"AEF_XPS\" , \"availableVisualComponents\": , \"availableLanguageSets\": \"ALS_Standard\", \"ALS_DataCapture\", \"ALS_Artificial\", \"ALS_Programming\", \"ALS_User\", \"ALS_Chinese\", \"ALS_Hebrew\", \"ALS_Thai\", \"ALS_Vietnamese\", \"ALS_Arabic\", \"ALS_Japanese\", \"ALS_Korean\" , \"volumeRefreshingPeriod\": \"VRP_Infinite\", \"volume\": 10000, \"volumeRemaining\": 10000, \"serialNumber\": \"SWAT 1101 1004 1541 1060 5817\", \"allowedCoresCount\": 2, \"minimumCoresCountPerInstance\": 0 } The meaning of the fields in the response: Attribute Explanation availableTextTypes The set of the text types available in the license. availableBarcodeModules The set of the ABBYY FineReader Engine barcode modules available in the license. availableEngineModules The set of the ABBYY FineReader Engine modules available in the license. availableExportFormats The set of the export formats available in the license. availableVisualComponents The set of visual components available in the license. availableLanguageSets The set of the language sets available in the license. volumeRefreshingPeriod Information about the limitation period if the license limits the number of processed pages characters during this period. volume The total number of pages characters which can be processed during a period if the license has such a limitation. volumeRemaining The remaining number of pages characters which can be processed till the end of the current period if the license has such a limitation. When this property value reaches 0, analysis, recognition and export operations will not be possible. serialNumber The serial number of the license. allowedCoresCount The number of CPU cores that can be used simultaneously. If the value of this property is 0, the number of CPU cores is unlimited. minimumCoresCountPerInstance The minimum number of CPU cores which is allocated by ABBYY FineReader Engine at initialization. GET listLicenses Method returns list of all licenses connect to current ABBY Engine. Expand source { \"availableTextTypes\": \"ATT_Normal\", \"ATT_Typewriter\", \"ATT_Matrix\", \"ATT_Index\", \"ATTOCRA\", \"ATTOCRB\", \"ATTMICRE13B\", \"ATTMICRCMC7\", \"ATT_Advanced\" , \"availableBarcodeModules\": \"ABM_1D\", \"ABM_PDF417\", \"ABM_Aztec\", \"ABM_QRCode\", \"ABM_MaxiCode\", \"ABM_DataMatrix\", \"ABM_Autolocation\" , \"availableEngineModules\": \"AEM_ProcessAsPlainText\", \"AEM_Process\", \"AEM_Analyze\", \"AEM_Recognize\", \"AEM_Synthesize\", \"AEM_ExtendedCharacterInfo\", \"AEM_OpenPDF\", \"AEM_UserPatterns\", \"AEM_BalancedMode\", \"AEM_FastMode\", \"AEM_BCR\", \"AEM_Classification\" , \"availableExportFormats\": \"AEF_RTF\", \"AEF_HTML\", \"AEF_XLS\", \"AEF_PDF\", \"AEF_Text\", \"AEF_PDFImageOnly\", \"AEF_XML\", \"AEF_PPT\", \"AEF_PDFA\", \"AEF_PDFMRC\", \"AEF_ALTO\", \"AEF_EPUB\", \"AEF_FB2\", \"AEF_ODT\", \"AEF_XPS\" , \"availableVisualComponents\": , \"availableLanguageSets\": \"ALS_Standard\", \"ALS_DataCapture\", \"ALS_Artificial\", \"ALS_Programming\", \"ALS_User\", \"ALS_Chinese\", \"ALS_Hebrew\", \"ALS_Thai\", \"ALS_Vietnamese\", \"ALS_Arabic\", \"ALS_Japanese\", \"ALS_Korean\" , \"volumeRefreshingPeriod\": \"VRP_Infinite\", \"volume\": 10000, \"volumeRemaining\": 10000, \"serialNumber\": \"SWAT 1101 1004 1541 1060 5817\", \"allowedCoresCount\": 2, \"minimumCoresCountPerInstance\": 0 } GET api v1 metrics count Request example: Parameters: a ) status. Either one of TaskStatus values or aggregate DONE, PROCESSING, ALL b ) period. Number of minutes to substract from current time. By default the range is NOW MINUTES to NOW, when period is negative the range is BEGINNING to NOW MINUTES. Default is 60 minutes GET api v1 metrics stats Request example: Parameters: a ) period. Number of minutes to substract from current time. By default the range is NOW MINUTES to NOW. Default is 60 minutes b ) minProcessingTime. Tasks with processing time less than this value will be discurded. Default is 1000 (1 second) c ) stat. Descriptive statistic mean, max, n (number of values), std, percentileN (where N is any number 0 100) or all Multi worker processing When process starting to work with specific task it changes task status, so other processes can't also start working with that task thus one task can be processed only by one process worker. When server during task processing is terminated we not able to send that task to the queue immediately, but we have service that runs on a schedule and puts such tasks to the queue. Server specification and usage guidance Currently its low parameters instance don't submit many documents. License allows only 10K pages to be recognized License Update with latest info Practical Usage Tips In this paragraph described tips that was born during OCR api usage Recognition quality use custom dictionary fetch plain text from pdf and set it to originalText parameter don't use removeGarbageSize parameter for PDF searchable and carefully use it for scanned PDF or image. Sometime it can degrade results. use allowedRegionTypes=BT Table,BT Text,BT Barcode,BT VectorPicture, BT Separator,BT SeparatorGroup,BT Checkmark,BT CheckmarkGroup,BT _AutoAnalysis for aggressive recognition image blocks in documents train and use pattern for some repeatable unrecognized cases. By default used costa rica currency pattern PDF searchable document is preferable to recognition than high dpi image created from it Recommended resolution for source image: 300 dpi for typical texts (10pt or larger) and 400 600 dpi for texts in smaller fonts (9pt or smaller) Performance Improvements use skipPreprocessing=true at least for PDF searchable documents. It saves up to 30% processing time use xml:writeRecognitionVariants=false if you don't need char word variants in the output xml file. It saves up to 30% processing time, memory and disk space "},{"version":"10.0","date":"Aug-15-2019","title":"data-store-rest-api","name":"Data Store REST API","fullPath":"iac/core/api/data-store-rest-api","content":" DataStore Rest Service is used for remote DataStore management through WorkFusion application. Generally, rest service works with JSON data. Configuration Attention! For working with Data Stores you should use \" soap v1 datastores\" URL. URL Examples https: custom name.workfusion.com workfusion soap v1 datastores API Security All API postings are made over a Secure Sockets Layer (SSL) connection, which encrypts communications between the user and web server to ensure data remains private. Note All requests must be preceding with https: WorkFusion REST API supports both basic (login and password hash) and form based (login and password) authentication to ensure that APIs are only accessible to those with the proper credentials. By default, the basic authentication is used. Note Please note that only one authentication type can be enabled at a time. E.g. if you enable form based authentication = > basic authentication would be disabled. And vice versa. Basic Authentication username Your username in Control Tower. password Hash of your password in Control Tower. Form Based Authentication To enable the form based authentication, the following property should be set in the properties file ({TOMCAT_HOME} conf workfusion.properties) on WorkFusion server: Since SPA 9.2, CSRF protection is added to REST endpoints. To make REST call, add CSRF token to header. rest.form.auth.enable=true Before executing REST API requests, client should log in through form URL: POST method POST workfusion dologin Content Type: application x www form urlencoded j_username=username j_password=password j _username Your username in Control Tower. j _password Your password in Control Tower. Successful request body from server should look like: \" > \" > ... ... Get the JSESSIONID from the Set Cookie response header. When creating REST API requests: set this JSESSIONID to the Cookie header set application x www form urlencoded as Content Type header set received as header Here is the Postman login request example: Sample login with HttpClient public String post(String addressURL, AbstractHttpEntity body) throws IOException { HttpPost httpPost = new HttpPost(addressURL); System.out.println(\"POST > \" addressURL); httpPost.setEntity(body); HttpResponse response = httpClient.execute(httpPost); String stringResponse = convertStreamToString(response.getEntity().getContent()); System.out.println(stringResponse); return stringResponse; } public void login() throws IOException { List nvp = new ArrayList(); nvp.add(new BasicNameValuePair(\"j_username\", USERNAME)); nvp.add(new BasicNameValuePair(\"j_password\", PASSWORD)); simply post username and password to the server to login re use the httpClient instance to make sure same JSESSIONID cookie is used UrlEncodedFormEntity will set Content Type=application x www form urlencoded String loginResponse = post(LOGIN_URL, new UrlEncodedFormEntity(nvp)); Map jsonResponse = new Gson.fromJson(loginResponse, Map.class); String csrfToken = jsonResponse.get(\"csrfToken\"); String csrfHeader = jsonResponse.get(\"csrfHeaderName\"); then use csrfHeader and csrfToken for REST API calls } Using REST API with SSO When using REST API with SSO you can not use your SSO id password. The recommended approach for using REST API with SSO enabled is the following: Disable SSO (wf.sso.saml.enable=false) Restart Control Tower (wfmanager restart workfusion) Log in to Workfusion with pre SSO id password Create special REST API user and grant him necessary permissions (assign proper role) Enable SSO (wf.sso.saml.enable=true) Restart Control Tower (wfmanager restart workfusion) Now SSO is enabled, but you can use that REST API user id password for REST API authentication. Operations Rules applied to the Data Store queries You need to add the ds _ prefix to the Data Store name If you have some UPPERCASE characters in the Data Store name, you need to wrap the whole name in double quotes like this: \"ds01WLCLHT_DD\" Use the Microsoft SQL syntax Request Name Resource Path HttpMethod Parameters Permissions Example URL Execute execute POST query (any kind of SQL query to execute) required, query param Manage Data Stores Added fromSPA 9.2 soap v1 datastores execute query=DELETE FROM \"dstestDataStore\" Select select GET query (SQL select query) required, query param; maxRows (fetch row count limit) required, query param; View Data Stores Manage Data Stores Added fromSPA 9.2 soap v1 datastores select query=SELECT * FROM \"dstestDataStore\"&amp;maxRows=5 CreateOrUpdate {name} createOrUpdate POST {name} (DataStore name) required, path param; dataStoreData required, body param, consits of: columns (Key value map \"name:type\") required; originalCampaignUuid optional; automationUseCaseId optional; isAuthenticated soap v1 datastores testDataStore createOrUpdate Body (Content Type: application json) { &quot;columns&quot;: { &quot;name&quot;:&quot;TEXT&quot; } } Insert {name} insert POST {name} (DataStore name) required, path param; rowData required, body param, consists of: headers (array) required; values (array) required; Manage Data Stores Added fromSPA 9.2 soap v1 datastores testDataStore insert Examples: CreateOrUpdate soap v1 datastores datastore_name createOrUpdate Body { \"columns\": { \"name\": \"TEXT\", \"surname\": \"TEXT\" } } Insert soap v1 datastores datastore_name insert Body { \"headers\": \"name\", \"surname\" , \"values\": \"value1\", \"value2\" } CURL with basic authentication Please replace the username with proper user name, userpasswordhash with password hash (obtained from MsSQL ct . applicationuser table, field password), and your.server.name with your actual server name. You should get first 5 records from WFML Job Data datastore. curl u 'username:userpasswordhash' X GET 'https: your.server.name workfusion soap v1 datastores select query=SELECT%20%2A%20FROM%20%22dsWFMLJob_Data%22&maxRows=5' Response Consists of 3 parts: responseStatus (\"SUCCESS\", \"FAILURE\") body (response data DTO) errors (collection of business errors, if request failed) Examples: soap v1 datastores select query=SELECT * FROM \"ds_testDataStore\"&maxRows=5 Success { \"responseStatus\": \"SUCCESS\", \"body\": { \"rowData\": { \"columnDescriptions\": { \"type\": 4, \"name\": \"dstestDataStoreid\" }, { \"type\": 12, \"name\": \"column1\" }, { \"type\": 4, \"name\": \"column2\" } , \"rowData\": 1, \"value1\", 5 } }, \"errors\": } Failure { \"responseStatus\": \"FAILURE\", \"body\": null, \"errors\": { \"code\": 0, \"message\": \"StatementCallback; bad SQL grammar SELECT COUNT(*) FROM dscountryriskmay2018 ; nested exception is com.microsoft.sqlserver.jdbc.SQLServerException: Invalid object name 'dscountryriskmay2018'\" } } Accessing Data Store Service in Manual Tasks There is a possibility to insert Data Store REST API calls in Manual Tasks using AJAX – see Allowed Data Store Queries. In that case the service URL is: workfusion public datastores Security is provided by: unique task hitId parameter allowed Data Store queries set in Manual Task designer task status – works only for active tasks "},{"version":"10.0","date":"Aug-06-2019","title":"packages-api","name":"Packages API","fullPath":"iac/core/api/packages-api","content":" Since WorkFusion 7.5 If no binary files are transferred, the request body should be passed in JSON format: Request Headers: Authorization: Basic Accept: application json Content Type: application json Package import conflict resolution types CREATEWITHNEW_NAME REPLACE_EXISTING SKIP_IMPORT * Authorization is required * Method URL Description Parameters Example GET runs {uuid} packages retrieve list of existing packages for specified run curl i v H &quot;Accept: application json&quot; H &quot;Content Type: application json&quot; user user:password X GET https: instance_url mturk web soap api runs 621fcab5 9fca 4273 9ae3 3fafdd518f18 packages POST runs {uuid} packages create new package for specified run CreatePackageDto {&quot;name&quot;:&quot;rest package name&quot;,&quot;description&quot;:&quot;rest package description&quot;,&quot;dataStoreNames&quot;: &quot;test&quot; ,&quot;targetVersion&quot;:&quot;7.4.1&quot;} GET packages {uuid} file download package by specified UUID curl i v H &quot;Accept: application json&quot; H &quot;Content Type: application json&quot; user user:password o package.zip X GET https: instance_url mturk web soap api packages 539905d4 82bf 4ae5 b9ba 428beef49506 file DELETE packages {uuid} delete package by specified UUID GET runs {uuid} data stores retrieve list of used data stores in specified run POST packages import import provided package packageFile, Optional: conflictResolutions (PackageConflictsDto) curl H &quot;Content Type: multipart form data&quot; F &quot;conflictResolutions=@conflicts.json; type=application json&quot; F &quot;packageFile=@BP with DS 20 6 2016.zip; type=application octet stream&quot; X PUT https: instanceurl mturk web soap api packages import i v user user:password conflicts.json: { &quot;dataStoreConflictResolutions&quot;: {&quot;name&quot;:&quot;customersdatastore&quot;,&quot;type&quot;:&quot;SKIPIMPORT&quot;} , &quot;includedConfigsResolutions&quot;: {&quot;name&quot;:&quot;common functions&quot;,&quot;type&quot;:&quot;REPLACEEXISTING&quot;}, {&quot;name&quot;:&quot;rpa functions&quot;,&quot;type&quot;:&quot;CREATEWITHNEW_NAME&quot;,&quot;newName&quot;:&quot;rpa functions v2&quot;} } "},{"version":"10.0","date":"Aug-15-2019","title":"secrets-vault-api","name":"Secrets Vault API","fullPath":"iac/core/api/secrets-vault-api","content":" Description The \"password\" parameter requires BCrypt Encoded value, with 10 strength. Secrets Vault Plugins can be used as a part of Business Processes: Secrets Vault Plugins. Configuration Service is published under the \" api v1 secrets vault\" URL. URL Examples https: custom name.workfusion.com workfusion api v1 secrets vault API Security All API postings are made over a Secure Sockets Layer (SSL) connection, which encrypts communications between the user and web server to ensure data remains private. Note All requests must be preceding with https: WorkFusion REST API supports both basic (login and password hash) and form based (login and password) authentication to ensure that APIs are only accessible to those with the proper credentials. By default, the basic authentication is used. Note Please note that only one authentication type can be enabled at a time. E.g. if you enable form based authentication = > basic authentication would be disabled. And vice versa. Basic Authentication username Your username in Control Tower. password Hash of your password in Control Tower. Form Based Authentication To enable the form based authentication, the following property should be set in the properties file ({TOMCAT_HOME} conf workfusion.properties) on WorkFusion server: Since SPA 9.2, CSRF protection is added to REST endpoints. To make REST call, add CSRF token to header. rest.form.auth.enable=true Before executing REST API requests, client should log in through form URL: POST method POST workfusion dologin Content Type: application x www form urlencoded j_username=username j_password=password j _username Your username in Control Tower. j _password Your password in Control Tower. Successful request body from server should look like: \" > \" > ... ... Get the JSESSIONID from the Set Cookie response header. When creating REST API requests: set this JSESSIONID to the Cookie header set application x www form urlencoded as Content Type header set received as header Here is the Postman login request example: Sample login with HttpClient public String post(String addressURL, AbstractHttpEntity body) throws IOException { HttpPost httpPost = new HttpPost(addressURL); System.out.println(\"POST > \" addressURL); httpPost.setEntity(body); HttpResponse response = httpClient.execute(httpPost); String stringResponse = convertStreamToString(response.getEntity().getContent()); System.out.println(stringResponse); return stringResponse; } public void login() throws IOException { List nvp = new ArrayList(); nvp.add(new BasicNameValuePair(\"j_username\", USERNAME)); nvp.add(new BasicNameValuePair(\"j_password\", PASSWORD)); simply post username and password to the server to login re use the httpClient instance to make sure same JSESSIONID cookie is used UrlEncodedFormEntity will set Content Type=application x www form urlencoded String loginResponse = post(LOGIN_URL, new UrlEncodedFormEntity(nvp)); Document htmlDocument = getDocument(loginResponse); XPathExpression exprToken = getXPathExpression(\" meta @name='_csrf' @content\"); String csrfToken = exprToken.evaluate(document); XPathExpression exprHeader = getXPathExpression(\" meta @name='csrfheader' @content\"); String csrfHeader = expr.evaluate(document); then use csrfHeader and csrfToken for REST API calls } private Document getDocument(String content) throws ParserConfigurationException, SAXException, IOException { DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance(); DocumentBuilder db = dbf.newDocumentBuilder(); return db.parse(IOUtils.toInputStream(content, \"UTF 8\")); } private XPathExpression getXPathExpression(String xPath) throws XPathExpressionException { XPathFactory xpf = XPathFactory.newInstance(); XPath xpath = xpf.newXPath(); return xpath.compile(xPath); } Using REST API with SSO When using REST API with SSO you can not use your SSO id password. The recommended approach for using REST API with SSO enabled is the following: Disable SSO (wf.sso.saml.enable=false) Restart Control Tower (wfmanager restart workfusion) Log in to Workfusion with pre SSO id password Create special REST API user and grant him necessary permissions (assign proper role) Enable SSO (wf.sso.saml.enable=true) Restart Control Tower (wfmanager restart workfusion) Now SSO is enabled, but you can use that REST API user id password for REST API authentication. Get Secure Entry Resource path: v1 secrets vault entry Method: POST Retrieves secure entry from Secrets Vault. Parameter Description Required Param Type Data Type alias alias of the secured entry yes x www form urlencoded string Successful output: { \"responseStatus\": \"SUCCESS\", \"body\": { \"alias\": \"333\", \"key\": \"0000000\", \"value\": \"0000000000\", \"lastUpdateDate\": 1449814392229 }, \"errors\": } Error output: Invalid UUID { \"responseStatus\": \"FAILURE\", \"body\": null, \"errors\": { \"code\": 22, \"message\": \"Invalid password\" } } Save Secure Entry Resource path: v1 secrets vault entry put Method: POST Saves provided entry to Secrets Vault. Parameter Description Required Param Type Data Type alias alias of the secured entry yes x www form urlencoded string key entry key (usually username) yes x www form urlencoded string value entry value (usually password) yes x www form urlencoded string Successful output: { \"responseStatus\": \"SUCCESS\", \"body\": true, \"errors\": } Error output: Invalid UUID { \"responseStatus\": \"FAILURE\", \"body\": null, \"errors\": { \"code\": 20, \"message\": \"Secure Entry 1211 already exists\" } } Delete Secure Entry Resource path: v1 secrets vault entry delete Method: POST Deletes specified entry from Secrets Vault Parameter Description Required Param Type Data Type alias alias of the secured entry yes x www form urlencoded string Successful output: { \"responseStatus\": \"SUCCESS\", \"body\": true, \"errors\": } Error output: Invalid UUID { \"responseStatus\": \"FAILURE\", \"body\": null, \"errors\": { \"code\": 22, \"message\": \"Invalid password\" } } Update Secure Entry Resource path: v1 secrets vault entry update Method: POST Updates entry (key, value) by alias to Secrets Vault. Parameter Description Required Param Type Data Type alias alias of the secured entry yes x www form urlencoded string key entry key (usually username) yes x www form urlencoded string value entry value (usually password) yes x www form urlencoded string Successful output: { \"responseStatus\": \"SUCCESS\", \"body\": true, \"errors\": } Error output: Invalid UUID { \"message\": \"Validation Failed\", \"code\": \"com.workfusion.api.error.missing.request.parameter\", \"errors\": { \"message\": \"Required String parameter 'key' is not present\", \"field\": \"key\" } } Reset Secure Entry Resource path: v1 secrets vault entry reset Method: POST Resets value for specified entry from Secrets Vault. The new value is randomly generated (length is 20 symbols). Parameter Description Required Param Type Data Type alias alias of the secured entry yes x www form urlencoded string Successful output: { \"responseStatus\": \"SUCCESS\", \"body\": true, \"errors\": } Error output: Invalid UUID { \"message\": \"Secure Entry with alias '12342222' does not exist\", \"code\": \"com.workfusion.api.error.http.not.found\" } "},{"version":"10.0","date":"Aug-06-2019","title":"start-business-process-from-manual-task","name":"Start Business Process from Manual Task","fullPath":"iac/core/api/start-business-process-from-manual-task","content":" Create Business Process Prepare Input Data Create a CSV file with the input data, the file should contain only one column. Code Sample var campaignUuid = \"ff244048 031f 4fc4 82f3 b658d1b571f2\"; create csv input data containing only 1 column var dataToSend = \"mergedinputfields_json n\" \" \"\" JSON.stringify(combinedObject).replace( \" g,\" \" \"\") \" \"\"; var processData = btoa(dataToSend); encode to base64 format var startProcessData = {\"campaignUuid\":campaignUuid,\"mainData\": processData}; Get Campaign UUID Go to Campaigns → Open Campaign → Enter properties tab, find and copy the UUID value there. AJAX call to create and start Business Process Code Example Get ID of the started process Code Example Check BP execution status Code Example Get snapshot of the final step Code Example function retrieveSnapshot(processUuid) { retriveSnapshotUrl= instanceBaseUrl \" api v2 workfusion task {uuid} snapshot CSV\"; var startProcessPromise = .ajax({ type: \"GET\", url: retriveSnapshotUrl.replace(\"{uuid}\", processUuid), contentType: 'text', headers: {'Authorization': 'Basic ' btoa(userId \":\" pwd)}, dataType: 'text', cache:false, async: true }); startProcessPromise.then(snapshotRetrieveSuccess, snapshotRetrieveError); }; function snapshotRetrieveSuccess (response){ var decodedResponseString = atob(JSON.parse(response).content); }; "},{"version":"10.0","date":"Aug-15-2019","title":"stop-all-running-business-processes-with-rest-api","name":"Stop All Control Tower Running Business Processes with REST API","fullPath":"iac/core/api/stop-all-running-business-processes-with-rest-api","content":" In order to cut load on certain WorkFusion instance, we are going to stop all active Business Process runs (1000s of runs) programmatically. We need to do a few steps to achieve our goal: Connect via SSH into target WorkFusion instance. With SQL query fetch all IDs of Business Processes which are running, paused or stuck with exceptions. Store fetched IDs in file. Prepare shell script which will in a loop for every BP ID (from file) execute REST API call to delete BP Run shell script on target WorkFusion instance Verify on WF instance Connect In order to connect to cloud instance, you should generate SSH keys, provide Public to WF Ops. Then you'll be able to connect. In case of On premises type of WF installation, your Ops team can provide instructions. ssh i C: Users USER USER ssh key USER @ INSTANCE .workfusion.com Prepare Folder for MySQL Write In order to allow MySQL to create files we need to create folder and change owner cd home USER mkdir mysql data sudo chown root:root mysql data sudo chmod 777 mysql data cd mysql data Query for IDs mysql u USER p PASSWORD use mturk; SELECT DISTINCT rootrunuuid FROM Run WHERE status not in ('COMPLETED', 'DELETED', 'DELETING') INTO OUTFILE ' home USER mysql data bpuuids.txt'; This will create bpuuids.txt file with all BP IDs (one per line). Shell Script Create stop bps.sh file on same folder with generated by MySQL bpuuids.txt file: ! bin bash filename='bpuuids.txt' echo Start while read p; do echo p curl X PUT header 'Content Type: application json' header 'Accept: application json' user WFINSTANCEUSER : WFINSTANCEUSERPASSWORD \"https: WFINSTANCE .workfusion.com workfusion api v1 bps p end ignoreStepsErrors=true\" done < filename Run Shell Script sudo chmod 777 stop bps.sh . stop bps.sh Restart tomcat sudo service tomcat restart Verify Finally, we can log into the target WorkFusion instance, go to Business Processes list, filter out to see Active Business Processes only. If all previous steps are done correctly, this list MUST BE EMPTY. "},{"version":"10.0","date":"Aug-15-2019","title":"swagger-configuration","name":"Swagger configuration","fullPath":"iac/core/api/swagger-configuration","content":" Swagger auto generated documentation for Spring based REST API is configured in Workfusion with regard to Springfox libraries. Being enabled via specific property, it requires standard login to WF platform. The documentation allows to examine all available public API endpoints with calls description and even execute selected requests pressing 'Try it out!' button. How to enable swagger Due to security reasons we disabled the access to swagger by default. It can be enabled via adding the specific property file to tomcat: Version 8.3 {catalina.home} conf rest settings.properties Version 8.4 {catalina.home} conf workfusion.properties with entry: swagger.apidocs.enabled=true Which url to use to access swagger documentation https: your instance.workfusion.com workfusion api swagger ui.html "},{"version":"10.0","date":"Aug-19-2019","title":"workfusion-rest-api","name":"WorkFusion REST API","fullPath":"iac/core/api/workfusion-rest-api","content":" The WorkFusion API allows you to post data to, launch, and receive results from WorkFusion use case in an automated fashion. Using the RESTful API one can manage simple use cases, called tasks, and more complex use cases, called business processes (also called BPs), which represent a workflow of various tasks both manual and automated. Groups of contributors working on submitting manual tasks are referred as Workforces. A workforce may represent one or more different groups both from Cloud worker pools such as UpWork, Mechanical Turk as well as internal collaborators, business partners, and outsourcers. Use cases such as business processes and tasks follow a certain lifecycle within WorkFusion. There are few states such as draft, processing, paused, completed, etc. The API allows to manage use cases by providing unique operations applicable for various states such as create, start, pause, stop, etc. API Security All API postings are made over a Secure Sockets Layer (SSL) connection, which encrypts communications between the user and web server to ensure data remains private. Note All requests must be preceded by https: Form Based Authentication WorkFusion REST API uses form based (login and password) authentication to ensure that APIs are only accessible to those with the proper credentials. Since SPA 9.2, CSRF protection is added to REST endpoints. To make REST call, add CSRF token to header. Before executing REST API requests, client should log in through form URL: POST method j _username Your username in Control Tower. j _password Your password in Control Tower. Successful request body from server should look like: { \"success\": true, \"csrfToken\": , \"csrfHeaderName\": } Get the JSESSIONID from the Set Cookie response header. When creating REST API requests: set this JSESSIONID to the Cookie header set application x www form urlencoded as Content Type header set received as header Here is the Postman login request example: Sample login with HttpClient public String post(String addressURL, AbstractHttpEntity body) throws IOException { HttpPost httpPost = new HttpPost(addressURL); System.out.println(\"POST > \" addressURL); httpPost.setEntity(body); HttpResponse response = httpClient.execute(httpPost); String stringResponse = convertStreamToString(response.getEntity().getContent()); System.out.println(stringResponse); return stringResponse; } public void login() throws IOException { List nvp = new ArrayList(); nvp.add(new BasicNameValuePair(\"j_username\", USERNAME)); nvp.add(new BasicNameValuePair(\"j_password\", PASSWORD)); simply post username and password to the server to login re use the httpClient instance to make sure same JSESSIONID cookie is used UrlEncodedFormEntity will set Content Type=application x www form urlencoded String loginResponse = post(LOGIN_URL, new UrlEncodedFormEntity(nvp)); Map jsonResponse = new Gson.fromJson(loginResponse, Map.class); String csrfToken = jsonResponse.get(\"csrfToken\"); String csrfHeader = jsonResponse.get(\"csrfHeaderName\"); then use csrfHeader and csrfToken for REST API calls } Using REST API with SSO When using REST API with SSO you can not use your SSO id password. The recommended approach for using REST API with SSO enabled is the following: Disable SSO (wf.sso.saml.enable=false) Restart Control Tower (wfmanager restart workfusion) Log in to Workfusion with pre SSO id password Create special REST API user and grant him necessary permissions (assign proper role) Enable SSO (wf.sso.saml.enable=true) Restart Control Tower (wfmanager restart workfusion) Now SSO is enabled, but you can use that REST API user id password for REST API authentication. URLs All URLs begin with the following pattern: https: %HOSTNAME% workfusion api UUIDs Each use case such as task or Business Process, and workforce have unique identifier which is called UUID. It is used as a unique reference to invoke the WorkFusion API and identify those entities. UUID Example db1cf0a8 1be4 4842 aec9 4ab0196bc9f1 How to Obtain Definition UUID You can get the required UUID in Control Tower User Interface. Business Process UUID Go to Business Processes List. Click Open Process Definition In New Tab The Business Process Definition opens in a new tab. UUID is displayed as shown on the screenshot below: Workforce UUID Go to Workers → Workforces Copy UUID of the required Workforce from the table. Documentation Links to docs: SPA 10.0 Sample implementation Refer to the sample Java implementation below to see how login, launching and getting a status of a Business Process can be implemented with a help of Apache HttpClient: API sample "},{"version":"10.0","date":"Aug-22-2019","title":"architecture","name":"Intelligent Automation Cloud Components Architecture","fullPath":"iac/core/architecture/architecture","content":" WorkFusion provides full range of components allowing comprehensive Enterprise automation. To understand goals of each and how components work together, first let's group them on high level. Application Level Control Tower Control Tower is the central managing web application, which orchestrates automation of processes and tasks, and provides a user interface for power users, administrators, and developers. It allows to perform the following actions: Manage Automation Processes Create and edit workflows using GUI tools Run and monitor automations Handle input output data Manage users and roles Manage Manual Tasks Manage Workforce Configure all components of the system Related components: WorkSpace, Workflow, RPA, AutoML, OCR, Object Storage, WorkFusion Studio. Learn more: Control Tower WorkSpace WorkSpace is the web application where Subject Matter Experts complete manual tasks. It allows to perform the following actions: Manage manual tasks queue Fulfill manual task Manage users, roles, teams, qualifications Related components: Control Tower, Object Storage. Learn more: WorkSpace Bot Manager Bot Manager is the web application that uses Bot Agents on RPA servers to manage RPA units and send commands. It covers the following functionality: Server processes management Maintenance of active RDP sessions Hardware utilization monitoring Statistics for bots and instances Related components: Control Tower, RPA, Analytics, Database, Secrets Vault. Learn more: Bot Manager WorkFusion Studio WorkFusion Studio is an Eclipse based IDE used to develop Bot configurations. It allows to perform the following actions: Create, edit, and publish Bot configurations Create Bot Config Bundles Web scraping & ETL Use AutoML SDK Record user workflows Inspect desktop applications Related components: Control Tower, OCR, AutoML, RPA, Object Storage. Learn more: WorkFusion Studio Service Level AutoML AutoML finds the best ML model for a particular target function (expressed in a Manual Task), trains it, validates, and finally predicts the target variables. It allows to perform the following actions: Search for the best model within hypermodels space Training, validation of models using data from Manual Tasks Computing predictions for unseen input data Visualization of prediction quality Related components: WorkFusion Studio, Control Tower, Object Storage Learn more: AutoML RPA RPA Cluster is a scalable computing environment that executes automation of user scenarios within desktop or web applications. It takes control of the keyboard and mouse pointer to emulate user inputs. Related components: WorkFusion Studio, Control Tower. Learn more: RPA OCR OCR engine processes scanned documents: converts unstructured, image based data into structured character documents in several formats (html, xml). Related components: WorkFusion Studio, Control Tower, WorkSpace, Object Storage. Learn more: OCR Workflow Workflow Engine executes automation processes authored in Control Tower according to process definition. This is the server application configured within Control Tower. Execution is handled in different computing environments—RPA, WorkSpace, OCR, and AutoML. Related components: Control Tower, RPA, WorkSpace, AutoML. Analytics WorkFusion Analytics comprehensive dashboards present a reporting mechanism that aggregates and displays metrics and key performance indicators (KPIs). Dashboards help improve decision making by revealing and communicating in context insight into business performance. Related components: Control Tower, WorkSpace. Learn more: Analytics BEP Bot Execution Platform (BEP) is the platform that creates tasks, submits them to queues, routes to completely isolated workers, and receives the task perfomance results. It dynamically allocates resources depending on the task load. BEP covers the following functionality: Horizontal scaling of workers Bot execution isolation Reuse of the shared resources (cluster nodes) both by CT and AutoML tasks Task processing fault tolerance Aggregation of logs and metrics from tasks and workers Related components: Control Tower, AutoML, Workflow, RPA Learn more: Bot Execution Platform Data Level Database Database stores business process definitions, runtime and tracking information on their executions — including RPA and Cognitive automations. It is a reliable storage of the designed automation implementation. Related components: Control Tower, WorkSpace, Analytics. Secrets Vault Secrets Vault is a secured storage for sensitive data. It allows to: Store authentication data (login password) for applications involved in RPA Securely access applications involved in RPA during business process execution Store authentication data for WorkFusion SPA inter component interaction Related components: Control Tower, RPA. Learn more: Secrets Vault Object Storage Object Storage is a reliable storage of OCR results and trained AutoML models. It keeps binary file type content, such as OCR results, AutoML models, screenshots, and reports generation by automation. Related components: WorkFusion Studio, Workflow, OCR, AutoML. "},{"version":"10.0","date":"Aug-19-2019","title":"bot-execution-platform","name":"Bot Execution Platform","fullPath":"iac/core/architecture/bot-execution-platform","content":" Overview Bot execution platform (BEP) enables a horizontally scalable, predictable, and fault tolerant system. It delivers on demand automation with scalability and resilience designed specifically for enterprise level. The three immediate benefits are as follows: Dynamical resource allocation based on the task load. This results in better utilization of hardware available for other tasks. Worker isolation. One failed task is returned to the task queque to be automatically processed later and does not affect the overall Control Tower stability. Distinct Control Tower and execution platform separation. Now, the platform exposes an API for cluster execution, orchestration, monitoring, and analytics. Structure Workflow Control Tower sends Tasks to BEP. BEP Master arranges tasks in queque, and BEP Task Dispatcher dynamically allocates Agent servers' resources, invoking Workers to perform each Task. After the Task is performed, its results are returned to Control Tower. If the task failed, it is returned to the Queue. RPA RPA components employ different BEP workflow. To learn about it, refer to the RPA documentation. "},{"version":"10.0","date":"Aug-14-2019","title":"deprecation-notices","name":"Deprecation notices","fullPath":"iac/core/deprecations/deprecation-notices","content":" This page is used to notify about upcoming deprecations of outdated functionality and features. Usage detection and migration steps (if any) are described under individual subpages. "},{"version":"10.0","date":"Aug-14-2019","title":"deprecations","name":"Deprecations","fullPath":"iac/core/deprecations/deprecations","content":" SPA v10.0 is a fresh start release with major architectural changes, so a number of legacy items and components that are no longer in focus, or not supported altogether, were removed. These items include WorkSpace Sandbox, all Smartcrowd integrations, WFBI service, and a number of minor leftovers scattered around the system. Here's the list of the most noticeable changes: MySQL, OCR MongoDB, WS PostgreSQL are consolidated in a single MS SQL database. Bot Sources are removed. Platform Monitor is no longer available being replaced by ELK and Bot Manager. WorkSpace Sandbox environment is gone since most of enterprise customers use a 3 stage environment stack: Dev, UAT, and Production. Below is the full list of deprecations grouped by component. Control Tower Control Tower MySQL, OCR MongoDB, WS PostgreSQL consolidated in a single MS SQL database. Bot Sources are removed. WFBI Service on APP server is removed from Control Tower and its logic for statistics is moved to AutoML service. google places scanner and google api client are removed. Learn more: google places scanner and google api client deprecation. Bot Configurations: Spring beans and Control Tower service beans are removed. Bot Configurations: Classes Constants are removed from packages. Basic authentication option in REST API is removed due to security concerns. Now authentication is form based (ligin and password) only. Learn more: 10.0 WorkFusion REST API. userInternalCredentials object in Bot Tasks context is deprecated and contains only the username of the task author and empty password string. Learn more: Bot Tasks Context. Sandbox feature is removed. Confirmation dialog upon Run of the Business Process Manual Task in sandbox production is removed. Support of custom jars in Tomcat lib is removed. Qualifications Test Tab is removed. 'Support IE' inside Correlated Fields is removed. 'Bot assisted extraction for values close to accuracy threshold' functionality is removed. 'Online Learning' functionality support is removed. The following legacy items are removed from Control Tower, as SmartCrowds are not supported by WorkFusion since v8.5: Communication center Balances figures Mturk integration Bonus notifications Amazon notifications Idle predefined filter and settings. Analytics WF Automation r and WF Automation j dashboards were removed as obsolete. WorkSpace Sandbox feature is removed. Captcha during registration is removed. WorkSpace index page is eliminated (as referring to crowd system). Login page is used instead. Log4j logging service is replaced with Logback. The following legacy items are removed from WorkSpace, as SmartCrowds are not supported by WorkFusion since v8.5: Geo API functionality Elance, Facebook, Twitter, Clickworker, Upwork integration points Paypal integration Support of 'Payments' and earning related funionality RPA MongoDB is removed and replaced with MS SQL. `` plugin is removed in favor of RPA API. node id attribute is removed from the `` plugin. Now, RPA Workers take jobs from RPA queues, and there is always only one node in a bot unit. For surface based automation, the in house solution is used, thus SikuliX Robot Framework library cannot be imported into scripts. Learn more: Surface Based Robotics Driver. Custom capabilities cannot be used anymore. Use ` attribute in ` plugin instead. Learn more: Robotics Plugins. Selenium RemoteDriver cannot be used directly in the bot task code. OCR GET processDocument request is removed in OCR API due to its length limitations. Since 9.2, it is superseded with POST processDocument request method. OCR plugin attribute use get process document method is not available anymore. If previously used, it should be removed. Flushing documents is turned off by default. To enable flushing, the DflushEnabled=true parameter should be added to the config file. Platform Monitor Platform Monitor is removed and replaced with Elasticsearch and Kibana stack (ELK). RPA bot management functions are transferred to the Bot Manager application. Learn more: Bot Manager. Commands API: Running commands using ID only is deprecated. Replaced with a new version with namespace. "},{"version":"10.0","date":"Aug-06-2019","title":"google-places-scanner-and-google-api-client-deprecation","name":"google-places-scanner and google-api-client deprecation","fullPath":"iac/core/deprecations/google-places-scanner-and-google-api-client-deprecation","content":" google places scanner plugin The google places scanner plugin in its current state relies on outdated Google Places API, and as a result is not functional. This plugin is scheduled for deletion from the codebase in SPA 10.0 and later versions. google api client libraries To minimize security and operational risks the following outdated libraries are scheduled for removal from Control Tower: com.google.api client:google api client:1.6.0 beta com.google.http client:google http client:1.6.0 beta com.google.oauth client:google oauth client:1.6.0 beta As a result the APIs of these libraries won't be available in bot configs anymore. If your bot configs rely on these APIs, it is recommended to migrate to newer version of the APIs. Latest stable version of the libraries can be found at: "},{"version":"10.0","date":"Aug-14-2019","title":"data-stores-purging","name":"Data Stores Purging","fullPath":"iac/core/data-stores/data-stores-purging","content":" Note Refer to Schedule Settings to get information about how to set up the data purge for Business Processes. When your Data Stores size gets rather big (e.g. 100k records and more) it can slow down Bot task execution, therefore you need to perform Data Store purging procedure. This article describes how to perform Data Store purging using a one step Business Process. General Use Case Overview Create a single step Business Process with a Bot step to purge a specific data store. Specify a condition inside of Bot Step Config. Run the Business Process periodically, when a purge procedure is needed, or create a Schedule for this Business Process. Data Store Assume, a data store named as \"t_status\" has the next structure and is populated with data: name (type: TEXT) status (type: TEXT) one FINISHED two RUNNING three FINISHED four NEW We are going to remove all of records with status = \"FINISHED\". Performance note To speed up the table cleaning it's highly recommended to create an index on the \"status\" column of the data store in the MsSQL database wf_datastore before running the purging BP. Pay attention: the table name is extended with the prefix \"ds_\". create index dststatusIDXstatus on ds.dststatus(status); Bot Step Config Condition to purge: status = 'FINISHED' (row 5) Run the Business Process Data Store content BEFORE purging wfdatastore= select * from ds.dst_status; dststatus_id name status 1 one FINISHED 2 two RUNNING 3 three FINISHED 4 four NEW (4 rows) State of the completed BP Data Store content AFTER purging wfdatastore= select * from ds.dst_status; dststatus_id name status 2 two RUNNING 4 four NEW (2 rows) Schedule for data purging To fulfill purging of data stores periodically you may want to schedule the Business Process executing. Your DBA can observe increasing of rows in data stores. Tables with 100,000 records and more are good candidates for purging (524,288 rows in the example below). Determine a number of records in MsSQL console select count(*) from ds.dststatus; count 524288 Use Case: purge multiple data stores by last modification time Overview Create a Business Process that accepts a list of data stores to purge in a CSV file. Add one Bot Task (see its code below). Run the Business Process periodically, when a purge procedure is needed or create a Schedule for this Business Process. Input list of data stores list of data stores.csv datastorename, purgetimestamp_column, days t1, update_timestamp, 30 t2, change_timestamp, 10 datastore_name name of data store to purge; purgetimestampcolumn data store's column with time of the last modification; days number of days from the current date to start purging. Bot Task Config The Config below uses the SQL syntax to find records to purge: Machine Config Sample Data Stores Your Data Stores being purged need to have a time stamp column which shows the date of the last record update (t1.updatetimestamp and t2.changetimestamp accordingly in the example below). ds purge t1.csv file for the data store \"t1\" uniquename,updatetimestamp one,2017 01 01 12:00:00 two,2017 06 01 17:20:30 three,2017 08 01 09:40:45 ds purge t2.csv file for the data store \"t2\" uniquename,changetimestamp first,2017 01 02 11:12:13 second,2017 03 15 10:20:30 third,2017 07 01 09:40:45 fourth,2017 08 03 23:19:01 {width=\"822\" height=\"250\"} Remember: default data type is TEXT, not TIMESTAMP If a Data Store is created from a file, then the update_timestamp column has the TEXT type (by default). So it has been explicitly converted to ::timestamp in the config (see the row 5 in the Bot Config). Don't forgot to update timestamp All of Business Processes that use a data store should update a value of update_timestamp column each time when a change occurs. Sample of timestamp updating "},{"version":"10.0","date":"Aug-06-2019","title":"data-stores","name":"Data Stores","fullPath":"iac/core/data-stores/data-stores","content":" Managing a Data Store Creating a DataStore Viewing and Editing a DataStore Deleting a Data Store Using Filters Using a Data Store Answers Currency Bot Configs Business Processes Business Process Export Global Variables Data Store Query Manual Task Rules Data Store for Model Training Managing Field Schemes Data Store Performance Optimization Archive and Audit Data Store module is a web interface for managing DB tables in WorkFusion. Data Store table structure and column data format are defined by a Field Scheme. You can use Data Stores to keep lookup tables for multivalue Answer options, save BP results, and query Data Stores from Bot Configs and Rules. To manage Data Stores, users must have Datastore Modifications permissions (Configuration > User Management > Edit User > Select User Permission). Managing a Data Store To start managing Data Stores, go to Advanced > Data Stores. Creating a DataStore Click the Create button above the grid. Enter a unique Name and Description to distinguish the Data Store. Names can only contain alphanumeric characters, underscores, and spaces. To define the Data Store structure, select one of the following options: Field Scheme. Select a Field Scheme from a dropdown. To manage Field Schemes, go to Configuration > Field Schemes (details) or click the Edit link to the right of the dropdown. Upload File (CSV file, the first row should contain column headers). Click the Save button. You cannot select another Field Scheme or source CSV file after saving the Data Store. Column names cannot contain spaces. Add Data to the Data Store: Upload a CSV file by clicking the Upload Data button. To leverage data file creation, click the Download Data button and start populating the downloaded file that will contain all necessary columns. When the file is finished, upload it. You can choose the data format (CSV, XLSX) and data set (All Records, Matching Filter) for downloading. Or add Records manually one by one by clicking the Add Record button. Viewing and Editing a DataStore To view or edit a Data Store, click the appropriate Data Store name in the grid. You can perform the following operations with Records: view ( filter by column values; set columns to display in the grid) add edit – hover over the appropriate Record number and click the Edit button. Save the Record after editing. upload a new file if the new file column names coincide with the Data Store columns, all new records will be appended to the Data Store otherwise, you need to map columns from file to Data Store: delete ( delete all) Deleting a Data Store To perform a bulk Data Store deletion: Go to Advanced > Data Stores. Tick the appropriate Data Stores in the grid. Click the Delete button and confirm this action. You can also delete a separate Data Store from the Data Store Details page: Go to the Data tab. Click the Delete Data Store button in the bottom right corner and confirm this action. Using Filters For quick navigation through the Data Stores you can use filters. There are two types of filters: Predefined – you can filter out Data Stores created by username. Advanced – you can set a number of criteria for the filter: To add a parameter to your filter use the Select an Option dropdown. As the first parameter has been added to the filter you can add or delete the parameters using plus ( ) or minus ( ) sign. Name– text containing in the Data Store name; Description– text containing in the Data Store description; Author– name of the user who created a Data Store. With Name and Description you can use contains does not contain or equals to not equals to options. In the Author field you can specify a number of authors and define the contains does not contain options. Additionally, you can use Match All of The Following to match the search results with the all options defined in your filter. Using a Data Store Answers You can use a Data Store is to create a re usable lookup list for Task Answer, e.g. List of Countries. Instead of adding options manually, you can use Records from a Data Store for the following Answer Types: Free Text Select Multi Select One Check One Check Multi Taxonomy Mark the Input from Data Store checkbox and select an item from the list. To use a predefined list in Answers, you need to create a Data Store with 2 columns: id – unique identifier name – label to show in the dropdown Currency All currencies that can be used as an answer for the Currency Answer Type in the Invoice Answer Type are stored in a system Data Store called System Currency. In the Data Store the currency code represents the currency id and name contains the currency code and name. The names appear in the dropdown, where a Worker can select the appropriate one: Bot Configs Create a Bot Task Use Case (ETL) with a Data Store Answer. Using the plugin, you can use SQL queries to read and or modify the Data Store content. To create a Data Store, use the plugin. See the full list of Bot Config plugins here. Add this Bot Task to your BP and set its options: Select the Data Store name. Map Data Elements to the variables. Business Processes To automatically load the BP Final Results to a Data Store, tick the Copy results to DataStore checkbox. Therefore, you do not need to generate a BP Snapshot and store it on your local computer or in the cloud. For detailed info, see the Set BP Run Parameters topic. Business Process Export If a Business Process uses Data Stores in its steps, the Data Stores can be included to the package during the export procedure and created in another Control Tower as the package is imported. More details you can find in Migrating Packages of Tasks and BPs. Global Variables All the global variables are stored in one Data Store with the \"Global Variables\" name. You can use these variable values in the plugin. Data Store Query Manual Task You can develop a custom Manual Task that uses Data Store API calls from JavaScript. In such Manual Tasks you should create a set of Data Store Queries. For more details refer to Allowed Data Store Queries. Rules You can query a Data Store from a Rule: String query = \"SELECT * FROM @This WHERE color = '\" answer.getFormInputValue() \"';\"; List prohibitionRecords = ruleContext.queryDataStore(\"Prohibited Colors\", query); Data Store for Model Training During the Model Training a special Data Store is created to keep the results of and information about each training step. A record in the Data Store represents the results per Model Training. For more information refer to VDS Workflow Part 2 > > Step 3 Start Model Training Managing Field Schemes Field Scheme defines Data Store column names and content types. To start managing Field Schemes, go to Configuration > Field Schemes. Click the Create New Field Scheme button. Enter a unique Name and Description to distinguish the Field Scheme. Add Answers to the field Scheme. This step is similar to the adding Answers on the Task Design tab. Field Scheme allows to set data format for each Data Store column by using specific Answer Types (Email, Number, etc.). Click the Save button. For a Data Store created from file, an appropriate Field Scheme is created automatically. Data Store Performance Optimization Data Store functionality is implemented using relational database under the hood. In order to optimize performance for significant amount of data, developers need to think about indexes. There are 2 ways of managing Data Store indexes: Setup indices within Bot Task code. Example can be found in Datastore Plugins, Contact your Operations team to create index in PostgreSQL (WorkFusion 9.x) or MsSQL (WorkFusion 10.x) with given table and index details See the following articles about Data Store purging according to your WorkFusion version: 10.0 Data Stores Purging 9.x Data Stores Purging Archive and Audit All actions with Data Stores are tracked in DB in a special Audit table (data store audit). Removed records are saved in a special DB table with archive postfix and can be restored only via DB (e.g. for \"ds test\" table, archive is located in \"ds test archive\" table). A deleted datastore is not removed from DB, an archive table is created instead (e.g. \"ds currencies\" will be renamed to \"ds currencies deleted 20160129022959\"). "},{"version":"10.0","date":"Aug-06-2019","title":"gold-data","name":"Gold Data","fullPath":"iac/core/glossary/gold-data","content":" Gold Data is a file containing correct Answers for all the Questions that you intend to qualify the Workers on. A Gold Data file has an additional column with a \"gold _\" header prefix for each Answer in the Task. For example, if your manual task contains Answer Unique Codes \"invoice number\" and \"vendor name\", Gold Data should have additional columns with \"gold invoice number\" and \"gold vendor name\" headers. These columns should have the correct Answers. If necessary, Gold Data should have messages containing explanations for Workers to train them on the Task. The message column for each Answer should have headers with a \"gold \" prefix and a \" message\" postfix. For example,\"gold vendor name\" and \"gold vendor name _message\". To download gold data click on Upload Gold data in human task Gold data for the manual task illustrated above should have the following structure:"},{"version":"10.0","date":"Aug-06-2019","title":"qualifications","name":"Qualifications","fullPath":"iac/core/glossary/qualifications","content":" Qualifications are a means that the WorkFusion provides to ensure the quality of Workers and narrow your workforce for optimal performance. Qualifications limit the visibility of manual tasks to appropriate workers and groups. Qualifications can be assigned to Workers manually, automatically by Qualification Rules, or Workers can request some Qualifications. Accuracy Based Qualifications – Qualifications associated with a Qualification Task. They allow you to measure Worker quality with a Gold Data file. When the Qualification Task is launched, it appears to Workers like an actual Worker Task. But since you have provided the responses in terms of Gold Data, the system can measure how well they answered and assign them a Qualification Score based on their performance. The algorithm of Score calculation is determined by Qualification Rule. Not Accuracy Based Qualifications – Qualifications that are not associated with Gold Data and the Workers are not granted a Score. Workers either have this Qualification or they do not.Example: Terms and Conditions questionnaire that Workers have to read and sign before being allowed to work on your Task. Private Qualifications – Qualifications intended to mark some Workers internally in WorkFusion. They are not visible to Workers. Private Qualification is not included in qualification requirements for the Crowd. If you want to start a Qualification Task, you should add Accuracy Based Qualification in addition to a Private one. System Qualifications – Qualifications provided by WorkSpace. They are available by default within WorkFusion and so do not need to be created or configured. You simply select them when setting up your Task.Example: Worker Percent Assignments Approved. Qualification Types Qualification Types "},{"version":"10.0","date":"Oct-15-2019","title":"glossary","name":"Glossary","fullPath":"iac/core/glossary/glossary","content":" Control Tower Workflow Task – a unit of work assigned to Workers (contractors, SMEs, employees). The main goal of a Task is to take initial raw data (.csv file) and to structure, enrich, or check this data according to some rules. Manual Task Task completed by a human, either by a cloud Worker or an internal employee. Bot Task Task completed by some type of computer automation (data parsing, extraction, transformation, loading). Bot Tasks contain work to be done by WorkFusion's Bot. These Bots perform the steps in your Business Process which are more suited for automation than human labor. Bot Tasks can exist only as a Business Processes step. Example: address parsing, determining the width and height of an image, interacting with your API or repository, or filtering content that do not meet certain criteria. Qualification Task a test Task intended for screening your Workers and filtering out Workers who are unqualified to take your specific Task. Business Process (BP) is a combination of connected Manual Tasks, Bot Tasks, and or other BPs handled by a user defined flow. The BP Designer allows you to control the flow of the Manual and Bot Tasks. A BP has one input Data file which is enriched and or modified by each Task results. Tasks in BP are executed successively or in parallel depending on the Composite Rules and their Conditions. A BP Task can utilize data generated by previously completed Tasks or data from the input file. Use Cases serve as the foundation for both Tasks and Bot Configurations. They describe the workflow of a particular Business Process and thus serve as a template to create either Manual or Bot Tasks based on that workflow. For Manual Tasks, Use Cases simplify task creation by having predefined questions, answers, instructions, and sample input file and gold files. For Bot Tasks, Use Cases simplify development by providing a baseline of the configuration code. Data Gold Data is a file containing correct Answers for all the Questions that you intend to qualify the Workers on. Learn more: Gold Data. Task Data is a .csv file with source input data needed to run the task. The column headers in the file should be the same as the variables used within the task design but can be remapped if necessary. Each row of the Task Data file defines a Record. Data Store module is a web interface for managing DB tables in WorkFusion. Data Store table structure and column data format are defined by a Field Scheme. You can use Data Stores to keep lookup tables for multivalue Answer options, save BP results, and query Data Stores from Bot Configs and Rules. BEP Available since v10.0. Learn more about Bot Execution Platform. BEP Worker – a job that is invoked by Master and performs on an Agent. It processes the task and returns its results to Control Tower. BEP Worker Profile – a type of client that sends tasks to BEP. Can be: Control Tower, AutoML training, AutoML execution. BEP Master – a server with a number of services that orchestrates Workers performing tasks from Control Tower, routes them in a queque, prioritizes them, allocates resources on Agent, and invokes Workers to perform a specific task on Agent. BEP Agent – a resource on which Workers perform. WorkSpace See full list of terms, including Workers, Qualifications, Instructions and more: WorkSpace Glossary. AutoML See full list of Automation and Machine Learning terms: AutoML Glossary. RPA See full list of Robotics terms: RPA Glossary. "},{"version":"10.0","date":"Aug-06-2019","title":"task","name":"Task","fullPath":"iac/core/glossary/task","content":" Task – unit of work assigned to Workers, who can be public Workers or private internal Workers (contractors, SMEs, employees). The main goal of a Task is to take initial raw data (csv file) and to structure, enrich, or check this data according to some rules. Manual Task Task completed by a human, either by a cloud Worker or an internal employee. Bot Task Task completed by some type of computer automation (data parsing, extraction, transformation, loading). Qualification Task a test Task intended for screening your Workers and filtering out Workers who are unqualified to take your specific Task. Task Data and Design Task Data and Design Task Statechart Task State Chart Task Available Actions ↓ Action Status → Draft Processing Paused Completed : : : : : : : : Copy Task V V V V Copy Task with Data V V V V Copy to New Task V V V V Create Qualification Task V V V V View Properties V V V V View Data V V V V Pause Task X V X X Resume Task X X V X Stop Task X V V X Delete Task V V V V Create Moderation Task X V V V Recalculate Accuracy X V V V Download Original Data * V V V V Download Gold Data * V V V V Export V V V V Run V X X X Preview V V V V Repair * X V V V * – You can download Original or Gold Data depending on the uploaded file type. – For Draft Tasks, preview is available if Data file was uploaded and mapped to Data Elements. * – You can repair only the Tasks with processing issues. "},{"version":"10.0","date":"Aug-14-2019","title":"workspace-glossary","name":"WorkSpace","fullPath":"iac/core/glossary/workspace-glossary","content":" Accuracy – a parameter determining the probability that the majority Answer is correct (calculation is based on previous Worker statistics in similar Tasks). Worker Accuracy – the percentage of all Worker Answers that coincided with Answers provided by other Workers (with majority). Block Size – the number of Records a Worker will get in each Task. Default value = 1. Confidence – a qualitative parameter showing the number of Workers with coincident Answers per Record. Example: Confidence = \"2 out of 3\". This means: 3 Workers have completed this Record; 2 of them provided coincident Answers. Crowd – a group of Workers from WorkSpace that suits a set of pre defined requirements and Qualifications. Qualifications are a means that the WorkFusion platform provides to ensure the quality of Workers and narrow your Workforce for optimal performance. Qualifications can be assigned to Workers manually, automatically by Qualification Rules, or Workers can request some Qualifications. Learn more: Qualifications. Workforce – a grouping entity that specifies who is working on the Task. A Workforce can contain one or several Crowds. Instructions – explanatory part of a Task intended to provide initial data, Task goal, steps to complete, examples. Proper Instructions should contain the following parts: Overview – brief explanation of task objective. Background – some context or background that may be necessary. Steps – clear, step by step instructions to follow. Common Mistakes – explanations of things you DO NOT want to be completed. Examples – specific examples of correct responses. Instruction item can be published or marked as Test. Each time the Instructions are updated, the Worker should acknowledge the changes and mark them as read. FAQ (Frequently Asked Questions) – a Task section comprising some common questions that the Workers can ask about the Task and respective answers. FAQs, like Instructions, can be added in the Design Task tab dynamically while the Task is still running. "},{"version":"10.0","date":"Aug-06-2019","title":"allowed-data-store-queries","name":"Allowed Data Store Queries","fullPath":"iac/core/security/allowed-data-store-queries","content":" If you develop a custom Manual Task that uses Data Store (DS) API calls from JavaScript, you need to create a set of* Allowed Data Store Queries*. By default, all Data Store (DS) API calls from Manual Tasks, except Allowed DS Queries, are blocked by WorkFusion security. Creating Allowed DS Query To start managing Allowed DS Queries: Open the Design tab of your Manual task. Switch to Code Editor. Click the Allowed Data Store Queries button: You will see a popup containing a grid with DS Queries: Click the Create Query button or click an existing query name. Provide parameters for your DS Query: Enter a unique Name Select an existing Data Store. You can click the eye icon to open a selected DS in a new browser tab. enter a valid Query. Click the Save button. All allowed queries are expected to return some data back to Manual Task. That is true for SELECT type queries. If you need other query type – please make sure they return something. For example: INSERT INTO @this (ID, document) values (:id, :document) RETURNING id; UPDATE @this SET workerid=:workerid WHERE id=:id RETURNING id; Using Allowed DS Query in Manual Task Code The Data Store API call in this case will have the following URL pattern: for WF 8.3 workfusion public v2 datastores executeNamedQuery queryName=demo&nativeId=hitId&parameters={status:\"COMPLETED\"} for WF 8.2 and older workfusion public datastore executeNamedQuery queryName=demo&nativeId=hitId&parameters={status:\"COMPLETED\"} Where: queryName is a name of your Allowed DS Query; nativeId is the Task hitId parameters is a JSON representation of an object containing dynamic query parameters Add the following code snippet at the bottom of your Manual Task code. This code is valid for SPA Version 8.3 or newer. JS code snippet The result of this AJAX request will be seen only on: Task Preview > External URL. This link appears for task that are not in a draft state (they should be already run) in WorkSpace application when you have accepted this task "},{"version":"10.0","date":"Aug-06-2019","title":"filters-for-collaborative-work","name":"Filters for Collaborative Work","fullPath":"iac/core/security/filters-for-collaborative-work","content":" Introduction Control Tower provides a capability for collaborative work to multiple teams on the same environment. For this purpose, you can use filters to allow a Group of users accessing only certain items in Control Tower, while the others are hidden. The capability is very helpful in case when a few teams use the same Control Tower to perform different tasks, as the teams do not interfere each other work. By creating and applying an advanced filter, you can define an available set of the following objects: Tasks Business Processes Data Stores Note All other items, that do not match with the filters will be hidden and can not be accessed. Filters applied can not be cleared from Tasks, Business Processes or Data Stores Lists. Introduction Tasks and Business Processes Data Stores Secrets Vault How To Use Filters Create Filter Assign Filter to Group Assign Filter to User Tasks and Business Processes For Tasks and Business Processes the main criteria for filters are: Tags – tags assigned to Tasks or Business Processes; Author – name of the user, who created the Task or Business Process; Title – exact title or text containing in the Task or Business Process. Data Stores For Data Stores the main criteria for filters are: Name – text containing in the Data Store name; Description – text containing in the Data Store description; Author – name of the user who created a Data Store. Secrets Vault Available since SPA 9.2 For Secrets Vault the main criteria for filters are: Alias – text containing the name of Secret entry Hint For more information about filters refer to the following sections in the Knowledge Base: Business Process List Data Stores Secrets Vault How To Use Filters Create Filter For example, let's create a Business Process filter for Invoice Processing team (team members will see only Business Processes that match criteria defined in this filter): Note You can create multiple filters (Business Process, Task, etc.) for different teams (ops, devs, analysts). Go to Business Processes List, create an advanced filter and save it under a unique name. Alternatively, you can create filters in Configuration > Filters page. Attention Make sure, that the filter contains at least one parameter defined, as a filter without criteria can not be applied and is skipped. Assign Filter to Group To assign the filter created to a User Group, go to Configuration > User Group Management and click an appropriate Group. Click the Add button. Select the Filter Type (in this example Business Process) Select the Filter Name (filter that you've created on the 1st step) (optional) Add another filter. Click the Save button. You can check, that the filter has been applied to users in the Group. Go to Configuration > User Management, click a user belonging to the Group. The filters applied are listed in the Group Level Filters section. Attention As a filter or a group of filters has been applied to the Group, all objects (Tasks, Business Processes or Data Stores), that do not match to the filter criteria are hidden from all users belonging to the Group! As a result, users from different teams (with different available filters) will have access only to their Tasks, Business Processes or Data Stores. If a user opens a direct link to a Task, Business Process, or another object that is not included in the assigned filters, the access will be denied. In Business Process Designer a user cannot see details of Tasks and Business Processes that are not included in the assigned filters, if these steps are already active. If a user attempts to create a new filter that contains elements excluded from the applied filters, these elements will not be displayed. Example: a filter applied to Tasks: \"Tags doesn't contain PROD\"; a user applies a filter on the View All Tasks page: \"Tags contains PROD\"; Result – no Tasks are displayed. Assign Filter to User Another option is to apply the filter to a single user (without using Groups). Go to Configuration > User Management and click an appropriate username. Click the Add button. Select the Filter Type (in this example Business Process) Select the Filter Name (filter that you've created before) (optional) Add another filter. Click the Save button. Repeat steps 1 6 for all users to apply the same filters to. "},{"version":"10.0","date":"Aug-06-2019","title":"links-to-s3-secure-buckets","name":"Links to S3 Secure Buckets","fullPath":"iac/core/security/links-to-s3-secure-buckets","content":" You can use links to secure S3 buckets in the input data files for Human Tasks and BPs. Suitable use cases Prepare an S3 Secure Bucket Amazon or React S3 Client Minio S3 Client Generate Secure Document Links Configure WorkFusion Instance SPA Version 8.x RPA Express SPA Version 9.x Restart Control Tower Result in WorkFusion and WorkSpace Using Secure Links inside other URLs Troubleshooting checklist Suitable use cases IE task with private documents Private files are accessible in IE (PDF, XML) through a temporary link. If user copies the link, he cannot open it outside the task. User wants to render PDF outside of IE task, e.g. insert an iframe with PDF to the task Private files are accessible in the task through a temporary link. If user copies the link, he cannot open it outside the task User acquired a public link to the task, i.e. ... public HTMLRenderer... User can not see private documents inside the task if the HIT is not active. If user does preview inside WF, it should render the document in any case. Prepare an S3 Secure Bucket Amazon or React S3 Client Upload your files to S3 bucket. (see How to upload files to S3) Set bucket permissions: Clear all checkboxes for all users to disable all permissions. Select Make private to enable Full Control only for one user. S3 Browser Minio S3 Client Go to your Minio instance UI client. It should have the following format: https: bucket name db1.instance name.com minio Select you bucket and select ... > Edit policy. Remove all policies so that the policies list looks the following way: Generate Secure Document Links To generate secure link, use s3 put plugin (learn more: S3 put Plugin) with option expires in seconds. expires in seconds parameter denotes the time after creation when the link will stay live. If this parameter is used, the BP won't cache anything. If expires in seconds is empty, secure links will not be created. The code below uses the signedUrl property: For more details, refer to the S3 Plugins page. Links with the https: bucket _name.s3.amazonaws.com path to file format are NOT supported in WorkFusion for secure buckets. Configure WorkFusion Instance SPA Version 8.x SPA 8.x To use links to S3 secure buckets, WorkFusion operations team needs to add the following JNDI variables: JNDI Variables bucket _name – S3 secure bucket name. access key secret key – S3 secure bucket Access Key and Secret Key. RPA Express Find the following file, where X.X stands for your RPA Express version: Find a line containing s3.context.key.map.presign and edit it the following way, providing your actual secret key and access value: secure.properties (from 9.x) SPA Version 9.x Open the database instance and find the following folder and .sh file: Prepare a file with any name (my.properties in this example) with the following text, providing your actual secret key and access value: secure.properties (from 9.x) Run the following command using Bash: Restart Control Tower Go to application server. In console, run the following command: Result in WorkFusion and WorkSpace Each link to the S3 secure bucket will be presigned and available for downloading (in WF preview or in WorkSpace) only during a defined time period: in WorkFusion, when opening Preview or External Preview, presigned links will be available for downloading during 3000 seconds. in WorkSpace, when viewing a task or after accepting the task, presigned links will be available for downloading during 30 seconds. If user has opened External Preview from WorkFusion and copied the External URL (... public HTMLRenderer...), this URL will be unavailable for opening in another browser window, i.e. users cannot share links to external preview of a task with S3 secure links. In Information Extraction tasks, for Original Document field link, a new link is generated (e.g. public generatePreSignedS3Link assignmentId=assignmentI id&dataUUID=generated data _uuid). A query to this link returns a document link available during 30 seconds. GeneratePreSignedS3Link is available only for active Assignments (accepted by Workers), after Assignment submitting or returning it becomes unavailable. Using Secure Links inside other URLs To forcefully presign an S3 secure bucket link, for example when you need to include this link into another URL for PDF preview (src=\"https: s3.amazonaws.com crowdcontrol.taglib pdf.js 1.1.366 web viewer.html file=question.data 'url' \"), you should use a Macro template `` . It creates a new resultUrl variable containing the presigned link. Troubleshooting checklist Review settings specified in the workfusion.properties file located in opt workfusion workfusion conf folder. Make sure s3.endpoint.url and s3.bucket.URL are correct and have no https: prefix. Default port for https should be specified. Correct settings example: s3.server.com:443. Review the keys in secure storage settings. Make sure secure key and access key are not mixed. Review links specified inside .CSV file. All links should have the following format: https: s3.server.com:443 bucket file path file name. Port number is compulsory. Review security settings for the bucket: there should be no read access by default. For TOOD OCR version, review the xml content and make sure there is a signed link to image. "},{"version":"10.0","date":"Oct-04-2019","title":"role-management","name":"Role Management","fullPath":"iac/core/security/role-management","content":" Add Edit a Role Permissions Description and Default Role Setup Permission Restrictions Main menu items are not displayed Action buttons links are not displayed 'Access Restricted' message when trying to open a direct URL Known Issues Versioning Info Available in SPA 10.0. To learn about previous version, see 9.2 Role Management. The Role Management lists all the current roles in the system and provides access to all Role Management Functions. It is accessed via the Configuration Menu: System Settings > Role Management. On this page, you can perform the following actions: create a new role edit an existing role delete an existing role Add Edit a Role Click Create Role (or click the appropriate link in the \"Name\" column to edit). The following screen is displayed: Enter a unique Name for the role. Provide role Description. Tick checkboxes with appropriate Permissions. Click the Save button. As a result, the created role appears in the Role Management table and can be granted to WorkFusion users on Configuration > User Management page. Default role setup Permissions Roles Administrator Developer Operator View Analytics ✓ ✓ ✓ View Tasks ✓ ✓ Task Designer ✓ ✓ View Results ✓ ✓ ✓ Task Business Process Actions ✓ ✓ ✓ View Business Processes ✓ ✓ ✓ Schedules ✓ ✓ ✓ Manage Templates ✓ ✓ Manage Bot Configurations ✓ ✓ Manage Rules ✓ ✓ Manage Data Stores ✓ ✓ View Data Stores ✓ ✓ ✓ Manage Workers ✓ ✓ ✓ Manage Qualifications ✓ ✓ ✓ System Preferences ✓ ✓ Manage Data Purge ✓ ✓ Manage Data Sources ✓ ✓ ✓ Manage Answer Types ✓ ✓ Manage Use Cases ✓ ✓ Manage Users and Groups ✓ Activity Log ✓ View Secrets Vault Aliases ✓ ✓ ✓ Manage Secrets Vault Aliases ✓ View Secrets Vault Entries Manage E mails ✓ Import Export ✓ ✓ ✓ Advanced Package Import ✓ ✓ Process Deep Copy ✓ ✓ Bot Task Monitoring Access ✓ ✓ Manage Platform Monitor ✓ ✓ View Platform Monitor ✓ ✓ Manage AutoML Settings ✓ ✓ Permissions Description To be added Permission Restrictions When a user has a Role without appropriate Permissions, the following restrictions can be applied in the user interface: Main menu items are not displayed Restricted Permissions All Permissions Action buttons links are not displayed Restricted Permissions All Permissions 'Access Restricted' message when trying to open a direct URL Known Issues Impossible to delete a role assigned only to group. No messages appear after Delete Confirmation operation. "},{"version":"10.0","date":"Oct-04-2019","title":"secrets-vault","name":"Secrets Vault","fullPath":"iac/core/security/secrets-vault","content":" WorkFusion platform provides Secrets Vault (formerly Secure Storage) functionality intended for storing sensitive data (i.e., login, password). To start managing Secrets Vault, go to System Settings > Secrets Vault. Managing Secure Storage from Bot Configs You can read, write, and delete records from Secrets Vault by using Secrets Vault Plugins (Secure Store Plugins). Documentation on Secrets Vault Plugins is available in the Help section in RPA Express. To access it, in RPA Express menu, go to Help > Help Contents > Bot Task Plugins > Secrets Vault Plugins (Secure Store Plugins). Filtering Starting with SPA 9.2, standard filter feature was introduced for Secrets Vault which allows the following: filtering the required aliases as per your rules limiting visibility of the aliases to other users See Filters for Collaborative Work for more information. Assigning Permissions From SPA 9.2, it is possible to assign granular permissions to users in Secrets Vault. Refer to the 9.2 Role Management page. WorkFusion platform provides Secrets Vault (formerly Secure Storage) functionality intended for storing sensitive data in (login, password, alias) format. Managing Secure Storage from Bot Configs Filtering Assigning Permissions Adding Secure Entry Adding Bulk Entry Viewing Secure Keys and Values Updating Existing Secure Entry Deleting Secure Entries Known Issues Adding Secure Entry To add a new Secret Entry, do as follows. Click the Add button above the grid. The following popup appears. Enter Alias, Key and Value for this Secret Entry. Alias serves to distinguish secure entries and must be unique. Click Save. As a result, a new entry appears in the grid. Adding Bulk Entry You can also create Secure Entries from file (for example, if you want to create multiple entries). Create a CSV file with 3 columns: alias, key, value. See the example below. Click the Upload Data button. The following popup appears: Click Add and browse the CSV file on your local machine. (Optional) Click the Options link and set File Separator and or File Encoding, if needed. Tip We recommend using Comma separator and UTF 8 encoding. Viewing Secure Keys and Values All existing Secure Entries are displayed in the grid with hidden Key and Value column items ( *). To view Key or Value, do as follows. Hover over appropriate grid cell. The View Update button appears. Click the View Update button. Enter Password popup appears. Enter the master password and click the Ok button. As a result, the appropriate key or value is displayed in the grid without asterisks. You can hide key or value by hovering over a cell and clicking the* Hide key* or Hide value button. All Keys and Values are automatically hidden when user leaves the page or reloads it. Updating Existing Secure Entry To edit a Secure Entry, proceed as follows. Hover over appropriate grid cell in the Alias column. The Update button appears. Manage Secrets Vault Aliases permission provided View Secrets Vault Entries permission provided Click the Update button. The following popup appears. Manage Secrets Vault Aliases permission provided View Secrets Vault Entries permission provided Enter New Key and New Value. Alias cannot be modified. Click the Update button. Deleting Secure Entries Tick check boxes opposite appropriate grid entries. Click the Delete button. Known Issues Known Issue Workaround The following combination is not saved in Secrets Vault: ! ! Do not use the combination When importing list of Secret Entries ( ) symbol is not recognized Do not use the combination "},{"version":"10.0","date":"Aug-22-2019","title":"security","name":"Security","fullPath":"iac/core/security/security","content":" This section contains security features that are intended for protecting data in WorkFusion, segregation of duties, controlling access etc. "},{"version":"10.0","date":"Aug-06-2019","title":"user-group-management","name":"User Group Management","fullPath":"iac/core/security/user-group-management","content":" The User Group Management dashboard lists all the current user groups in the system and provides access to all User Group Management functions (create,edit, delete, enable,disable). It is accessed via the Configuration Menu: Configuration > User Group Management. Creating a User Group You can organize and manage users by including them into specific groups and setting Roles and Filters on a group level: Click the Create User Group button. Enter the Group Name and Owner Contact. Start typing or selecting user names in the Add Users field. Assign a Role to this group under the Select User Roles panel. Add Available Filters if you want limit access for this user group. Click the Save button. As a result, each user will have both group level and personal permissions and available filters which can be viewed on user management page. "},{"version":"10.0","date":"Aug-06-2019","title":"user-management","name":"User Management","fullPath":"iac/core/security/user-management","content":" The User Management dashboard lists all the current users in the system and provides access to all User Management Functions. It is accessed via the Control Tower Menu: System Settings > User Management. On this page, you can perform the following actions: create user edit user properties and permissions delete user (starting from Version 9.2, it is restricted to self delete) disable enable user profile. Disabled users cannot login to WorkFusion until they are enabled again. Adding Editing a User Click Create User: or click the appropriate link in the Username column to edit: Set all the required User Properties. Select a User Role. (Optional) Add Available Filters – this option is used to define a set of Tasks, BPs, and other lists available for this user. See full description here. Password policy Password should not contain the user name Passwords should be longer than 6 characters Password should not contain 2 or more adjacent repetitive characters Password contain at least 1 alpha, 1 numeric character, 1 character in lower case and 1 character in upper case Password should be different from any of 6 previous passwords User Roles and Permissions Each WorkFusion user must have one role in the system. To view or modify Permissions for a User Role, go to System Settings > Role Management. You can also create a new Role with a custom set of Permissions and grant it to a user. "},{"version":"10.0","date":"Aug-06-2019","title":"workspace-localization","name":"WorkSpace Localization","fullPath":"iac/core/localization/workspace-localization","content":" Overview Default File Translation Customization Apply changes Overview WorkSpace has built in localization and customization mechanism. To localize WorkSpace to different language, localization file with translations is required. Application will use this file to display appropriate language, based on browser language settings detected automatically. If no browser language is defined, English translation will be used. To localise the WorkSpace to new language, file for tranlsation should be translated into required language and then the file needs to be included into WorkFusion infrastructure. Alternatively, you can use this mechanish to change the wording of particular menus and messages and avoid Workers' confusion. Default File The i18n.properties file contains all interface wording that can be changed. See and download the default file below: i18n.properties Translation To translate or modify items' names, you need to edit the corresponding properties, see the example in English and Russian: i18 _ru.properties tab.tasks=Задачи Non ascii symbols are encoded and the Russian file looks like the one below: tab.tasks= u0417 u0430 u0434 u0430 u0447 u0438 i18n.properties tab.tasks=Tasks Customization Let's say, the following UI message confuses internal workers: Your results have been submitted and will be approved or rejected shortly. You can use the following code as a unique reference for the task you have just submitted. In this case, you need to find the corresponding section in the i18n.properties file and change its wording according to your goals: messages.info.yourResultsHaveBeenSubmittedAndWillBeApprovedOrRejectedShortly.acceptNewTask=Your results have been submitted. Apply changes Translate i18n.properties file to required language or introduce the required changes to items. (translation only) Add language code to file name, for example rename to i18n_sv.properties for Swedish language. (translation only) Encode to unicode, if required. This can be done using ascii utility: Put or add the modified i18n *.properties files in WEB INF classes folder without rebuilding the JAR: Alternatively, you could add or update translation file to workspace WAR with the following commands: "},{"version":"10.0","date":"Aug-06-2019","title":"barcodes-recognition","name":"Barcodes recognition","fullPath":"iac/core/ocr/barcodes-recognition","content":" Barcodes extraction mode Prerequisites License info Barcode recognition settings Example of barcodes recognition via API Profile Custom region Barcode types 1D barcodes Codabar Code 128 2D barcodes PDF 417 Aztec QR Code DataMatrix MaxiCode Barcodes extraction mode OCR provides document processing with \"barcodeRecognition\" profile which is designed for barcode extraction scenarios. Use of this profile as 'profile' parameter in API activates barcode extraction mode. This mode extracts only barcodes (texts, pictures, or tables are ignored). Prerequisites Prerequisites Barcodes should be available in your License License info License info API Use APIs to define all available information about license: api v1 cloud activeLicense api v1 cloud listLicenses According to License parameters the following types of barcodes(1D, 2D) can be recognized with OCR: activeLicense response example ABM_1D ABM_PDF417 ABM_Aztec ABM_QRCode ABM_MaxiCode ABM_DataMatrix ABM_Autolocation Barcode recognition settings Barcode recognition To enable barcode recognition: document with barcode only is recognized automatically when profile=barcodeRecognition is used document with text requires use of either profile=barcodeRecognition or custom region of type = BT _Barcode usage of profile or custom region is mandatory barcode extraction contains barcode only with activated profile barcode and other blocks with custom regions Example of barcodes recognition via API Profile profile=barcodeRecognition Barcode only is displayed on provided image Results of recognition: A31117013206375B Barcode and additional text is displayed on provided image Results of recognition: (only barcode is recognied when profile was using) 1ode *O28 Custom region customRegions= {\"page\":1,\"left\":60,\"top\":50,\"right\":245,\"bottom\":205, \"type\":\"BT _Barcode\" } To define custom region on the image you can use simple picture editor(e.g. \"Paint\"), switch on rulers feature for example and try to define points(coordinates) of borders(px) Results of recognition: Code 1ode O28 Code 128 is an alphanumeric very high density, compact variable length barcode scheme that can encode the full 128 ASCII character set. Each character is represented by three bars and three spaces totaling 11 modules. Each bar or space is one, two, three, or four modules wide with the total number of modules representing bars an even number and the total number of modules representing a space an odd number Three different start characters are used to select one of three character sets. 1ode O28 Notes: recognition can be not successful, when: when page size is larger of the provided region page size is smaller of the provided region Barcode types 1D barcodes Codabar {.media width=\"300\"} Codabar is a Self checking, variable length barcode that can encode 16 data characters. It is used primarily for numeric data, but also encodes six special characters. Codabar is useful for encoding dollar and mathematical figures because a decimal point, plus sign, and minus sign can be encoded. Code 128 {.media width=\"230\"} Code 128 is an alphanumeric, very high density, compact, variable length barcode scheme that can encode the full 128 ASCII character set. Each character is represented by three bars and three spaces totaling 11 modules. Each bar or space is one, two, three, or four modules wide with the total number of modules representing bars an even number and the total number of modules representing a space an odd number. Three different start characters are used to select one of three character sets. And others. 2D barcodes PDF 417 {.media width=\"200\"} PDF 417 is a variable length, stacked symbology that can store up to 1,850 printable ASCII characters or 1,100 binary characters per symbol. PDF417 is designed with selectable levels of error correction. Its high data capacity can be helpful in applications where a large amount of data must travel with a labeled document or item. Aztec {.media width=\"100\"} Aztec is high density two dimensional matrix style bar code symbology that can encode up to 3750 characters from the entire 256 byte ASCII character set. The symbol is built on a square grid with a bulls eye pattern at its center. QR Code {.media width=\"100\"} Next generation format used already in Japan for both image and text data with ability to store URLs which can be read and launched by appropriate software. DataMatrix {.media width=\"100\"} MaxiCode {.media width=\"100\"} created and used by United Parcel Service "},{"version":"10.0","date":"Aug-06-2019","title":"business-process-for-checkmarks-recognition","name":"Business Process for Checkmarks Recognition","fullPath":"iac/core/ocr/business-process-for-checkmarks-recognition","content":" The main purpose of Business Process is to define state of checkmarks checkboxes radio buttons in recognized documents. Business process is available since 9.2 Required Setup ABBYY FineReader Engine has the option to recognize checkmarks in Windows version only. This means that to use this feature in SPA, all OCR should be installed on Windows machines. OCR on premise setup on Windons (not on Linux) to use OMR capabilities. Refer to OCR Windows Server Installation. License for checkmarks recognition (Windows OMR) Available for RPAx and SPA with OCR on Windows Installed ImageMagic to split multi page documents. Supported version 7.0.7 7 Installed GhostScript to process PDF files. Supported version 9.22 Artifacts Business process definition: Checkmarks recognition 9.2.0.3.zip (attachments 67505668 70067367.zip) Checkmarks recognition 10.0.0.1.zip (attachments 67505668 77365510.zip) Last changes: self signed certificates in HTTPS URL of documents can be processed Authorization request header was added, e.g. in case of basic auth BP uses credentials from 'Global variables' DS Business Process Flow Recognition of checkmarks has the following phases. OMR Settings – setup of main settings which OCR will use: S3 bucket, pre processing parameters Split document into PNG pages – some files formats, e.g. pdf, are not supported for checkmarks recognition, that is why we need additional splitting. All splitted images are saved in the BP folder (the path file is ... ocr bp bp _uid) on s3. Try to load existing template – Business Process tries to find the existing json template from storage for processing the document by provided original document type. In case, when a template with the needed type does not exist, Manual Task is created, where checkmark groups and checkmarks for one document are specified. For details, refer to Document template preparation. Definition of document type If two documents have the same checkmarks in the same places, they are considered to be of one type. The manual task output is a .json file with a template for the document type. The template is saved on s3 after Task completion. If a template exists, the file is moved to OCR to be processed by applying of the existed template as the customRegions value. As a result, a link with OCR response is provided in xml format with the checkmarks state. Input Data The required column names in the input file are as follows: original document url – the link to the document, e.g. s3 url original document type – the document type Manual Task for Checkmark Groups and Checkmarks Specification The main Idea of Task is to define coordinates of checkmarks without additional tools and application To do it manually, refer to Document template preparation. First of all, you need to define the image zone recognized either as a group of checkmarks or a single checkmark. In case of group, the area for each checkmark in block should be additionally defined. The accuracy of the selection area affects the quality of recognition directly. That is why, the area should be defined as accurately as possible.** Region Selection To increase selection accuracy, you can zoom a document. Click on the top left point of the region and drag to the bottom right point using the cursor. Add the region name and fill in the required fields. Item type (OMR support extra configuration): BT _Checkmark BT _CheckmarkGroup Checkmark type: CMT _Square CMT _Empty Circle (CMT _Circle) NOT SUPPORTED FOR NOW As a result, coordinates are defined and displayed in the table. Coordinates are used in json template generation for checkmarks recognition. Checkmarks Results After completion of BP, OCR response, the recognition results are saved in the BP folder as an .xml file (... ocr bp bp uid). The url is prepared, available in the results table as ocr document _url. Checkmark group results Single cheсkmark results "},{"version":"10.0","date":"Aug-06-2019","title":"comparison-of-ocr-platforms","name":"Comparison of OCR Platforms","fullPath":"iac/core/ocr/comparison-of-ocr-platforms","content":" The document describes the differences between OCR for Windows and Linux, RPA Express and SPA. Installation platform Licensing Architecture Rest API OCR recognition capability Installation platform RPA Express SPA Linux Windows Licensing RPA Express SPA License type trial license or paid runtime license runtime license License activation not required (only for trial license type) required (instruction) Architecture RPA Express SPA Scaling Process images synchronous, not parallel asynchronous Data storage in memory DB Queue in memory LRU RabbitMQ Rest API RPA Express SPA submitImage submitPattern processImage processDocument getTaskStatus download cleanup summary cancelTasks trainPattern (since v. 9.0.0.1) activeLicense listLicenses prepareLicense activateLicense OCR recognition capability RPA Express (Windows) SPA (Windows) SPA (Linux) OMR (Optical Mark Recognition, i.e. check boxes and radio buttons) (new license is required) ICR (Handprint Recognition) Barcodes Recognition (new license is required) (may require license update) "},{"version":"10.0","date":"Aug-06-2019","title":"how-to-use-ocr-api","name":"How to use OCR API","fullPath":"iac/core/ocr/how-to-use-ocr-api","content":" Introduction Docs recognition via processImage API Docs recognition via processDocument API OCR pattern creation mergePattern processImage: OCR pattern applying processDocument: OCR pattern applying Introduction Optical Character Recognition (OCR) it is technology for convert document images into editable text. There is ability to recognize document by several ways: via processImage API via processDocument API Example of picture recognition by several ways will be provided below. Docs recognition via processImage API Authentication should be set if needed POST host:port api v1 cloud processImage Start image processing with processImage. Upload doc image as a file Add parameters for correct recognition Send request Response has a lot of useful information: taskId, processing time, result Urls, status of recognition. taskId will be used during the next requests in the following steps(copy taskId from response). ResultUrl result of recognition will be available by this link after completion of recognition. GET host:port * api v1 cloud getTaskStatus taskId=* value Check task status with getTaskStatus (specify correct taskId). taskId value was determined in previous step. Set taskId parameter and send request. As a results, status of Task with resultUrls are provided in response. If task is complete (status=\"Completed\") open results from resultUrl, resultUrl2 or resultUrl3 fields (replace &amp; with &). GET host:port api v1 cloud download taskId= value If task is complete (status=\"Completed\") , you can also download results from links in resultUrl, resultUrl2, resultUrl3 (depends on export format). Docs recognition via processDocument API POST host:port api v1 cloud processDocument *GET* host:port api v1 cloud processDocument* is deprecated from 9.2* POST host:port api v1 cloud submitImage Upload file with submitImage, copy taskId from response. In case, when need to upload several files for recognition need to: Submit image Copy id Add taskId parameter to submitImage API Submit one more image POST host:port api v1 cloud processDocument Start image processing with processDocument taskId value was determined in previous step.Set taskId parameter. Add parameters for correct recognition Send request GET host:port api v1 cloud getTaskStatus taskId=** value Check task status with getTaskStatus (specify correct taskId) If task is complete (status=\"Completed\") open results from resultUrl, resultUrl2 or resultUrl3 fields (replace &amp; with &). GET host:port api v1 cloud download taskId=value If task is complete (status=\"Completed\") , you can also download results from links in resultUrl, resultUrl2, resultUrl3 (depends on export format). OCR pattern creation Extended Recognition with custom trained pattern can be used for: texts set in decorative fonts texts containing unusual characters (e.g. mathematical symbols) long documents of low print quality (more than a hundred pages) For example: OCR product provides a possibility to create train a user pattern, which will be used for the further recognition. Pattern training works as follows. Symbols are recognized in training mode,and, subsequently, a pattern is created. The pattern is used as a source of additional information during recognition to aid recognition of the remaining text. At first, need to prepare images and data for training. Need to prepare several example of symbol in different views and define parameters of symbol(baseLine, smallSymbolHeight)** . baseLine contains the distance from the base line to the top edge of the cropped image of the character. The base line is the line on which the characters are located. The top edge of the image is determined by the character orientation. H1 on the picture. smallSymbolHeight specifies the height of small characters in pixels on the source image. H2 on the picture. Examples of images and values baseLine =22 smallSymbolHeight =14 baseLine =19 smallSymbolHeight =10 ** baseLine = 27 smallSymbolHeight = 18 * * baseLine = 34 smallSymbolHeight = 26 * * baseLine = 18 smallSymbolHeight = 12 baseLine =28 smallSymbolHeight =18 ** baseLine =21 smallSymbolHeight =14 ** baseLine = 19 smallSymbolHeight = 12 ** baseLine = 23 smallSymbolHeight = 15 * * baseLine = 22 smallSymbolHeight = 14 You need to experiment with baseLine and smallSymbolHeight parameters to optimize recognition. To determine baseLine and smallSymbolHeight parameters, consider position of the symbol on the text line within the actual document. E.g. if we consider checkbox in a tax form, it is much bigger than normal letters, and base line is slightly higher (1 pixel for W 8BEN E) than bottom line of the checkbox. So don't guess the parameters looking at the symbol in isolation, consider the layout of the document. Greater pictures of symbols are more efficient for training, preferably around 40 50 pixels. Generally, you need multiple input images for training. POST host:port api v1 cloud trainPattern Set all available examples of symbol with parameters and required parameters to API request. file image file with the symbol. baseLine value from previous step. smallSymbolHeight value from previous step. symbol the symbol that is associated with picture(s). (symbol=₡) Send and download trainPattern result as PTN file. mergePattern mergePattern file upload for a pattern, if provided the training output pattern will be combined with the uploaded pattern. Prepare images and data for training. As was described in previous section \"OCR pattern creation\". POST host:port api v1 cloud trainPattern Set all available examples of symbol with parameters and required parameters to API request. file image file with the symbol baseLine value from previous step smallSymbolHeight value from previous step symbol the symbol that is associated with picture(s) (symbol=₡) And use mergePattern parameter to combine existed pattern with trained in advanced. In current case ae.ptn trained pattern for recognition of Æ symbol. Send and download trainPattern result as PTN file. As a results, trained pattern is available for recognition of two symbols. Apply pattern for the first symbol(\"applying of pattern\" in details see in the next section). Apply pattern for the second special symbol(\"applying of pattern\" in details see in the next section). Pattern file upload for a pattern to be applied to recognize special symbols. Note: You also need the symbol to be added to alphabetExtension (alphabetExtension = Æ) You can use either explicit pattern uploaded with the field or useDefaultPattern=true, not both processImage: OCR pattern applying POST host:port api v1 cloud processImage Add trainPattern file to ProcessImage request. pattern file parameter for better recognition. Download results where trainPattern was applied = > symbol ₡ was recognized as expected. processDocument: OCR pattern applying At first, need to submit image for recognition. POST host:port api v1 cloud submitImage Copy id for the next steps. After that need to submitPattern, which was trained and downloaded before(see \"OCR pattern creation\" section). POST host:port api v1 cloud submitPattern taskId value was determined in previous step. Set taskId parameter and pattern file. POST host:port api v1 cloud processDocument Start image processing with processDocument, trained pattern was uploaded on previous step. Set taskId parameter Add parameters for correct recognition Send request Download results where trainPattern was applied = > symbol ₡ was recognized as expected. "},{"version":"10.0","date":"Aug-06-2019","title":"detect-language-using-text-or-html","name":"Detect language using text or HTML","fullPath":"iac/core/ocr/detect-language-using-text-or-html","content":" Open source library is used for language detection in SPA. It contains 71 built in language profiles: See full list of built in language profiles af Afrikaans an Aragonese ar Arabic ast Asturian be Belarusian br Breton ca Catalan bg Bulgarian bn Bengali cs Czech cy Welsh da Danish de German el Greek en English es Spanish et Estonian eu Basque fa Persian fi Finnish fr French ga Irish gl Galician gu Gujarati he Hebrew hi Hindi hr Croatian ht Haitian hu Hungarian id Indonesian is Icelandic it Italian ja Japanese km Khmer kn Kannada ko Korean lt Lithuanian lv Latvian mk Macedonian ml Malayalam mr Marathi ms Malay mt Maltese ne Nepali nl Dutch no Norwegian oc Occitan pa Punjabi pl Polish pt Portuguese ro Romanian ru Russian sk Slovak sl Slovene so Somali sq Albanian sr Serbian sv Swedish sw Swahili ta Tamil te Telugu th Thai tl Tagalog tr Turkish uk Ukrainian ur Urdu vi Vietnamese wa Walloon yi Yiddish zh cn Simplified Chinese zh tw Traditional Chinese This library estimates precision during the detection process using embedded method languageDetector.getProbabilities(\"text to detect\"). The bot config attached is able to detect only one main langugage and has no functionallity to detect mutiple languages. {.rwuibutton .rwuiinline block .rwuiid292c9194 ef38 46af b267 44d9a6b1dafe} detect _lang.xml Detect Language Bot Config "},{"version":"10.0","date":"Oct-04-2019","title":"improve-ocr-results","name":"Improve OCR Results","fullPath":"iac/core/ocr/improve-ocr-results","content":" What are quality requirements of an input document Scanned images may require some pre processing prior to recognition, for example, if scanned documents contain background noise, skewed text, inverted colors, black margins, wrong orientation or resolution. Resolution r ecommendation for source image: 300 dpi for typical texts (10pt or larger) and 400 600 dpi for texts in smaller fonts (9pt or smaller). Brightness: This image is suitable for recognition. Lower the brightness to make the image darker. Increase the brightness to make the image brighter. Why PDF searchable document is preferable for recognition than high dpi image created from it OCR analyses internal information within the source PDF files such as: annotations, metadata, text objects, font dictionaries, content stream. OCR enchanges PDF conversion performance and speed by efficient and accurate text selection. If text is embedded into the PDF ifle, the OCR engine examines the integrity of the text layer, and makes a decision as to whether or not to extract the text or apply OCR on a block by block bases Why we should cache OCR results Abbyy OCR is part of WorkFusion Product. The way WF partners with Abbyy requires licensing for every commercial user whom we re sell OCR capability. License cost is based on pages images processed. This is why it is important to avoid re processing of exactly same files. WorkFusion core feature is exposure business Processes as use Cases allows us to create and distribute OCR sub process, which contains solution for caching. Approach of caching is: For the file which we need to OCR calculate unique hash based on binary of this file Query DataStore is we already have this hash If NO execute OCR Write to DataStore new hash with results of OCR If YES just get OCR results from DataStore How to improve OCR results Preprocessing use ImageMagic or GhostScript convert colorspace rgb density 300 input.pdf monochrome output.tif Use PDF searchable document instead of high dpi image created from it. Use custom dictionary. Use removeGarbageSize parameter for scanned images to remove garbage (excess dots that are smaller than a certain size) from the image Use allowedRegionTypes parameter to exclude incorrect region types. Use xml:writeRecognitionVariants to understand the recognition problems Example 1 Step 1 (default properties) file exportFormat = html, xml Output results: HTML XML You can see that by default OCR recognize this text with garbage like an image (see html output img tag). Xml format more informative. We can see recognized block types: Picture and Separators Step 2.A (default removeGarbageSize properties) We can try to remove garbage and will see if result changed removeGarbageSize=1 Output results: Federal Credit Union, July 25, 2005 HTML XML Like we can see block type was changed from Picture to Text and image recognized successfully. Step 2.B (default allowedRegionTypes properties) Usage of allowedRegionTypes Please, be careful using allowedRegionTypes properties as it may break original document layout. We can choose second way to fix previous problem We will exclude Picture block type from allowed regions allowedRegionTypes=BT Table,BT Text,BT Barcode,BT Separator,BT SeparatorGroup,BT Checkmark,BT _CheckmarkGroup HTML XML Example 2 Step 1 (default properties) file = Output results: Facility Namer Nortfawoods Home Health and Hospice Cit> : Lancaster State r NH HTML Step 2 (default properties image preprocessing) We will improve vertical and horizontal density of the image (dpi) ImageMagic convertion tool command: convert colorspace rgb density 250 example2.png example2 250.tif file = example2 250.tif Output results: Facility Name: Northwoods Home Health and Hospice City: Lancaster State :NH Example 3 Step 1 (default properties) file = Output results: Step 2 (default properties customRegions) Single table was devided on 2 tables. To fix this issue we can try to use customRegions = { \"type\": \"BT _Table \" , \"left\":0, \"top\":0, \"right\":10000, \"bottom\":10000} select full page like a table On this page: What are quality requirements of an input document Why PDF searchable document is preferable for recognition than high dpi image created from it Why we should cache OCR results How to improve OCR results Example 1 Step 1 (default properties)) Step 2.A (default removeGarbageSize properties)) Step 2.B (default allowedRegionTypes properties)) Example 2 Step 1 (default properties).1) Step 2 (default properties image preprocessing)) Example 3 Step 1 (default properties).2) Step 2 (default properties customRegions)) See Also "},{"version":"10.0","date":"Aug-15-2019","title":"ocr-api-reference","name":"OCR API","fullPath":"iac/core/ocr/ocr-api-reference","content":" General information Primary API's: api v1 cloud submitImage api v1 cloud processDocument api v1 cloud getTaskStatus ABBYY's User guide attached OCR API ABBYY's User guide with short examples: OCR API or OCR API Authentication As some other parts of WorkFusion, OCR uses JWT for authentication. To access most of OCR resources you need to retrieve JWT token. For that you can access any Workfusion instance, provide a users login password and retrieve short living JWT token: curl X POST H Content Type:application json d '{\"username\":\"'\",\"password\":\"\"}' http: workfusion api v1 jwt login That token you can use to access any OCR resource, e.g.: curl form \"file=@FILENAME\" H \"Authorization:Bearer \" \"http: api v1 cloud processImage\" Using command line Process image specify appropriate location of file (FILENAME) and OCR parameters to processImage API call curl s form \"file=@FILENAME\" \"http: ocr.hostname:8080 api v1 cloud processImage correctSkew=true&xml:writeRecognitionVariants=false&profile=documentConversion&exportFormat=txt&language=English&correctOrientation=true\" Output copy task id for next command Get status by task id: curl s \"http: ocr.hostname:8080 api v1 cloud getTaskStatus taskId=5693d8f77b78005fcbbfbe84\" Output if message=\"OK\" you can download results from links in resultUrl,resultUrl2,resultUrl3 (depends on export format) API Usage We implement API to be compatible with ABBYY Cloud OCR API sequence call Upload file with submitImage Copy taskId from response: Start image processing with processDocument Check task status with getTaskStatus (specify correct taskId) If task is complete (status=\"Completed\") open results from resultUrl, resultUrl2 or resultUrl3 fields (replace &amp; with &) Custom request parameters We implement custom additional request parameters: GET processDocument OCR API export format (supports html) html html page pdfSearchable text can be searched in such file xml file contains characters words along with their location in the original document (coordinates frames) xmlForCorrectedImage the same as xml, except location is taken from a processed adjusted document txt – plain text the default format For multiple export formats you can use combinations delimited by the comma: pdfSearchable,xmlForCorrectedImage,html Attention You can use maximum 3 types at once. customRegions (json variable) { \"type\":\"BT_Table\", \"page\":1, \"left\":0, \"top\":0, \"right\":2000, \"bottom\":2000 }, { \"type\":\"BT_Table\", \"page\":1, \"left\":0, \"top\":2000, \"right\":4000, \"bottom\":4000 }, { \"type\":\"BT_Table\", \"page\":2, \"left\":0, \"top\":0, \"right\":4000, \"bottom\":4000 } Example to select one table region for all pages { \"type\":\"BT_Table\", \"left\":0, \"top\":0, \"right\":2000, \"bottom\":2000 } correctSkew (default true) if true, the page skew will be detected and automatically corrected correctOrientation (default true) if true, the page orientation will be detected and if it differs from normal, will be automatically rotated skipPreprocessing (default false) skips preprocessing stage. It will increase performance up to 30% useOnlyCustomRegions (default false) skips original analyzing stage extract information from custom regions only alphabetExtension string with specials symbols to extend already defined alphabet useDefaultPattern (default false) if true, it requires to apply the default pattern (from the OCR application bundle) removeNoiseModels removes noise on the image (optinal, valid value comma separated values: CorrelatedNoise, WhiteNoise). Important! This method can be used for color and 8 bit gray images only. removeGarbageSize removes garbage (excess dots that are smaller than a certain size) from the image (optinal, valid value > 0 and 1 for automatically detect garbage size) allowedRegionTypes allowed region types for identified blocks classification. If allowedRegionTypes=Empty, all types will be processed. For example, to suppress classifying any block as picture (BT RasterPicture) specify the parameter value as BT Table,BT Text,BT Barcode,BT VectorPicture, BT Separator,BT SeparatorGroup,BT Checkmark,BT _CheckmarkGroup.NOTE narrowing down type of regions can break page layout. Do not use it if you're not sure you need it. discardColorImage If you work with black and white images or the color of images is not important, set the discardColorImage to true. enhanceLocalContrast Specifies whether the local contrast of the image should be increased. Such preprocessing may increase the quality of recognition. It is effective for: photos or scans of documents with texture or pictures in the background. photos or scans of documents with highly colorful background or text highlighting. language Specify predefined language ex. English You can also define multiple languages and use comma as a separator: English,German,Polish lowResolutionMode: true,false improves recognition of images with low resolution, e.g. faxes. Pre processing parameters(available from 9.1): invertImage(default false) inverts Image discardColorImage(default false) leaves only black and white plane in the prepared image removeColorObjects removes color objects from the image, colors values: Blue, Green, Red, Yellow removeColorObjectsType(default Background) removes color objects from the image, modes values: Background, Full, Stamp convertTo(value=tiff) auto detection of the file type and conversion to TIFF(convert before processing). Accept images: PDF, PNG, JPG, JPEG pages selection of pages from PDF files for recognition(example:pages=1,2,3,10 15) changeDPI contains the new value of dpi, changeDPI available values from 50 to 3200 (example:changeDPI=300) useAutoDetectedDPIFromRange({\"min\": 50,\"max\": 3200}) DPI detection from defined range to detect best resolution and apply it before further processing.Note: changeDPI is not applicable when useAutoDetectedDPIFromRange is defined.When engine cannot define best DPI then original DPI is used. priority: 0 10 defines priority in the image processing queue POST processImage file file in format PDF, PNG, JPG, JPEG OCR API export format: html html page pdfSearchable text can be searched in such file xml file contains characters words along with their location in the original document (coordinates frames) xmlForCorrectedImage the same as xml, except location is taken from a processed adjusted document txt – plain text the default format For multiple export formats you can use combinations delimited by the comma: pdfSearchable,xmlForCorrectedImage,html Attention You can use maximum 3 types at once. xml:writeRecognitionVariants makes xml xmlForCorrectedImage formats to contain all variants of character word OCR considered as a possible recognition customRegions (json variable) correctSkew (default true) if true, the page skew will be detected and automatically corrected correctOrientation (default true) if true, the page orientation will be detected and if it differs from normal, will be automatically rotated skipPreprocessing (default false) useOnlyCustomRegions (default false) skips original analyzing stage extract information from custom regions only alphabetExtension string with specials symbols to extend already defined alphabet pattern file upload for a pattern to be applied to recognize special symbols. Note: 1 . you should also add the symbol to alphabetExtension 2 . you can use either the explicit pattern uploaded with the field or useDefaultPattern=true. Both can not be used at a time. useDefaultPattern (default false) if true, it requires to apply the default pattern (from the OCR application bundle) dictionary file where each line contains a word or combination of characters which can be used to improve OCR recognition. The set of words extends, not limits, the default dictionary. removeNoiseModels removes noise on the image (optinal, valid value comma separated values: CorrelatedNoise, WhiteNoise). Important! This method can be used for color and 8 bit gray images only. removeGarbageSize removes garbage (excess dots that are smaller than a certain size) from the image (optinal, valid value > 0) allowedRegionTypes allowed region types for identified blocks classification. If allowedRegionTypes=Empty, all types will be processed. For example, to suppress classifying any block as picture (BT RasterPicture) specify the parameter value as BT Table,BT Text,BT Barcode,BT VectorPicture, BT Separator,BT SeparatorGroup,BT Checkmark,BT CheckmarkGroup. To narrow down type of regions even more you can use the following set: BT Table,BT Text,BT Separator,BT _SeparatorGroup.NOTE narrowing down type of regions can break page layout. Do not use it if you're not sure you need it. enhanceLocalContrast Specifies whether the local contrast of the image should be increased. Such preprocessing may increase the quality of recognition. It is effective for: photos or scans of documents with texture or pictures in the background. photos or scans of documents with highly colorful background or text highlighting. language Specify predefined language ex. English lowResolutionMode: true,false improves recognition of images with low resolution, e.g. faxes. Pre processing parameters(available from 9.1): invertImage(default false) inverts Image discardColorImage(default false) leaves only black and white plane in the prepared image removeColorObjects removes color objects from the image, colors values: Blue, Green, Red, Yellow removeColorObjectsType(default Background) removes color objects from the image, modes values: Background, Full, Stamp convertTo(value=tiff) auto detection of the file type and conversion to TIFF(convert before processing). Accept images: PDF, PNG, JPG, JPEG pages selection of pages from PDF files for recognition(example:pages=1,2,3,10 15) changeDPI contains the new value of dpi, changeDPI available values from 50 to 3200 (example:changeDPI=300) useAutoDetectedDPIFromRange({\"min\": 50,\"max\": 3200}) DPI detection from defined range to detect best resolution and apply it before further processing.Note: changeDPI is not applicable when useAutoDetectedDPIFromRange is defined.When engine cannot define best DPI then original DPI is used. priority: 0 10 defines priority in the image processing queue Custom API's GET summary return the total number of tasks with status QUEUED. Could produce json and xml GET cancelTasks changed status for all NEW, QUEUED and INPROGRESS tasks to CANCELLED. response example: Recommendation: will be good if we add the new parameter that will be define, what tasks we need to cancel. For example date of upload or task id. POST trainPattern trains user pattern for the symbol. file image file with the symbol baseLine contains the distance from the base line to the top edge of the cropped image of the character. The base line is the line on which the characters are located. The top edge of the image is determined by the character orientation. H1 on the picture smallSymbolHeight specifies the height of small characters in pixels on the source image. H2 on the picture symbol the symbol that is associated with picture(s) mergePattern – a pattern is uploaded as a file. If provided, the training output pattern will be combined with the uploaded. request example: Response is a downloadable pattern file in a proprietary binary format. This is different from version 8.0.0, where the pattern was referred by the patternName. Note: when using a custom pattern, you need to specify alphabetExtension with the trained symbol in processImage or processDocument. POST submitPattern The method attaches a pattern file for recognition of special symbols to an (existing) task created with submitImage. To be consequently processed with processDocument action. Parameters: pattern – the pattern file to upload taskId – mandatory GET api project info GET activeLicense Method returns current active license and all available information about this license. Result example: Expand source { \"availableTextTypes\": \"ATT_Normal\", \"ATT_Typewriter\", \"ATT_Matrix\", \"ATT_Index\", \"ATTOCRA\", \"ATTOCRB\", \"ATTMICRE13B\", \"ATTMICRCMC7\", \"ATT_Advanced\" , \"availableBarcodeModules\": \"ABM_1D\", \"ABM_PDF417\", \"ABM_Aztec\", \"ABM_QRCode\", \"ABM_MaxiCode\", \"ABM_DataMatrix\", \"ABM_Autolocation\" , \"availableEngineModules\": \"AEM_ProcessAsPlainText\", \"AEM_Process\", \"AEM_Analyze\", \"AEM_Recognize\", \"AEM_Synthesize\", \"AEM_ExtendedCharacterInfo\", \"AEM_OpenPDF\", \"AEM_UserPatterns\", \"AEM_BalancedMode\", \"AEM_FastMode\", \"AEM_BCR\", \"AEM_Classification\" , \"availableExportFormats\": \"AEF_RTF\", \"AEF_HTML\", \"AEF_XLS\", \"AEF_PDF\", \"AEF_Text\", \"AEF_PDFImageOnly\", \"AEF_XML\", \"AEF_PPT\", \"AEF_PDFA\", \"AEF_PDFMRC\", \"AEF_ALTO\", \"AEF_EPUB\", \"AEF_FB2\", \"AEF_ODT\", \"AEF_XPS\" , \"availableVisualComponents\": , \"availableLanguageSets\": \"ALS_Standard\", \"ALS_DataCapture\", \"ALS_Artificial\", \"ALS_Programming\", \"ALS_User\", \"ALS_Chinese\", \"ALS_Hebrew\", \"ALS_Thai\", \"ALS_Vietnamese\", \"ALS_Arabic\", \"ALS_Japanese\", \"ALS_Korean\" , \"volumeRefreshingPeriod\": \"VRP_Infinite\", \"volume\": 10000, \"volumeRemaining\": 10000, \"serialNumber\": \"SWAT 1101 1004 1541 1060 5817\", \"allowedCoresCount\": 2, \"minimumCoresCountPerInstance\": 0 } The meaning of the fields in the response: Attribute Explanation availableTextTypes The set of the text types available in the license. availableBarcodeModules The set of the ABBYY FineReader Engine barcode modules available in the license. availableEngineModules The set of the ABBYY FineReader Engine modules available in the license. availableExportFormats The set of the export formats available in the license. availableVisualComponents The set of visual components available in the license. availableLanguageSets The set of the language sets available in the license. volumeRefreshingPeriod Information about the limitation period if the license limits the number of processed pages characters during this period. volume The total number of pages characters which can be processed during a period if the license has such a limitation. volumeRemaining The remaining number of pages characters which can be processed till the end of the current period if the license has such a limitation. When this property value reaches 0, analysis, recognition and export operations will not be possible. serialNumber The serial number of the license. allowedCoresCount The number of CPU cores that can be used simultaneously. If the value of this property is 0, the number of CPU cores is unlimited. minimumCoresCountPerInstance The minimum number of CPU cores which is allocated by ABBYY FineReader Engine at initialization. GET listLicenses Method returns list of all licenses connect to current ABBY Engine. Expand source { \"availableTextTypes\": \"ATT_Normal\", \"ATT_Typewriter\", \"ATT_Matrix\", \"ATT_Index\", \"ATTOCRA\", \"ATTOCRB\", \"ATTMICRE13B\", \"ATTMICRCMC7\", \"ATT_Advanced\" , \"availableBarcodeModules\": \"ABM_1D\", \"ABM_PDF417\", \"ABM_Aztec\", \"ABM_QRCode\", \"ABM_MaxiCode\", \"ABM_DataMatrix\", \"ABM_Autolocation\" , \"availableEngineModules\": \"AEM_ProcessAsPlainText\", \"AEM_Process\", \"AEM_Analyze\", \"AEM_Recognize\", \"AEM_Synthesize\", \"AEM_ExtendedCharacterInfo\", \"AEM_OpenPDF\", \"AEM_UserPatterns\", \"AEM_BalancedMode\", \"AEM_FastMode\", \"AEM_BCR\", \"AEM_Classification\" , \"availableExportFormats\": \"AEF_RTF\", \"AEF_HTML\", \"AEF_XLS\", \"AEF_PDF\", \"AEF_Text\", \"AEF_PDFImageOnly\", \"AEF_XML\", \"AEF_PPT\", \"AEF_PDFA\", \"AEF_PDFMRC\", \"AEF_ALTO\", \"AEF_EPUB\", \"AEF_FB2\", \"AEF_ODT\", \"AEF_XPS\" , \"availableVisualComponents\": , \"availableLanguageSets\": \"ALS_Standard\", \"ALS_DataCapture\", \"ALS_Artificial\", \"ALS_Programming\", \"ALS_User\", \"ALS_Chinese\", \"ALS_Hebrew\", \"ALS_Thai\", \"ALS_Vietnamese\", \"ALS_Arabic\", \"ALS_Japanese\", \"ALS_Korean\" , \"volumeRefreshingPeriod\": \"VRP_Infinite\", \"volume\": 10000, \"volumeRemaining\": 10000, \"serialNumber\": \"SWAT 1101 1004 1541 1060 5817\", \"allowedCoresCount\": 2, \"minimumCoresCountPerInstance\": 0 } GET api v1 metrics count Request example: Parameters: a ) status. Either one of TaskStatus values or aggregate DONE, PROCESSING, ALL b ) period. Number of minutes to substract from current time. By default the range is NOW MINUTES to NOW, when period is negative the range is BEGINNING to NOW MINUTES. Default is 60 minutes GET api v1 metrics stats Request example: Parameters: a ) period. Number of minutes to substract from current time. By default the range is NOW MINUTES to NOW. Default is 60 minutes b ) minProcessingTime. Tasks with processing time less than this value will be discurded. Default is 1000 (1 second) c ) stat. Descriptive statistic mean, max, n (number of values), std, percentileN (where N is any number 0 100) or all Multi worker processing When process starting to work with specific task it changes task status, so other processes can't also start working with that task thus one task can be processed only by one process worker. When server during task processing is terminated we not able to send that task to the queue immediately, but we have service that runs on a schedule and puts such tasks to the queue. Server specification and usage guidance Currently its low parameters instance don't submit many documents. License allows only 10K pages to be recognized License Update with latest info Practical Usage Tips In this paragraph described tips that was born during OCR api usage Recognition quality use custom dictionary fetch plain text from pdf and set it to originalText parameter don't use removeGarbageSize parameter for PDF searchable and carefully use it for scanned PDF or image. Sometime it can degrade results. use allowedRegionTypes=BT Table,BT Text,BT Barcode,BT VectorPicture, BT Separator,BT SeparatorGroup,BT Checkmark,BT CheckmarkGroup,BT _AutoAnalysis for aggressive recognition image blocks in documents train and use pattern for some repeatable unrecognized cases. By default used costa rica currency pattern PDF searchable document is preferable to recognition than high dpi image created from it Recommended resolution for source image: 300 dpi for typical texts (10pt or larger) and 400 600 dpi for texts in smaller fonts (9pt or smaller) Performance Improvements use skipPreprocessing=true at least for PDF searchable documents. It saves up to 30% processing time use xml:writeRecognitionVariants=false if you don't need char word variants in the output xml file. It saves up to 30% processing time, memory and disk space "},{"version":"10.0","date":"Aug-06-2019","title":"ocr-health-check","name":"OCR Health Check","fullPath":"iac/core/ocr/ocr-health-check","content":" Overview How It Works Configuration Generic OCR Health Check Configurations Configuring Storage Health Check Configuring Message Queue Health Check Configuring Worker Health Check Configuring ABBYY License Health Check Overview OCR health check functionality allows monitoring of OCR Service state dynamically with certain intervals. You can retrieve the results in the JSON format directly at path api v1 health check of OCR Service API. How It Works OCR health check tests the following components, dependencies, or connectivity to them: Microsoft SQL Server, RabbitMQ, S3, ocr worker, ocr task, ABBYY License. OCR health check tries interacting with the components in a way close to that OCR Service would do in its normal operation. When you request api v1 health check endpoint, it checks the components, collects and reports errors, warnings, and stats. Errors and warnings absence indicates fact that OCR Service functions properly. Further details for specific components are provided in the corresponding subsections of the Configuration section. Configuration Generic OCR Health Check Configurations OCR health check executes checks of individual components concurrently. ocr.health.timeout specifies in seconds for how long OCR health check will wait for checks of individual components. Effectively, this configuration property specifies how fast health check will respond. This also means that reports from all individual component checks will not be completed before the timeout is represented in the report only partially. ocr.health.executor.pool.size defines how many component checks can be executed concurrently. ocr.health.executor.queue.capacity defines how many component checks can be queued. These are advanced options. Generally, you will not need the former to be higher. Health check as of version 8.5 starts only five component checks. Default configuration values for application.properties of ocr rest ocr.health.timeout: 29 ocr.health.executor.pool.size: 10 ocr.health.executor.queue.capacity: 0 Configuring Storage Health Check S3 bucket is configured by ocr.bucket . It should have consistent values in ocr rest and ocr worker. ocr.bucket: doc upload Configuring Message Queue Health Check When a message queue is tested, ocr rest sends a request message to a request queue, while ocr worker receives it, then sends a respond message to a response queue, and finally ocr rest receives it. ocr rest additionally does some validation and retries, if needed . ocr.health.messaging.receive.timeout specifies in seconds for how long ocr rest will wait for a response message from ocr worker. ocr.health.messaging.receive.retryCount specifies how many times ocr rest will retry attempts to read a response message if a previous attempt was unsuccessful. ocr rest will use exponential backoff for wait time before a consequent attempt to retry a receive operation. ocr.health.messaging.receive.maximumRetryInterval defines the maximum time. In case of some special case when you need to redefine RabbitMQ queue and exchange names used for health check, you can use the corresponding properties mentioned below. Make sure that the configurations are equal for ocr rest.yml and ocr worker.yml in application.properties. If rabbitmqQueue profile is active, ocr.health.messaging.requestQueue, ocr.health.messaging.requestExchange, ocr.health.messaging.requestRoutingKey, ocr.health.messaging.responseQueue, ocr.health.messaging.responseExchange and ocr.health.messaging.responseRoutingKey will be applied for the rabbitmqQueue profile. Default configuration values for application.properties of ocr rest ocr.health.messaging.receive.timeout: 1 ocr.health.messaging.receive.retryCount: 5 ocr.health.messaging.receive.maximumRetryInterval: 4 Default configuration values for application.properties of ocr worker ocr.health.messaging.executor.pool.size: 1 ocr.health.messaging.executor.queue.capacity: 0 Default configuration values for application.properties of ocr rest and ocr worker Health Check : message queue with rabbitmqQueue ocr.health.messaging.requestQueue: healthrequestqueue ocr.health.messaging.requestExchange: healthrequestexchange ocr.health.messaging.requestRoutingKey: healthrequestrouting_key ocr.health.messaging.responseQueue: healthresponsequeue ocr.health.messaging.responseExchange: healthresponseexchange ocr.health.messaging.responseRoutingKey: healthresponserouting_key Configuring Worker Health Check OCR workers periodically evaluate their own health statuses and save the results to Microsoft SQL Server. When api v1 health check request is sent to ocr rest by a client, ocr rest collects the worker health checks from the database and provides a summary computed from those health checks. ocr.health.worker.cron property has a cron like format and specifies schedule of starting worker health checks. It has space separated fields that define repetition pattern for seconds, minutes, hours, days of month, months, days of week, respectively. The default schedule written below specifies to execute worker health check every 10 minutes: \"0 0 10 * * * *\". An extra health check is done right after ocr worker start. You can disable that by specifying ocr.health.worker.runOnStart=false . If you have multiple ocr worker instances which may have the same hostname, configure ocr.health.worker.name in ocr.yml (or command line) for each ocr worker instance to have a unique name among your ocr worker instances. During worker health check, an ocr task is executed (the process that does the actual text recognition). In order not to waste license volume, actual recognition is not performed during a health check. However, ABBYY FREngine can be loaded during task health check. This will check that ABBYY FREngine is installed properly and its license is available. Specify ocr.health.task.loadEngine=true to force loading of ABBYY FREngine during task health check. ocr.health.task.timeout specifies timeout in seconds for ocr task process during task health check. You can configure worker health check to report warnings and errors depending on the expected number of healthy workers. If the number of workers that update their report timely (less then ocr.health.worker.errorThreshold.reportTtl seconds ago) is less than ocr.health.worker.errorThreshold.minHealthyCount, an error will be reported. ocr.health.worker.warningThreshold.reportTtl and ocr.health.worker.warningThreshold.minHealthyCount properties have the same effect, with a warning being reported. If an ocr worker is scaled down, you can tune ocr.health.worker.cleanup.reportTtl property. Worker health reports older than the value will be completely ignored in the health check report. Default configuration values for application.properties of ocr rest ocr.health.worker.warningThreshold.reportTtl: 900 ocr.health.worker.warningThreshold.minHealthyCount: 1 ocr.health.worker.errorThreshold.reportTtl: 1800 ocr.health.worker.errorThreshold.minHealthyCount: 1 ocr.health.worker.cleanup.reportTtl: 3600 Default configuration values for application.properties of ocr worker ocr.health.worker.name: default_worker ocr.health.worker.cron: 0 0 10 * * * * ocr.health.worker.runOnStart: true ocr.health.task.loadEngine: false ocr.health.task.timeout: 60 ocr.health.task.debug: false FREngine configuration is propagated automatically from ocr rest to ocr worker through Microsoft SQL Server. If ocr worker starts earlier during the first launch, it may test itself before getting the configs from ocr rest and report an error accordingly. The error will disappear on the next check, once ocr worker gets the configs. Configuring ABBYY License Health Check During license health check, ABBYY license is loaded and the remaining volume (number of pages to be recognized included into the license) is checked. If remaining volume of pages is less or equal to zero, a warning will be reported. Your OCR license has expired. You have to purchase a new license in order to continue using OCR. If the number of remaining pages is less than ocr.health.license.warningThreshold.minVolumeRemaining, a warning will be reported. Remaining number of pages to be processed under the current OCR license is below warning threshold: 500. Contact WorkFusion support team to obtain a new license. Additionally, if the percentage of remaining pages is less than ocr.health.license.warningThreshold.minPercentageOfVolume, a warning will be reported as well. Your OCR license is about to expire. Less than 10% of pages left. You will need to purchase a new license in \" \"order to continue using OCR. Default configuration values for application.properties of ocr rest ocr.health.license.warningThreshold.minVolumeRemaining: 500 ocr.health.license.warningThreshold.minPercentageOfVolume=10 "},{"version":"10.0","date":"Aug-06-2019","title":"ocr-input-data-quality-assessment-using-rpax","name":"OCR Input Data Quality Assessment using RPAx","fullPath":"iac/core/ocr/ocr-input-data-quality-assessment-using-rpax","content":" Download Standard OCR Business Process. Import OCR BP. Provide Global Variables: OCR url, username, password. Provide OCR Settings. Select Standard for OCR Type. Specify pdfSearchable as Export Format. Provide original document url as BP input data. Run BP. Check the result manually. Select the Prepare URL step. Find ocr document url. Open ocr document url and review OCRed PDF document for correctness. "},{"version":"10.0","date":"Aug-06-2019","title":"ocr-tuning","name":"OCR Tuning","fullPath":"iac/core/ocr/ocr-tuning","content":" OCR Process Overview Standard Workfusion OCR use case provides set of Bot Configurations that may be re used or extended to address custom OCR challenges. OCR Architecture UML Diagram Usage Example OCR Metrics and Licensing Workfusion OCR capabilities are limited by the corresponding license. Every OCR license can be checked via Platform Monitor, or directly via URL (for RPA Express 2.0, license check url is ). Upon OCR license expiration Workfusion provides license upgrades. Every OCR document processing request provides short metrics snippet to estimate the OCR load and predict the completion of the task. OCR Client Features Comparison Table OCR Plugin OCR REST Execution Synchronous Asynchronous Input Format PDF, TIFF, JPEG, PNG PDF, TIFF, JPEG, PNG OCR Result Format txt,xml,html,pdf txt,xml,html,pdf Usage low age volume, good document quality, no sla, OCR test Enterprise grade BP Common OCR Parameters Parameter Recommended Value Notes Export Format xmlForCorrectedImage Document Language Language parameter must be supported by the license Character Recognition Variance false Allowed Region Type BTTable,BTText,BTSeparator,BTSeparatorGroup,BT_AutoAnalysis Custom Region * Change DPI Correct Orientation true Correct Skew true * Invert Image * Discard Color Image * Remove Color Objects * Remove Color Object Types Warning! Bar Code recognition is not working with TOOD OCR, page is empty with Bar Code region specified. Required to use Standard OCR to get Barcodes working. OCR Optimization Instructions Validate source image DPI. Recommended DPI for all source documents is 300 350. When resolution is 25% lower than recommended by Workfusion, it is suggested to add image pre processing parameter Change DPI or use ImageMagick to bump up the DPI to suggested level. When DPI is significantly lower, for example is within 72 110 DPI, increasing the DPI increases OCR recognition errors. It is suggested to use color documents within OCR process. A color image, since it has different colors presence, adds ability to remove RGB color channels prior to the processing, decreasing the document noise and providing a way to stabilize the image prior to applying gray scale conversion. When looking at poor quality documents the only three pre processing techniques proven to work within Workfusion OCR Rest: Adjusting contrast Adjusting brightness Inverting the image The OCR recognition result varies upon applying Allowed Region Type parameter values in a certain order. When setting array of two elements BT Table, BT Text as the value, the OCR result XML will have more OCR blocks as tables, than when the array has elements in reverse order BT Text, BT Table. See also Improve OCR Results for more improvements "},{"version":"10.0","date":"Aug-06-2019","title":"optical-character-recognition","name":"Optical Character Recognition (OCR)","fullPath":"iac/core/ocr/optical-character-recognition","content":" title: Optical Character Recognition (OCR) TOD Tagging Over Document OCR API Improve OCR Results Detect language using text or HTML OCR Health Check Windows: CheckMarks Recognition Barcodes recognition OCR Input Data Quality Assessment using RPAx Comparison of OCR Platforms "},{"version":"10.0","date":"Aug-08-2019","title":"tagging-over-document","name":"TOD - Tagging Over Document","fullPath":"iac/core/ocr/tagging-over-document","content":" Overview Tagging over Document (TOD) business process (BP) allows users to label the original document, so that they do not face the OCR results. Before: User tagged the result of OCR, which could look very differently from the original document, and struggled to find a proper value. After: User labels the original, taking no time to recognize the familiar document, its structure, the position, font and font size of the required value. Result: For classification models, the labeling speed increase is at least 50%, for extraction models at least 30%. Labeling speed increase, combined with model training time reduction, decreases the time required to deliver a use case to customer. Demo TOD offers several workflows: Tag: select and label values, chunks of sentences, or entire table rows and columns. Search: find other occurrences of a certain value across the document. Evaluate: check the tagged values. Click on the extracted value to jump to it even if the document has many pages. Your browser does not support the HTML5 video element Features The OOTB TOD Use Case is intended for the easy setup of the TOD BP. TOD OOTB Use Case uses preprocessed OCR xml output to: split text into words calculate absolute coordinates of each symbol group them into words, lines, columns generate HTML in which a tag over the original document corresponds to an OCRed text underneath According to the execution engine architecture, there are no limitations inside BP for the number of pages records or the number of documents on any step. Note that each step must complete processing of all records before moving to the next step. Limitations In SPA Version 10.0, OOTB TOD BP has the following limitations: Does not include a manual task and ETL step. Does not support predefined autoselect OCR parameters. Only supports OCR with basic or no authentication, no OCR JWT authentication support. Overview Demo Features Limitations Environment configuration OCR REST API Data Stores OCR cache data store Global variables Business Process BP Structure OCR Settings OCR Workflow Manual Task Configuration Add TOD Answer Input Output Certificates Environment configuration AutoML10.0 or later. ImageMagick (windows: v7.0.4 ) and GhostScript (windows: v9.20) installed to perform multi page document split (in case when TOOD OCR type is used). Global variables configured according to Globalvariables section. OCR REST API The new OCR BP implemented in AutoML10.0 extends the OCR API with retrieving a preprocessed document in order to use it for labeling in IE manual task. This wasn't a stumbling stone before the users tagged an OCRed document. While, once we are about to let user labeling the original document, there is a problem with the coordinates of recognized symbols. The reason is that OCR performs internal preprocessing of a document regardless of the parameters which were fed such as rotating, improving the quality, etc. The coordinates of the recognized symbols relate to the internally preprocessed document, and mismatch to the very original document. Hence, the user does not actually label the very original document, but the differences are such tiny, that he she does not notice it. OCR preprocessed document can be retrieved using following methods: POST processDocument or submitImage with parameter storePreprocessedDoc=true curl Response GET download preprocessed with the following parameters: taskId OCR task id (returned from processDocument or submitImage) page number of document page curl Data Stores OCR cache data store Cache storage is optional for TOD but highly recommended, as it allows to decrease requests to the OCR cluster. Create Data store for OCR cache in Control Tower. The Data store with the name you specified will be created automatically if it doesn't exist. The most important Data store parameters are: Parameter name Description Mandatory Example ocrjson Cached response from OCR yes { &quot;originaldocumenturl&quot;: &quot;https: services rnd.s3.amazonaws.com testing%20OCR newfiels timo0002125.tif&quot;, &quot;ocrexportformat&quot;: &quot;xmlForCorrectedImage&quot;, &quot;originaldocumentname&quot;: &quot;timo0002125&quot;, &quot;ocrdocumentname&quot;: &quot;timo0002125 xmlForCorrectedImage&quot;, &quot;ocrdocumentformat&quot;: &quot;xml&quot;, &quot;ocrdocumenturl&quot;: &quot;https: vision automl db1.workfusion.com:8443 doc upload timo0002125 xmlForCorrectedImage.xml&quot; } key Document hash yes 208682471691263be19a1fc0909d3efa ocrtype OCR flow used in BP run yes tod exportformat List of formats used in BP yes xmlForCorrectedImage, xmlWithoutRecognitionVariants metainfojson Contains all required data for TOD manual task, including chars positions, preprocessed document URL etc. yes { \"pages\": \"https: vision s3.workfusion.com:9000 doc upload tod images preprocessed links 4e5d19df b421 4638 8eee 0b723df92438.json\", \"ocrXmlUrl\": \"https: vision s3.workfusion.com:9000 doc upload 6b674b22 6825 4383 8e37 adea5e6bf54d 39203488 1102 4a5f acc0 de0d347e9716 1916260 xmlWithoutRecognitionVariants.xml\", \"imageUrl\": \"https: bep vision db1.workfusion.com doc upload 1916260.pdf\" } Global variables Global Variables are used for the global configuration of TOD. The following parameters should be specified: Parameter name Description Mandatory Example convertlibpath Specify path to ImageMagick library if it was not added to PATH no usr bin convert ocrurl URL to OCR service with context path yes https: velcom ocr1.workfusion.com api v1 cloud ocrusername OCR username no &lt;someusername&gt; ocrpassword OCR password no Business Process BP Structure The business process must include the following steps: OCR Settings OCR Workflow TOD Manual Task OCR Settings If the ETL settings step is inserted by drag and drop from the bot library to the right of the workflow field, the settings will be rewritten for all business processes which use this step. To avoid this, drag an empty bot task from the toolbar above the workflow field and define it using the OCR Settings use case as described below. OCR settings step should be inserted into the BP before OOTB TOD BP: On the Workflow tab, drag an empty bot step to BP designer. Double click on the bot task and select an ELT settings use case. Specify OCR settings according to the instructions below: Setting Description Required Example Options S3 bucket S3 bucket for TOD OCR results yes doc upload OCR Type OCR flow for business process—specifies the OCR BP use mode yes Enriched output (default) Standard Export Format Specifies the storage format for the OCR result yes html pdfSearchable txt xml xmlForCorrectedImage xmlWithoutRecognitionVariants Document Language no English (default) Change DPI DPI value for documents no from 50 to 400 Invert image no true false Discard Color Image no true false Remove Color Objects Remove color objects according to the selected option no Blue Green Red Yellow Remove Color Objects Type Remove color objects type no Background Full Stamp Custom Regions Custom regions for OCR no &lt;some region&gt; If use line height in letter size computation Specifies if we want to use line height in letter size computation no true false Allowed Region Types Allowed region types for OCR no BTAutoanalysis BT Barcode BTCheckmark BTCheckmarkGroup BPRasterPicture BTSeparator BTSeparatorGroup BTTable BTText BTVectorPicture Cache datastore OCR cache datastore no ocrcachedatastore (described in the OCR cache datastore section) When Enriched output OCR Type is selected, Export Format field is hidden and xmlWithoutRecognitionVariants, xmlForCorrectedImage are selected. OCR Workflow To add the OCR Workflow step to your business process, perform the following actions: On the Workflow tab of your BP, select Sub Process on the right tab. Expand OCR section and select OCR Worklow. Drag and drop OCR Workflow step to the left on the BP Structure field. {width=\"600\" height=\"326\"} Manual Task Configuration In order to label documents after recognition, Information Extraction Manual Task should be inserted into the BP after OOTB TOD step. The IE Manual Task is generic and accepts both old and new format. It displays xml or the original document based on the input format. It includes an original document if the input file refers to the corresponding json. Otherwise, the information extraction task works with any type of document as previously. To learn about new input format, see this section. Add TOD Answer Configure TOD answer the following way: Unique Code: documentxmllink(default) or mapped to a column containing xml link to xml from the previous steps. Answer Type: Information Extraction Content Source: Input Data TOD data:metainfojson (default) or mapped to a column containing json from OCR. The following Unique code values are not allowed: See full list of forbidden Unique values Forbidden Unique values block formatting line p par pre rect region td text tr document separator page regionrect textregion lineformatting textpar blocktext ptext pline rectrect end cell row start table th sec document sec header document type sequence description text filename br To learn more, see Create IE Answers. Input Input data must contain originaldocumenturl. Input file content example originaldocumenturl https: vision automl db1.workfusion.com:8443 doc upload d1ceba.tif https: vision automl db1.workfusion.com:8443 doc upload b5ytx.tif https: vision automl db1.workfusion.com:8443 doc upload p4exv.tif https: vision automl db1.workfusion.com:8443 doc upload 6ng9y.tif https: vision automl db1.workfusion.com:8443 doc upload j7ter.tif Output TOD OCR BP produces metainfojson in case Enriched ouput flow is selected in OCR Settings. Below is the example of metainfojson structure: meta info json pages: a link to json document which contains an array of links to characters recognized on specific page of document and preprocessed image url pages charsUrl content chars ocrXmlUrl: OCR result in Export Format format imageUrl: original document Certificates OOTB TOD should work in all environments. In case of untrusted certificates, it's necessary to download SSL db server certificate and add it to trusted list. Otherwise, the document image won't be available in WorkSpace in TOD manual task. "},{"version":"10.0","date":"Aug-06-2019","title":"windows-checkmarks-recognition","name":"Windows: CheckMarks Recognition","fullPath":"iac/core/ocr/windows-checkmarks-recognition","content":" Prerequisites Checkmark Recognition (OMR)) Technical Implementation of OMR Detection of Checkmarks on a page OCR API: Checkmark recognition \"customRegions\" json structure Document template preparation: example Checkmarks in group Single checkmark Example with checkmark in group and single checkmarks Checkmark results of recognition CustomRegions for checkmark recognition via OCR plugin in Bot Task in CT * *Business Process for Checkmarks Recognition Prerequisites Prerequisites OCR Setup on Windows License for checkmarks recognition (Windows OMR) OPEN QUESTIONS: OCR on premise setup* on Windons (not on Linux) to use OMR capabilities* Windows: CheckMarks Recognition Document template preparation Windows Installer exists only as part of RPAx installation, currently there is no ability to use it for on premise installation (out of the box) Notes: Default RPAx trial license has all features enabled.** Checkmark Recognition (OMR) A checkmark field is an element on a machine readable form (usually rectangular in shape and often called a “check box”) in which a mark should be made (a check tick, an X, a large dot, inking over, etc.). OCR is able to read and process checkmarks, the technical term for this is “Optical Mark Recognition” (OMR) The OMR technology recognizes simple checkmarks, grouped checkmarks, OMR delivers a very accuracy rate of up to 99.995 % {.media width=\"150\"} {.media width=\"150\"} Technical Implementation of OMR Layout analysis and the underlying recognition technology works with Checkmark Block (Single Checkmark) and a Checkmark Group (Group of checkmarks or Radio buttons) objects. Checkmark block group corresponds to an image zone recognized as checkmark. {.mediacenter width=\"300\"} The state of a checkmark can be Selected (state=\"CMCS _Checked\") Not selected (state=\"CMCS _NotChecked\") Supported checkmark types: Square (CMT _Square) {.media} Circle (CMT _Circle) NOT SUPPORTED FOR NOW {.media} Empty (CMT _Empty) {.media} Detection of Checkmarks on a page Checkmark areas cannot be detected automatically by the document analyser. “Area” of checkmark should be defined prior the recognition. OCR API: Checkmark recognition Checmarks support Checkmark recognition is based on document template with defined checkmark areas. Document template should be formatted as JSON string and passed as 'customRegion' parameter which describe to the engine where a specific block is located (e.g. checkmark). Custom regions types for OMR support extra configuration: BT _Checkmark BT _CheckmarkGroup EXPORT Only XML export supports checkmarks recognition results. INPUT Image file format (.png, .jpeg, .tiff) is recommended. (warning){.emoticon .emoticon warning} Do not use PDF as input. (warning){.emoticon .emoticon warning} Text inside checkmark area is not recognized. \"customRegions\" json structure Custom region is a JSON variable that defines area of special type. Schema (red required): type one of allowed region type BT Table, BT Text, BT Barcode, BT VectorPicture, BT Checkmark, BT CheckmarkGroup . page number of page of custom region to be placed. Default: 0 left left border of area top top border of area right right border of area bottom bottom border of area name name of region. Default: '' checkmarkConfiguration extra configuration for checkmarks maximumCheckedInGroup maximum count of items allowed for checking minimumCheckedInGroup minimum count of items allowed for checking type type of checkmark CMT Square, CMT Circle(not supported), CMT Empty . Default: CMT Square innerRegions inner custom regions (inside parent). !!! Checkmark group must contain at least 1 inner checkmark definition Example of json structure customRegions Value { \"type\": \"BT_Checkmark\", \"page\": 1, \"left\": 20, \"top\": 350, \"right\": 70, \"bottom\": 395, \"name\": \"Other Conditions and Document to be tendered to prove compliance\", \"checkmarkConfiguration\": {\"type\": \"CMT_Square\"} } Document template preparation: example {width=\"502\" height=\"250\"}{width=\"627\" height=\"250\"} Checkmarks in group first of all need to define image zone recognized as checkmark for group of checkmarks (see the previous picture, you can use simple picture editor(e.g. \"Paint\"), switch on rulers feature for example and try to define points(coordinates) of borders(px)) then define area for every checkmark in block json for group of checkmarks Expand source { \"type\": \"BT_CheckmarkGroup\", \"page\": 1, \"left\": 10, \"top\": 80, \"right\": 950, \"bottom\": 315, \"name\": \"Date Of Application\", \"checkmarkConfiguration\": { \"maximumCheckedInGroup\": 4, \"minimumCheckedInGroup\": 1, \"type\": \"CMT_Square\" }, \"innerRegions\": { \"type\": \"BT_Checkmark\", \"page\": 1, \"left\": 15, \"top\": 80, \"right\": 70, \"bottom\": 140, \"name\": \"Issue by email\" }, { \"type\": \"BT_Checkmark\", \"page\": 1, \"left\": 420, \"top\": 80, \"right\": 470, \"bottom\": 140, \"name\": \"Pre advice\" }, { \"type\": \"BT_Checkmark\", \"page\": 1, \"left\": 15, \"top\": 170, \"right\": 70, \"bottom\": 220, \"name\": \"Issue by teletransmission\" }, { \"type\": \"BT_Checkmark\", \"page\": 1, \"left\": 15, \"top\": 250, \"right\": 70, \"bottom\": 305, \"name\": \"Transferable credit\" } } Single checkmark define area only for single checkmark json for single checkmark Expand source { \"type\": \"BT_Checkmark\", \"page\": 1, \"left\": 20, \"top\": 340, \"right\": 70, \"bottom\": 395, \"name\": \"Other Conditions and Document to be tendered to prove compliance\", \"checkmarkConfiguration\": { \"type\": \"CMT_Square\" } } Example with checkmark in group and single checkmarks Template example for image above Expand source { \"type\": \"BT_CheckmarkGroup\", \"page\": 1, \"left\": 10, \"top\": 80, \"right\": 950, \"bottom\": 315, \"name\": \"Date Of Application\", \"checkmarkConfiguration\": { \"maximumCheckedInGroup\": 4, \"minimumCheckedInGroup\": 1, \"type\": \"CMT_Square\" }, \"innerRegions\": { \"type\": \"BT_Checkmark\", \"page\": 1, \"left\": 15, \"top\": 80, \"right\": 70, \"bottom\": 140, \"name\": \"Issue by email\" }, { \"type\": \"BT_Checkmark\", \"page\": 1, \"left\": 420, \"top\": 80, \"right\": 470, \"bottom\": 140, \"name\": \"Pre advice\" }, { \"type\": \"BT_Checkmark\", \"page\": 1, \"left\": 15, \"top\": 170, \"right\": 70, \"bottom\": 220, \"name\": \"Issue by teletransmission\" }, { \"type\": \"BT_Checkmark\", \"page\": 1, \"left\": 15, \"top\": 250, \"right\": 70, \"bottom\": 305, \"name\": \"Transferable credit\" } }, { \"type\": \"BT_Checkmark\", \"page\": 1, \"left\": 20, \"top\": 340, \"right\": 70, \"bottom\": 395, \"name\": \"Other Conditions and Document to be tendered to prove compliance\", \"checkmarkConfiguration\": { \"type\": \"CMT_Square\" } } Checkmark results of recognition Recognition result Single checkmark Expand source Recognition result Checkmark group Expand source CustomRegions for checkmark recognition via OCR plugin in Bot Task in CT Recognition of checkmarks via OCR plugin in CT's Bot Task > > Results: xml(or other export format) data with checked unchecked state of checkmarks (see \"Checkmark results of recognition\" section) config for checkmark recognition Expand source Custom regions value(json, should be provided for this parameter, e.g. see \"json for single checkmark\" \"Example with checkmark in group and single checkmarks\") and original document _url value were provided from input file or previous machine config data for current example. Or user can provide value directly in config. Best practices: Use processImage in Bot Tasks(the same as in example), instead of processDocument( If user uses processDocument, there is character limits: ~2000 characters will be available). User can use ocr by direct urls, out of plugin, for better performance in production cases. If you use customRegions for checkmarks recognition in table, table can be broken and there is possibility that text in table should be recognized separately. "},{"version":"10.0","date":"Aug-06-2019","title":"train-to-recognize-additional-symbols","name":"Train to Recognize Additional Symbols","fullPath":"iac/core/ocr/train-to-recognize-additional-symbols","content":" User pattern training To train custom symbol pattern you need image of a symbol Important! You should set the smallSymbolHeight and BaseLine properties to correct values, otherwise the trained pattern will not function properly. smallSymbolHeight specifies the height of small characters in pixels on the source image. By default, the value is 0. BaseLine contains the distance from the base line to the top edge of the cropped image of the character. The base line is the line on which the characters are located. The top edge of the image is determined by the character orientation. By default, the value is 0. Example: The currency image. Its BaseLine=31, smallSymbolHeight = 22; Code: private ITrainingImage loadImage(String filename, int baseLine, int smallSymbolHeight){ ITrainingImage image = engine.CreateTrainingImage(); image.setBaseLine(baseLine); image.setsmallSymbolHeight(smallSymbolHeight); IImageDocument imageDoc = engine.OpenImageFile(filename, null, null, null); IImage bwImage = imageDoc.getBlackWhiteImage(); IRegion region = engine.CreateRegion(); region.AddRect(0, 0, bwImage.getWidth(), bwImage.getHeight()); image.SetImageData(imageDoc, region); region.Release(); imageDoc.Release(); return image; } ..... How to test your code You can look at samples in opt ABBYY FREngine11 Samples Java on an OCR server: create your own sample folder with java file export LD LIBRARY PATH= LD LIBRARY PATH: opt ABBYY FREngine11 Bin build.sh run.sh Look at the example ocr ABBYY FREngine11 Samples Java TrainUserPattern2 How to avoid memory leaks ABBYY FineReader Engine provide Java support through JNI (Java Native Interface). JNI allows running native code from Java using special wrapper in C In this case we should care about releasing objects that is created through engine methods. For example: To be sure that all objects are releasing correctly you must enable debug mode engine.StartLogging(\"filename.log\", true); then check log file to be sure that following message does not appear there Not all objects were released before DeinitializeEngine() "},{"version":"10.0","date":"Aug-06-2019","title":"set-up-workfusion-samples-project-from-github","name":"Set up WorkFusion Samples Project from GitHub","fullPath":"iac/core/studio/set-up-workfusion-samples-project-from-github","content":" General Overview Clone and Setup Samples Import with WorkFusion Studio Clone from GitHub General Overview Install WorkFusion Studio and create a new workspace. Then you have two options how to get Samples project from WorkFusion public GitHub repository: 1. import with WorkFusion Studio manual clone with Git Clone and Setup Samples Import with WorkFusion Studio Go to File > Import in the main menu. Select Projects from Git. Select Clone URI. {width=\"500\" height=\"350\"} Paste the link copied from Automation Academy Samples Repo on GitHub: . Select the master branch. Specify the local path. Import as the existing Eclipse project. Press Finish. To check if everything is ok, launch one of the samples as a bot task. {width=\"500\" height=\"527\"} Clone from GitHub Open the console and go to the created workspace. Type Git clone command (you can get links to clone by GitHub web app on our repo page). Launch WorkFusion Studio and open the cloned project. {width=\"400\" height=\"174\"} Select the folder you have cloned. To check if everything is ok, launch one of the samples as a bot task. {width=\"500\" height=\"527\"} How to Contribute More Samples Before contributing, make sure that code you contribute is well formatted. You can see the actual way GitHub requires you to follow in order to propose a pull request. For more information, go here (even more details are here). See an example of making a pull request below. Let's pretend you have found an issue and wish to fix it, e.g., there is a typo in WorkFusion Studio installation. Press Fork. After forking, GitHub creates a new project in your account, and you are redirected to its page. {width=\"800\" height=\"524\"} Configure remote branches in WorkFusion Studio.{width=\"800\" height=\"820\"} Change the origin branch to your fork. To change the origin, paste the link to your fork. Make the needed fixes. Commit the fixes.{width=\"600\" height=\"882\"} Provide a clear and full description of your changes and push your commit(s). Check your fork after the commit has been done.{width=\"800\" height=\"518\"} (Optional) Check the commit list. Open WorkFusion Studio repository and press Pull Request.{width=\"800\" height=\"89\"} Press New pull request.{width=\"800\" height=\"444\"} Click compare across forks.{width=\"800\" height=\"273\"} Select base and head branches.{width=\"800\" height=\"260\"} Press Create pull request.{width=\"800\" height=\"632\"} Provide title and description of the pull request.{width=\"600\" height=\"318\"} A newly created pull request looks like that. If needed, provide some documentation and fixes before your pull request is accepted. {width=\"800\" height=\"673\"} "},{"version":"10.0","date":"Aug-09-2019","title":"tips-tricks-in-workfusion-studio","name":"Tips & Tricks in WorkFusion Studio","fullPath":"iac/core/studio/tips-tricks-in-workfusion-studio","content":" General Recommendations If your configuration does not run or some features do not work, use the checklist below. Make sure you have opened the Code perspective. Save your configuration before running it. Use Content Assistant (Ctrl Space) to view: web harvest plugins context variables (item, sys, etc.) RPA simplified API classes and methods include config names suggestions (WorkFusion Studio scans all configs from the current config folder) The `` tag must have the xmlns=\"\" attribute. If your copied configuration does not have this attribute, add it manually. Make sure your Run Debug Configuration is set for the script you are trying to launch. Periodically update WorkFusion Studio (Help > Check for Updates) and relaunch Eclipse. Some old projects may work unstable after WorkFusion Studio update. Create a new project and write configs there. To enable Expressions view while debugging, go to Window > Show View > Expressions. Content Assistant In WorkFusion Studio, you can adjust Content Assistant to purposes of your development tasks. Go to Windows > Preferences > WorkFusion Studio > Content Assistant to fine tune the function to get the maximum help from it. Content Assistant for bot task scripts is disabled by default to make scripting faster. It is recommended not to enable it in case you use the Code perspective. The following options can be adjusted. Auto activation – define how fast Content Assistant prompts the suggestions (in milliseconds) and keyboard keys to activate it in Groovy. Relevance – rank the constructs according to their importance to your programming preferences. The higher the relevance, the higher the place of a construct in the list. Packages – define packages to be used as a source for Content Assistant and their ranking order. "},{"version":"10.0","date":"Aug-09-2019","title":"troubleshooting-in-workfusion-studio","name":"Troubleshooting in WorkFusion Studio","fullPath":"iac/core/studio/troubleshooting-in-workfusion-studio","content":" Eclipse proxy settings When executing bot task plugins that access external resources, you may get an error message similar to this one: org.webharvest.exception.HttpException: IO error during HTTP execution for URL: https: workfusion.com Proceed as follows. Try to add settings to use proxy in the eclipse.ini file that is located in the eclipse installation folder. Add proxy settings. eclipse.ini proxy settings: Dorg.eclipse.ecf.provider.filetransfer.excludeContributors=org.eclipse.ecf.provider.filetransfer.httpclient Dhttp.proxyPort=8080 Dhttp.proxyHost=XXX Dhttp.proxyUser=XXX Dhttp.proxyPassword=XXX Dhttp.nonProxyHosts=localhost 127.0.0.1 NullPointerException When running a bot task configuration, you may get a NullPointerException in Eclipse. If Eclipse restart does not help, start it in the clean mode in the command prompt. The sample command is follows: D: WorkFusion Studio folder>eclipse.exe clean :::tip For more information, see here. ::: OutOfMemoryError When launching or editing an existing bot task configuration, you may get OutOfMemoryError. !ENTRY org.eclipse.jdt.core 4 4 2016 08 22 10:59:46.260 !MESSAGE Background Indexer Crash Recovery !STACK 0 java.lang.OutOfMemoryError: Java heap space at org.eclipse.jdt.internal.compiler.util.HashtableOfObject.(HashtableOfObject.java:39) at org.eclipse.jdt.internal.compiler.util.HashtableOfObject.rehash(HashtableOfObject.java:165) To increase the maximum heap size, update eclipse.ini that is located in the eclipse installation folder. To do this, open eclipse.ini and add change value of Xmx to Xmx1600m :::tip For more information, see here. ::: Find log files In case of unexpected errors in WorkFusion Studio, you can attach logs to JIRA issues. Find a log following this path: %way to current _workspase% .metadata .log In addition to the log file, specify full OS version (e.g., Microsoft Windows 10 Pro x64) and version of Java installed (for that, execute java version in cmd). To access log, you can also go to Help > About WorkFusion Studio > Installation Details > Configuration > View Error Log. Run Eclipse with custom Java runtime environment In case of several JRE installed on you environment, you can specify which one will be used to run Eclipse with WorkFusion Studio. Go to %eclipse installation folder% eclipse.ini Insert the following text on two separate lines into the beginning of the file (before line with vmargs). vm %path to java 8 runtime% bin javaw In this case, Eclipse will run using the specified JRE. :::note You can also copy the JRE folder directly to the WorkFusion Studio installation directory and rename it to jre. Thus, Eclipse will use it automatically. ::: "},{"version":"10.0","date":"Aug-09-2019","title":"use-rpa-recorder-in-workfusion-studio","name":"Use RPA Recorder in WorkFusion Studio","fullPath":"iac/core/studio/use-rpa-recorder-in-workfusion-studio","content":" If you have installed the WorkFusion Studio only (without RPA Package or Intelligent Automation Cloud Express or Business), RPA Recorder will not be able to execute scripts and perform other actions. There are two options to make RPA Recorder work in SPA WorkFusion Studio. Option 1: Install RPA package and configure Server Profile OCR action will not work without RPA Express installation Control Tower version should be 9.0 or higher Run recordings in WorkFusion Studio Download RPA installer and install it. Update files inside the RPA rpa grid folder. File name Before After node0.json \"port\":5000 \"port\":15410 node0.json \"hubPort\":4444 \"hubPort\":15444 hub.json \"port\": 4444 \"port\": 15444 start hub.bat %javaPath% bin java Dfile.encoding=UTF 8 %logConfig% cp %javaPath% lib tools.jar;;dependency com.workfusion.grid.CustomGridLauncher %role% %config% %javaPath% bin java Drpa.root.folder=\"C: RPAExpress RPA \" Dfile.encoding=UTF 8 %logConfig% cp %javaPath% lib tools.jar;;dependency com.workfusion.grid.CustomGridLauncher %role% %config% start node.bat %javaPath% bin java Dfile.encoding=UTF 8 %logConfig% cp %javaPath% lib tools.jar;;dependency com.workfusion.grid.CustomGridLauncher %role% %config% %javaPath% bin java Drpa.root.folder=\"C: RPAExpress RPA \" Dwebdriver.firefox.bin=\"C: RPAExpress Applications FirefoxPortable FirefoxPortable.exe\" Dwebdriver.chrome.binary=\"C: RPAExpress Applications ChromiumPortable ChromiumPortable.exe\" Dfile.encoding=UTF 8 %logConfig% cp %javaPath% lib tools.jar;;dependency com.workfusion.grid.CustomGridLauncher %role% %config% Publish recordings to Control Tower Open WorkFusion Studio and make the following changes. Go to Window > Preferences > WorkFusion Studio > Server Profiles. Enter the Control Tower parameters: URL in the following format: http: {instamce _url}:{port} workfusion, for example, https: copernicus.workfusion.com:443 workfusion :::note Default port numbers: for http = 8080 for https = 443 ::: Username and Password of your existing user in Control Tower Option 2: Install Intelligent Automation Cloud – Express or Business Edition :::tip Intelligent Automation Cloud Express and Business installation is covered here. Mind the installer size (2 GB) and system requirements. ::: Run Intelligent Automation Cloud – Express or Business for Bot Relay to start automatically on port 15444. Run WorkFusion Studio. Switch to the Record perspective. Use RPA Recorder inside WorkFusion Studio to record, edit, play scripts, or publish to Control Tower. "},{"version":"10.0","date":"Aug-09-2019","title":"workfusion-studio","name":"Use WorkFusion Studio","fullPath":"iac/core/studio/workfusion-studio","content":" :::note WorkFusion Studio is an Eclipse based IDE intended to develop WorkFusion bot task examples. Java 8 is required for WorkFusion Studio to work correctly. ::: :::tip Useful materials: XML processing in Groovy: JSON processing in Groovy: ::: Create new project Within WorkFusion Project Navigator, right click and select New > Project. In the window that opens, select WorkFusion Studio project and click Next. Enter the project name and click Finish. A new folder with a predefined structure is created in Project Navigator. To create a bot task config, right click Configs and select New > Other. Select Bot Task and click Next. Specify the the bot task name and click Finish. A newly created bot task configuration has a predefined structure and the ` and ` plugins. Create Bot Config Bundle project – WorkFusion repository To use this feature, you need to install and setup Java SE Development Kit in Eclipse. The Repository feature allows developers to package RPA tasks in WorkFusion Studio and publish them to a repository in the WorkFusion SPA instance for inclusion in business processes. These artifacts contain everything needed to run an RPA task, including Java and Groovy classes, input data and data stores, and any required settings. For this purpose, WorkFusion Studio provides a new project type – WorkFusion Bot Task Bundle (or Bot Config Bundle BCB). The bot task bundle project offers two starter templates: \"kickstart\" – contains a set of demo files and working config examples, and perfectly fits for understanding and testing the whole Artifactory workflow \"simple\" – contains only required folders and files without working samples WorkFusion project types Bot task bundle \"kickstart\" structure To create a bot task bundle, do as follows. Go to File > New > WorkFusion Bot Task Bundle. In the Create new WorkFusion Bot Task Bundle project wizard, click Configure next to Catalog. In Preferences > Maven > Archetypes, click Add Remote Catalog. Specify the link to your catalog file and its description. Click OK. Select the project type and click Next. \"kickstart\" – generates a bot task bundle with demo files for quick start \"simple\" – generates a bot task bundle with basic structure Specify the project parameters (Group Id, Artifact Id, Version, Package) and click Finish. In a bot task bundle, you can create your custom Java and Groovy classes and import them into bot configs. When Bot configuration has been tested, you need to run the project as a Maven build and set the clean deploy command as a target. Maven creates a Bot Config Bundle (BCB), that is a jar file that contains main and included bot configs together with Java and Groovy, – and uploads this bundle to the Nexus repository. Afterwards, all configs from the uploaded BCB are scanned and uploaded by WorkFusion Studio for you to add them to business processes. Write code configs To write config code, you should switch to the Code perspective. Go to Window > Perspective > Open perspective > Other. In the window that follows, select Code and click Open. To switch to the Code perspective, you can also use the Quick Access option or click the Code perspective button next to it. Open a config file and start coding. The `` tag must have the xmlns=\"http: web harvest.sourceforge.net schema 1.0 config\" attribute. Save your script before running or debugging. Debug Set brake point(s) by double clicking near line number. Click the Debug icon. Select one of recent Debug Configurations: Or click Debug Configurations and create a new one. Run configurations To run a configuration, select it in Project Navigator and click Run. You can use the Run As option, depending on the configuration type. As soon as the execution finishes, see the output folder and Console to view results. Input & output data Put a .csv file with the input data into the input project folder and select it in the Script Input Data field. :::note If the input file contains multiple records, the config will be executed: for a defined range of records (the Start and End Record fields) OR for each record (check the Process the whole file option) ::: By default, script results are recorded to the output project folder. Each config execution creates a new .csv file \"export _ \". You can choose a custom project folder to store the configuration output data. Set up RPA environment Install RPA package as described in the RPA Environment Setup guide. Start your Bot(s) to register to Bot Relay. Go to Run > Run Configurations screen, check the Bot Relay URL option is as follows: http: localhost:15444 wd hub and run your RPA script. S3 access After you enter your S3 keys, you can use S3 Plugins (without hard coding access and secret key attributes, which is a bad practice and is deprecated). View S3 access settings on the Run Configurations screen. OCR In Bot Tasks you can use the OCR plugin to process documents or images. OCR Sample Code To enable processing with OCR, define the needed settings. Go to Run > Run Configurations > OCR. Define the following parameters required to process your documents or images with OCR: OCR Instance – target location with the OCR service installed and running to enable documents images recognition JWT Issuer – username to authenticate to the OCR service JWT Secret – key to authenticate to the OCR service Polling interval (seconds) – time period defining how often to retry the OCR service if it fails Timeout (seconds) – time period defining how long to wait for the OCR service to complete before stopping the execution process. Secrets Vault Using Secrets Vault functionality, you can create secret entries locally to store sensitive data and use it in your bot configs. To manage Secrets Vault, go to Window > Preferences > WorkFusion Studio > Secrets Vault. When a secret entry is added, it becomes accessible in bot configs by a unique Alias. For more details, see `` plugins. Global variables If your config uses global variables to store common values used in different processes and dependent on the SPA environment, you need to use the `` plugin. WorkFusion Studio supports the `` plugin and inserts values of variables defined in Window > Preferences > WorkFusion Studio > Global Variables. Import and export bot tasks Instead of copying and pasting bot config code between WorkFusion Studio and Control Tower, you can use more simple and reliable feature – Import Export. If the bot configuration you are using has included configs, these configs will be transferred together with the main bot config between WorkFusion Studio and Control Tower. The Input data step is downloaded when importing a bot config from Control Tower to WorkFusion Studio. Import bot from Control Tower to WorkFusion Studio (Optional) Run the following REG file to skip the copy paste step by registering the wfstudio protocol. REG file content HKEYCLASSESROOT wfstudio @=\"URL:WorkFusion Studio Protocol\" \"URL Protocol\"=\"\" HKEYCLASSESROOT wfstudio DefaultIcon @=\"eclipse.exe,1\" HKEYCLASSESROOT wfstudio shell HKEYCLASSESROOT wfstudio shell open HKEYCLASSESROOT wfstudio shell open command @=\" \"PATHTOSTUDIO_INSTALLATION eclipse.exe \" \"%1 \"\" Replace PATHTOSTUDIO_INSTALLATION with your path to WorkFusion Studio. Mind to use double back slashes, for example: @=\" \"D: Work WorkFusion SPA Studio 10.0.1 spa studio.exe \" \"%1 \"\" In Control Tower, open any business process in Workflow > Edit Process. Right click a bot step you want to open in WorkFusion Studio. Select the Edit in WF Studio option. The following popup appears. Click Copy to clipboard. Open WorkFusion Studio > File > Import. This step is not required if you have already registered the wfstudio protocol (step 1). Select WorkFusion > Bot Task. Paste the Bot task URL copied on the step 5 and choose an existing Project if needed. Specify your user credentials for the Control Tower you imported the current bot task from. You can save these credentials not to enter them for each import procedure. There can be issues in case you provided wrong credentials or Control Tower outage. As a result, your bot task is displayed in Project Navigator together with the input data (and included configs if they exist). Publish bot task from WorkFusion Studio to Control Tower When the bot config is tested and runs smoothly, you can publish it to Control Tower: Right click the bot config file in Project Navigator or in the code editor itself. Select one of the following options: Publish Bot Task – use to overwrite the step in existing business process Publish Bot Task to new BP – use to create a new business process with one bot step Enter Control Tower Instance. In case you selected Publish Bot Task to new BP, specify Process name for a new business process. Specify the credentials for the Control Tower you are publishing the current bot task to. You can save these credentials not to enter them for each publish procedure by checking save credentials for this instance. As a result, a success message is displayed. :::warning There can be issues in case you provided wrong credentials or Control Tower outage. ::: If you have selected the new process creation option (step 2, option b), this new process is opened in your default browser. Set up import & export Upon import or export of a bot task from to Control Tower, the configuration file in WorkFusion Studio is linked to the bot task on the instance where Control Tower is running. :::note If you rename the configuration file, the link is broken, so you should repeat the Import Export procedure. ::: To view the links of the files to bot tasks in Control Tower, do as follows. Go to Window > Preferences > WorkFusion Studio > Imported Bot Tasks. Here, there is the list of all your configurations linked to bot tasks in Control Tower. Select a bot task and click Remove to break the link between the configuration and the bot task in Control Tower. If you have selected the save user credentials for this instance option during Import Export, your credentials are saved in WorkFusion Studio. To view the saved credentials to all instances, proceed as follows. Go to Window > Preferences > WorkFusion Studio > Saved Credentials. You can add, remove, or edit the credentials. Add – click Add, select the new row appeared in the list, click each item in the row to enter the required data, and press Enter Edit – select a row with the credentials you are going to edit, click the item containing the information to be modified, enter the required data, and click Enter Remove – select a row with the credentials you are going to remove and click Remove Modify log level You can customize the WorkFusion Studio logger which records events produced by WorkFusion Studio during execution of bot tasks. For example, you can change the level of a message to make it less or more critical, or add a new trigger (appender) for a log message. Go to Window > Preferences > WorkFusion Studio > Log Settings. You can add, remove, or edit appenders for log messages. Add – click Add to set up a new appender. Edit – select an appender you are going to edit, click Edit, modify the appender in the dialog appeared and click Enter. Remove – select an appender you are going to delete and click Remove. :::tip For more information on how to configure the log level in WorkFusion Studio, refer to the following web resources: Apache Logging Services Log4j 1.2 Manual IBM Knowledge Center Configuring the log4j.properties file ::: "},{"version":"10.0","date":"Aug-09-2019","title":"workfusion-studio-installation","name":"Install WorkFusion Studio","fullPath":"iac/core/studio/workfusion-studio-installation","content":" :::note Java 8 JDK is required for WorkFusion Studio correct work. ::: Download WorkFusion Studio Download the package ( ~ 500 MB) with WorkFusion Studio: WorkFusion Studio for SPA 10.0 RECOMMENDED Windows (64bit): Windows (32bit): Install WorkFusion Studio Unzip the downloaded package to a folder on your local hard drive. Go to the folder with the extracted content and run workfusion studio.exe. Configure RPA Recorder and Inspector :::note The present section is valid for cases when you need to use RPA Recorder or Inspector and WorkFusion Studio is not a part of Intelligent Automation Cloud Express or Business installation. ::: Go to the WorkFusion Studio installation directory and open the file workfusion studio.ini in a text editor. On the first line, write vm. On the second line, write the path to your JDK installation (usually something like this: C: Program Files Java jdk1.6.0 _31 bin javaw.exe on Windows). vm C: Program Files Java jdk1.8.0_151 bin javaw.exe Link WorkFusion Studio to RPA package Before you start WorkFusion Studio, link it to PRA package. Go to WorkFusion Studio installation directory and open the configuration folder. Open config.ini and add the workfusion.rpa.package.path property that defines the path to the RPA package, e.g.: bash workfusion.rpa.package.path=c: RPA. The default path to the RPA package is C: RPA. If you selected another directory, make sure to specify it correctly. Configure secure connection to Control Tower :::note The present section is valid for cases when WorkFusion Studio is used to modify Bot Tasks in Control Tower. ::: If Control Tower uses the SSL certificates, you need to complete the steps below to configure your WorkFusion Studio to enable connection to Control Tower over HTTPS. Copy the current cacerts file from its default location (for example, C: Program Files Java jdk1.8.0 144 jre lib security cacerts) to your user home directory (for example, C: Users java _security). The certificate should be the same as for Control Tower (ask your system administrator to assist you). Otherwise, you can take it from Firefox browser configuration. Run the following command (the default password for the cacerts file is changeit). keytool import alias icarootcert keystore C: Users javasecurity cacerts file \" CA.cer\" Edit wf studio.ini in the WorkFusion Studio working directory by adding the following line with the path to your cacerts file: Djavax.net.ssl.trustStore=C: Users javasecurity cacerts Restart WorkFusion Studio. "},{"version":"10.0","date":"Aug-19-2019","title":"templates","name":"Templates","fullPath":"iac/core/automation-engineer/templates","content":" Overview Templates provide a way to reuse objects within the WorkFusion platform. Almost every object within the WorkFusion platform, including Tasks and BPs, is based on Templates. To start managing Templates, go to Advanced > Templates, where you can apply the following actions: create edit copy export import delete filter Templates by type Creating Editing a Template While editing or creating a Template, you can access the following tabs: Content You can write plain text, XML, HTML, CSS, JavaScript, and FreeMarker directives in the Template Content box and save the result. You can also modify the Template Type. Template Content Tab History The History tab displays all saved versions of a particular Template along with the author names and modification dates. To view the Template code, click the appropriate link in the Template Name column – the code will be displayed under the grid. Template History Tab To compare two different Template versions, tick the appropriate checkboxes and click the Compare button. These two versions will be displayed under the grid with all their differences highlighted. Template Comparing Versions Usage The Usage tab displays all Use Cases (for Base, Base _Machine) or Tasks (all other) that use this particular Template. Template Usage Tab If a Template is used for a Task or Use Case, a clickable plus ( ) icon is shown under the Template name in the grid. Template Usage Copying Tasks with and without Templates When copying Tasks, you have the option of creating a copy of the existing Template, or creating a link to it that will enable changes to propagate when either Task is modified. When copying Templates, changes made to the new Task will not affect the original Task and its associated Template, while changes made to a Task with a linked Template will. Task Copying See the Copy a Task and Copy a BP sections for detailed info. Template Variables FreeMarker engine variables: To use an object in code, you need put it inside a tag: Worker Name: {worker.firstName} You can also create a hidden answer and pass some info into it to view it on Results > Data. For example, this code snippet submits the Worker ID in a separate column: Passing Worker ID to Task Results (document).ready(function() { (\"input name= {question.identifier!}workerid_skipCheck \").val( (' workerId').val()); }); You can use the following variables in all Templates, except Machine and Base _Machine: Variable Type Description questions list of object Questions list usually used in following form: Variable Usage Example &lt; list questions as question&gt; Question text:&lt;br &gt; Field 1: {question.data.field1}&lt;br &gt; Field 2: {question.data.field2}&lt;br &gt; &lt;@answers question=question &gt; &lt; list&gt; question object class: com.freedomoss.appliance.model.QuestionDTO question.data map Main data. Can be used in form: question.data.column_name question.isGold boolean Is gold question question.qualificationTraining boolean Is training question question.order number Question order question.answers list of objects Question answers submissionUUID string UUID of Task's submission workerId string Worker ID assigned on endpoint (Mturk or Virtualizer) worker object Object with current worker info. Can be empty (null) when worker is previewing the Task and has not accepted it yet. class: com.freedomoss.worker.model.Worker worker.id number Worker ID in WorkFusion database worker.uuid string Worker UUID worker.nativeWorkerId string Same as workerId (see above) worker.firstName string First name worker.lastName string Last name worker.nickname string Nickname worker.creationDate datetime Worker's registration datetime. worker.country.name string Worker's country (can be empty if worker's country cannot be determined) worker.lastActivity datetime Worker's last activity datetime countryCode string Worker's country code (if detected) assignmentId string Assignment ID mturkExternalSubmit string URL to submit Task form (for unsecure submit through http) secureMturkExternalSubmit string URL to submit Task form (for secure submit through https) hitTypeId string Hit type ID (as defined by endpoint) Template Types Macro Macro Templates (MT) are libraries that contain custom tags used to build the Task and Base Templates. They contain necessary imports, scripts, structure, and styling. Macro Templates can have the following extensions: ftl – used in Task Code Editor css – for answer type styling js – answer behavior Updating Macro Template There are 2 ways to update Macro Template: Go to Configuration > System Preferences > Advanced tab and click the Install Macro Templates Version button. Automatic update (since WF version 7.2.11): Automatic update is executed on application startup for each new revision. Manual modification of MT though the WF UI is strictly forbidden. If you need to make changes to Macro Templates, add a feature request or create a bug ticket to WorkFusion dev team. WorkFusion tracks history of Template updates, so if necessary, Template content can be restored. List of Basic Macro Templates Name Description 1 html.ftl Base Template which converts data, scripts and formatting to html 2 answers.ftl Contnains macros for all defined Answer Types 3 extras.ftl Includes gold and training macros 4 answer information extraction.ftl Used to render \"Information Extraction\" Answer Type Base ( Base _Machine) WorkFusion's Use Cases are based on this type of Template. These Templates contain the code for the Task Design including the Instructions, Questions, and Answer choices if any. They specify the format of the input file. Designed into the application by default. When you create a Task using a Base Template, a copy of the Template is created and linked to that Task. Duplication of the Base Template ensures that the original Template will remain consistent and will not be altered unless required. As Tasks are created, a new copy of the Base Template will be associated to the new Task. The following graphic illustrates this inherent design: Base Template Task Every time a new Task is created or copied, a Template for that Task is created. This Template contains the code for the Task Design including the Instructions, Questions, and Answer choices if any. It also handles the passing of information such as the input data for the Task and the answer data entered from workers. FreeMarker code of a Task is available in Task Code Editor. Task Templates have \"include\" links to .ftl files (extras, html, answers) Macro Templates. Machine Bot config XML code written using Web Harvest language. Availability Availability Templates allow you to define an alternative UI for the Task when the Task is not available for execution. An example scenario for implementing this Template would be when time constraints exist within the timing of a Task. If a Task is set up to call a businesses, you might require the Task to only run during normal business hours. This Template controls the timing of the Task. Bonus Used to control bonuses granted to workers. Applied when setting bonus strategies. Applied via the Worker’s Bonus Worker section (Communication Center tab). Text message when Worker gets a bonus. Set in Task Advanced Options > Notifications. Notify, Crowd _Notification Notification Template used to communicate with workers concerning Tasks. Text message when new Tasks are available for Workers. Set in Task Advanced Options > Notifications. Accept Reject Notification Template used to convey to a worker that a Task has been accepted rejected. Message text sent to Worker when HIT is accepted rejected. Set in Task Advanced Options > Notifications. Example Accept Template Your Task is approved: 0) > Assignment {assignment_index} : Title: {data.question.title} Reward: {data.question.amount} "},{"version":"10.0","date":"Aug-19-2019","title":"connect-rpa-express-to-shared-automl-server","name":"Connect RPA Express to Shared AutoML Server","fullPath":"iac/core/automation-engineer/dev-env/connect-rpa-express-to-shared-automl-server","content":" Configuring Local HTTP Proxies for RPA Express 2.0 Validate Prerequisites RPA Express machine has access to AutoML and the following information is available The URL for gateway service on the shared AutoML Server The URL for wfbi service on the remote AutoML Server Authorization credentials for wfbi servive on the remote AutoML Server Pre trained presentation model Experiment ID Experiment Group Model Version Download and install Nginx Nginx can be downloaded from Configure Nginx to proxy remote gateway and wfbi services through localhost. Change conf nginx.conf Additions to HTTP block Additions to SERVER block Start Nginx Use Windows command prompt window (cmd.exe) Navigate to Nginx folder execute start nginx.exe check if Nginx is working via tasklist fi \"imagename eq nginx.exe\" Modify RPA Exress secure properties Edit rpaExpress _folder Workfusion tomcat conf secure.properties: Restart Control Tower Use RPA Express context menu to restart control tower Preparing Automation within RPA Express 2.0 Environment Create or Import Automation Manual Task The easier way just import existing Automation BP which is attached (Classification Cognitive Automation Task 23 5 2018.zip) it will create everything that you need.* * But if you want to create everything by yourself you need to: Create campaign. (Unfortunately rpa express doesnt have campaign in menu you have to find some bp that contains necessary campaigns and import it) Then create Business Process Use Case which is using bp that you created before. Then create automation manual task using the following Use Case If you want use trained model setup it in manual task ( link to the documentation VDS Workflow using Trained Model) sometimes when you choose bot workflow and you dont see element in the image bellow and cant choose trained model. So you need press Apply and save Manual task. And then open AutoML tab again start enter campaign to find your model select experiment that you need and model version "},{"version":"10.0","date":"Aug-19-2019","title":"dev-environment-with-rpa-express-spa-developer-edition","name":"Dev Environment with RPA Express SPA Developer Edition","fullPath":"iac/core/automation-engineer/dev-env/dev-environment-with-rpa-express-spa-developer-edition","content":" Using RPA Express as Integrated RPA Developer Environment WorkFusion's Customer and Partner Automation Engineers are suggested to utilize the RPA Express for all ongoing automation efforts. It is convenient to use RPA Express SPA Developer Edition as local DEVELOPMENT environment by each engineer who contributes into automation using WorkFusion SPA. Robitic RPA, Machine Learning, OCR, Manual Tasks, development in WF Studio IDE all tools and components are provided. "},{"version":"10.0","date":"Aug-19-2019","title":"install-rpa-express-spa-developer-edition","name":"Install RPA Express SPA Developer Edition","fullPath":"iac/core/automation-engineer/dev-env/install-rpa-express-spa-developer-edition","content":" Step by step guide Validate RPA Express 2.0 system requirements Windows 7 (or higher) x64 16GB RAM 4 logical CPUs 50 GB disk space Obtain RPA Express 2.0 via https: www.workfusion.com rpa express Obtain SPA Developer Edition (former license for SPA Sandbox) WF Account Manager Delivery Manager opens \"SPA License Request for RPAx\" ticket in OPS project IT Security approves the request Validate you have Administrative rights to execute the installation After installation is complete, Admin user must add read write permissions to the following log folders c: RPAExpress Workfusion tomcat logs c: RPAExpress RPA logs c: RPAExpress OCR logs c: RPAExpress RPA wfagent logs Identical to Workfusion SPA, the OCR operations executed within RPA Express would require the following third party products to be installed alongside with SPA Sandbox: ImageMagick 7.0.4 4 Q16 x64 2017 01 14 GPL Ghostscript 9.20 (2016 09 26) https: github.com ArtifexSoftware ghostpdl downloads releases download gs920 gs920w64.exe Modify System Environment PATH variable to include path to the gs9.20 bin and ImageMagick 7.0.4 Q16 folders After installing ImageMagick and Ghostscript, open cmd.exe and try to execute magick help gswin64c.exe Run RPA Express via icon on you Desktop "},{"version":"10.0","date":"Aug-06-2019","title":"use-nexus-repository-with-rpa-express","name":"Use Nexus Repository with RPA Express","fullPath":"iac/core/automation-engineer/dev-env/use-nexus-repository-with-rpa-express","content":" To work with BCBs on local environment or to use shared Nexus repository with RPA Express Step by step guide on how to use Nexus repository with local Control Tower Install RPA Express, obtain SPA Developer license. Install Nexus Repository Manager (version 2.7. *) Add property to RPAExpress Workfusion tomcat conf secure.properties Restart RPA Express and Import from Repository button will appear. Change your mcb.repo.url property in your BCB Related articles Page: Use Nexus Repository with RPA Express "},{"version":"10.0","date":"Aug-19-2019","title":"run-pra-express-on-public-ip-address-or-dns","name":"Run PRA Express on public IP address or DNS","fullPath":"iac/core/automation-engineer/dev-env/run-pra-express-on-public-ip-address-or-dns","content":" By default, all WorkFusion applications within SPA Sandbox are accessible on the localhost interface. To access the applications on the public IP or public DNS, use the below guide. Step by step guide Login to Process Monitor , go to Processes tab and stop the nginx service Open file site from SPASandbox nginx conf sites enabled folder Add the IP address or and DNS name for the machine where SPA Sandbox is installed from SPASandbox nginx conf sites enabled site server { listen 15280 default; server_name localhost; .... change the configuration to server { listen 15280 default; server_name 10.101.12.12 another.server.name.com localhost; .... where 10.101.12.12 and another.server.name.com should be replaced by IP and or DNS accessible in your network. From the Process Monitor start the nginx server back Validate that WorkFusion Control Tower and WorkSpace are available on all interfaces. can be accessible via and http: another.server.name.com:15280 workfusion It is important to understand that SPA Sandbox comes standard with the following applications: Name URL Primary URL Secondary Workspace http: localhost:15280 workspace http: localhost:15380 workspace Control Tower http: localhost:15280 workfusion http: localhost:15380 workfusion Process Monitor http: localhost:15100 Minio S3 Web UI http: localhost:15110 RPA Grid Console http: localhost:15444 grid console "},{"version":"10.0","date":"Aug-06-2019","title":"workfusion-standard-use-cases","name":"WorkFusion Standard Use Cases","fullPath":"iac/core/automation-engineer/dev-env/workfusion-standard-use-cases","content":" Almost all attachments that are presented here you can find in repository: Name Description Link Language Detection Workfusionlanguagedetection.v1.0.0.xml Email message (MSG) parsing Messages file should be in \"inputmsg\" folder. Temp files are not deleting in this version. DWorkfusionemailmessageparsing.v1.0.0.xml FTP Download using for download file from ftp without authentication Workfusionftpdownload.v1.0.0.xml SFTP Download using for download file from FTP with authentication UC Workfusionsftpdownload.v1.0.0.xml html2xml UC Workfusionhtml2xml.v1.0.0.xml html2text UC Workfusionhtml2text.v1.0.0.xml pdf2tiff convert from PDF file format to tiff file format. Program ImageMagic should be installed UC Workfusionpdf2tiff.v1.0.0.xml Execute shell command with optional parameters Workfusionexecuteshellcommandwithparams.v1.0.0.xml Save all input records to Data Store UC Workfusionsaverectods.v1.0.0.xml Read Variables from DataStore do output as map of two columns from DataStore UC Workfusionreadvarsfromds.v1.0.0.xml Analyze BP Performance Use Case to do time study for BP performance UC Workfusionbpperformance.v1.0.0.xml Page Type classification Rule based classification of page type, based on the keywords and keyword combinations. Change to your specific keywords for positive and negative cases UC WorkfusionPagetype_classification.v1.0.0.xml WorkFusion Standard BP Name Description Link Health check Check that components are working fine BP Healtcheck.v1.0.0.zip Cleanup Tagged Documents BP to use to cleanup already tagged documents BP Workfusioncleantags.v1.0.0.zip Split OCR into asynchronous two step process To eliminate OCR bottlenecks. For input should be provided variables: documentimagelink, ocrs3bucket, ocrs3folder. Should be installed global variable: ocrurl NEW OCR v.2 Export 01 2 2018.zip OCR BP Sample of BP OCR, also you can find all attachments to ocr here https: github.com syemialy WF COE PLATFORM tree master standart_bp ocr BP OCR v2.2.9.zip "},{"version":"10.0","date":"Aug-15-2019","title":"adjudication-rules","name":"Adjudication Rules","fullPath":"iac/core/automation-engineer/rules/adjudication-rules","content":" Adjudication is a process that ensures the quality of answers provided by Workers. These Rules allow you to configure this Adjudication process. There are several such rules which are created by default on your instance of the platform. These can be accessed from the Rules page. In addition, you can also create your own rules. Every Human Task that you create has an Adjudication Rule associated with it. You can see the rule associated with your Task and even change the rule by going to the Run Task > Advanced Options > Adjudication. Adjudication Rule The Adjudication Rule Configuration in the screenshot above tells the WorkFusion platform to use a minimum of 2 Assignments and a maximum of 3 Assignments and to use the \"2 1, pay all\" Rule for this particular Task. This means that the platform will initially post two assignments of the Task on the endpoint (for example WorkSpace or Amazon Mechanical Turk). If there are multiple Records in the input file, it will post two assignments for each of these records. Then as the Workers complete these assignments, the platform will check if the answers match or not between the two assignments. If both match then the Task is accepted and no further assignments are posted. If they do not match, then a third assignment is posted. When this third assignment is submitted, the platform compares this result with the previous two. If it finds a match then it accepts the answers that were matching. If there is still no match, then since the Max of Assignments field was set to 3, no further assignments are posted. The output file (snapshot) will contain the answers provided by the 3 Workers and that no confidence was found. Adjudication Rules can be configured by Rule Parameters in the Basic View and the actual Rules Code in the Advanced View. Adjudication Rule Example package com.freedomoss.requester; list any import classes here. import com.freedomoss.objective.model.RuleContext; import com.freedomoss.objective.model.RuleContext.MajorityType; import com.freedomoss.objective.model.RuleAssigmentContext; import com.freedomoss.objective.model.RuleQuestionContext; import com.freedomoss.requester.model.AwsHitQuestion; import com.freedomoss.requester.model.AwsHitQuestionItem; import com.freedomoss.requester.model.AwsHitQuestionAnswerItem; import org.slf4j.Logger; import java.util.ArrayList; import java.util.List; import java.util.Set; import java.util.Map; import java.util.Iterator; declare any global variables here global RuleContext source global Logger log global Map params rule \"Rule context initialization\" auto focus true no loop dialect \"mvel\" agenda group \"initialization group\" when ctx:RuleContext(initialized == false); then set parameters ctx.properties RuleContext.MAJORITY_TYPE = RuleContext.MajorityType.COUNT; ctx.properties RuleContext.MAJORITY_VALUE = new Integer(2); ctx.properties RuleContext.MAXASSIGNMENTLIMIT = new Integer(3); ctx.properties RuleContext.MAJORITYHITTHRESHOLD = new Double(100 100); ctx.properties RuleContext.ASSIGNMENTAPPROVETHRESHOLD = new Double(0.5); insert processed facts into memory ctx.updateWorkingMemory(); move to business rules kcontext.getKnowledgeRuntime().getAgenda().getAgendaGroup(\"calculation\").setFocus(); ctx.logExecutedRule(kcontext.getRule().getName()); end rule \"Worst accuracy rule Evaluate every 5 gold question and set Accuracy based qualification score to 70 if gold accuracy goes up 0) evaluate run average response time eval( campaignStatistic.totalGoldQuestions > 0 && ( campaignStatistic.totalGoldQuestions % 5) == 0 && campaignStatistic.goldAccuracy = 75 percents and 0) evaluate run average response time eval( campaignStatistic.totalGoldQuestions > 0 && ( campaignStatistic.totalGoldQuestions % 5) == 0 && campaignStatistic.goldAccuracy >= (75 100) && campaignStatistic.goldAccuracy = 90 percents\" agenda group \"calculation\" dialect \"mvel\" salience 250 no loop when ctx:RuleContext(initialized == true) rac:RuleAssigmentContext( campaignStatistic:campaignStatistic, runStatistic.totalGoldQuestions > 0) evaluate run average response time eval( campaignStatistic.totalGoldQuestions > 0 && ( campaignStatistic.totalGoldQuestions % 5) == 0 && campaignStatistic.goldAccuracy >= (90 100) ) then rac.grandQualification(RuleContext.ACCURACYBASEDQUALIFICATION, 95); ctx.logExecutedRule(kcontext.getRule().getName()); end rule \"0. Check, if exist majority\" agenda group \"calculation\" dialect \"mvel\" salience 100 no loop when ctx:RuleContext (initialized == true, gold:gold, threshold:properties.MAJORITYHITTHRESHOLD) eval( ctx.majorityWithoutGold().size() >= ( ctx.questions.size() gold.size()) * threshold) then insert(new String(\"MAJORITY_FOUND\")); ctx.logExecutedRule(kcontext.getRule().getName()); end rule \"1. Approve question by majority\" agenda group \"calculation\" dialect \"mvel\" salience 90 no loop when ((String(toString == \"MAJORITY_FOUND\") and ctx:RuleContext (initialized == true)) or (not String(toString == \"MAJORITYFOUND\") and ctx:RuleContext (initialized == true, assignments.size == properties.MAXASSIGNMENT_LIMIT))) rqc:RuleQuestionContext() then rqc.approveQuestion( ctx.majority()); ctx.logExecutedRule(kcontext.getRule().getName(), rqc); end rule \"3. Approve assignment (always)\" agenda group \"calculation\" dialect \"mvel\" salience 50 no loop when ctx:RuleContext(initialized == true) rac:RuleAssigmentContext() then insert(new String(\"ASSIGNMENT_PROCESSED\")); ctx.addApproved( rac); ctx.logExecutedRule(kcontext.getRule().getName(), rac); end rule \"5. Extend HIT\" agenda group \"calculation\" dialect \"mvel\" salience 50 no loop when not String(toString == \"MAJORITY_FOUND\") ctx:RuleContext(initialized == true, assignments.size = properties.MAXASSIGNMENTLIMIT); rac:RuleAssigmentContext(); then ctx.addApproved( rac); ctx.setDisposeHit(true); ctx.logExecutedRule(kcontext.getRule().getName()); end Adjudication Rule Parameters Majority Value Determines how many Workers need to agree in their answers for the Assignment to be accepted and the Task to be closed. So for example if this parameter is set at 2, the WorkFusion platform will create 2 Assignments (instances) of the Task on the endpoint (WorkSpace, Mechanical Turk, etc.). If Workers submit the same answers for both Assignments then the Task is closed, meaning it is not posted again and the Workers are paid. But if it return different answers, then one more assignment is created. WorkFusion will continue posting Assignments until 2 Workers provide the same answer. Increasing this value will ensure further quality of your work but it may result in more assignments being created and thus more Workers needing to be paid. Maximum Assignments Determines the maximum number of assignments that will be created during Adjudication. If Worker do not provide the needed majority and the Task needs to be extended by posting more assignments, then the WorkFusion platform will not exceed the value specified in this parameter when posting more assignments. For example if the Majority Value is 2, and more assignments need to be posted to reach that majority, and the Maximum Assignments is 4, then at most 4 assignments of that Task will be posted. If after 4 assignments, the majority still has not been reached then the Task will be closed. Increasing this value will ensure further quality of your work but it may result in more assignments being created and thus more Workers needing to be paid. Evaluation Frequency The Rule is also known as the CHECK _EVERY Rule. When performing Ongoing Qualifications, it determines after how many Gold Tasks a Worker performs to evaluate that Worker's score on this Task. For example, if this parameter is set to 5 then after every 5 Gold Tasks, the system will evaluate Worker performance and update Worker score. Normal Accuracy % vs Super Accuracy % The WorkFusion platform currently has two rankings that it assigns to Workers, Normal and Super. Normal Workers for a given Task usually are those who are known to perform it adequately well. Super Workers usually are those who are known to perform the given Task exceptionally well. These two parameters allow you to make the determination of what constitutes as \"Normal\" and \"Super\". They do this by allowing you to decide the percentage of correct answers that need to be present from the Worker in responses to Gold Data Tasks to achieve either the Normal or Super rankings. For example if you set Normal Accuracy to 75% then if the Worker answers at least 75% correct on Gold Tasks they will be given a Normal Ranking. And if you set the Super Accuracy to 90% then the system will only grant them Super Accuracy if they respond correctly to at least 90% of the Gold Tasks presented to them. This parameter is sometimes referred to as \"UPPER LIMIT\" and \"LOWER LIMIT\". Normal accuracy score vs Super accuracy score On some endpoints, like Mechanical Turk, a Qualification needs to have a score associated with it. This score is different from the Gold Accuracy score which is the percentage the Worker answered correctly on the Gold Data Tasks. So based on the Gold Accuracy Score, the Worker is assigned a ranking as described above. Then based on this ranking, the Worker is assigned a score based on these two parameters. In essence these parameters determine the score granted to the Workers based on the Ranking they achieved. So for example if they achieved a ranking of Super and the Super Accuracy Score parameter is set to 90 then they will be assigned a score of 90 on the Qualification associated with this Task. Adjudication Rules can contain other Rules that verify Worker statistics and execute some actions based on the results. See the rules below. Retracting Worker Answers The functionality is intended to exclude answers from final result (for example, if Worker is a cheater). If a Task contains retracted answers, the Majority and Confidence will be recalculated. Retract button is available only for human not qualification Tasks. Answers can be retracted: Manually (Task > View Results > Workers tab > Retract Worker Answers). Worker can continue to work on tasks in this run. Answers that have been given by this Worker after manual retract won't be retracted automatically. By the Adjudication Rule. If answers have been retracted by the rule, future answers of the Worker in this run will also be retracted and rejected. Retracted assignment won't be included in Max assignment limit counting. If task contains more than 1 assignment, additional assignment will be created instead of retracted one. Retracting rule example rule \"Retract less 7 sec\" agenda group \"calculation\" dialect \"mvel\" salience 20 no loop when ctx:RuleContext(); rac:RuleAssigmentContext(); eval( rac.assignment.submitTime.time rac.assignment.acceptTime.time Disqualifying Workers This rule allows to disqualify Workers if their Performance, Accuracy, or Gold Accuracy does not meet the specified requirements. Discqualify rule example package com.freedomoss.requester; list any import classes here. import com.freedomoss.objective.model.RuleContext; import com.freedomoss.objective.model.RuleContext.MajorityType; import com.freedomoss.objective.model.RuleAssigmentContext; import com.freedomoss.objective.model.RuleQuestionContext; import com.freedomoss.requester.model.AwsHitQuestion; import com.freedomoss.requester.model.AwsHitQuestionItem; import com.freedomoss.requester.model.AwsHitQuestionAnswerItem; import com.freedomoss.objective.model.RuleContextUtils; import com.freedomoss.objective.facts.RetractState; import org.slf4j.Logger; import java.util.ArrayList; import java.util.List; import java.util.Set; import java.util.Map; import java.util.Iterator; declare any global variables here global RuleContext source global Logger log global Map params rule \"Rule context initialization\" auto focus true no loop dialect \"mvel\" agenda group \"initialization group\" when ctx:RuleContext(initialized == false); then set parameters ctx.properties RuleContext.MAJORITY_TYPE = RuleContext.MajorityType.COUNT; ctx.properties RuleContext.MAJORITY_VALUE = new Integer(2); ctx.properties RuleContext.MAXASSIGNMENTLIMIT = new Integer(3); ctx.properties RuleContext.MAJORITYHITTHRESHOLD = new Double(100 100); ctx.properties RuleContext.ASSIGNMENTAPPROVETHRESHOLD = new Double(0.5); insert processed facts into memory ctx.updateWorkingMemory(); move to business rules kcontext.getKnowledgeRuntime().getAgenda().getAgendaGroup(\"calculation\").setFocus(); ctx.logExecutedRule(kcontext.getRule().getName()); end rule \"Worst accuracy rule Evaluate every 5 gold question and set Accuracy based qualification score to 70 if gold accuracy goes up 0 && ( campaignStatistic.totalGoldQuestions % 5) == 0 && campaignStatistic.goldAccuracy = 75 percents and 0 && ( campaignStatistic.totalGoldQuestions % 5) == 0 && campaignStatistic.goldAccuracy >= (75 100) && campaignStatistic.goldAccuracy = 90 percents\" agenda group \"calculation\" dialect \"mvel\" salience 250 no loop when ctx:RuleContext(initialized == true) rac:RuleAssigmentContext( campaignStatistic:campaignStatistic) evaluate run average response time eval( campaignStatistic.totalGoldQuestions > 0 && ( campaignStatistic.totalGoldQuestions % 5) == 0 && campaignStatistic.goldAccuracy >= (90 100) ) then rac.grantQualification(RuleContext.ACCURACYBASEDQUALIFICATION, 95); ctx.logExecutedRule(kcontext.getRule().getName()); end rule \"0. Check, if exist majority\" agenda group \"calculation\" dialect \"mvel\" salience 100 no loop when ctx:RuleContext (initialized == true, gold:gold, threshold:properties.MAJORITYHITTHRESHOLD) eval( ctx.assignments.size() > 0 && ctx.majorityWithoutGold().size() >= ( ctx.questions.size() gold.size()) * threshold) then insert(new String(\"MAJORITY_FOUND\")); ctx.logExecutedRule(kcontext.getRule().getName()); end rule \"1. Approve question by majority\" agenda group \"calculation\" dialect \"mvel\" salience 90 no loop when ((String(toString == \"MAJORITY_FOUND\") and ctx:RuleContext (initialized == true)) or (not String(toString == \"MAJORITYFOUND\") and ctx:RuleContext (initialized == true, assignments.size == properties.MAXASSIGNMENT_LIMIT))) rqc:RuleQuestionContext() then rqc.approveQuestion( ctx.majority()); ctx.logExecutedRule(kcontext.getRule().getName(), rqc); end rule \"3. Approve assignment (always)\" agenda group \"calculation\" dialect \"mvel\" salience 50 no loop when ctx:RuleContext(initialized == true) rac:RuleAssigmentContext() then insert(new String(\"ASSIGNMENT_PROCESSED\")); ctx.addApproved( rac); ctx.logExecutedRule(kcontext.getRule().getName(), rac); end rule \"5. Extend HIT\" agenda group \"calculation\" dialect \"mvel\" salience 50 no loop when not String(toString == \"MAJORITY_FOUND\") ctx:RuleContext(initialized == true, assignments.size = properties.MAXASSIGNMENTLIMIT); rac:RuleAssigmentContext(); then ctx.addApproved( rac); ctx.setDisposeHit(true); ctx.logExecutedRule(kcontext.getRule().getName()); end rule \"Init retract states\" agenda group \"calculation\" dialect \"mvel\" no loop when ctx:RuleContext(initialized == true) rac:RuleAssigmentContext() then insert(new RetractState( rac.assignmentId), true); end rule \"7. Retract Worker when response time is too high\" agenda group \"calculation\" dialect \"mvel\" salience 20 no loop when ctx:RuleContext(initialized == true) rac:RuleAssigmentContext( aggregatedWorkerStatistics:aggregatedWorkerStatistics) rs:RetractState(assignmentId == rac.assignmentId, assignmentRetracted == false) eval( aggregatedWorkerStatistics.otherWorkersAnswerCount > 1 && rac.responseTime 0 && ( ctx.completeHitCount % 2) == 0 && ( gold.size() > 0 runStatistic.totalGold > 0 ) && RuleContextUtils.getTaskWorkerGoldAccuracy( rac) * 100 0 && ( ctx.completeHitCount % 2) == 0 && runStatistic.totalMajorityCount > 0 && RuleContextUtils.getTaskWorkerAccuracy( rac) * 100 1) (! rs.roundRetracted && rac.retractedRounds > 1) ) and eval( aggregatedWorkerStatistics.otherWorkersGoldAnswerCount > 0 && RuleContextUtils.getTaskWorkerGoldAccuracy( rac) 1) (! rs.roundRetracted && rac.retractedRounds > 1) ) and eval( aggregatedWorkerStatistics.WorkerTaskResponseTimeMedian 1) and (eval( rs.assignmentRetracted && rac.retractedTasks 1 > ( aggregatedWorkerStatistics.totalTaskCount * 50) 100) or eval(! rs.assignmentRetracted && rac.retractedTasks > ( aggregatedWorkerStatistics.totalTaskCount * 50) 100) ) then disqualifyWorker( rac, 65, log); ctx.logExecutedRule(kcontext.getRule().getName()); end function void disqualifyWorker(RuleAssigmentContext rac, int score, Logger log) { RuleContext rc = rac.getParent(); log.info(\"Disqualifying Worker: \" rac.getNativeWorkerId()); String autoGrantedQualificationUUID = RuleContextUtils.getAutoGrantedQualificationUuid(rc); if (autoGrantedQualificationUUID != null) { log.info(\"Changing auto granted qualification \" autoGrantedQualificationUUID \" score to \" score); rac.forceGrantQualification(autoGrantedQualificationUUID, score); } else { String accuracyQualificationUUID = RuleContextUtils.getAccuracyBasedQualificationUuid(rc); if(accuracyQualificationUUID != null) { log.info(\"Changing accuracy based qualification \" accuracyQualificationUUID \" score to \" score); rac.forceGrantQualification(accuracyQualificationUUID, score); } } } Retract Worker when Response Time is too high This part marks assignments as Retracted in DB if Worker answers very fast. To mark an assignment: Should be more than 1 answer from other Workers (number of answers can be specified in Time Check threshold parameter) Time spend on task should be lower than (Time Median of other Workers 2). All answers greater than 90th percentile will be excluded from median counting. ( aggregatedWorkerStatistics.otherWorkersAnswerCount > 1 && rac.responseTime 0 && ( ctx.completeHitCount % 2) == 0 && ( gold.size() > 0 runStatistic.totalGold > 0 ) && RuleContextUtils.getTaskWorkerGoldAccuracy( rac) * 100 0 && ( ctx.completeHitCount % 2) == 0 && runStatistic.totalMajorityCount > 0 && RuleContextUtils.getTaskWorkerAccuracy( rac) * 100 1) (! rs.roundRetracted && rac.retractedRounds > 1) ) and eval( aggregatedWorkerStatistics.otherWorkersGoldAnswerCount > 0 && RuleContextUtils.getTaskWorkerGoldAccuracy( rac) 1) (! rs.roundRetracted && rac.retractedRounds > 1) ) and eval( aggregatedWorkerStatistics.WorkerTaskResponseTimeMedian 1) and (eval( rs.assignmentRetracted && rac.retractedTasks 1 > ( aggregatedWorkerStatistics.totalTaskCount * 50) 100) or eval(! rs.assignmentRetracted && rac.retractedTasks > ( aggregatedWorkerStatistics.totalTaskCount * 50) 100) ) `"},{"version":"10.0","date":"Aug-06-2019","title":"composite-rules-advanced-options","name":"Composite Rules - Advanced Options","fullPath":"iac/core/automation-engineer/rules/composite-rules-advanced-options","content":" Composite Rules are used to define the variant of BP flow depending on the Record column values. In most cases these Rules can be configured through UI – see the basic description here. Rule Advanced Options You can also create a custom Rule by editing Composite Rule source code (Drools language) in Code Editor. Composite Rule Example Expand source declare any global variables here global CompositeRuleContext source global Logger log global Map params RuleContext.SPIT_RULE:usa rule \"Rule context initialization\" auto focus true no loop agenda group \"initialization group\" when ctx:CompositeRuleContext(initialized == false); then insert processed facts into memory ctx.updateWorkingMemory(); move to business rules kcontext.getKnowledgeRuntime().getAgenda().getAgendaGroup(\"calculation\").setFocus(); end RuleOutcome.Definition={\"outcomeId\":\"Group I\",\"conditional\":false,\"conditions\": } RuleOutcome.Definition={\"outcomeId\":\"Group II\",\"conditional\":false,\"conditions\": } RuleOutcome.Definition={\"outcomeId\":\"Group III\",\"conditional\":false,\"conditions\": } RuleOutcome.Definition={\"outcomeId\":\"Group IV\",\"conditional\":false,\"conditions\": } RuleOutcome.Definition={\"outcomeId\":\"Group V\",\"conditional\":false,\"conditions\": } RuleOutcome.Definition={\"outcomeId\":\"outcome1\",\"conditional\":false,\"conditions\": } rule \"Send unprocessed result to unconditional outcome(s)\" agenda group \"calculation\" dialect \"mvel\" salience 80 when ctx:CompositeRuleContext(initialized == true); item:CompositeRuleDataItemContext(); then ctx.sendResultToUnconditionalOutcomes( item, true); ctx.logExecutedRule(kcontext.getRule().getName() \" item:\" item.toString()); end rule \"Logging all items\" agenda group \"calculation\" dialect \"mvel\" salience 60 when ctx:CompositeRuleContext(initialized == true); item:CompositeRuleDataItemContext(this memberOf ctx.lastStepDataItems); then ctx.logExecutedRule(kcontext.getRule().getName() \" item:\" item.toString()); end You can use additional functions of Composite Rules by enabling Advanced Rule Options and or Advanced Outcome Options. Rule Advanced Options Option Description APPROVE ALL RULE If set, then Drools execution is replaced with Java loop that sends all Records to the next step unconditionally. MODERATION _RULE This flag is used in Moderation Flow, before Moderation Step. SPLIT _DATA This flag is used to split Records for the next step according to the number of created Assignments. For example, 1 Record is sent to the 1st step with Adjudication Rule \"1 20\", then 20 or 21 Assignments will be created, and they will go to the next step as input Records. This feature can be used for Surveys, Moderation Flow, and Training Tasks. You need to create a composite Rule with SPLIT _DATA flag after a Bot Task that uses multi column Export plugin (with split results=\"true\") to split one Record into multiple. Since WF 8.2: In order to minimize data errors when splitting data records is required, it is not necessary to add the SPLIT _DATA rule after a step which splits records in its export section. Previously, to produce multiple rows along with split results=\"true\" after a step, it was necessary to create a rule with SPLIT _DATA advanced option checked. SPLIT _RULE These flags are used in BPs where all Records that go to the fork, should pass through all branches and join after. The step after JOIN rule won't start until all branches are completed. Split Rule Group label (value inserted after flag) is used to differentiate split join groups, in case it is used more than once, e.g. if BP has sub processes with split and join. JOIN _RULE STREAMING This flag overrides BP streaming settings for the consequent step. If set, you should define the number of Tasks or percentage value for streaming. Streaming in Rule NAMED _RULE This flag is used to build cut off rules in Automation BP according to model results (this rule is used after the extract step and allows to generate condition automatically). Outcome Advanced Options These options are used for Moderation Flow. Option Description Rework task by the same worker Used in Moderation Flow in moderation decision rule, new assignment is created after Moderation step and is assigned to the same worker (in WorkSpace– shown on Dashboard, in mTurk – not available for others). After rework, new assignment is created, original one is in Review. The Worker will not get Reward for reworking. Assign task to the same worker Used in Moderation Flow in moderation decision rule, new assignment is created after Moderation step and is assigned to the same worker. After rework, new assignment is created, original one is Done. The Worker will get Reward for each reworking iteration. Prohibit current worker to work on this task Used in Moderation Flow. After Moderation decision, the task will not be available for rework by current Worker. Approve initial worker's task After Moderation, Worker gets the Task Reward. Reject initial worker's task After Moderation, Worker DOES NOT get Reward. Enable logging Enables rule logging – rule execution is written to rules.log. RegEXP in Conditions RegEXP allows the user to use OR in composite rules for some parameter. In the image above: num 'matches regexp' 1 2 means that this outcome will accept records that have num value 1 OR 2 otherwise record will be redirected to Default outcome. "},{"version":"10.0","date":"Aug-06-2019","title":"creating-adjudication-rules","name":"Creating Adjudication Rules","fullPath":"iac/core/automation-engineer/rules/creating-adjudication-rules","content":" Select* Campaigns→ Rules* Select the rule with template you need and copy it. Attention! Do not change the settings of the initial rule! * * * * Select the rule you have created. Name the rule, so you could possibly reuse it for other questions. Set up the majority values and max assignments limits depending on a use case you have Majority values is the minimum number of workers required to complete the task so that the results are available in the output snapshot. The task will not be completed unless the number of workers participating in the task reaches or exceeds majority values. Max of Assignments is the total amount of workers allowed to participate in the task. For that reason, Max of Assignments CANNOT be less than Majority values. Example For example, Majority values* (Min of Assignments) = 4, Max of Assignments = 6 means that 4 workers’ answers are enough to find the majority, 5th worker’s answer is utilized when the majority is not found between the two. If the majority is found between 5 workers, no work is distributed beyond 5 workers count.* ** Save the rule. Select your Manual Task, go to the Run tab → * Advanced Options. Choose the rule you have created, set up Min and Max of Assignments* and click OK. * * Run the task. In order to make not all fields required for adjudication, you can go to your Manual Task, select the Design tab, click the answer button to view Advanced option and select \"Do not use in Adjudication (SA)\" How to Proceed with Adjudication Steps in Case of 6 and More SMEs Number of SMEs Adjudication rule 1 5 SMEs Start with one assignment 2 1 pay all, Start with one assignment 3 2 pay all 6 SMEs Start with one assignment 2 1 pay all, Start with one assignment 3 2 pay all or create your own adjudication rule In case you have more than 5 SMEs our advice is to set up 2 1 or 3 2 rules. Before launching the task make sure you set up appropriate min and max numbers of assignments. (min and max number of workers required to complete the task). "},{"version":"10.0","date":"Aug-06-2019","title":"qualification-rules","name":"Qualification Rules","fullPath":"iac/core/automation-engineer/rules/qualification-rules","content":" The Qualification Rules are used to evaluate Worker performance during Qualification Runs. They allow the WorkFusion platform to grant Accuracy Based Qualifications to Workers based on their performance on such runs. The Qualification Rules are executed instead of Adjudication Rules for Qualification Tasks. WorkFusion platform provides the Generic Qualification Rule by default. You can modify this rule or create a new one. Qualification Rule The first part of the rule serves to initialize the rule parameters. The next part of the Rule evaluates the Workers' score after they have taken a specified number of Tasks, this number is defined by the TOTAL HITS LIMIT parameter of the rule. Example So for example after a Worker has taken 10 Tasks, this part of the rule fires and begins to evaluate Worker's performance so far on the Qualification Run. If the Worker scores higher than or equal to the value defined in the NORMAL ACCURACY parameter and less than the value defined in the ELITE ACCURACY parameter then the Worker is granted the Accuracy Based Qualification with a score defined in the NORMAL _SCORE parameter. If the Worker scores higher than the value defined in the ELITE ACCURACY parameter then the Worker is granted the Accuracy Based Qualification with a score defined in the ELITE SCORE parameter. Some Qualification Rules also have a BUSTLER TIME IN _SECONDS parameter. This parameter detects bustlers, people who try to complete Tasks without properly attempting them or reading the given instructions. If the response time for the Tasks was less than or equal to this parameter, then the Assignment is rejected and retracted from that Worker. Finally, the system checks the values of the MAX ASSIGNMENT LIMIT parameter. If the number of assignments created for this Task equals or exceeds this parameter, then the HIT is disposed (meaning no further assignments are created for this run). Qualification Rule Example Expand source rule \"Rule context initialization\" auto focus true no loop agenda group \"initialization group\" when ctx:RuleContext(initialized == false); then set parameters ctx.addProperty(RuleContext.MAJORITYTYPE, RuleContext.MajorityType.COUNT); ctx.addProperty(RuleContext.MAJORITYVALUE, new Integer(2)); ctx.addProperty(RuleContext.DISTINCTANSWERS, true); ctx.addProperty(RuleContext.SKIPANSWER \"toCheck\", \"\"); ctx.addProperty(RuleContext.ACCEPTANSWER \"toCheck\", \"b\"); ctx.addProperty(RuleContext.SKIPANSWER, \"\"); ctx.addProperty(RuleContext.ACCEPTANSWER, \"\"); ctx.addProperty(RuleContext.SKIPEMPTYANSWERS, true); ctx.addProperty(RuleContext.MAJORITYTYPE, RuleContext.MajorityType.PERCENTAGE); ctx.addProperty(RuleContext.MAJORITYVALUE, new Double(0.5)); insert processed facts into memory ctx.updateWorkingMemory(); move to business rules kcontext.getKnowledgeRuntime().getAgenda().getAgendaGroup(\"calculation\").setFocus(); end rule \"000. Test Rule Evaluate every hit\" agenda group \"calculation\" dialect \"mvel\" salience 260 no loop when ctx:RuleContext(initialized == true) rac:RuleAssigmentContext( campaignStatistic:campaignStatistic) evaluate run average response time eval( campaignStatistic.totalHits >= 30 && campaignStatistic.accuracy >= 90) then ctx.logExecutedRule(\"Accuracy :\" campaignStatistic.getAccuracy()); ctx.logExecutedRule(\"GAccuracy :\" campaignStatistic.getGoldAccuracy()); ctx.logExecutedRule(\"TotalHits :\" campaignStatistic.getTotalHits()); end rule \"001. Evaluate every 20 hits and grant Qualified to work on Spanish sentiment Tasks set score 70 \" agenda group \"calculation\" dialect \"mvel\" salience 250 no loop when ctx:RuleContext(initialized == true) rac:RuleAssigmentContext( campaignStatistic:campaignStatistic) evaluate run average response time eval( campaignStatistic.totalHits >= 30 && campaignStatistic.goldAccuracy = 30 && campaignStatistic.goldAccuracy >= 0.8 && campaignStatistic.goldAccuracy = 30 && campaignStatistic.goldAccuracy >= 0.9) then rac.grandQualification(RuleContext.ACCURACYBASED_QUALIFICATION, 90); ctx.logExecutedRule(kcontext.getRule().getName()); ctx.logExecutedRule(\"GAccuracy :\" campaignStatistic.getGoldAccuracy()); end rule \"3. Approve assignment\" agenda group \"calculation\" dialect \"mvel\" salience 20 no loop when ctx:RuleContext(inAdvanceKnownValues:inAdvanceKnownValues, inAdvanceKnownValues.size != 0) rac:RuleAssigmentContext() eval( rac.containAll( ctx.inAdvanceKnownValues(), 10)) then ctx.addApproved( rac); ctx.logExecutedRule(kcontext.getRule().getName(), rac); end rule \"4. Reject assignment\" agenda group \"calculation\" dialect \"mvel\" salience 20 no loop when ctx:RuleContext(inAdvanceKnownValues:inAdvanceKnownValues, inAdvanceKnownValues.size != 0); rac:RuleAssigmentContext(); eval(! rac.containAll( ctx.inAdvanceKnownValues(), 90)) then ctx.addRejected( rac); ctx.logExecutedRule(kcontext.getRule().getName(), rac); end rule \"5. Extend HIT\" agenda group \"calculation\" dialect \"mvel\" salience 10 no loop when ctx:RuleContext(assigments.size() = 8) then ctx.addApproved( rac); ctx.addRejected( rac); ctx.logExecutedRule(kcontext.getRule().getName()); end Qualification Rule Parameters Parameter Parameter Description Maximum Assignments Determines the maximum number of assignments that will be created for the Qualification Run. If the number of Workers needed to be qualified is not reached and the Task needs to be extended by posting more assignments, then the WorkFusion platform will not exceed the value specified in this parameter when posting more assignments. Normal Accuracy % Elite Accuracy % The WorkFusion platform has two rankings assigned to Workers: Normal and Elite. These two parameters allow you to make the determination of what constitutes as \"Normal\" and \"Elite\". You can set the percentage of correct answers needed to be present from the Worker in responses to Gold Data Tasks to achieve either the Normal or Elite rankings. For example, if you set Normal Accuracy to 75% then if the Worker answers at least 75% correct on Gold Tasks, this Worker will be given a Normal Ranking. This parameter is sometimes referred to as \"UPPERLIMIT\" and \"LOWERLIMIT\". Normal accuracy score Elite accuracy score On some endpoints, like Mechanical Turk, a Qualification needs to have a score associated with it. This score is different from the Gold Accuracy score which is the percentage the Worker answered correctly on the Gold Data Tasks. So based on the Gold Accuracy Score, the Worker is assigned a ranking as described above. Then the Worker is assigned a score based on these two parameters. In essence these parameters determine the score granted to the Workers based on the Ranking they achieved. For example, if a Worker achieved a ranking of Elite and the Elite Accuracy Score parameter is set to 90, then this Worker will be assigned a score of 90 on the Qualification associated with this Task. Bustler Time ====================== ================ Bustlers are Workers who do not spend enough time on each Task. This often is an indication that their answers are of poor quality and should not be accepted. This parameter allows you to determine how much time a Worker needs to spend on each of your Tasks in order not to be labeled a bustler. "},{"version":"10.0","date":"Aug-19-2019","title":"rules-and-rule-templates","name":"Rules and Rule Templates","fullPath":"iac/core/automation-engineer/rules/rules-and-rule-templates","content":" To manage Rules in WorkFusion platform, go to Advanced > Rules. You can create, edit, delete, copy, and export import Rules. Rules can also be created from templates (Advanced > Rule Templates). Rule Structure To edit Rules in advanced mode, you need to learn the Drools language. A typical rule file can contain one or several rules with the following structure: Basic Rule Structure rule \"name\" attributes when LHS conditional part then RHS executional block end Rule Types in WorkFusion WorkFusion platform has the following Rule Types: Adjudication. Adjudication Rules use a majority rule algorithm that you can set to determine how many Workers you want to have to agree in order to reach a consensus. Gold Adjudication. These Rules are used to automatically generate gold questions based upon Adjudication (Majority found). Gold Adjudication Rule Example Expand source This rule first checks to see if a majority has been reached and then it marks these records as gold. rule \"0. Check, if exist majority\" agenda group \"calculation\" dialect \"mvel\" salience 100 no loop when ctx:RuleContext (initialized == true , assigments.size >= 2, majority.size >= 1, threshold:properties.MAJORITYHITTHRESHOLD) eval( ctx.majority().size() >= ctx.questions.size() * threshold) then insert(new String(\"MAJORITY_FOUND\")); ctx.logExecutedRule(kcontext.getRule().getName()); end rule \"1 Determine new gold questions\" dialect \"mvel\" salience 90 no loop when String(toString == \"MAJORITY_FOUND\") ctx:RuleContext() rqc:RuleQuestionContext() eval( rqc.count( ctx.majority()) == rqc.answers.size()) then rqc.markAsGoldQuestion(); ctx.logExecutedRule(kcontext.getRule().getName(), rqc); end Qualification. Qualification Rules are used to evaluate Worker performance in Qualification Tasks. They allow the WorkFusion platform to grant Accuracy Based Qualifications to Workers based on their performance on such Tasks. Composite. Composite Rules Advanced Options control Task progression and escalation through Business Process. Price. Price Rules are intended to update Task price automatically. Task Comparator. Task Comparator (Manual tasks sorting) is used as a global parameter for environments (separately for Sandbox and Production) which defines the order of displaying Tasks of the same type (Campaign) to Workers. Task Distribution. This Rule is used as a Workforce parameter to make possible to complete the Tasks in a custom order. See the Dynamic Task Distribution among Crowds topic. Task Distribution Rule Example package com.freedomoss.requester; list any import classes here. import com.freedomoss.objective.model.RuleContext; import com.freedomoss.objective.model.DistributionRuleContext; import com.freedomoss.requester.service.mes.TaskDistributionStrategyContext; import com.freedomoss.requester.service.mes.TaskDistributionStrategyEnum; import com.freedomoss.requester.service.mes.ITaskDistributionStrategy; import com.freedomoss.requester.service.mes.impl.TaskDistributionStrategyFactory; import com.freedomoss.requester.service.mes.impl.AdvancedStrategy import com.freedomoss.requester.model.AwsHit; import com.freedomoss.workfusion.model.mes.Workforce; import com.freedomoss.workfusion.model.mes.Crowd; import com.freedomoss.model.HitSubmissionDataItem; import com.freedomoss.model.HitSubmissionDataItemValue; import org.slf4j.Logger; import java.util.ArrayList; import java.util.List; import java.util.Set; import java.util.Map; import java.util.HashMap; import java.util.Iterator; declare any global variables here global DistributionRuleContext source global Logger log global Map params rule \"Distribute\" auto focus true no loop when ctx: DistributionRuleContext(); then ctx.setDistribution(getTasksDistribution( ctx.getDistributionContext())); end function Map> getTasksDistribution(TaskDistributionStrategyContext context) { Map> result = new HashMap>(); Workforce workforce = context.getWorkforce(); List crowds = workforce.getEnabledCrowds(); for (Crowd crowd: crowds){ result.put(crowd, new ArrayList()); } List hits = context.getHits(); Map> hitDataItemsMap = context.getHitDataItemsMap(); for (AwsHit hit: context.getHits()) { for(HitSubmissionDataItem item: hitDataItemsMap.get(hit.getId())) { for(HitSubmissionDataItemValue val:item.getValues()){ System.out.println(\"================== VALUES language: \" val.getName() \" =====================\"); } HitSubmissionDataItemValue languageObj = item.getValue(\"language\"); if(languageObj != null){ String language = languageObj.getItemValue(); if (\"en\".equals(language)){ Crowd crowd = crowds.get(0); result.get(crowd).add(hit); } else { Crowd crowd = crowds.get(1); result.get(crowd).add(hit); } } } } return result; } Rule Parameters Overview All of the Rules in the WorkFusion platform have rule parameters associated with them. These parameters allow non technical users of the platform to configure and customize the behavior of the rules. Rule Parameters are created in Rule Templates > Edit Template > Params tab. Rule Parameters Creating a Rule To create a new Rule in WorkFusion platform: Go to Campaigns > Rules.** Rules** Click the Create button. Create Rule Enter a unique Rule Name. Select Rule Type from the appropriate dropdown. Select a Rule Template from the Adjudication Strategy dropdown. (Optional) If you want to create a Rule from scratch (without selecting a template), click the Switch to Advanced View link and write a custom Drools code. Click the Save button in the bottom right corner. Creating a Rule Template Rule Templates are intended for re using code when creating new Rules and for creating Rule Parameters. To create a new Rule Template in WorkFusion platform: Go to Campaigns > Rule Templates. Rule Templates Click the Create button. Create Rule Template Enter a unique Rule Template Name. Add Description. Write a custom Drools code in the Code Editor. Click the Save button. Adding Rule Parameters Rule Parameters allow non technical users of the platform to configure and customize the behavior of the rules. To add Rule Parameters: Go to Campaigns > Rule Templates. Click an appropriate Rule in the Name column. Rule Template Params Switch to the Params tab. Add Rule Params Click the Add Param button – the Edit Param dialog will be displayed under the grid. Edit Rule Param Dialog Enter the parameter Name (will be displayed for user). Enter the unique parameter Code (this code will be used in the Rule code). Example of using parameters in rule code: {{RETRACT ROUND SIZE}} Select the parameter Type (control type: number, text, or text area). Enter the parameter Default Value. Add Description. Click the Save button. "},{"version":"10.0","date":"Aug-06-2019","title":"task-comparator","name":"Task Comparator (Manual tasks sorting)","fullPath":"iac/core/automation-engineer/rules/task-comparator","content":" title: Task Comparator (Manual tasks sorting) This functionality is intended to sort Tasks having the same template (Campaign), and make Workers complete these Tasks in a custom order. To enable this feature: Select a Task Comparator (Configuration > System Preferences)Task Comparator is available separately for Sandbox and Production environments. Check the Enable Dynamic Task Rendering parameter (Run Task > Advanced Options > Properties). Select the Task Priority on the Run Task tab.Priority will work by default for both endpoints. Priority allows to decide which tasks will be displayed for the Workers primarily. Example You have 2 similar Tasks with different input Data files. And you want Workers to complete the 2nd Task primarily. For this purpose, set the Priority = Max for the 2nd Task. When a Worker finds this type of Task and clicks the View link, the Worker will see the Tasks with the highest priority on top. (Optional) Set other Task parameters that are used as Task Comparator attributes: Due Date, of Max Assignments, etc. Task Comparator Attributes You can use the following attributes in Task Comparator: priority dueDate maxAssignments creationTime expirationTime Data from Input file columns To sort Tasks by these attributes, the Task Comparator Rule should be accordingly modified. The following Rule sorts Tasks by Priority, Due Date, and Creation Date: Task Comparator Example Expand source Task Comparator Example. Sorting by creation time only Expand source result = 0; for (Object attribute : attributes) { if (task1 attribute == null && task2 attribute != null) { return 1; } else if (task1 attribute != null && task2 attribute == null) { return 1; } else if (task1 attribute != null && task2 attribute != null) { if (dates.contains(attribute)) { date1 = new java.text.SimpleDateFormat('MM dd yyyy hh:mm:ss').parse(task1 attribute ); date2 = new java.text.SimpleDateFormat('MM dd yyyy hh:mm:ss').parse(task2 attribute ); result = date1.compareTo(date2); } else { result = task1 attribute .compareTo(task2 attribute ); } if (result != 0) { return result; } } } return 0; Task Comparator Example. Sorting by Y N input field and creation time Expand source Sort by isexpedited asc, creation date asc. If task doesn't have isexpedited input field, sort by creation date only This method expected to be reusable. For the given two objects (which are usually simple java.util.Map) and given array of attributes it returns 1, 0 or 1 comparing fields sequently. Attributes are being compared one by one in the order they are specified in the list. This method is expected to be reusable unless you need more advanced comparation logic. Call of sorting method for given objects 'first' (task1) and 'second' (task2) and list of attributes to compare. Please note, you can use custom attributes specified in CCF and 3 task properties: 'maxAssignments', 'creationTime' and 'expirationTime'. task1 = first; task2 = second; All list of attributes for sorting attributes = \"yorn_field\",\"creationTime\" ; the variable 'dates' defines list of field names which are in the date format. This fields require special comparation logic. dates = \"creationTime\" ; the variable 'yorn' defines list of field names which have Y or N values. This fields require special comparation logic. yorn = \"yorn_field\" ; int res = 0 ; for (Object attribute : attributes) { boolean isDateFilled = (task1 attribute != null && !task1 attribute .isEmpty()) && (task2 attribute != null && !task2 attribute .isEmpty()); if (yorn.contains(attribute)) { in string Y and N comparison, N N result, so we simple invert comparison order res = org.apache.commons.lang3.ObjectUtils.compare(task2 attribute , task1 attribute , true); } else if (dates.contains(attribute) && isDateFilled) { date1 = new java.text.SimpleDateFormat('MM dd yyyy hh:mm:ss').parse(task1 attribute ); date2 = new java.text.SimpleDateFormat('MM dd yyyy hh:mm:ss').parse(task2 attribute ); res = date1.compareTo(date2); } else { res = org.apache.commons.lang3.ObjectUtils.compare(task1 attribute , task2 attribute , true); } if (res != 0) { return res; } } return 0; Sorting by data from input file ascending (file → country city sorting.csv) Task comparator can be written in MVEL (or since 8.2 in Groovy). Add the following line to include input data into context : include INPUT _DATA Since 8.2: For better performance particular used columns should be specified explicitly, e.g. when comparison is based on attributes some date, some priority: include INPUT DATA(some date,some _priority) Creating a New Task Comparator A new Task Comparator can be added on the Campaigns > Rules page: Click the Create button. Add your code and enter the Rule parameters. Rule Type should be \"Task Comparator\". for (Object attribute : attributes) { int res = org.apache.commons.lang3.ObjectUtils.compare(task1 attribute , task2 attribute , true); if (res != 0) { return res; } } return 0; Task Comparator life cycle Default behavior Manual task sorting is scheduled process. It means that once tasks posted to workspace they won't be sorted immediately, but only after next sorting cycle. By default, every 2 minutes (120000). It also means that once you changed behavior of a task comparator, changes will appear in 1 2 of sorting cycles. Change behavior Time period between two sorting cycles can be changed. To do that you need you need add update parameter task.delay.processHitSorting to workfusion.properties file, near to other task.delay parameters. Location of properties file may looks like opt workfusion apps webapps apache tomcat 8.0.33 conf workfusion.properties file. After update of the file, tomcat need to be restarted Before change Before change of task.delay.processHitSorting parameter it is good to know how much time at current moment manual tasks sorting procedure takes (depends on amount of tasks). To do that you should check HitSortingService.processHitSorting data from performance metrics.log file. If it already takes 2 minutes then there no reason to decrease default parameter. "},{"version":"10.0","date":"Aug-21-2019","title":"workspace","name":"WorkSpace","fullPath":"iac/core/workspace/workspace","content":" Workspace is an application for WorkFusion human in the loop approach. When primitive work has been automated with RPA, there is also work that can only be done by humans, for example, decision making. A person—Worker—is presented with the task, and then he she submits the answer, so that the bots can continue the work based on this answer decision. In SPA with AutoML WorkFusion's AI learns on the Workers' answers, so that even cognitive work gets automated. Workspace serves as a web portal for internal or external Workers where they can: register look for tasks watch statistics accept and submit tasks created in the Control Tower application pass Qualification tests See full list of WorkSpace Glossary. "},{"version":"10.0","date":"Aug-14-2019","title":"hit-lookup-strategies","name":"HIT Lookup Strategies","fullPath":"iac/core/workspace/admin/hit-lookup-strategies","content":" CV has two strategies to lookup HITs in HitType. This is due to major performance impact of the old method we used. The old method: Search for 'ASSIGNABLE' HIT, Filter HITs by their assignment: AND NOT EXISTS (SELECT 1 FROM assignment a WHERE a.hitid = h.id AND a.workerid = AND a.assignment_status IN ('SKIPPED', 'ACCEPTED', 'SUBMITTED', 'APPROVED', 'REJECTED')) AND NOT EXISTS (SELECT 1 FROM assignment a WHERE a.hitid = h.id AND a.workerid = AND a.assignment_status IN ('RETURNED', 'ABANDONED') AND a.deadline > ) `AND (SELECT COUNT(id) FROM assignment a WHERE a.hitid = h.id AND a.assignmentstatus IN ('ACCEPTED', 'SUBMITTED', 'APPROVED', 'REJECTED')) `"},{"version":"10.0","date":"Aug-14-2019","title":"workspace-application-properties","name":"WorkSpace Application Properties","fullPath":"iac/core/workspace/admin/workspace-application-properties","content":" At least 2 properties are required for correct work: core.socialApplicationUrl web.workerMessageReceiverUrl For WorkSpace secret properties see WorkSpace Secure Properties. property name description default value ws.datasource.connection pool.dataSourceClassName Name of the DataSource class provided by the JDBC driver. ws.datasource.connection pool.minimumIdle This property controls the minimum number of idle connection that will be to maintained in the pool. ws.datasource.connection pool.maximumPoolSize The maximum size that the pool is allowed to reach, including both idle and in use connections. ws.datasource.connection pool.connectionTimeout The maximum number of milliseconds that a client will wait for a connection from the pool. ws.datasource.connection pool.validationTimeout The maximum amount of time that a connection will be tested for aliveness. email.from email.host google.com email.port 8080 core.defaultPendingAssignmentsLimit This value is used when CrowdControl did not specify PendingAssignmentsLimit when registering HitType. Default value is 10, but can be changed. The variable is used for old data as for new HitTypes CC saves 1 when no limit is come from CC, which means unlimited amount of assignments per user per HitType. 10 core.defaultSkippedAssignmentDurationInSeconds This amount is used as a delay to delete assignments with status SKIPPED or RETURNED. User will see the skipped returned Hits again after this delay. 600 core.abandonAssignmentsJobChunkSize The size of chunk used to query database and abandon expired assignments. 100 core.countAvailableHitsUpTo Sets upper limit of tasks to count in each HitType and Category. Default is 100 (if not set) 100 core.useFastHitLookupStrategy See HIT Lookup Strategies for reference. false core.hitSortingStrategy Strategy to sort HITs using task comparator. It must be either \"HitType\" or \"Category\". If strategy is \"HitType\", hits will be sorted independently for each HitType. For \"Category\" the sort order bill be set for all HITs which are the members of any particular category. The value is optional, default is \"HitType\". HitType core.templateFilesExternalLocation Defines the folder with CV message templates opt cv templates core.instanceIdentifier CV core.socialApplicationUrl Required. It should be set to full URL value of workspace. It's used for example in registration email confirmation OR for sending push notifications password.workerPasswordExpirationDurationInSeconds This number is used as worker password expiration duration. If 0 set worker password does not expire 0 password.requesterPasswordExpirationDurationInSeconds This number is used as requester password expiration duration. If 0 set requester password does not expire 0 password.resetPasswordTokenExpirationTimeInMinutes Sets the timeout for reset password token. If user didn't reset his password during the lifetime of this token, he will be prompted to ask for password reset again. 15 web.googleAnalyticsEnabled Disables google analytics for (useful for on premise local deployment with no Internet access) true web.enableWorkerEmailConfirmation Disables email confirmation while registering (useful for on premise deployment) true web.displaySupportEmail If true email support is shown in footer true web.manualBalanceUpdateAllowed false web.socialPostSignInUrl The path to redirect user after he logged in with OAuth. Used by ClickWorkers at the moment to redirect them directly to categories dashboard web.workerMessageReceiverUrl Required. The callback URL to send worker messages to CrowdControl. This messages will appear on CC dashboard. The URL should be like following: https: somehost.com workfusion api v1 feedbacks web.resourceUrl Path to workspace resources. The resources include help and branding images. https: s3.amazonaws.com workspace resources web.tourEnabled If true tour modal is shown on first worker sign in true securityLogin.lockoutType time based securityLogin.lockoutTimeout 15 securityLogin.timeframeToCountAttempts 1440 securityLogin.maxAttemptsAmount 6 ldap.enabled false ldap.internal.authorization.enabled If set to true then groups filters ignored and ldap is used for authentication only (check that username password match) false ldap.user.base ldap.user.filter (sAMAccountName={0}) ldap.group.base ldap.group.filter (member={0}) ldap.requester.groups If user belongs to one of groups he's signed in as requester ldap.worker.groups ldap.user.attribute.email mail ldap.user.attribute.firstName givenname ldap.user.attribute.lastName sn ldap.user.attribute.country country ldap.search.subtree ldap.search.timelimit The time limit for user and groups search in milliseconds. If the value is 0, this means to wait indefinitely. 0 ldap.search.dereflink if true links will be dereferenced; if false, not followed. false ws.sso.saml.entity.base.url See SAML Integration for reference. none ws.sso.saml.response.skew 60 ws.sso.saml.enable false secure.storage.safe.customer.default See WorkSpace Secure Properties for SMS config localWFApp secure.storage.safe.internal localWFInternal secure.storage.type LOCAL secure.storage.serverApi secure.storage.platformId WFApplication secure.storage.client.certificate secure.storage.client.keyPass secure.storage.username.customer Sample workspace.properties file Sample workspace.properties Expand source ws.datasource.connection pool.dataSourceClassName=com.microsoft.sqlserver.jdbc.SQLServerDataSource ws.datasource.connection pool.minimumIdle=5 ws.datasource.connection pool.maximumPoolSize=40 ws.datasource.connection pool.connectionTimeout=3000 ws.datasource.connection pool.validationTimeout=300 core.defaultPendingAssignmentsLimit=10 core.abandonAssignmentsJobChunkSize=100 core.defaultSkippedAssignmentDurationInSeconds=600 web.workerMessageReceiverUrl=https: localhost mturk web rest feedback email.host=localhost email.port=25 email.from=crowdsupport2@workfusion.com core.socialApplicationUrl=https: localhost virtualizer web core.instanceIdentifier=LCLPROD core.hitSortingStrategy=Category web.manualBalanceUpdateAllowed=True web.tourEnabled=true web.socialPostSignInUrl= categories core.useFastHitLookupStrategy=true mobile.downloadPageUrl=https: s3.amazonaws.com cv mobile index.html password.resetPasswordTokenExpirationTimeInMinutes=15 core.templateFilesExternalLocation= opt cv templates core.countAvailableHitsUpTo=100 web.resourceUrl=https: s3.amazonaws.com workspace resources2 ldap.enabled=false ldap.internal.authorization.enabled=false ldap.user.filter=(uid={0}) ldap.group.base=ou=Groups,dc=crowdcomputingsystems,dc=com ldap.group.filter=(memberUid={1}) ldap.worker.groups=CC Dev ldap.requester.groups=CC Req ldap.user.attribute.email=mail ldap.user.attribute.firstName=givenname ldap.user.attribute.lastName=sn ldap.user.attribute.country=country ldap.user.base=ou=People,dc=crowdcomputingsystems,dc=com ldap.search.subtree=true ldap.search.timelimit=0 ldap.search.dereflink=false secure.storage.safe.customer.default=local_WFApp secure.storage.safe.internal=local_WFInternal secure.storage.type=LOCAL secure.storage.serverApi= Users workfusion Documents Work localsecurestorage secure.storage.platformId= {catalina.home} conf secure.properties secure.storage.client.certificate= secure.storage.client.keyPass= secure.storage.username.customer= "},{"version":"10.0","date":"Aug-14-2019","title":"workspace-customization","name":"WorkSpace Customization","fullPath":"iac/core/workspace/admin/workspace-customization","content":" Localization To learn more about WorkSpace built in localization mechanism, refer to WorkSpace Localization. This mechanism can also be used for changing the titles or wording of WorkSpace messages and menu. Customization Change Logo If it is required to replace image at resources img workspace.png (or at resources img workfusion workspace.png for versions earlier than 9.0). In the same directory where workspace.war is located create the resources img directory and put workfusion workspace.png or workspace.pnglogo in it) Run the following command: jar uvf workspace.war resources img workspace.png Link to manual Use web.resourceUrl application property. Manual should be located at web.resourceUrl help help.html Tour display on first sign Use web.tourEnabled application property. Display of company email in footer Use web.displaySupportEmail property. It can be switched off. If you want to change company email, project rebuild is required. Footer twitter link Use web.twitterLink application property. It's not displayed if the value is 'null'. "},{"version":"10.0","date":"Aug-14-2019","title":"workspace-secure-properties","name":"WorkSpace Secure Properties","fullPath":"iac/core/workspace/admin/workspace-secure-properties","content":" Connection to secure properties is set up in the workspace.properties file. Set the following properties: secure.storage.safe.customer.default secure.storage.safe.internal secure.storage.type secure.storage.serverApi secure.storage.platformId secure.storage.client.certificate secure.storage.client.keyPass secure.storage.username.customer For development purposes, secure storage can be a plain text file. In this case secure.storage.type should be set to LOCAL, secure.storage.serverApi should be a path to any empty folder (WorkFusion won't work in this case) and secure.storage.platformIdshould be a path to file with secure properties. Sample secure properties section in workspace.properties Secure properties block in workspace.properties secure.storage.safe.customer.default=local_WFApp secure.storage.safe.internal=local_WFInternal secure.storage.type=LOCAL secure.storage.serverApi= Users workfusion Documents Work localsecurestorage2 secure.storage.platformId= {catalina.home} conf secure.properties secure.storage.client.certificate= secure.storage.client.keyPass= secure.storage.username.customer= List of secure properties Property Notes Default Required ws.datasource.username Username for MsSQL Database connection true ws.datasource.password Password for MsSQL Database connection true ws.secure.jwt.secret Not used false ws.secure.ldap.server.url dc=not defined false ws.secure.ldap.bind.dn not defined false ws.secure.ldap.bind.password not defined false ws.sso.saml.idp.metadata See SAML 2.0. none false ws.sso.saml.idp.file.metadata none false ws.sso.saml.username.attribute mail false ws.sso.saml.sp.metadata none false ws.mail.username false ws.mail.password false Related articles: Secrets Vault Manage secure properties "},{"version":"10.0","date":"Aug-06-2019","title":"changing-profile-settings","name":"Changing Profile Settings","fullPath":"iac/core/workspace/worker/changing-profile-settings","content":" Profile Menu The user profile menu enables you to edit your profile information and log out. To access your user profile, select My Profile from the user profile menu in the upper right corner. Your profile page contains both read only and input fields. Read only fields: Your ID Profile Name Input fields: Email – the email address used for notifications and authorization Locale Name Last Name Organization Time Zone Passwords (Current, New Password, Repeat New Password) To change your password, you must enter your current password together with a new one, and repeat the new password to confirm. Receive email notifications Setting Interface Language WorkSpace automatically detects your browser's language and sets the application interface language from the following list: English (default) Russian Japanese "},{"version":"10.0","date":"Aug-06-2019","title":"getting-started","name":"Getting Started","fullPath":"iac/core/workspace/worker/getting-started","content":" Register, verify your account by clicking the link in the registration email, and log into WorkSpace. Search for Tasks Type the keywords (e.g. Classification, Translation, Information Extraction) in the search field and review the search results. Or drill down to Task categories and sub categories that fit your skills and interests. If you haven't found available Tasks, try to obtain different Qualifications. Workers with Qualifications can access the bigger number of Tasks with higher reward. Work on Tasks If you have found Tasks with suitable subject, complexity, reward, and allotted time, perform the following actions: Carefully read all the Task instructions. Accept the Task and start working on it. Watch the timer to complete the Task in time. Submit the Task when you are done. You can accept and submit other Tasks of the same group in succession, no need to search them again. "},{"version":"10.0","date":"Aug-06-2019","title":"obtaining-qualification","name":"Obtaining Qualification","fullPath":"iac/core/workspace/worker/obtaining-qualification","content":" Worker Qualifications control which Tasks are available to you, and what Tasks could become available to you once you obtain the correct Qualifications. Each Qualification has a Score parameter (from 0 to 100) showing your skills in the appropriate area. To access interesting well paid Tasks, obtain different available Qualifications with maximum Score. My Qualifications To obtain a Qualification, you can request a Qualification or complete a certain number of Qualification Tasks. Requesting Qualification Go to Qualifications >* Available for me*. Click the appropriate Request it or Accept Agreement link. Pass the Qualification Test or accept the Agreement. Available Qualifications In some cases Qualifications are auto granted and you do not need to pass a test. Auto Granted Qualifications Searching and Completing Qualification Tasks If you have found a Task that requires a Qualification, search the Qualification by copying and pasting its name to the Search field. If you do not have the Qualification required for a Task, the View link is inactive and you cannot access it until you get this Qualification. Task that Requires Qualification Complete the required number of Qualification Tasks and get the Qualification. Qualification Task The Qualification Score depends on the number of correct Answers. You must submit all the required Qualification Tasks to obtain the Qualification. (20 in the example above) Alternatively, you can search Tasks with the following keywords: test, qualification, and complete them. "},{"version":"10.0","date":"Aug-06-2019","title":"previewing-and-assigning-tasks","name":"Previewing and Assigning Tasks","fullPath":"iac/core/workspace/worker/previewing-and-assigning-tasks","content":" Previewing Tasks Assigning Tasks These features can be unavailable for some tasks or totally disabled by WorkSpace administrator. If you are using WorkFusion version less than 8.0, see this version of the same page. Previewing Tasks > Download this video (12 > MB): https: pub _demo.s3.amazonaws.com tutorial videos release 7.4 RN%20 %20WorkSpace%20Preview.mp4 To pick a particular Worker Task from a Task type, click the Show Tasks button: A grid containing all Tasks of the same type is displayed: You can do the following actions: View a particular Task by clicking the appropriate link and then accept it. If a grid contains column(s) with links to images or documents (like the Document Preview column on the screenshot), you can open these links without viewing the whole Task. Results can be filtered by using the Search box above the table. The search field accepts keywords as well as more advanced search terms, such as column specific filtering (e.g. Type: Loan to filter the results based on the term \"Loan\" within the \"Type\" column). Refresh grid to display only currently available Worker Tasks. Assigning Tasks > Download this video (6 > MB): To assign a particular Worker Task to a person do the following actions: find your Task in the Tasks menu or using the search function; click the Show Tasks button to display all available Tasks of the same type; select Worker Tasks by ticking appropriate checkboxes; click the Assign button – a popup will appear. select a Worker from a dropdown list; (optionally) add a Task comment if needed; click the Assign button; As a result, the Worker's name will be displayed in the Assigned To column. If a Task is assigned to you, it means you are expected to accept this Task and submit it. You can also make Tasks unassigned by selecting the unassigned option in the Worker dropdown. Assigned Tasks can be re assigned to another person or unassigned. A Worker Task can also be assigned, reassigned, or unassigned from the Task Preview screen: All the Task comments are displayed under the task, and you can add a new one if needed: "},{"version":"10.0","date":"Aug-06-2019","title":"working-on-tasks","name":"Working on Tasks","fullPath":"iac/core/workspace/worker/working-on-tasks","content":" After you have found an appropriate Task, you need to accept it before start completing the Task. Accepting Tasks Carefully read all the Task instructions. You can expand or collapse the instructions section by using the Click to show hide instructions link. Accept the Task by clicking the Accept button in the top right corner and start working on it. All accepted Tasks are displayed on your Dashboard as Active Tasks. Click the Automatically accept the next task checkbox, to accept all tasks and have them load automatically after you submit your answer. Accepting and Submitting Tasks You have an option to Skip a Task by clicking the appropriate button. In that case, the Task will not be available for some time and you will be able to accept other Tasks in the same Task Group. Executing Tasks Task Actions Panel After you have accepted the Task, the Task Timer starts. Watch the timer to complete the Task in time. If you cannot complete a Task (lack of time, Task is too complex, problems with internet, etc.), click the Return button. It is better to return a Task than to submit wrong or incomplete answers. Requester can reject inconsistent Tasks, and your Approval Rate will be reduced. Workers with a low Approval Rate cannot access interesting well paid Tasks. If you have any questions about the task, click the Send Feedback button and enter your name and message. Worker Feedback on Task Submitting Tasks When you have provided all the required Task answers, check yourself and click the Submit button in the bottom left corner. Your submission will be reviewed and approved or rejected. You will receive a notification in your Messages and on your Dashboard. Only approved Tasks are paid. You can check your statistics and earnings on your Dashboard. You can accept and submit other Tasks of the same Task Group in succession, no need to search them again. "},{"version":"10.0","date":"Aug-06-2019","title":"working-with-dashboard","name":"Working with Dashboard","fullPath":"iac/core/workspace/worker/working-with-dashboard","content":" Active Tasks Messages Statistics Tasks Assigned to Me Available Tasks The Dashboard provides a summary of the significant events associated with your user profile and by default is the first screen displayed when logging in to WorkSpace. Active Tasks The Active Tasks section displays the tasks that you are currently working on. They are tasks that you've accepted and have not submitted yet and can be revisited (until they expire). You can keep track of active tasks by using the Time left field which is dynamically calculated based on the expiration time set for the Task. Messages WorkSpace enables messaged communication so that inquiries can be answered or notification about Tasks and Qualifications can be made. To set the way in which messages are filtered, click the appropriate filter button (All messages, Unread only). Statistics WorkSpace accumulates statistics based on the Tasks you have accepted and submitted. The User Dashboard also lists the total amount of hours worked. You can filter your statistics based on the time period that best suits your needs. To set the way in your statistical data is filtered, click the appropriate filter button (All, Last 24 hours, Last week, last 30 days, Past month). The following panels can be disabled by WorkSpace administrator. Tasks Assigned to Me This panel contains all the Tasks assigned to you. To start working on these tasks you need to click an appropriate link and accept it. You can also reassign any Task or make it unassigned. Available Tasks This panel indicates the total number of available Tasks and the number of unassigned Tasks vs. assigned Tasks. "},{"version":"10.0","date":"Aug-06-2019","title":"worker-manual","name":"Worker Manual","fullPath":"iac/core/workspace/worker/worker-manual","content":" This manual covers the WorkSpace basic functionality from the Worker point of view and describes the following topics: search and complete Tasks qualify for Tasks that require specialized restrictions See the following topics: Getting Started Searching for Tasks Previewing and Assigning Tasks Working on Tasks Obtaining Qualification Working with Dashboard Deprecated Getting Paid Changing Profile Settings "},{"version":"10.0","date":"Aug-06-2019","title":"searching-for-tasks","name":"Searching for Tasks","fullPath":"iac/core/workspace/worker/searching-for-tasks","content":" Searching A Task is a web form with questions that you need to answer and submit. Workers accept Tasks, begin working, submit an answer, and collect a reward for completing. Task Group is a set of Tasks with the same parameters (Reward, Qualifications, Time Allotted, etc.) and questions. Tasks in a Task Group vary only by the input data. For example: Task Group: Find organization official website Task 1: Find the official website for the American Red Cross Task 2: Find the official website for the UNICEF Task Groups are joined into Sub Categories, and Sub Categories (Video Tagging, Identify Number of People from Image) are joined into Categories (Media Tagging, Translation, Information Extraction). Searching Task Categories and Search Navigate to the Tasks in the top menu. You can start searching Tasks by doing one of the following actions: Type the keywords (e.g. Classification, Translation, Information Extraction) in the search field, click the Search button, and review the results. Or click Task categories or sub categories that fit your skills and interests. The number next to Category name shows the amount of Tasks available for you in this Category. You will see the list of Task Groups. Expand the Task Group by clicking its name or by clicking the Expand all link. Tasks Expanded View If the Task Group Description, Time Allotted, Reward, and other parameters are suitable for you, click the View link in the right corner. If you haven't found available Tasks, try to obtain different Qualifications. Workers with Qualifications can access the bigger number of Tasks with higher reward. "},{"version":"10.0","date":"Aug-06-2019","title":"advanced-settings","name":"Advanced Settings","fullPath":"iac/core/workspace/requester/advanced-settings","content":" To open Advanced Settings, click the user name link in the top right corner and select an item from the dropdown menu. Advanced Settings Advanced Settings contain the following items: My Profile Custom Profile Fields Balance Application Settings Terms and Conditions Worker Channels Deprecated Financial Accounts "},{"version":"10.0","date":"Aug-06-2019","title":"application-settings","name":"Application Settings","fullPath":"iac/core/workspace/requester/application-settings","content":" On the Application Setting page (available for Requester User in his context menu on right top corner), you can enable disable different payment, worker, sign in, notification, and other options. Application settings are as follows: Skip operation: Worker can perform \"Skip Task\" action on Available tasks Return operation: Worker can perform \"Return Task\" action on already Accepted task Show \"Auto accept\" checkbox on the task view Worker registration: Self registration is available for new Workers Show Agreement License on registration: Check to include Terms and Conditions on registration See non qualified tasks: Worker is allowed to see non qualified tasks Show empty categories: Display categories without tasks to Worker Override email notification settings: If enabled worker's notification settings are overridden Allow workers to receive email notifications Show \"Remember Me\" on login Show Task Type Preview: Allow a Worker to pick a particular Task from a pool Enable Task Notifications Show Task Assignment: Allow a Worker to assign a particular Task to another Worker Show Task Assignment History "},{"version":"10.0","date":"Aug-06-2019","title":"how-to","name":"How To","fullPath":"iac/core/workspace/requester/how-to","content":" Hiding Zero Reward from the Tasks List Changing Currency of the Tasks Reward Translating UI Paying a Worker Hiding Zero Reward from the Tasks List Advanced Settings > Application Settings > Show empty categories Changing Currency of the Tasks Reward Advanced Settings > Worker Channels > Currency Symbol Translating UI WorkSpace automatically detects your browser's language and sets the application interface language from the following list: English (default) Russian Japanese To add a new translation, contact the developers. Paying a Worker Enable PayPal for WorkSpace Advanced Settings > Application Settings > Enable PayPal Change Worker Payment System to PayPal Workers > View > Profile > Payment Preference Enter and verify the PayPal credentials Deprecated Financial Accounts > PayPal Account (optional) Change Payment System for virtual paymentsPayments > Select Payments > Change Payment System Approve PaymentsPayments > Select Payments > Approve You can skip this required action in Advanced Settings > Application Settings > Payments Approval Required. "},{"version":"10.0","date":"Aug-06-2019","title":"managing-assignments","name":"Managing Assignments","fullPath":"iac/core/workspace/requester/managing-assignments","content":" Assignment is an object that is created when a Worker accepts or skips a Task. Assignments serve as connections between Workers and Tasks. Assignment Lifecycle Initially Workers can view all available Tasks (if they have required Qualifications). When a Worker has accepted a Task, a new Assignment is created and nobody can accept this Task until it will be Abandoned or Returned. If a Worker has skipped a Task, this Task will not be shown to this Worker during timeout (10 minutes for now). If an Assignment is Approved, it can become a part of a Payment. Assignments Lifecycle When as Assignment is Abandoned, Returned, Skipped, or Deleted, the Task itself is not removed and is still available for other Workers. Assignments Page To start managing Assignments, click the Assignments item in the top menu. You will see the following grid with a filter: Managing Assignments On the Assignments page, you can perform the following actions: filter Assignments You can filter by Title, Accept Time, Expiration Time, Worker Name, and Status. To apply filters, click the Refresh button. view Assignment parameters To expand and view the HIT Keywords, Price, and Username, click the corresponding arrow in the leftmost column. preview Tasks Click the HIT Title link, and the Task preview will open in a new tab. Return back to work queue Select Assignment(s) and click this button to return accepted Tasks (take away the Task from the current Worker and make it available for other Workers). "},{"version":"10.0","date":"Aug-06-2019","title":"managing-licenses","name":"Managing Licenses","fullPath":"iac/core/workspace/requester/managing-licenses","content":" Versioning Info The functional scope described in this topic is available since WorkFusion 8.4.5 Since WorkFusion 8.4.5 the access and secret keys have been moved from the Requester to a new entity – License. This approach is more consistent and platform oriented, as it allows a few Requesters to manage data (Qualifications) for the same license Migration to 8.4.5 During migration from older versions a new license is created with the same name for each Requester. The access key, the secret key and the license id are set in the account. Managing Licenses A Requester manager can create, edit license on the new Licenses tab. A name and access secret keys should be set for the license. Once set, the access and secret keys cannot be updated (same as previously for a Requester): Additionally, a mandatory license field is available for Requesters. The respective license should be selected there while creating a new Requester. Useful links For more information you can refer to Requester Manual. "},{"version":"10.0","date":"Aug-06-2019","title":"custom-profile-fields","name":"Custom Profile Fields","fullPath":"iac/core/workspace/requester/custom-profile-fields","content":" On the Custom Profile Fields page, you can enable additional fields for Worker Profile. Each custom field can be added with the following options: Enabled – the field is added to the Worker Profile as an optional one. Required – the field is added to the Worker Profile as a required one. Request on Registration – the field is added to the Worker Profile as a required one and is requested on registration. For the Required and Request on Registration fields, check the Enabled option too. Custom Profile Fields Tick the necessary checkboxes and click the Preview button. If the preview result is fine, close it and click the Update button. "},{"version":"10.0","date":"Aug-06-2019","title":"managing-requesters","name":"Managing Requesters","fullPath":"iac/core/workspace/requester/managing-requesters","content":" Requester with manager role can manager requesters. After login with requester account go to Requesters tab. Create edit enable disable operations are available here If tab is not available it means requester doesn't have manager role OR old version of WorkSpace is used. There are 2 ways to add manager role to requester: execute database script. ask another requester manager to add this role by checking \"Requester Manager\" checkbox "},{"version":"10.0","date":"Aug-06-2019","title":"managing-teams","name":"Managing Teams","fullPath":"iac/core/workspace/requester/managing-teams","content":" A Team is a group of Workers that can be assigned different Qualifications. To start managing Worker Teams, click the Teams item in the top menu. You will see the following grid: Managing Teams On the Teams page, you can perform the following actions: view edit Teams (Name, Number of Workers) add Teams Click the Add button. remove Teams Select Team(s) and click the Remove button. assign Qualification to selected Teams Select Teams(s) and click the Assign button. All the Team Workers will have the selected Qualifications. Editing a Team Editing a Team To edit a Team: On the Teams page, click the appropriate Team Name link. Add or Remove Workers. To navigate to the Worker Profile, click the appropriate Worker Email link. On the Team Info tab, edit the Team Name . Click the Save button. "},{"version":"10.0","date":"Aug-06-2019","title":"my-profile","name":"My Profile","fullPath":"iac/core/workspace/requester/my-profile","content":" Personal Account On the My Account page, you can view the following info: Access Key ss Secret Key E Mail You can also view and edit the following fields: Name Last Name Time Zone Password (Current, New, Repeat New) To change your password, you must enter all the 3 passwords: your current password, the new one, and repeat the new password. When you have finished editing your account info, click the Update button. "},{"version":"10.0","date":"Aug-06-2019","title":"managing-workers","name":"Managing Workers","fullPath":"iac/core/workspace/requester/managing-workers","content":" To start managing Workers, click the Workers item in the top menu. You will see the following grid with a search field: Managing Workers On the Workers page, you can perform the following actions: view Worker parameters (ID, Name, Organization, Status, Qualifications) search Workers You can search Workers by their ID, Name, or Organization. view edit Worker Profile, Teams, Qualifications History Click the appropriate View link in the right column. add Workers Click the Add button. assign Qualification to selected Workers Select Worker(s) and click the Assign button. remove Workers Select Worker(s) and click the Remove button.* * enable disable Workers Disabled Workers cannot log in and accept Tasks. generate Report in a CSV file This report displays all the Worker Assignments with payment details over a certain period. Adding a Worker Create Worker To add a new Worker: On the Workers page, click the Add button in the top right corner. Fill the worker Profile fields. (Optional) Add Worker Teams and or Qualifications. Click the Create button. To edit the Worker Profile: On the Workers page, click the appropriate View link. Edit fields on Profile, Teams, or Qualifications tab. You can also review the Qualifications History tab. Click the Update button. "},{"version":"10.0","date":"Aug-06-2019","title":"managing-qualifications","name":"Managing Qualifications","fullPath":"iac/core/workspace/requester/managing-qualifications","content":" Qualifications are a means that the WorkFusion platform provides to ensure the quality of Cloud Workers and narrow your Workforce for optimal performance. Qualifications can be assigned to Workers manually, automatically by Qualification Rules, or Workers can request some Qualifications. For more information, see the following topics: Qualification Create and Assign Qualification Create a Qualification Task To start managing Qualifications, click the Qualifications item in the top menu. You will see the following grid with a search field and a filter: Managing Qualifications On the Qualifications page, you can perform the following actions: view Qualifications (Name, Keywords, Author, Description, Auto Granted) search Qualifications** You can search Qualifications by their Name or Keywords. filter Qualifications (All Custom, My, System) view edit Qualification Info and view Change History Click the appropriate View link in the right column. add Qualification Click the Add button in the top right corner. remove Qualification Select Qualification(s) and click the Remove button. Adding a Qualification Create Qualification To create a Qualification: On the Qualifications page, click the Add button. Enter the Qualification info in the appropriate fields. Click the Save button. Name and Keywords modification is not available for a saved Qualification. "},{"version":"10.0","date":"Aug-06-2019","title":"terms-and-conditions","name":"Terms and Conditions","fullPath":"iac/core/workspace/requester/terms-and-conditions","content":" On the Terms and Conditions page, you can: view the actual Terms and Conditions for WorkSpace upload a new Terms and Conditions document in HTML, TXT, or PDF format Terms and Conditions Workers will read and accept this document during the registration. "},{"version":"10.0","date":"Aug-06-2019","title":"requester-manual","name":"Requester Manual","fullPath":"iac/core/workspace/requester/requester-manual","content":" WorkSpace is a crowd sourcing application intended for publishing and submitting Tasks and managing your Workforces. Task creation is done via the Control Tower application. See the Control Tower documentation for detailed info on Task creation. You also need to create a WorkSpace License in WorkFusion. Once the License has been created, you get access to the following environments: Sandbox – https: worker provider name.workfusion.com workspace sandbox Production – https: worker provider name.workfusion.com workspace Who Is Requester Generally, a Requester plays a role of a WorkSpace Administrator who can create edit Workers and Qualifications, change application settings, and generate reports. Before 8.4.5 In addition, a Requester links the Control Tower License to WorkSpace. The Control Tower License has the access key and the secret key; the same fields are defined in the WorkSpace Requester as it's designed iin Amazon Mturk. It works well with a single Requester, but produces some issues when a few Requesters are required . Mostly the issues are related to the case, that a Requester is able to manage only his her Qualifications. From 8.4.5 The License entity has been introduced. Now both Control Tower and WorkSpace have separate License objects and the connection is more obvious. Qualifications are related to the License now and multiple Requesters can access the same License. How It Works Design a Business Process, Task, or Qualification Task in Control Tower. Select a Workforce containing WorkSpace Workers. Set the Task Reward. Pay attention to the Environment (Run in Production option). Run your Business Process or Tasks – the Tasks will be published to WorkSpace. Workers start viewing and accepting Tasks. Workers submit Tasks for review. WorkFusion engine compares Worker answers according to the Adjudication rule and approves or rejects submitted Tasks. The payment is made only for the approved work. Optional: Depending on the Application Settings, you may need to approve or disapprove Payments. See the How To section for the information about basic WorkSpace use cases. "},{"version":"10.0","date":"Aug-06-2019","title":"worker-channels","name":"Worker Channels","fullPath":"iac/core/workspace/requester/worker-channels","content":" Note. Since version 10.0 all external integrations are removed On the Worker Channels page, you can set the following channel parameters: Fee – service fee for posting Tasks (e.g. for 5%, enter 0.05). Currency Symbol – Worker will see this symbol next to the Task Reward. Default = US Dollars ( ). Exchange Rate – actual currency exchange rate relative to US Dollar. Worker Channels Currently the following channels are available: USAMP ELANCE REAL _WORLD NATIVE CLICK _WORKER UPWORK "},{"version":"10.0","date":"Oct-04-2019","title":"business-process-modelling","name":"Business Process Modelling","fullPath":"iac/core/delivery-manager/business-process-modelling","content":" Introduction One of the most powerful weapons is the process map. As professionals, we are tasked with creating efficient and useful tools for our organizations, often with little understanding of the underlying business process. Often we are collaborating with customers who don’t understand their own business processes, much less the upstream and downstream processes. A process map can go a long way to building that understanding while highlighting problems, miscommunications, gaps, redundancies, workarounds, rework loops and waste. In short, process maps show us “what” we do, “how” we do and “where” we do it. And above every process map should hang a huge sign that reads: “Why ” Understanding how to create and analyze business processes is a skill that can benefit anyone, in any role, in any industry. Business Process Defined “Processes are how people within an organization collaborate in order to accomplish a goal. Essentially everything we do in an organization involves or contributes to some type of process.“ A business process has typical characteristics: It exists to meet a specific business need Collaboration between multiple people or groups Takes place over a period of time Often has more than one iteration ¾ is repeatable. So what is a process map Process mapping consists of a collection of tools and methods used to understand an organization and its processes. Those tools allow us to document, analyze, improve, streamline and redesign business processes to realize organizational efficiencies. A process map is a visual aid for picturing work processes and shows how inputs and tasks are linked and highlights the steps required to consistently produce a desired output. A process map encourages new thinking about how work is done, where it is done, who performs it, what problems frequently occur and how best to solve them. A common analogy relates to road maps: you can’t plot a route to get to where you want to go…unless you know where you are. Process models and maps can also be used to identify appropriate Process Modelling & Mapping: The Basics quality improvement team members, identify who provides inputs or resources to whom, establish important areas for monitoring (critical control points) or data collection and to identify areas for improvement. Flowcharts can help us understand the flow of a wide variety of things: information, documentation, forms, patients, products, supplies, customers or employees. Benefits of using Process Maps Benefits of process maps: Enables everyone to visualize (see) the process in the same way Acts as a training and educational tool for new and existing staff and help reduce procedural errors Focuses stakeholders on the process itself Builds understanding between cross functional work areas Provides a “current state” upon which to base future improvements Identifies objective measurements and metrics for ongoing evaluation and future improvement activities Identifies existing workarounds, rework loops and information gaps Illustrates opportunities for improvement Improves compliance with, or provide documentation for, quality and regulatory standards (SOX, C SOX, CCHSA, JCAHO, OH&S, etc) Business Process Modelling Tools Process Modelling levels Level one: this very high level map outlines the operational levels of an organization and are rarely, if ever, actually drawn. Examples include: customer processes, administrative processes. Level two: shows end to end processes across the above operational areas. For example a level two process for purchasing capital equipment would cross several operational areas: the requesting department, purchasing, accounts payable, asset management, receiving and biomedical maintenance. Also called top down or high level process maps. They are quick and easy to draw, but may not provide the detail necessary to build understanding or realize improvements. Level three: shows the roles, inputs, outputs and steps required to complete a specific process within an operational area. For example, the purchasing process (request, sourcing, cut PO) might be depicted as a process map. Also called cross functional or deployment diagrams. Usually contain enough information for improvement efforts, but often miss inefficient details and don’t function well for training or as operational documentation. Level four: is the documentation of systems, instructions and procedures required to complete steps in the level three processes and shows inputs, outputs, associated steps and decision points. For example, specific steps necessary to cut a PO in the enterprise application would require a level four process map. The procedures and system instructions can be represented as text, an algorithm or detailed process map. Because of the level of detail, they can be resource intensive to create, but offer the greatest improvement potential. Since they illustrate decisions and subsequent actions, they are excellent training and reference materials. But if your organization is new to process mapping, use these sparingly. The time and effort may turn stakeholders off before they’ve had a chance to experience the benefits of the work. Process Maps Choose the type of process map appropriate for your specific goal. What follows is a description of different types of process maps, when to use each, and a brief outline of how to create them. What is common to all of them is the “process oriented approach”. This approach: Defines the process first Specifies the customers of the process Defines and refines the requirements for all process customers Specifies the steps involved in getting something accomplished Indicates the sequence of steps Top Down Process Maps Top Down or High level process maps (Parking Lot diagrams to some) are good for illustrating the major clusters of activity in a process; those key steps essential to the process while limiting the amount of detail documented. The top down flowchart starts with the major steps drawn horizontally. The detail is provided in numbered sub tasks under each major task. The top down flowchart does not show decision points or rework loops and does not show the same detail as a cross functional chart or detailed flowchart. How to Draw Top Down Diagrams (Level 2) List the most basic steps in the process. Limit yourself to no more than 5 or 6 basic steps and place them in order horizontally across a whiteboard, flip chart or large piece of paper Under each step, list the sub steps that make up that element, list them in the order they occur and again, limit yourself to 5 or 6 sub steps Cross Functional or Deployment Diagrams Use cross functional flowcharts to show the relationship between a business process and the functional units (such as departments) responsible for that process. Bands represent the functional units. Shapes representing steps in the process are placed in bands that correspond to the functional units responsible for those steps. You can represent a process either vertically or horizontally. A vertical layout, where bands representing the functional units run vertically from the top to the bottom of the page, places slightly more emphasis on the functional units. In a horizontal layout bands representing functional units run horizontally across the drawing page, highlighting the process. These charts emphasize where the people or groups fit into the process sequence, and how they relate to one another throughout the process. Cross functional or deployment charts are excellent tools for illustrating how a process flows across organizational boundaries. They can be very effective at identifying delays, redundancy, excessive inspection, rework and potential points of process failure. How to Draw a Cross Functional Diagram (Level 3) Gather together a team representing the different functional areas Place a large piece of paper on a wall or flat surface Identify your process stakeholders players (people or functional areas) Down the left hand side of the paper starting with the process customer at the top, list the process players in order based on the closeness of their relationship to the process customer. Draw horizontal lines between each process stakeholders, using a double line if they are external to your organization (customer, supplier, regulatory body). These are referred to as swim lanes and may also be used to represent different roles in a process or a key piece of software. The bottom axis is time and moves from left to right Write out the process steps in order and on the line of the person functional area that performs that task, moving from left to right as time elapses As you draw boxes, connect them with lines and arrowheads showing the direction of input. Concurrent activities should be aligned vertically and shared activities (between people or functional areas) should be drawn on the swim lane (when possible). Alternatively you can also write the process steps on 'sticky” notes and place them on the map. You can then move them around until the team members are satisfied that the steps are properly identified and in the correct order. Finish by adding labels, decisions, and arrows (showing direction of input) to the map. Detailed Flowcharts Flowcharts are maps or graphical representations of a process. Steps in a process are typically described within a specific set of symbols, which communicate ‘what’ or ‘where’ something is happening. There are literally hundreds of different symbols to choose from, but most experienced process analysts will recommend that you choose a handful of basic symbols and stick with them. Accurate flowcharts can be created using very few symbols (e.g. oval, rectangle, diamond, delay) and the International Standards Organizations standard 9004.4 recommends just four. A chart of commonly used symbols is shown on the right. In quality improvement work, flowcharts are particularly useful for establishing a common understanding of a process, or allowing people to “see” the process in the same way. Displaying the “current state” of a process helps identify illogical flows, potential miscommunications, redundancy, rework, delays, dead ends, and missing critical control points that might otherwise go unnoticed. Another methodology, from the Ben Graham Group (www.worksimp.com) is a very effective method of collecting data and drawing detailed process charts. Ensure that you are working with the people actually doing the work. Managers will often want to be involved in process work, and their support is critical. But unless they do the tasks on a day to day basis, they can often be more of a hindrance than help. Ensure team members feel safe in sharing and understand that this isn’t a fault finding mission. Their description of the process as it is, is critical, good, bad or otherwise Decision symbols are appropriate when those working in the process make a decision that will affect how the process will proceed. For example, when the outcome of the decision or question is YES, the person would follow one set of steps, and if the outcome is NO, the person would do another set of steps. Be sure the text in the decision symbol would generate a YES or NO response, so that the flow of the diagram is logical In deciding how much detail to put in the flowchart (i.e., how much to break down each general step), remember the purpose of the flowchart. For example, a flowchart to better understand the problem of long waiting times would need to break down in detail only those steps that could have an effect on waiting times. Steps that do not affect waiting times can be left without much detail. Detailed Flowcharts How to draw a detailed process flow chart (Level 4) Describe the process to be charted and define the process boundaries. (scope) Always start with the process trigger (business need that drives the process) You may want to start by determining major and minor inputs into the process with a cause & effect (Ishikawa) diagram or using the six sigma SIPOC tool Complete the big picture before filling in the details, often by sketching a cross functional diagram first Keep the descriptions concise. If necessary, cross reference other maps or documentation If you are the facilitator, make generous use of the Socratic dialogue. Ask lots of questions. Ask lots of ‘why’ questions. It is best not to facilitate mapping your own process. If the participants are able to describe the process in a way that makes sense to someone who knows nothing about it...then you are probably capturing enough detail Note down each successive action taken. Actions should be clearly described in as few words as possible Pay attention to the questions. These are often critical control points: places in the process where multiple alternative flows appear, based on questions, inspections etc. When you are charting a branch, always follow the most important alternative to flow chart first. Then go back and complete the remaining flows Validate the process chart with others involved in the process. 9. Identify responsibility for each step Process Analysis Processes are rarely designed intentionally and they all evolve over time. Throughout this evolution, inefficiency, redundancy and waste creep in. It is critical to avoid finding fault with a specific process: recognize that people and departments make changes in an effort to get the job done, often without knowledge of how those changes might impact upstream and downstream processes. The performance of individuals is only as good as the process will allow it to be. Processes, especially cross functional business practices, are usually not documented, not standardized, not measured, not systematically and continually improved and not managed by the micro process doer or owner. Analyzing Process Maps Processes evolve (or devolve) over time as staff or business conditions change and in response to other changes in the organization. This evolution often results in growing process complexity. The trouble spots in a process usually begin to appear as a team constructs a detailed flowchart. Begin by challenging the necessity of each process step. Is this step redundant Does it add value to the product or service Is it problematic Could errors be prevented in this activity How Does this process step physically change the product Is it done right the first time Is it another inspection, control point or approval Is it required to meet a regulation or legislation If you stopped doing this step, would your customer even notice Asking these questions will help determine if the process step adds value for your customer or your organization. Combine, simplify or eliminate activities that do not contribute value. Watch for over inspection or multiple approvals in your process as they may suggest a lack of confidence in the process. Eliminate control steps that are not critical for quality outcomes Identify time lags or delays in the process. Look for ways to eliminate those delays Watch for multiple hand offs and specialized work areas. Who is involved What could go wrong Is the output of one area meeting the input needs of the next Could 30 seconds of effort at point A save 3 minutes of effort at point B or service meeting the needs of the next person in the process Each time a process crosses organizational boundaries, there is increased potential for delay or miscommunication. Consider opportunities to increase the scope of functional areas to allow for more work to be done in a specific area. Some efficiency proponents argue that reducing specialization of tasks can lead to decreased turnaround times, fewer errors and more efficient processes of work tasks. Cellular work areas are taking the place of assembly line processes, both in operational and administrative work environments. Communication is improved, silos are taken down and the workforce is better trained and more flexible to upcoming changes Examine decision points. Does it evaluate an activity to see if everything is going well Is it effective Is it redundant Is it designed to catch an error that could be prevented (or caught) earlier in the process Look carefully at rework loops and quality control points. Does this rework loop prevent the problem from recurring How is feedback from this loop returned to the area of origin Could the problem be prevented or caught earlier in the process Are repairs being made long after the step where the errors originally occurred Are their methods for preventing the problem that could be employed Look hard at each document symbol and ask if this document or data entry is necessary Where does the form or data go afterward Is it up to date Is it redundant Is there another way of capturing this data Is there a single source for the information Could this information be used for monitoring and improving the process Evaluate each Delay symbol. Is the delay necessary For what Some delays are valid, i.e. “delay while resin cures,” or “wait for Lab results” but many are not. Ask, “How long is the wait” Can it be eliminated or reduced Examine the overall process: Is the flow logical Are the steps in the right order Could some work be done in parallel to reduce overall processing time > Are there places where the process leads off to nowhere Is there repetition Other Questions for Analyzing Process Maps Challenge your map and process by asking the following questions: Is this map comprehensive Are there other key outputs What activities lead to this output What are the major inputs into these activities (Think of inputs as both physical entities and information.) How is information tracked Written specs or paper invoices What percentage of this information is automated What technology or application is used to convert a particular input into an output What equipment or job aids are used What are the major decisions made with the process Where and when are these decisions made By whom Are approval signatures required When How long does this step take Why does it take this long Is there a range Why is there a range What is the cost of performing these activities Can you give me an estimate What are the problems you encounter in performing this step What causes If everyone is thinking alike, then somebody isn't thinking. George S. Patton Page 11 Process Modelling & Mapping: The Basics these problems What are the roadblocks in this process What are the strengths of this process How do we know when this process is successful Is this process successful as it currently exists How do we know Conclusion There are many benefits of using process maps, and many different ways to draw them. Because people have limited understanding of an organizations processes and usually visualize them differently, a process map can build understanding and often create the ‘aha’ moment necessary to drive action, improvement or change. From a hastily sketched top down diagram on a flip chart to a complex detailed computer drawn deployment chart, visual representations of “how” we do things helps to crate understanding of “where we are” and sometimes “how we got here”. But you can’t set a new course or plan a new route if you don’t know where you are to begin with. Process Modelling & Mapping: The Basics by Kelly Halseth Regional Coordinator, Forms Management & Production and* David Thompson Health Region* "},{"version":"10.0","date":"Aug-06-2019","title":"delivery-manager-manual","name":"Delivery Manager Manual","fullPath":"iac/core/delivery-manager/delivery-manager-manual","content":" This Delivery Runbook is a useful asset for anyone engaged in Delivery of Use Cases with WorkFusion. The aspects covered will help you understand how to professionally deliver the project (both with RPA and or ML) from initiation to production in a managed and agile fashion. Audience Delivery Managers Pre requisites Courses: Power User certification ML Basics IT project management background Duration You can complete this course in 30 hours. What you will learn As an outcome of this course, you will: Understand the specifics of delivering automation projects with WorkFusion and get acquainted with typical use cases Know what pre requisites have to be fulfilled before the implementation can start Be able to staff appropriate an team for the automation project Learn the phases specific to RPA and ML projects and be able to build a corresponding project plan Be prepared to analyze the results of a Machine Learning project and speak with customers on the topic of ML Understand the peculiarities of delivering an RPA project and using OCR in WorkFusion Be able to estimate the timeline, resources and hardware required for RPA and ML projects Know what support is available during and after implementation "},{"version":"10.0","date":"Aug-29-2019","title":"delivery-manager-role-and-responsibilities","name":"Delivery Manager Role and Responsibilities","fullPath":"iac/core/delivery-manager/delivery-manager-role-and-responsibilities","content":" Competency Important Tasks KSA (knowledge, skills, ability) Solution Design Business Tech Requirement assessment Validate business process design data flows, rules, RPA &amp; ML steps. Define which models will be needed to train. Describe solution design to the customer. Knowledge of all major releases of SPA and RPAx, key components and its purpose SOW requirements and success criteria review and acknowledgement Knowledge of RPA and ML use cases: how to automate typical processes in key industries (banking, finance services, healthcare, insurance, retail) Knowledge of data types and its usage in WorkFusion SPA Customer Education Explain the basics of ML to customer and respond on typical questions. Consult customer about WorkFusion products and offering. Justify a solution to the customer and required resources, for example: explain the statistics, reasons, ways, needed resources to improve. Communicate and explain to the customer major implementation risks Coordination of Installation and Upgrade activities Cover basic WF architecture related questions Knowledge of current WorkFusion offering, current and planned versions of products and its capabilities. Knowledge of the key risks of WF SPA implementations and ways how to mitigate it in typical projects environments. Ability to prepare and communicate plans, proposals, estimations by the project and present it to the customer. Project Management Prepare project planning of WorkFusion implementation. Estimate each phase of implementation project data tagging, preparing data set, train model, develop bots, testing whole process. Optimize plan to meet customer expectations. Plan the project team capacity the number of SMEs, DA, MLE and other roles. Staff a project team to conduct an interview, ask the right questions and make a decision (with help of Exerts in areas where needed). Present the project plan to the management and approve it. Continuous monitoring of project status. In time, identify and eliminate the risks arising in the project. Quickly find solutions for project related issues like: installation issues, improving data quality, improving OCR quality, deployment issues etc. Motivate team members to achieve results. Knowledge of RPA and ML projects life cycle, typical phases and tasks for each type of implementation. Knowledge of which tasks can be performed in parallel. Knowledge of typical estimations for key project phases: data tagging, preparing data set, train model, develop bots. Ability to prepare and conduct project kick off meeting. Ability to prepare weekly RAG status by the project. Proper business communication skills (emails, face 2 face, onsite visits, ). Knowledge of all support options DM has (Forum, KB, Internal and External CoEs, Product Service Desk). Knowledge of best practices of SPA development: OCR, RPA, data tagging. Ability to calculate team capacity for the product relates to amount of expected data set and use case complexity. "},{"version":"10.0","date":"Aug-19-2019","title":"development-environment-checklist","name":"Development Environment Checklist","fullPath":"iac/core/delivery-manager/development-environment-checklist","content":" Hardware CPU: 4 RAM: 16 GB HDD: 500 GB Operating Systems Microsoft Windows 7 64 bit only Checklist for customers Developers software checklist v0.4.xlsx Software Software Comment Justification Java SE Development Kit 1.8 Required to run WF Studio. Microsoft Outlook Public inbound outbound. Internal communication. Required to exchange emails (alternative software would do) Chrome 63 Default browser to access WorkFusion SPA. Postman cUrl The latest version Postman is required to run direct API calls to WorkFusion Platform. Speed up development and simplify analysis of issues. Chrome selenium driver Without this driver applications cannot be automated through Chrome. Mozilla Firefox 59 Optional browser to access WorkFusion SPA; can be used instead of Chrome Mozilla Firefox selenium driver Without this driver applications cannot be automated through Firefox. Microsoft Internet Explorer 11.x.x Required on developers machines if an application that has to be automated works only with IE browser. MS Internet Explorer selenium driver Depending on the version of IE, it can be necessary to update IE selenium driver in RPA Express. Without this driver applications cannot be automated through IE. Maven Git etc version repository client Depend on a version control system used in the company. To centrally store code and development artifacts with version control WorkFusion RPAx Pro (includes WorkFusion Studio WorkFusion IDE) The latest version compatible with installed WorkFusion SPA. Full package to develop and run WorkFusion solutions. ImageMagick The latest version A free and open source software suite for displaying, converting, and editing raster image and vector image files. GhostScript The latest version An interpreter for the PostScript language and for PDF. Cyberduck S3 Browser any other tool to access S3 Storage The tool should be compatible with minio S3 server Utility to connect to S3 emulator (part of WorkFusion Platform) used to store binary files. MySQL Workbench Connect to MySQL DB to run SQL requests or investigate issues. pgAdmin Connect to PostgreSQL DB to run SQL requests or investigate issues. Notepad Text editor to work with text format files (config ones, json, xml, yaml, csv, etc). Notepad json plugin To work with json (displays the selected JSON string in a tree view) Putty any other ssh terminal client Tool to connect to Linux machines of WorkFusion setup. WinSCP any other FTP and SCP client Tool to copy files to from Linux machines of WorkFusion setup. Webex GTM Skype for Business or any other communication tool To run screen sharing meetings Oracle Virtualbox with Guest Extensions or VMware Workstation Player (15.0 ) To install and run virtual machines Deployed DA VDI image to Oracle Virtualbox Virtual machine that contains helpful scripts for Data Analysts Deployment guidance Deployed DA VDI image to VMware Workstation Player Virtual machine that contains helpful scripts for Data Analysts Deployment guidance LibreOffice OpenOffice Tool to automate Docx to PDF conversion process CSV processing If BP performs extraction on large docs (40 pages) and we have a large set of such files, the output CSV file DA exports through CT can have 50 Mb size & 2 Mb of XML content per cell. Standard tools, such as Libre Office can't process such files. Below is the list of tools you can use for CSV files processing (but not limited). Tested on the following sample: File: 25Mb CSV with 100 rows, each row contains extracted XML (cell). Cell size: paste read edit operations with 2.5Mb of escaped XML file. Tool Link to download Cell limit Open Read CSV 1 Record editor 2.5 Mb Can use File or DB for data processing. Opens 40 MB CSV file that contain 2.5mb cells with no lags. Allows to modify cells & save files. (java 1.5 HSQL). 2 OpenOffice 1Mb Cell's size is limited to 1mb, can break the layout on file load. 3 LibreOffice 1Mb Cell's size is limited to 1mb, can break the layout on file load. 4 OnlyOffice 1Mb Opens large files, doesn't allow to modify large cells but allows to read navigate through the content (low performance). 5 MS Excel 1Mb Well known data formatting issues, limit cell size by 1 mb. 6 MS Access 65Kb Fails on upload CSV file that contains > than 65000 characters per table row. 7 Enterprise class SQL Database (like Postrgre) Easy to use export from file, update content through SQL queries. 8 ReCSVEditor 30Kb Openes large files with big cells, but limit cell size by 30 kb on save (java 1.8) 9 OpenRefine (Google Refine) 1Mb Self hosted opensource web app for big data processing pretty fast, but has a 1mb per cell limitation on edit save. 10 Rons Editor 2 Mb Tested on 16mb per cell file was opened but impossible to edit. Wokrs fine for 2 MB cells. 11 CSVKIT Python lib CLI to process CSV files (opensource) usage samples 12 TadViewer Unable to test in WF network as it conflicts with pre installed AV software (Symantec Endpoint Protection) 13 Custom BP in Control Tower WF SPA Design custom business process that replaces CSV cells by S3 links to extracted files Internal resources Internal resource Comment Access to WorkFusion web applications of development environment ( URL) Control Tower, WorkSpace, Platform Monitor, Tableau, Mesos, Marathon. Access to Windows Linux machines of development environment Need to have access to debug review logs start stop restart WorkFusion services. Access to S3 (Access, URL and Secret keys of development environment) Put files into the storage to have direct access to them from WorkFusion Platform. Allows to split development of huge process on chunks. Access to version repository provided by customer with an appropriate client to use it (Git Maven etc) Entry point of a version system used to store code developed by the team. Access to applications being automated ( their URLs); backend documentation if possible Allows the team to analyse applications and find the best approach of automation. selenium grid URL Check the list of connected RPA machines (helpful for debug purposes) RPA Nodes access Allows to monitor Robots activities, debug execution. All RPA nodes need to be able to access internal and external URLs used in the business processes and applications that have to be automated by Robots RPA machines running Robots should have access to all Urls (internal and external ones) and applications that Robots touches emulating Human activities. nodes require installation of API SSL certificates required for each API If applications being automated require SSL connections, appropriate SSL certificates have to be deployed to Robots to establish secure connections. Access to External resources (may be restricted in client network) External resource Comment www.google.com Search answers on technical questions https: kb.workfusion.com WorkFusion Knowledge Base Access required with downloadable permissions https: forum.workfusion.com Access to WorkFusion forum and community http: automationacademy.com Access to WorkFusion Automation Academy educational portal All videos from Academy are stored here. https: stackoverflow.com The main Q&A site for developers in internet https: developers.google.com Google technical forum for developers http: toolsqa.com Technical forum about selenium automation Access to external tools if any are used to implement a solution E.g.: Google Geocoder to verify address Access to proxy if direct connection to public network is restricted If access to external network (internet) is configured through proxy appropriate settings have to be provided to the team to configure WF Studio to connect to external repositories to download frameworks libraries HowTos How to integrate RPAx with standalone AutoML "},{"version":"10.0","date":"Sep-02-2019","title":"gdpr-playbook-for-service-provider","name":"GDPR Playbook for Service Provider","fullPath":"iac/core/delivery-manager/gdpr-playbook-for-service-provider","content":" Study How To Handle European GDPR"},{"version":"10.0","date":"Oct-04-2019","title":"project-tracking","name":"Project Tracking","fullPath":"iac/core/delivery-manager/project-tracking","content":" Do not read this section simply as hints for some helpful tools; project tracking is a must regardless of in which format you end up doing it. Why is it necessary You will not be able to structure and track all the events, plans and dependencies solely in your mind unless you possess super powers; thinking that you possess them will not help, either For each decision, issue, delay there must be a written proof that was shared with all the parties (dev team, customer, partner) involved at the time when event occurred Each project should produce a set of materials that would allow to make future use of its experience and outcomes The below paragraphs describe project tracking components based on Project Tracking Template. Project Plan is a visual representation of each project task timeline with their dependencies (like Gantt chart). It should be prepared before the project start and briefly presented on the kickoff. The plan should be prepared by PM or DM with the help of the development team; all the timeline estimates should come from or be agreed by the developers who are going to implement the project. Apart from day to day tasks, it is useful to integrate holiday team vacations schedule here as it is going to affect timelines and should be clarified in advance. Milestones Tracker (UC1 UC2 or \"Delivery Plan\" in the template) lists milestones and dates in an easy to read format. Day to day plan is useful for the implementation team but contains too many details for executive level stakeholders. List of milestones is useful for high level tracking and for presenting to wider circle of stakeholders. Weekly Monitor lists tasks for each week based on project plan and additional activities that arise. Weekly monitor is the most concise and useful way for DM to stay on track with the progress. With the help of a simple list on a weekly (for some projects daily) call you can check status of each item, plan activities for next week. Each week's activities include: (1) items based on project plan; (2) additional actions that came up in progress (e.g. \"organize call with IT team to resolve access problem\"; \"contact X in another department to check on potential use case\"); (3) items not completed last week and thus moving to next week. While high level milestones can be set far in the future so it's hard to understand if you are on track to reach them until it becomes too late, such short term aims allow to check the health of implementation and adjust the course regularly e.g. if at each status call you find some items not done as planned. If any of the planned items is not done, you need to clearly understand the reason and the impact. Risks & Issues Log lists all risks with their impact, mitigation, dates, owners. This list must be reviewed regularly (especially it's top priority items) and updated whenever something changes regarding its items. Every risk and issue should be tracked, none can be left without owner and mitigation. Additional Templates Summary another way to represent progress in concise and visual way, can be useful for high level stakeholders Releases can use to plan and track process releases, along with their major features Features may be used to list and present features that are planned developed. It is not necessary to use each and every of the trackers proposed here. All in all, as a DM you need to have a detailed plan and track risks and issues. In other respects just choose what formats fit best your purpose and your audience, balancing their visual appeal with their ease to use; after all you want not only to accurately reflect all that is happening on the project but also not turn it into a tedious task that occupies hours of work. RAG Status RAG is a popular method of rating issues or status based on Red, Amber (yellow) or Green colours. In the above examples of documentation you can find it used both for issues rating and for reporting status in the tracker. GREEN status is applicable when: the project is on track and on budget quality is at expected levels and success criteria are likely to be met stakeholders and teams are generally satisfied no issues minor issues AMBER status is applicable when: overspend and or delays are taking place problems with quality dissatisfaction of customer stakeholders other risks and issues where mitigation an plan to resolve are identified RED status is applicable when one or more of the following is taking place: significant time slippage customer unhappiness, escalation acceptance success criteria not being met legal risk or inpact financial impact blocker (technical, organization, any other) Oftentimes putting a red status means the situation is going beyond your control and you are asking for help from senior management. RAG status is provided along with other key information, including reasons for this status and, most importantly, mitigation plan (aka Path to Green) in case of red or amber. Also status report includes risks and issues, progress from the previous status and activities planned for the next time period. Here is a sample template of status report: Status report Status Update: RED AMBER GREEN If Red or Amber, why are we in Red or Amber status And what is the Path to Green Risks Issues: Are there any risks or issues that have a probability of arising that will impact the budget, scope, quality, resources or timelines of the project If so, summarize the risk issues, impact to the project customer and mitigation strategy and action plan. Progress this week sprint month: Summarize progress made since the last update. Next week's plan: Summarize the key action items for the next period (week sprint month) Cc: Executive Sponsor, Account Manager, Project Team Project Plan Milestones Tracker Weekly Monitor Risks & Issues Log Additional Templates RAG Status "},{"version":"10.0","date":"Aug-06-2019","title":"supervised-production","name":"Supervised Production","fullPath":"iac/core/delivery-manager/supervised-production","content":" Introduction Supervised Production (SP) means stabilization and incremental FTE load transfer to Bots Unsupervised production (UP) fully functional production Bot implementation: data is flowing through the Business Process workflow, Bots are interacting with applications and processing data in autonomous way, only exceptions are processed manually. The goal of Supervised Production process is to provide a smooth transition from the existing process workflow to the automated Bot system, give a proper experience and confidence to the Business Team with the new approach. Process Description Prerequisites and Preparation Environment Production infrastructure is properly configured and environment is ready for the Bot deployment. Business Systems and Applications are prepared for Bot interactions (created proper User Ids, granted access, etc.) Automation Business Processes are successfully tested on UAT and migrated to PROD (additional logs timeouts may be added to initial transition stage to simplify monitoring) Teams Business Team is trained and ready to monitor Bot execution and process exceptions: access to the Bot VDIs and WorkSpace environment is provided, audit logs are enabled and transaction analytics is set up Infrastructure Team is making sure all systems are integrated and functioning Automation Team is monitoring Business Process workflow, and prepared to fix issues in high priority mode WorkFusion product Support Team is monitoring Service Desk for any product related bugs during transition Success criteria and SLA Transition timeline and phases are agreed between the teams Baseline is set for a record transaction processing time (i.e. SLA) Expected automation results are well defined (e.g. X% on N number of records) Out of scope cases (e.g. known out of scope document types, invalid data values, etc.) are excluded from work and handled separately Transition Phase Normal flow Based on agreed schedule, process load is incrementally transitioned to the Bots (e.g. 25%→ 50%→ 100%) Business Team is handling exceptions in WorkSpace Results are being reviewed by the teams on a scheduled basis (e.g. daily for initial transition phase) Issues All occurring issues are triaged (reviewed and classified) by Automation and Business teams Performance and stability issues are addressed by Infrastructure and Automation teams Unclear or recurring issues like performance degradation or memory leaks are reported to the WF Support Team Final Phase Production system is fully switched to Unsupervised Production mode All Bots are working in autonomous way Business exceptions are manually handled by SMEs Business Operations Business and Automation teams are monitoring automation results through the BI reporting tools Customer IT is automatically alerted by infrastructure monitoring system in case of issues Example Load transfer: SP (25% load) → SP (50% load) → SP (100% load) → UP (100% load) Introduction Process Description Prerequisites and Preparation Environment Teams Success criteria and SLA Transition Phase Normal flow Issues Final Phase Example "},{"version":"10.0","date":"Aug-06-2019","title":"team","name":"Team","fullPath":"iac/core/delivery-manager/team","content":" Staffing Key implementation roles that are specific to WorkFusion are: RPA Developer Machine Learning Engineer Data Analyst Delivery Manager Other roles engaged in implementation (but not limited to) are: QA, BA, Project Manager, Program Manager, Data Scientist, Enterprise Architect, IT Ops, SME Operators Particular team is identified based on complexity of the use case by CoE and highlighted in Project Plan. DM's responsibility is to match resource requirements with team staffing plan. Note for roles start dates It's usually the case that RPA dev and DA can start from day 1, while MLE should only be engaged to the project once training set is in place. Actual start date depends on the size of training set and number of SMEs available to tag, but usually takes up to 2 weeks from the project start. See below as a reference: Team Enablement and Training Please refer to roles description and associated learning paths at How to enroll To check if a particular developer has the relevant certification track progress please use: Reports: How to check trainees progress and certification Staffing Team Enablement and Training "},{"version":"10.0","date":"Aug-06-2019","title":"workfusion-spa-local-development-environment","name":"WorkFusion SPA local development environment","fullPath":"iac/core/delivery-manager/workfusion-spa-local-development-environment","content":" This page summarizes requirements and configuration guide for local development environment for efficient project implementation and delivery work. Target audience IT Ops admin, configuring development environment (desktop or VDI) and provisioning accesses for the implementation team. SPA developer setting up own development environment. Prerequisites Server SPA installation: Current Legacy Setup System requirements Windows VDI or desktop: 4 cores 32 GB RAM 100 GB HDD Platform engineer list RPA Express 2.0 instruction where to get, and what version to use, depending on the SPA Server version. Request SPA license to fully unlock Control Tower how to do this Is it for every developer Can license from dev server be used Link Control Tower and Nexus to create and check BCB up to date instructions Connect local Control Tower to AutoML server Connect RPA Express to Shared AutoML Server MLE list Development software: IntelliJ Idea Community Edition Java SE Development Kit 8u171 Maven 3.5.4 Sublime (Text editor) Git Putty as SSH client WinSCP S3 Browser (S3 Storage Client) Office software: Microsoft Excel, Word Skype Email(Outlook) with ability to send emails outside network Accesses Access to Internet (access to google etc.) Access to Dev Control Tower and WorkSpace Access to Dev Mesos and Marathon Access to Dev app linux box, and dev ML linux box ML SDK itself Nexus access S3 access SPA developer As a SPA developer I would like to have following items after installation regadless on premises or cloud: URLs (incl. host port) of all components with credentials to access it (For example: Control Tower: Credetials: name pwd) Control Tower URL default creds Master password for Secret Vault (it will be removed in next releases, but for now it is still required) OCR URL with API credentials S3 URL with S3 public private key Marathon URL credentials AutoML REST API credentials BI host RDP creds RPA URL RDP creds (for all machines) Nexus URL nexus admin credentials SSH username private key if feasible "},{"version":"10.0","date":"Aug-06-2019","title":"initiation-and-planning","name":"Initiation and Planning","fullPath":"iac/core/delivery-manager/initiation-and-planning","content":" Initiation Checklist As a Delivery Manager below points should be in place before you proceed to implementation: Scope Defined (SOW with clear success metrics acceptance criteria). At least for first iteration Solution Design, High level Project plan prepared based on 1 High level project plan example: Detailed Project plan can be found in the section Project Tracking. WorkFusion Software Installation (incl. Current Licenses, version and components required in 2) Implementation Team assigned On boarding, accesses, permissions provisioned for Project Team Development Environment prepared on team's laptops, including all the required software independent of the roles External apps access provisioned, third party soft installed Key stakeholders from Delivery, IT and Business are identified and engaged Communication and tracking tools defined Constant management of test systems ensured (actual snapshots, test data replenishment) "},{"version":"10.0","date":"Aug-06-2019","title":"installation","name":"Installation","fullPath":"iac/core/delivery-manager/installation","content":" Installation is an important milestone in the delivery process. Please note: Non standard infrastructure requirements architecture should be reviewed with WorkFusion Lead Delivery Managers before any installation. SPA Installation Below is a common flow for SPA installation: Define infrastructure requirements done by Delivery Manager with help of Senior RPA Developer (Solution Architect) and or Senior MLE based on the scope and timeline. Refer to standard build , note difference in Dev and Prod environments. For On premise installation: Checklist coordination Once requirements are clear they are communicated to Customer IT. The latter reverts with provisioned infra via Checklist filled in with actual data on infra. DM verifies checklist: If some components don't have required specification it's essential to communicate how crucial it is to run WF product on required infra; otherwise it may lead to additional risks, non stable and non resilient environment which will dramatically affect development process, site reliability and customer success as the result If all requirements are met DM creates a ticket for Installation team to plan installation. For on premise installation IT person from customer end and his timezone are required to coordinate timing for installation. Agree on installation support approach: Customer gives WF Partner team a direct access (e.g. ssh) to instances, WF Partner team installs components thyself. WF Partner team setups WebEx sessions, Customer team gives remote control, installation is done by WF Partner team, Customer just observes. WF Partner team setups WebEx sessions, Customer team has control and perform installation according to the Current SPA Installation guide with WF Partner team supervising the process. In such case Customer Partner IT person is required to complete IT Support Specialist course and familiarize himself with SPA Installation in advance. For Cloud installation ticket is raised in PPSU with following details: Customer Project (usually name of the use case) Due date Defined Infrastructure from 1 Note! For Cloud based installations it's obligatory to make sure no personal sensitive data is hosted on WF managed Cloud, e.g. for POCs.* *Otherwise choose On prem installation. See more about sensitive data at GPDR Playbook for Service Providers Licenses are provided to Installation team by WF IT once Licence Agreement is signed: For APP it's provided on the first day of installation. License Validity is defined in License Agreement. For Current OCR License tied to actual server after OCR service was installed on it. Jira ticket is raised by DM in relation to Installation request (e.g. via PPSU ) of pages supported languages (basic, CJK, all) customer name license type (Prod non Prod) Current WorkFusion Analytics License, requested by DM via PPSU (required info customer name, project name). DM tracks installation process and helps remove impediments if any. Final stage of installation are Current Post Installation health checks that are applied to confirm everything is setup properly. Once installation is completed it's transferred to support with all required info on the servers, so that in case of need Support may refer to it. See Completed Installation example below: Click here to expand... Completed Installation example Value Account Manager &lt;Name&gt; Delivery Manager &lt;Name&gt; Installation Engineer &lt;Name&gt; Installation JIRA Ticket Link to jira Product version e.g. 9.1 or 8.5.3 Deployment type Prod Env Checklist Config.yml Manifest.yml OCR license BI Server license BI Desktop license Start Date End date Purpose Name (DNS entries) IP address CPU RAM Disk Space OS WF Control Tower wfapp aaa.bbb.com XX.XX.XXX.XXX 8 32 250GB RHEL 7.3 DB server wfdb aaa.bbb.com XX.XX.XXX.XXX 8 16 1TB RHEL 7.3 OCR server wfocr aaa.bbb.com XX.XX.XXX.XXX 8 32 250GB RHEL 7.3 VDS Cluster (Master NFS) wfml aaa.bbb.com XX.XX.XXX.XXX 32 128 2TB RHEL 7.3 VDS Slave 1 wfmls1 aaa.bbb.com XX.XX.XXX.XXX 16 64 250GB RHEL 7.3 АPM server wfapm aaa.bbb.com XX.XX.XXX.XXX 2 4 100GB RHEL 7.3 BI server wfbi aaa.bbb.com XX.XX.XXX.XXX 4 16 100GB Windows 2012 R2 RPA server wfrpa1 aaa.bbb.com XX.XX.XXX.XXX 8 16 250GB Windows 2012 R2 RPAx Pro as Development Environment In addition to server installation, it is required for development team to have RPAx Pro installed on their local machines as development environment (for non commercial usage). To get it, do as follows: Download latest RPAx from official site at Request Pro license via PPSU Besides RPAx, it is strongly recommended to install several other tools on developers machines. SPA Installation RPAx Pro as Development Environment "},{"version":"10.0","date":"Aug-06-2019","title":"post-implementation-testing-uat-and-go-live-production","name":"Post-implementation: Testing (UAT) and Go-Live (production)","fullPath":"iac/core/delivery-manager/post-implementation-testing-uat-and-go-live-production","content":" title: Post implementation: Testing (UAT) and Go Live (production) UAT User Acceptance Testing is the last phase that takes place before process rollout to production and leads to end user sign off on the process both from technical and business logic perspectives. Consequently, this activity may or may not take place in smaller projects that do not lead to production rollout. In a POC the sign off is often received on the basis of a demo; still the end user can choose to test the outcome. In this case the aim of such testing would be more to ensure that the result meets their expectations and or to check how end users feel about the product they potentially will have to work with. In a production project the aim of UAT as a final test includes both of these aims but also target to find and eliminate all the possible remaining bugs or flaws in logic, as well as all the corner cases and exceptions that could have been missed in implementation and developer unit tests. UAT is not only needed, but usually also officially required by customer's internal compliance. You will have to check with customer's Compliance Department their requirements to putting in production new products or changes and follow the process and format of documenting UAT that they demand. It might be the case that internal compliance requires documenting user acceptance testing but does not specify the format then you can choose it yourself. Checklist for UAT: 1. include sufficient time for UAT in your project plan (e.g. a week) start documenting test cases for UAT in advance with the help of a subject matter expert to cover all corner cases book the needed resources for UAT with customer in advance and for sufficient amount of time UAT has to be done by end users and test cases for it should be written by the expert in each specific process make sure test data for UAT is available in test systems and is sufficient to cover test cases document the result of UAT and keep it as part of your project documentation receive sign off on UAT approval for production from relevant responsible parties internal compliance, process owner (or business, system, tech owner) Sample of Test Cases for UAT. Presenting Results As a template for results debrief you can use Results Deck Short Template. You might also use an extended deck as an example of how results could be presented to the client: Results Deck Example. Note, you're encouraged to only pick slides relevant to your particular case. Supervised Production and Go live To provide a smooth transition from the existing process workflow to the automated Bot system and give a proper experience and confidence to the Business Team with the new approach, Supervised Production phase is used before the work is fully transitioned to bots. Go live requires intensive preparation and planning from technical, compliance and user training perspective. Typically a Rollout plan is prepared to keep all the involved parties aware of the planned activities and the dates. Depending on the organization scale and the compliance requirements the plan can be more detailed or high level. Technical prerequisites: Production environment installed Access to production systems received Code migrated to production Testing and stress testing completed where applicable Compliance: Get acquainted with internal compliance requirements to production rollouts Prepare all needed documentation (technical, user; Jira tickets with corresponding approvals, etc.) Review with internal compliance team and receive their written sign off Receive approval from all the stakeholders who have to provide it for the given process change, e.g. process owner, business owner, application owner, etc. Change management (user preparation): Write user guide, including troubleshooting Conduct user training sessions, including support model Schedule user notifications of the coming changes Ensure access to internal Support desk, WorkSpace, etc. UATandGo Live(production) UAT) Presenting ResultsandGo Live(production) PresentingResults) Supervised Production and Go liveandGo Live(production) SupervisedProductionandGo live) "},{"version":"10.0","date":"Aug-06-2019","title":"before-ml-project-start","name":"Before ML project start","fullPath":"iac/core/delivery-manager/implementation/before-ml-project-start","content":" Basics As a preparation for starting on an ML project, familiarize yourself with the basic information: ML AutoML Glossary that explains the terms related to AutoML components in WorkFusion ML project Questionnaire it lists all the questions that should be answered for ML project to adequately estimate and plan it AutoML Architecture, including General AutoML Components Communication Scheme, Components Communication while Training Model, Components Communication while Extraction using Model and AutoML Installation Types (Standalone vs Cluster) along with explanation of Architecture Components ML Methodology is a collection of articles on the topic of best practices in managing ML use case. Some of the most relevant for DM topics include DA and SME Efforts Calculation, Use Case Lifecycle, ML Use Case Prerequisites. ML Methodology will be referred to further in this section where it is applicable System Requirements and how they affect the time of training (how many models fields can be trained in parallel) Prerequisites of delivering ML project: ML (and in majority of cases also OCR) server(s) installed according to requirements and based on required capacity scaling; ML health check run Scope is clear and validated Imagine you were given typical project where you need to: Classify 4 types of documents Type 1, Type 2, Type 3 and Type 4; Extract information from Type 3. Let's assume you're clear about what fields to extract, for classification though couple of hints you may use to be sure scope is defined well: Are those 4 types represent whole production throughput If not, what are the other classes and how many of them in total Are there any aggregate type of classes, like \"Other\" or \"General\" What is classes distribution You should focus on those covering most of the throughput How similar classes are E.g. if you consider color criteria: while it's easy to distinguish b w black or white, it can be much harder to do so against jeans color Make sure you know how multi class classification works at Machine Learning Basics MulticlassClassificationofResults Customer aware of the need and ready to provide (1) documents grouped by structures and (2) SMEs for tagging of the training set in sufficient number The number of documents to be tagged will likely be mentioned in SOW or needs to be determined with the help of MLE and based of what customer has available. It is also better to ask the customer to group the documents by unique structures to make preparation for tagging and OCR tuning quicker. If the documents are not grouped by customer, it will have to be done by DA. In this case DM should take into account that such grouping requires additional time. To request sufficient number of SMEs, you need to go by how many documents need to be tagged, how long it takes to tag one documents and what timeframe can be allocated for tagging in your project plan. To calculate the number or SMEs you should request use DA and SME efforts Calculator. It might also be the case that customer can provide only specific number of SMEs e.g. one and you need to plan the tagging timeframe and start of model training iterations based on these resources availability. You also might find that the needed resources are available only during specific time period e.g. due to vacation, being busy due to end of financial quarter or month closure, etc. Make sure you communicate all the resources requirements and the impact of lack of resources delay on the project plan and end date. Providing sufficient resources in timely manner is customer responsibility that is defined in SOW. Though DAs can participate in tagging where it necessary or beneficial for the project, tagging of the dataset is usually delegated to customer's SMEs for a couple of reasons: 1. Data Analyst does not always have the time to tag training set they need to be dedicated to other tasks described below Data Analysts do not have the expert knowledge of the data specific to the customer and use case that SMEs have When DAs take part in tagging, they still need the help and advice of SMEs from customer side. When making a decision to involve DA in tagging make sure it is not preventing them from performing their main tasks and they are not becoming a bottleneck for example, in checking the work of SMEs. Success criteria for ML model specified in SOW (numeric criteria usually, automation rate accuracy in percentage). See example below. Team: MLE DA(s) Machine Leaning Engineer (MLE) MLE is a specialist who possesses technical knowledge and corresponding certification to run model training, implement post processing, as well as fix OCR errors and help DA with statistics. MLE also is responsible for creating an automatically tagged training set when it is applicable in the project. Oftentimes on smaller projects the same person combines roles of RPA developer and MLE, both training the model and building the rest of the business process around it. For more details review MLE role and responsibilities.If you have several models that need to be implemented in parallel, you will likely need several MLEs. Data Analyst (DA) Data Analyst is a specialist who analyzes the ML use case (documents and extraction logic) and prepares the data set and calculates the statistics. Managing the data set is the overriding task of the DA that includes documents templates analysis, manual task creation; communication with and qualification of SMEs, checking their work; maintaining the highest possible quality of the data set at all time. The model can be only as good as the training set is. DA can also help MLE with the logic for post processing as DA generally has the deepest knowledge of the use case while MLE possesses technical skills to actually run model training and code the post processing. For more details review DA role and responsibilities. Depending on the number of documents that will be tagged, the number of SMEs provided and the timeframe, you might need several DAs. Use DA and SME effort Calculator to determine the number of DAs needed dependent on the factors specific to your use case. SOW of ML project: Before an ML project starts, customer should be already educated on some peculiarities like resources needed in it, applicable deliverables and success criteria. Such agreements will be documented in SOW and are the first thing you should review before starting ML project. Pay attention to the following: Success criteria should be expressed in concrete numbers (minimal acceptable percentage), usually specified for automation rate and accuracy. Make sure you know these and other Automation Quality Metrics, what they mean and how they are calculated. There are no recommended or universal success criteria; all depends on specific model and use case. Typically criteria around 90% accuracy and 50 60% automation rate can be encountered, at least in POCs Pilots. how they are calculated: typically success criteria is expressed in terms of average rate on document level. By \"document level\" here we mean the average across fields and document types. DAs and MLEs look at stats at more detailed level for specific field or even specific document to analyze and understand results. However, for customers it usually does not make sense to go to field level, as their main point of interest is where business value is derived e.g. FTE savings and overall effort reduction of the process. It is also disadvantageous to measure result on field level as it is highly unlikely that the model will have close enough stats on all fields some might be higher, some lower than the desired average. fields with low representation (appearing <25% of times) are excluded OCR errors excluded from stats Minimal requirement to dpi of image documents (300 dpi) Number of documents in the training set and test set Assurance that test set is equally distributed as a training set Handwriting is typically not digitized and processed Assumptions Client will provide sufficient SME (Subject Matter Expert) resources to perform tagging of documents Client will provide all training data in at the start of the project Data set is representative based on historic current production data (rather than a biased sample of convenient documents) Customer Responsibilities Again, make SMEs available to create training set SOW for ML example 1 4.7 Success Criteria 4.7.1 For fields appearing in documents more than 25% of the time: 4.7.1.1 on average, more than 90% accuracy, excluding errors due to OCR conversion (for PDF documents) into XML format that occur from low quality source documents 4.7.1.2 on average, more than 50% per field automation rate 4.7.2 Execution of robotic bot(s) for non exceptional scenarios where the inputs are valid and the business applications are not triggering any processing errors 4.7.3 Manual tasks permitting to add missed or correct wrong machine extracted data for achieving 100% completeness and correctness of processing term sheets and then verifying the data with rules based reconciliation as specified by the customer. 4.8 Assumptions 4.8.1 WorkFusion and its subcontractor personnel will have access to all necessary systems and data to implement the process before day one of the project. 4.8.2 Tagging and processing of 1,000 email bodies containing exhaustive term sheet information and 1,000 PDF machine typed term sheets (400x2 for training 200 for test in both cases). Every field that needs to be extracted features 1,000 times in each case and if not, supplementary examples will be provided. 4.8.3 Sample is based on historic current production data (rather than a biased sample of convenient term sheets that isn't representative). ... * 5. Customer Responsibilities:* as part of this SOW, Customer will commit to the following responsibilities in order for WorkFusion to provide its services ... 5.3 Make analysts available to create training set (tagging of both 1,000 PDFs and 1,000 emails) and then review data extracted by the cognitive bot. SOW for ML example 2 1.1 Cumulative Success Criteria 1.1.1 Test 1 – Document categorization 1.1.1.1 Automation Rate: average 40% automation across document types, calculated on a test batch of 100 documents, as per results and after manual review of each document categorization within WorkFusion's Workspace. Test batch can be provided as the existing result of current categorization process and needs to be equally distributed as the training set in terms of formats. 1.1.1.2 Accuracy Rate: average 90% across document types, calculated on a test batch of 100 documents, as per results and after manual review of each document categorization within WorkFusion's Workspace. Test batch can be provided as the existing result of current categorization process and needs to be equally distributed as the training set in terms of formats. This rate excludes errors due to OCR conversion of PDF documents into XML format occurring from low quality source documents (below 300 dots per inches definition). ... 1.1.2 Test 2 – Data extraction 1.1.2.1 Test to be performed on the batch of 100 documents. The test batch needs to be well represented within the training set in terms of formats, representation and document quality. 1.1.2.2 Automation: 40% average rate at document level, evaluated based on the manual review of documents within WorkFusion's Workspace. 1.1.2.3 Accuracy: 95% average rate at document level, evaluated based on the manual review of each document within WorkFusion's Workspace, excluding errors due to OCR conversion of PDF documents into XML format occurring from low quality source documents (below 300 dots per inches definition). ... 1.3 Assumptions 1.3.1 Client will provide sufficient SME (Subject Matter Expert) resources to perform tagging of documents in scope for generating the training sets for Machine Training 1.3.2 Client will dedicate SME resources for validating Business Process implementation in User Acceptance Testing 1.3.3 Client will provide valid inputs for RPA automation development and sufficient number of test cases in advance of development process. 1.3.4 Client will provide all training data in the first weeks of the project. 1.4 Out of Scope 1.4.1 Integration with systems other than the ones mentioned in this SOW 1.4.2 Extraction of data fields other than the ones mentioned in this SOW 1.4.3 Handwritten input will not be digitized and processed by WorkFusion SPA's OCR and cognitive bots "},{"version":"10.0","date":"Aug-06-2019","title":"da-and-mle-step-by-step-guide","name":"DA and MLE step-by-step guide","fullPath":"iac/core/delivery-manager/implementation/da-and-mle-step-by-step-guide","content":" Introduction WorkFusion SPA puts in place Human in the Loop (HITL) technique meaning that machine and human works together to achieve the best results in documents processing. This concept is crucial and results in the requirement that machine and Human should produce the same outputs. In terms of WorkFusion, Model's output should be identical to the output of Manual Task submitted by experts from the customer's side or subject matter experts (SMEs). SMEs use WorkFusion Workspace application to extract values from documents (or run any other actions on documents) via different Manual Tasks (MT). It's possible to think of Manual Tasks as buckets (for tech folks queues), where running Business Processes (BP) put documents and Users pulls documents from them by opening appropriate Manual Tasks in Workspace. Automation Business Process used to automate a single Manual Task does the following: Pre processes a document before sending it to AutoML server (e.g.: remove additional header tags that decrease models' confidence) Sends a document with the required model details to AutoML server Runs post processing if it's necessary Checks the response and if it's failed forwards document to a Manual Task If no periodically updated external reference data is used for post processing, it is strongly recommended to implement it inside a model, not to break the concept described above and results into: incorrect model's statistics (as a model produces not ideal values and there is no way to correct them) when PP is implemented as a separate step, a developer operates only with extracted values without its context (e.g.: a model extracted \"Tom Sayer\" company name. It looks like a correct value, but there is a requirement to have a company type abbreviation as part of the value if it exists. \"Ltd.\" string exists in the document and if you have access to the context it's rather easy to analyze it and form the correct value concatenating the extracted value and found \"Ltd.\" one. But in case of a separate step, there is no access to the context, so it's impossible to form the correct value) ToDo: it's better to insert a screenshot with an example MLE has to switch between different pieces of solution (model's and step's sources) to make automation part (model PP step) produce good results Phases of ML implementation Each step in the flow significantly influences the result and an output of one step drastically influences complexity of the next ones. So, if the initial quality of documents is poor, it will be difficult to receive good OCR output and virtually impossible to have accurate training set that will lead to building very unstable model producing low results. While working on ML use cases, it's important to focus on \"happy path\" scope: the goal should be to implement a stable solution for the most frequent documents cases; exceptional ones can be handled by SMEs via Manual Task. ML Use Case Flow Taking the chain of steps in the mind, lets go through ML use case flow and think about what DA and MLE folks have to do on each step: Initial analysis of documents by DA: evaluate the quality of documents, if OCR produces good output. If no, raise it to Delivery Manager to negotiate it with a customer. Potential issues: documents are dirty, some texts are not readable there are lots of handwritten notes signatures above values being extracted documents scanned with low quality that leads to problems of reading texts Potential solutions: customer improves quality of scanning (increase DPIs, replace scanners, etc) use ImageMagick utility to clean documents from \"noisy\" signs (e.g.: remove shadows, decrease size of hand written notes, etc) apply symbols' patterns to improve recognition of some letters numbers (usually when non standard font is used in documents) collect requirements for each field. DA can use the following template for it. This information is used by DA and MLE to build Manual Task and Model producing the same output (e.g.: all dates should be formatted in a similar manner) This activity requires involvement of the most experienced SMEs as any errors the team find during the tagging will require reconfiguration and retagging of a data set. Potential issues: experienced SMEs are busy and can't join the team it's impossible to collect all requirements due to huge number of exceptions (not possible to detect \"happy path\" set of documents values) customer tries to start tagging immediately saying that SMEs will have the time only now All these issues should be immediately raised to Delivery Manager and he has to negotiate it with the customer. Skipping this step can lead to increase of use case complexity and struggling with model's retraining. Delivery Manager has to educate a customer about importance of this step and explain the risks team can face with on the next phases of the project (retagging of documents, model producing incorrect results, mismatch of output formats, etc) Implementation of Manual Task for tagging Manual Task for tagging should be implemented based on the requirements collected by DA with the help of SMEs. The following parts of Manual Task have to be coded: reading and transforming values (e.g.: there is a requirement that dates have to be output in mm dd yyyy format. So, if there is a value \"September 1, 1999\" in a document, it should be correctly parsed and transformed into the correct output one \"9 1 1999\") validation rules (e.g.: there is a \"Total\" field and a grouped set of products' prices and there is a rule that sum of prices is a total sum. So, an appropriate validation rule has to be implemented) validation on reference data (e.g.: based on the collected requirements, there is a limited list of currencies that are in scope. An appropriate validation rule has to be implemented to validate an extracted value against reference data) normalisation of values (e.g.: documents contain country names in different notation \"Germany\", \"GERMANY\". Transformation has to be coded to normalise extracted values) Training and qualification of SMEs When Manual Task for tagging is ready, DA starts educating SMEs on tagging rules and run qualification training on it. Description: here Note: tagging of training set can be started only after SMEs can tag a selected batch correctly. Tagging The most critical and risky phase of ML use case: time consuming depends on customer's folks high cost of post fixing When SMEs tag training set, DA validates tagged documents on the following: accuracy and consistency of tagging. If DA notices any issue, (s)he immediately raises it to Delivery Manager and SMEs team consistency with the collected requirements. It's possible that new ones are found during tagging phase. DA has to collect them and discuss the influence with Delivery Manager and SMEs. distribution of fields' values across training set. Should be done on daily basis to raise a risk as soon as it comes to life. E.g. SMEs tagged 300 documents and DA sees that values for 2 fields exist only in 10 documents. So, it's clear that even if SMEs tag 1K documents, the team still won't have enough samples to train a stable model. Delivery Manager has to raise it to a customer: these fields can be exceptional ones and a customer agrees to exclude them from the scope these fields are still required and a customer has to collect more documents containing these fields Model Implementation Details can be found: ... PP should implement the same logic, that was coded for Manual Task. It guarantees that values produced by these two artefacts will be the same. Model tuning Introduction Phases of ML implementation ML Use Case Flow "},{"version":"10.0","date":"Aug-06-2019","title":"design","name":"Design","fullPath":"iac/core/delivery-manager/implementation/design","content":" RPA effort estimation Common customer questions What is the basis of time estimate for implementation RPA projects vary in complexity and resource requirements depending on the complexity and stability of the underlying technology. We distinguish 5 levels of RPA automation, starting from the simplest: 1. Level 1: API Automation (APIs, File Exchange, Database, ETL)) Level 2: Web Scraping and Crawling (static web applications sites)) Level 3: Web Automation (dynamic web applications sites)) Level 4: Desktop Automation (legacy applications and Internet Explorer applications)) Level 5: Surface Automation (screen only tools, image recognition)) To get details on the estimation of effort for RPA projects depending on the type of application, type of project and level of detail required, as well as to access effort calculator for RPA projects refer to the page How to estimate effort for RPA Capacity planning Common customer questions How many bot accounts do you need At what stage do I need to scale the hardware for running bot processes The number of nodes available on the RPA machine was determined before installation under hardware requirements and in default installation will be 5. At the beginning of an RPA project the question will usually come up again, e.g. for determining the number of bot accounts that need to be created in the target systems. To determine the correct number, use function for Number of bot estimation. Having determined the number of bots required, you can proceed to determining the needed hardware based on deployment (VDI or server) see RPA scaling for this information. Find the standard hardware requirements for RPA under Current System Requirements. Design in large projects Design stage in a short and simple RPA project may be not pronounced. In such cases, where the implementation team is small and follows simplified development lifecycle, RPA Express with it's Recorder is the main tool. Implementation with recorder will start shortly after requirements and target systems review. On the contrary, design stage is important for large RPA implementations. In this case we recommend using Enterprise RPA Framework to help eliminate chaos by introducing project standard to be followed by all involved teams, using version control, common programming language; improve speed with reusable automation code and quality with automated unit testing. Design phase prerequisites and output Getting access to target systems for developers and for bots is a prerequisite for proceeding to the next stage implementation. Other prerequisites include having documented requirements from customer that include step by step guide to the process along with screenshots of each page, button and action, and preferably accompanied with a walk through of the process in target systems by SMEs. If the requirements are not detailed enough, walk through is a must. Based on these requirements and system investigation by the dev team they will produce (typically in larger projects) a technical design document, describing target applications' pages' functionality that needs to be automated with corresponding classes and methods, as well as process block diagrams and manual tasks where applicable. When completed, this document is sent to customer for sign off. "},{"version":"10.0","date":"Aug-06-2019","title":"how-to-read-model-statistics","name":"How to Read Model Statistics","fullPath":"iac/core/delivery-manager/implementation/how-to-read-model-statistics","content":" DM must be able to read model statistics. To make use of this guide, make sure that you are familiar with: (1) the notions of true positive (TP), false positive (FP), true negative (TN), false negative (FN), precision and recall you can find them described in Data Analyst documentation; (2) mapping of this terminology to customer friendly names. All the tools of presenting model statistics that are describe below follow common structure: they contain data on statistics per field (accuracy and automation rate, result type (TP, TN, FP, FN)) and detailed gold vs extracted comparison (usually including the gold and extracted values themselves, result type for each and some ID of the record document where they were observed). Statistics per field is what is usually presented to customer and also the first level at which you view the model results. Once you identify the problematic fields based on these stats, you can go deeper into specific gold vs extracted comparison to understand the reasons of such results. For example, you observe low precision or recall for some field. You can find the corresponding columns in the detailed results and look through the error types to see which ones occur most frequently: did the model extract a lot of values incorrectly (FP) or did it miss many values (FN) Further you can look at the extracted value itself and compare it to the gold value to see what caused the mistake. If extracted fields are counted as false positives, is it because the gold is missing or it is different from the extracted value If missing, is it due to inattentiveness of SME or it indeed should not have been tagged If extracted is different from gold, understand what the difference is: number written with without comma, date in different format such differences are fixed by post processing and normalization; value completely different might be due to incorrect gold or the field is complicated for the model or represented not well enough. Check other typical kinds of mistakes in Interpreting results. Automation Dashboards Dashboards are a convenient and user friendly way to present and read model statistics. Information contained in them is calculated based on processes where both model extraction was applied (from which we get extracted values) and human workers SMEs submitted human tasks (from that we get gold values). Note that dashboard is filled in only if Automation Use Case containing \"Statistics calculation\" step is used in Automation BP (see Automation Settings). AutoML Dashboard WF 9.1 Link to Analytics documentation TBD by BI AutoML dashboard contains analytics on manual work reduction resulting from automation, ML performance forecast, and statistics of the models. All the information can be filtered by process name and specific instance execution of this process, as well as process status and execution time range. In AutoML Statistics by Field chart you can find results for each field. Black (66.67% for order _total) is Correct TP; orange (16.67%) is Mistake FP; grey (16.67%) is Not Learned FN. Gold vs Extracted Statistics chart can be filtered by process name and execution and field name. In addition 'Gold vs Extracted Statistics' allows to filter by: values: all, incorrect only (FP FN), correct only (TP TN). By default this filter is set to \"Incorrect only\". result type: in the dashboard for the purpose of being easy to read by customer FP is referred to as \"Extracted with Errors\"; FN as \"Failed to Extract\"; TP as \"Correctly Extracted\", TN as \"Should not be Extracted\". confidence: the confidence with which a field was extracted is reported by the model Apart from field name, confidence and result type, the table contains: Gold Values: the values that were extracted by SME human worker in the manual task and is supposed to be correct Extracted Values: the value extracted by the model Documents: documents in which this combination of gold extracted values occurred. At the bottom of the report you can see the number of expired human tasks (4 in this example). It is mentioned in the report, because they are excluded from the calculation of statistics. Cognitive Automation Breakdown Dashboard WF 8.5 From version 8.5 till version 9.1 you can Cognitive Automation Breakdown dashboard to read model stats. In this dashboard, once you choose a specific business process, you will be able to see Statistics per Field and Gold vs. Extracted charts. In* Statistics per Field* chart you can find results for each field; specifically, accuracy and automation rate. Result type contains colour coded details on TP (Correctly extracted), FP (Extracted with errors) and FN (Not extracted). For example, on the image below you can see that invoice _amount has 97.75% of TP (green), while the remaining mistakes are somewhat equally divided into FP (orange) and FN (grey). Gold vs. Extracted chart can be used for analysis of the model results and their reasons. It contains: Gold Value value tagged by Human; Extracted Value value that was extracted by model; Result Type (TP, FN, TN or FP, FN) result of Gold and Extracted Values comparison; Score the number assigned by the model to each value to indicate how well it fits the criteria to be extracted Confidence the level of confidence of the model in that the score it assigned to the value the decision it made is correct This information is listed for each field, accompanied with the name of the field, uuid of the task and uuid of the document where this result was collected from. Excel report Another commonly used way to report model statistics (especially for WF versions before 9.1) i s via Excel spreadsheet. Such report is described in DA methodology and in the project it will be prepared by DA, if needed with the help of MLE. Model statistics are presented in the first tab of the report. To learn how to read them, refer to the page Interpreting results. Precision and recall are related to each other: the higher precision threshold is set for a trained model, the lower recall will be as the model would be able to extract fewer fields at the desired level of confidence in their correctness. Vice versa, if smaller precision is sufficient, the model would consider more fields to be correct with less confidence in that decision. The second tab of the report contains the detailed results based on which the statistics in the first tab were calculated. Refer to the page Report for the guidance on this table."},{"version":"10.0","date":"Aug-06-2019","title":"implementation-pitfalls","name":"Implementation Pitfalls","fullPath":"iac/core/delivery-manager/implementation/implementation-pitfalls","content":" Scope Creep This pitfall is applicable to POC projects mostly, as the scope is fixed in them unlike in production projects that follow agile workflow. Naturally, customers want to get the most out of their money and often try to squeeze in additional tasks in the process of implementation. It is also normal that good ideas of new features appear once scope was agreed and implementation has started. Otherwise, the dev team might be getting such requests from SMEs they work with and who do not necessarily know what scope of work was agreed so propose new features that they believe will improve the process. The most important aspect here is to recognize in time that scope creep is happening. For that you need to be well acquainted with the contract on the one side and with what developers are actually doing on the other side. If there's divergence and a risk of timeframe and resources extension, it is necessary to re align expectations with the manager and team from client side and tasks with the dev team. How to avoid scope creep: 1. Write a design document based on requirements before implementation start and receive written sign off from customer that it is what they expect; Do not reject new feature; suggest to put it on a list of features to be considered for the later full scope project production implementation; Explain the full consequences of proposed action that are not seen by the customer. E.g. a request to simply add one more field to the model training in the middle of implementation might mean re tagging of the whole dataset tagged so far (even though to add only one field to the tagged ones), booking customer SMEs again for this period; re checking of the same dataset by DA; adjusting the manual task and all related bot steps, possibly; retraining the model; writing post processing for the new field. With all that the timeline is inevitably pushed forward. So, depending on complexity of the ask, you can estimate the time and resources needed for the change (in this case SMEs, MLE and DA) and ask if the change is important enough for successful proof of concept. Re iterate the purpose of POC to prove that functionality works potentially identify items to be tackled in the next phases production. All the new fields, rules and dependencies discovered are falling well under the second category and it is normal that they appear when you dig deeper. Trying to add them to the current scope is more of an obstacle of reaching the POC aim of producing a working MVP. Remind of the need to apply 80 20 rule for quickest outcome of the greatest benefits and address remaining opportunities in future reviews. Implementation Team: insufficient wrong skill set experience Make sure you're starting the project with the right team. Based on use case specific and project plan you're able to identify needed skillset and correspondent number of resources of each role. When defining what people should be involved into project consider the following basics bullet items: Evaluate candidate's previous experience and it's relevancy to your project Get feedback from previous involvements of the candidate Make sure candidate has correspondent certificates in Automation Academy (use Reports: How to check trainees progress and certification) Depending on the project you might also need to consider candidate's foreign language, ability to travel, visa's etc If you still got team members with wrong insufficient skills into the team, do not panic. Depending on the project duration you might have a chance to get them up to speed with necessary knowledge and skills. If you do not have enough time on project schedule, either try to replace a team member that doesn't fit or just add more senior resource into the team for guidance and mentoring others. Your focus as a DM should be successful delivery of the project and happy client in the end, but please also keep in mind that mentoring and guiding team members is a very important part of your responsibilities. When the project is finished do not forget to note feedback on every team member and share it with your colleagues, so that they could pick proper team members for their next projects. Customer Dependencies (Business, IT, etc.) During use case implementation you will always face dependencies on 3rd party (usually client). Frequent examples are: hardware or software needed for the implementation, questions on requirements, documents and other use case related data, progress and schedule of the other team you need to integrate to, etc. It is strongly recomended to define list of critical dependencies on early project stages (preferably on estimation and planning phase) and manage them as project risks. In a perfect world all these critical dependencies should be listed in SOW as pre requisites to successfull delivery, however it is not always possible to have all of them defined on early project stages. Maintain list of critical dependencies always actual in terms of severity, potential impact and planned dates of resolution. Keep it transparent to the client, team and all involved parties. Regular project meetings should include walk through sessions where you should be highlighting most risky dependencies and their impact on budget schedule. Focus on helping to find solutions or workarounds to customer problems, not on proving that these problems are not yours otherwise at the end of the project you might find you were right all way through but customer is unhappy and the project is not seen successful. Project Underestimation Underestimation is a regular pitfall faced by all DMs. As a result you might run out of budget before you successfully deliver the project. When faced on a project, try to do the analysis of the reasons of underestimation. It could be insufficient information provided by client as well as mistakes done during initial project analysis. Sometimes it could be related to misunderstanding on client team side of what should be done in scope of use case implementation. As always DMs focus is successful delivery. Work closely with client to understand what could bring you back on track in terms of budget: cut down the scope, agree with client for additional budget allocation or take a decision to cover exceeding budget at the cost of vendor. The important thing is to learn the lesson why underestimation took place and how to prevent this in future. Make sure factors that impacted the original estimate are considered during future project estimations. Budget and Schedule Slippage Rule of a thumb DM should regularly track project budget and schedule. As soon as you notice the possibility of being behind the schedule or out of budget you should understand the root cause. Very often the root cause will be one of the above pitfalls, but it could also be many other reasons: underestimation or wrong planning, misunderstanding of requirements or success criteria, bugs in automated apps, etc. While it is important to understand who's responsibility it is, your focus should be on getting back on track in terms of schedule and budget. Depending on the use case client root cause specifics you might consider agreeing with client to increase the budget or prolong the schedule; decreasing project scope could also be an option. As a vendor you might also consider a decision to cover budget slippage or add resources to get back on schedule. Main recommendation would be to focus on successful delivery and leave searching for guilty for retrospective or post mortem. Scope Creep Implementation Team: insufficient wrong skill set experience Customer Dependencies (Business, IT, etc.)) Project Underestimation Budget and Schedule Slippage "},{"version":"10.0","date":"Aug-06-2019","title":"implementation","name":"Implementation","fullPath":"iac/core/delivery-manager/implementation/implementation","content":" Typical Phases in Implementation "},{"version":"10.0","date":"Aug-06-2019","title":"implementation-support","name":"Implementation Support","fullPath":"iac/core/delivery-manager/implementation/implementation-support","content":" Delivery support model To set project for Success Delivery Manager shall make sure that support channels are established and utilized properly while Delivery. Support during implementation is provided on several levels via multiple means. On Developer level Automation Academy Best in class training providing you skills for the future work: from learning about bots to Solutions Architecture certification Forum Join our community of practitioners to get tips and support from expert users. Knowledge Base Wikipedia on WorkFusion: Release notes, detailed product information, and support. On Program level Expert Connect unique WorkFusion program that connects partner Expert to WorkFusion Experts in key implementation areas (ML, RPA, Architecture, IT Ops, Solutioning) Partner Service Desk get support in a managed and effective way. Refer to internal lead of WF practice if you don't have access to it WorkFusion Delivery Manager leverage best practices from seasoned DMs, SPOC for all technical questions related to correctly applying WF platform On Customer IT level even though customer may not see why it's important, it's essential to establish that support channel early Business Applications Support ensure you have identified key person and he she's is engaged in delivery Infra and IT Support ensure you have identified key person and he she's is engaged in delivery Product Support get support from Product Team in an effective and managed way according to SLA . See more details in Product Support,Go Live(production)andSupport Post ImplementationSupport) Expert Connect Program Where partner has already established internal WorkFusion COE for delivery there's Expert Connect Program that helps to sort out issues in delivery with the help of WorkFusion Experts. This is mainly done via weekly calls. If no such COE exist Delivery Manager should refer to Partner's WF practice Lead. Delivery support model Expert Connect Program "},{"version":"10.0","date":"Aug-06-2019","title":"ml-project-implementation","name":"ML project implementation","fullPath":"iac/core/delivery-manager/implementation/ml-project-implementation","content":" ML Implementation Phases You can get acquainted with full ML Use case Life Cycle in ML COE documentation. On high level, its typical phases include: Preparation of the dataset, including analysis of documents and fields that would allow to choose the right approach to creating a training set of them. At this stage MLE is not involved; DA(s) work with SMEs. Initial tagging, where SMEs get qualified and tag a set of documents and DA checks their work. Usually adjudication is used to ensure quality of tagging at initial stage. The number of documents tagged at this stage is chosen arbitrarily, based on the speed of tagging, size of the dataset and project plan in general. However, this number has to be sufficient for model to train (e.g. couple of hundreds records). It only makes sense to differentiate such stage if the dataset is big (that is, not equal to this initial batch) and at least two trainings of the model will be conducted. Then the best practice is to start the first training before the whole dataset is tagged: it allows to see the first results and have the MLE start working on model tuning based on these results in advance, without waiting for the whole dataset to be tagged. It is critical especially if tagging takes long time and the project timeframe is limited. Model training & tuning is the phase where MLE kicks in. Once the number of documents sufficient for training is accumulated, MLE trains the model and starts to work on its tuning. SMEs continue to work on tagging (if more tagging is left to be done). DA helps MLE to analyze results and come up with logic for post processing and continues to check the quality of SME tagging. This phase can occur only once in case of very short POCs or can repeat multiple times in case of bigger POCs, pilots and especially production projects as many times as needed to arrive at the desired quality and coverage of automation. In the latter case once MLE applies some tuning to the model and SMEs accumulate new batch of tagged documents, the model is trained again on the increased dataset. Then analysis and tuning continue on new results, and tagging continues if applicable. Final testing is conducted once the iterations of model training and tuning produce the result required by success criteria. Once customer provides sign off on the result, model can be launched in production. Details of Final test are covered in ML Project Results section. If the project involves both ML and RPA, the development of RPA part will happen in parallel with ML and usually independent of it. HPO training In WorkFusion training of new ML models is done in two stages: Hyper Parameter Optimization (HPO) finds the best parameters Metamodel for the supplied Training Set and then Model training itself uses the Metamodel to train the ultimate model. See HPO vs Metamodel for more information. Default HPO time limit configuration is 2 hours per field since SPA v9.0; until SPA 9.0 it was 10 hours per field. It is possible but highly unlikely that HPO for a field finishes before this time limit, because the optimization aim is to reach 100% quality (F1 metric) in experiments, which is something unlikely to happen. It is not recommended to lower the time limit, either, as the quality of the resulting features will likely be lower if you do not allow HPO to run enough experiments. How to work with the different input formats How to prepare for AutoML Format of input file on which the model will be trained is important both from the perspective of pre processing (e.g. digitizing it in case of pdf) and from the perspective of how successful the training on it will be. Let's consider some possible input formats and how to deal with them. HTML Can be directly processed with AutoML Image Send to OCR PDF Digital PDF can be processed with Java code. PDF doesn't have fixed format and new documents will require an additional effort to process. Document formatting will be lost in most of the cases. Send to OCR. After OCR of an image or pdf, we receive output in two format: xml and html. For ML it's important to use xml, as it contains more information about the original document. It is also advisable to use ABBYY OCR (its engine is used by default in WorkFusion) where possible because it produces xml that is most useful for WorkFusion ML models (e.g. contains tags that are directly used by some of the ML features). Excel Rule based approach using Java Convert to HTML before process with AutoML Excel input is complex: excel files can be huge in size and after conversion to HTML it might be an issue to display them in human task in WorkSpace. An approach in such case would be to split excel files by 50 100 rows before displaying these parts in human task and then combine the output back to what is required. Word Get plain text using Java Convert to HTML using Java Convert to PDF or Image and send to OCR Email Work with email body like with the plain text Plain Text Use directly with AutoML What approach to choose: ML vs Rule Based IMAGE (General approach: ML) This type of documents usually comes as scans. Typical workflow for images is the following: 1. Make sure that an image has sufficient resolution (at least 300 dpi) and convert it if needed (using ImageMagic library) OCR step HT or ML step to extract information from the document. PDF (General approach: ML) PDF documents can be categorized in three different types, depending on the way the file originated. How it was originally created also defines whether the content of the PDF (text, images, tables) can be accessed or whether it is “locked” in an image of the page. “True” or Digitally Created PDFs Digitally created PDFs, also known as “true“ PDFs, are created using software such as Microsoft® Word, Excel® or via the “print” function within a software application (virtual printer). They consist of text and images. Both the characters in the text and the meta information have an electronic character designation. With ABBYY FineReader 14 you can easily search through these PDFs and select, edit or delete text similar to how you would do that in other editable formats, such as Microsoft® Word. The images in digitally created documents can be resized, moved, or deleted. “Image only” or Scanned PDFs When scanning hard copy documents on MFPs and office scanners, or when converting a camera image, jpg, tiff or screenshot into a PDF, the content is “locked” in a snapshot like image. Such image only PDF documents contain just the scanned photographed images of pages, without an underlying text layer. Consequently, image only PDF files are not searchable, and their text usually cannot be modified or marked up. An “image only” PDF can be made searchable by applying OCR with which a text layer is added, normally under the page image. Searchable PDFs Searchable PDFs usually result through the application of OCR (Optical Character Recognition) to scanned PDFs or other image based documents. During the text recognition process, characters and the document structure are analyzed and “read”. A text layer is added to the image layer, usually placed underneath. Such PDF files are almost indistinguishable from the original documents and are fully searchable. Text in searchable PDF documents can be selected, copied, and marked up. Processing pdf workflow depends on type of PDF. In case of searchable or 'true' pdf we can get content of the files using pdfbox apache library. 'Image only' pdfs should go through OCR step at first (the same workflow as for images) EXCEL (general approach: rule based) Both approaches can be applied for this type of document, but generally it is much easy to implement rules based approch cause excel is structured document. ML approach can be used if the customer has many different templates of excel documents. Please note that you can face some issues while viewing and extracting data at the manual task step for the document with more than 50 rows. In addition to this please keep in mind that you should convert excel to html before sending the document to MT HTML (general approach: rule based) For html files we also can go with both aproaches. Please be aware that if customer has well structured HTML format probably the best solution is to use xpath to extract data from documents PLAIN TEXT (general approach: ML) ML approach is preferred for this format, but rule based also can be applied (for example we 100% sure that invoice number is the first word in the document). Please note that plain text is the worth case for the ML approach cause this format does not have any additional information (like html tags). OTHER FORMATS You may encounter other types of documents. Review the structure of the documents to make the right decision on the use of the approach Model training & tuning Tuning of the model by MLE is done through AutoML SDK. While MLEs can run training from SDK, Control Tower has a set of AutoML components used to: Create dataset (Manual Task, Automation training set) Manage quality of dataset (Adjudication Rules, Qualification Task) Run HPO and extraction (Automation Business Process) Monitor & troubleshoot training (Marathon & Mesos, WFML Job data datastore) View & analyze results (Automation Chart and Dashboards) Running (HPO) training from Control Tower To start model (HPO) training from CT, the following steps are taken: Create manual task selecting ML Model, BP Use Case and Training Set (assuming training set is ready and uploaded as Automation training set to CT) in the Automation options Creating a Manual Task. Run and stop the task to start model evaluation Starting a Training Process. Note that to see the training BP, you need to select Automation Training filter in BPs view by default training processes are hidden. Once training finished, you will see AUTOMATION AVAILABLE label on the manual task. Then you can click on the label, set accuracy threshold and apply recommendation to create a process with Automation sub process in it Setting Up an Automation Chart. Now you can run the Automation process with the trained model extracting data Running a Cognitive Business Process. Note that you can automate manual tasks that are steps of a business process in the same way as you automate standalone manual tasks (see Automate Manual BP Steps). Automation Business Process in Detail Automation Business Process is a collection of steps through which training, extraction, automatic quality control, statistics calculation and all the other surrounding activities of automation are executed. Automation settings step is where BP parameters are configured (e.g. enable disable Statistical Quality Control and Statistics calculation). After the Settings step, BP divides in two flows: Training and Production. Training is launched first through the steps that were described above. Once a trained model is ready and you apply recommendation, manual task on which it was started (either within BP or standalone) is replaced with this Automation (sub)process and Production flow in it is activated. Production flow of the Automation BP is where trained model is applied for extracting data. It has the following structure: 1. At Extract Information ML algorithms try to extract data automatically The Post Extract step does Post Processing and Normalization of data extracted by ML Model. Prepare data, basically, prepares data for the Statistics Calculation step. A special Composite Rule (Skip manual steps or is the data extracted automatically ) checks the Extract Information step results: if failure or partial success, then Records are sent to Human Workers. Note that in case of failure, the data extracted by the model is cleared and human tag all from scratch; in case of partial success the data extracted by the model is shown to human and they complete what was missed. if success, then apply SQC (Statistical Quality Control) or go to Statistics calculation is SQC is disabled. If SQC is enabled and inspection is required, then send a batch of Records to human Workers (the same original manual task is present inside the SQC subprocess). if a BP uses Gold Data (documents tagged by Human SMEs), the execution goes directly to the Statistics Calculation step without SQC. Statistics Calculation step provides calculations for quality statistics and saves results to data store. Enabling Statistics Calculation is required to use Automation dashboards because they use statistics that is saved in the database at this step. To find details on all the steps, go to Automation BP Description. Common customer questions Records where the model is not confident are verified by human but how do I know if the \"success\" decisions of the model are in fact correct SQC or AutoQC uses of statistical methods to the monitor and maintain the quality of model decision. AutoQC sub process chooses the optimally cost effective combination of automated machines and cloud worker that always deliver at or above the acceptable quality level. The main concept of AutoQC is to take a Sample from a defined Batch of items (BP records, documents, etc.) and verify each item in that sample (which is done by human in the manual step inside the AutoQC subprocess). After the sample verification, the whole batch is considered as accepted or rejected depending on the Rejection Limit. This way AutoQC makes conclusions on correctness of what the model considers to be success decisions based on a sample. Read AutoQC to understand this concept and its mechanics in WF. Model improvement workflow Model release and move to production Trained model can be moved from one environment to another for guidance on how to do it refer to Migrate Trained Model. When more data is accumulated, Model Retraining can be launched manually or set up to launch automatically. For a production process the latter case would mean that the model is retrained in production environment and automatically applied in place of the model that is currently running, therefore it is rarely used. Depending on customer architecture and security requirements, two Model (Re )Training StrategiesTraining) can be applied: re training in DEV or in PROD. ML Implementation Phases HPO training How to work with the different input formats How to prepare for AutoML What approach to choose: ML vs Rule Based Model training & tuning Running (HPO) training from Control TowertrainingfromControlTower) Automation Business Process in Detail Model improvement workflow "},{"version":"10.0","date":"Aug-06-2019","title":"kickoff","name":"Kickoff","fullPath":"iac/core/delivery-manager/implementation/kickoff","content":" Introduction A kickoff meeting is the first meeting with the project team and the client Kickoff is usually conducted after all the pre requisites are met and implementation is ready to be started. Preferably the DM should come onsite to the customer location for the kickoff. As the kickoff is often the first time when all the participants of the project get together (while results demo might be the last time), the main aim of the kickoff is to bring all the teams and stakeholders on the same page regarding what they are doing and with what aim, as well as to establish their roles on the project. Typically the meeting covers the following subjects: 1. Project Overview This section aims to establish or verify common ground for those aware of the project and give background to the attendees who are new to the project. Slides for it would contain extracts from SOW on project Description, Deliverables, Success Criteria, a map of the process flow to be automated is included to illustrate the scope of work. 2. Teams Introduction Here all the teams who work on the projects should be introduced, including: full names roles on the project escalation levels contact emails (phones where applicable) To better illustrate the escalation levels and roles, it's a good idea to put the names in a tree structure showing who reports to whom in each department squad. 3. Project Governance Responsibility matrix shall be provided to clearly articulate areas of responsibilities for each team member Project plan DM articulates in what format the progress will be tracked (see Project Tracking), with key milestones communicated to the stakeholders Communication schedule the implementation team should meet regularly, preferably on a daily basis. Other key stakeholders can meet on weekly, bi weekly or monthly basis 4. Immediate Priorities Next Steps The end of the kickoff is a good opportunity to list out all the open issues, urgent activities and blockers that impact the project. An owner shall be defined for each item. Point out the dependencies what activities cannot start until these actions are performed (e.g. until the customer provides the documents samples, team cannot start review of dataset and manual task implementation). Also here one could raise business questions (e.g. regarding logic in the flow or documents for ML), organizational questions (VPN, access to systems, team logistics) or let the customer team bring up what they want to know or add to the action plan again, if the time allows and the audience is willing to proceed to additional topics. Depending on the audience and overall status, these topics can be changed dropped or more topics can be introduced, such as: WorkFusion overview typically done by presales sales when there's someone new in the audience who has not participated in the sales cycles; Concerns it might be wise to bring up something that is out of general kickoff flow but you know is of interest to the customer (e.g. related to infrastructure, legal). Sample kick off deck.pptx Introduction 1. Project Overview 2. Teams Introduction 3. Project Governance 4. Immediate Priorities Next Steps "},{"version":"10.0","date":"Aug-06-2019","title":"ocr","name":"OCR","fullPath":"iac/core/delivery-manager/implementation/ocr","content":" This section provides information on working with OCR where it is used in WorkFusion projects: requirements to installation, scaling, relation to model quality, approaches to improving. Introduction OCR might be used for ML use cases when we are dealing with documents in the form of images or PDF files. It is also regular case when OCR is used for RPA use cases rule based extraction from documents. Some of the things to consider when your project involves OCR are: Installation if WF OCR is used, customer will need an OCR server License during the installation process you will be asked to request OCR license (see How to request OCR license ). This should be done in accordance with the contract SOW, which specifies the number of OCR pages granted to customer at set time periods Scaling in typical deployment only one OCR server is installed; however, if there is a need to scale OCR to process more volume, a cluster of several servers can be created (see Scaling OCR ). Check OCR Capacity Planning for information on what volumes can be processed with what hardware and when scaling would be required. Documents Quality Common customer questions Will the quality of our documents be sufficient for the model What is the required quality Why not all of the provided documents were used in model training At the stage of choosing documents for training set Data Analyst should have filtered out the documents where OCR has not recognized something or messed up some fields. Still if the quality of original documents is low ( <300dpi) for example, customer could not provide better quality documents you can encounter significant problems with statistics. If OCR does not recognize part of document you can end up with a lot of false negatives; if OCR recognizes some fields in a wrong way they can also be missed by the model (resulting in FN) or extracted by the model in the wrong form in which OCR recognized them (resulting in false positives (FP)). The action plan in this case depends on multiple factors: is customer able to provide higher quality documents, is the number of available documents sufficient to filter out the worst ones, are OCR errors expected to be counted towards model stats. If there is no way to receive better quality documents, the focus will be on tuning OCR, writing post processing that will help to mitigate OCR errors, filtering out the worst documents from the training set. In any case it is better to agree to exclude OCR errors from model stats and set it in the contract along with the desired dpi. Refer to the description of OCR influence on dataset) to understand the connection between OCR and model quality and find out how to act with documents that are candidates for training set in case of different level of OCR quality. OCR Results Improvements There're approaches how to improve OCR output for the documents even in case of insufficient quality: you can prepare input images (or PDFs) using imagemagick tool. Increase DPI, improve brightness, turn image to black and white, etc you can play with the parameters that are passed to OCR api to help OCR engine understand where on the document tables are usually located, what language is used in the documents, etc. See more info here: How to tune OCR How to improve OCR Results OCR FAQ OCR process is resource consuming. In case in scope of your use case it is expected to work with heavy flow of documents (especially multi page documents, 50 pages) you should be aware of OCR scaling approach. Introduction Documents Quality OCR Results Improvements "},{"version":"10.0","date":"Aug-06-2019","title":"ml-project-results","name":"ML Project Results","fullPath":"iac/core/delivery-manager/implementation/ml-project-results","content":" Measuring results Common customer questions What are model results calculated on Were these documents used for training ML model results are represented by statistics calculated on a test set of documents that are representative of the dataset and have not been used for model training. The statistics are calculated by DA and compiled into a report. It is beneficial to present final results of implemented ML use case in light of the impact on client business. Most often that would be potential savings in terms of decreased number of employees (FTE) that have to be involved in the process after use case is deployed to production. It is recommended to gather metrics reflecting how much time money is spent by client on running the process before automation is deployed and compare them to the metrics after automation is in place. In some cases overall process accuracy and predictability could also be improved after implementing automation. Try to analyze this aspect too and present it to client along with the ML model stats. Conducting Final test Final test should be agreed in detail with the client in advance, clarifying the following aspects: how the statistics are calculated. Usually the same formulas are used by WF unless customer specifies otherwise. However, you must clarify in advance if the customer expects the statistics to be calculated in a different way First of all, know your SOW the way of calculating statistics may be described there. Oftentimes SOW includes lines stating that OCR errors and fields with low representation are excluded from stats calculation. test set creation size of the test set (usually ~20% of the training set size) provisioning SMEs to tag the test set in advance to calculate adjust model stats who chooses documents for the test set. Customer might prefer to choose the documents in this case you must have your DA participate in choosing the documents and review the test set for the purpose of making sure it is representative. Otherwise the final stats will be skewed. way of presenting the results. Some of the ways (oftentimes combined) can be: showing the average document level stats in a deck table with field level stats launching the test set documents tagged by the model in a human task for SMEs to review manually send a report of extracted data to customer so they can also calculate the stats from their side Low stats and what to do if it happens to you In this section we will discuss the common reason that can lead to the model statistics being below the desired level, as well as the ways to overcome or manage them. The below problems can be identified either at the start in the middle of the implementation or at the very end close to presenting the final results. In the first case the preferred approach would be to try and adjust the plan, take corrective actions proposed here or identified by the implementation team. In the second case or due to other reasons such as lack of time or resources to take corrective action you might have to follow a different approach that would mostly focus on communicating poor results in the right and positive way. When you have to take this approach, it is key to be aware of the reason for the results you are getting to be able to explain them in constructive way and propose action plan that can be followed in the next phase or future project to build on the current achievements and improve them to desired level. Lack of understanding of the way ML works by the customer can lead to suspicion and negativity that is overcome with open and clear guidance through what was done, what it resulted in and why, as well as and most importantly what to do next. Start with Machine Learning Basics to be prepared to speak on the topic of Machine Learning. Dataset quality Though customers find it hard to believe, dataset quality is the most common reason of low model stats that WF come across in deliveries. Get acquainted with the dataset requirements to know what a quality dataset for model training is and be prepared to understand the pitfalls found by DA as well as discuss with customer the reasons of the low stats and ways to improve the results. \"Garbage in garbage out\" is the main principle of supervised machine learning, which means if the training set is not good enough and big enough, the model won't show good results. In other words, if the accuracy of you training set is, say, 87%, there is no way the model output will go above 87% and reach the desired 90%. Common customer questions What do you mean by \"incorrect tagging\" People who tagged the dataset are the experts, shouldn't they know better Why not all of the documents tagged by SMEs were used in model training Quality of tagging Most commonly it is the quality of tagging done by SMEs that undermines the dataset quality. It is also the most difficult reason for customer to accept, as SMEs are the experts who know best how and where to find the needed data so it seems they by default cannot be wrong. And to a large extent it is true the problem is that ML models perceive data different from how human do it, with a large focus on data context, not simply content. Therefore, for SME it does not matter where to take the data from as long as the data is right such as take customer name from the header of the invoice in one record and from the signature in the next record when the same name is available in both places. For the model such inconsistent tagging poses a huge problem, undermining model confidence and therefore lowering the results. For that reason it is so important to have a DA teach and qualify the SMEs, check quality of their tagging and correct them in timely manner, and use other tools for boosting tagging accuracy like Adjudication or Moderation tasks. Additionally, you can still find that SMEs and even DAs make mistakes just due to the nature of human. On average human accuracy is estimated at 90 95%, so reviewing the dataset is an always actual activity that can help to achieve better results without doing any other changes to the model that require developer involvement. Check out the typical cases of incorrect tagging. Common customer questions Why the model did not extract well from this document Why is it necessary to involve DA in choosing documents for the (final) test set Representativeness of the training set and test set Another driver of dataset quality is its diversity. One of the reasons of low stats may be that some document template is not represented or not enough represented in the dataset. To understand if that is the case, check the documents that have lowest stats after model extraction (records with highest number of false negatives and false positives). If you identify that there are very few or no documents with similar layout in the dataset compared to other layouts, it is quite understandable why the model has not learnt how to confidently extract data from them. A solution could be to add more documents of such template if the customer has them available or explain that the extraction can improve after more of such documents are accumulated in the normal course of work. Similar situation can occur in the calculation of the final stats on a test set, if the test set was not properly prepared to be representative of the population of documents in the training set. In such case the test set might contain template(s) that were not included in the training set, which means the model was not trained on them at all and is unlikely to extract data from them well. In this case the most logical solution would be to exclude such templates from the test set or at least explain how they skew the data and why. The option of reshuffling the training and test set and retraining the model to include the missing template is unlikely if we are dealing with the end of a POC, but might still be an option if it is a production project and the time allows to retrain the model. OCR quality Low model statistics could be caused by the issues with the OCR. In case of issues with the document quality or with OCR project team might need to do retagging, model retraining and do changes to post processing implementation. Hence it is a must for every project to plan activities related to verifying OCR output and making sure all necessary values are there. See more details in the below section related to OCR * * Common customer questions Why this field cannot be included in model training Why the model fails to extract this field properly Representativeness of fields Check the field level stats to see if some specific fields are pulling down the average figure. If so, one of the reasons it is happening might be that these fields appear less than others in the dataset. Check the number of these fields available in gold data (such stats can be checked in WF UI when automation training set is chosen in automation settings of a manual step). If these fields are hardly present in the dataset, it means the model would not be able to learn from them properly. In such case, similarly to the case when some document templates are not represented well enough, adding more samples with such fields to the training set should help. If such agreement was made with customer, these fields can be excluded from model stats calculation until their number in the dataset reaches desired level (e.g. 25%). In some situations the fields that were initially included to the list for automation in fact appear so rarely that no or hardly any samples for them can be found throughout the project. In such case it makes sense to exclude them from the training set at the very start. They can be still left in a manual task for the users to be able to extract manually when they encounter such fields in the course of work, but should not be tagged for the training set Difficult fields Common customer questions Why the model fails to extract this field properly Why do SMEs need to change the tagged text's value to a different format (normalize) Why are the stats so low in the first model training Another reason why some fields show lower stats than others might be that such fields are more complicated e.g. free text fields, address, requester name. Then you can focus on improving the average stats through accuracy&automation rate of simpler fields and adjustments to the model that would help to better recognize the difficult ones: Improve post processing identify if there are any validations and rules for example, that can be derived from other related fields that will help to make better decisions on the complicated field. MLE and DA can do this with the help of SMEs. Custom model required in the worst case you might find that the field is so unusual and complex that generic model just cannot handle it well enough. Usually complexity of fields will be estimated prior to project start, but such estimation cannot be exact until you actually try to train the model. Consulting with ML COE will help to understand if there are any ways to solve the problem through postprocessing or further development would require involvement of Data Scientist. Consistency in calculation Common customer questions Why are the stats lower on the model trained with more documents shouldn't the quality grow with addition of more docs This point is more related to model learning curve. If you expected to see model stats improve over time but cannot identify a pattern in statistics, one reason for it might be inconsistency in the way that stats were calculated. If you compare stats among model training done on different number of documents in the training set, the model version, postprocessing applied, and test set you are measuring on should all be the same. Another typical case where you can come across this problem is if you use the whole training set instead of a test set for calculating and comparing stats. Then with adding more documents in the next iteration you will have different number of documents you are comparing as well as potentially different template representation in the two training sets. It is natural to observe model stats (calculated on the whole set) dropping when you add new documents the model indeed might extract them worse, than the previous set of documents. However, in reality it does not mean that you actually got worse result now the model might extract from the new documents at least something, while before it could not extract anything. That is why it is so important to have a proper test set in the described case it should be comprised partially of the old documents from the training set, partially of the new documents added to it recently. Both models should be compared to this same test set and then you would be able to see the real change in result. 4 . Natural reasons 1. Dataset saturation flattening of learning curve To add more to the topic of learning curve, the model quality grows with addition of more documents only if new documents are adding more representation to the training set. If all of the variations of document templates are already present in the dataset in enough quantities for the model to learn them, then adding more documents of the same layouts will not increase the quality any longer. What you will observe is that the model learning curve flattens at some point, and depending on how complicated the documents are and how many variation they contain, the faster it will happen. E.g. if there were some 5 different templates, the documents have repeated simple structure, and you managed to collect an equally distributed training set containing all of these templates, you might have the model learn all it could already on ~300 documents and stats not growing further with adding more records. The only way to grow the quality and automation rate in such case would be to improve postprocessing or ultimately customize the model. In cases when the dataset is more complex, thousands of documents will be needed to achieve comparable results. Below is a representation of a typical learning curve of the model steep at the beginning and flattening at some number of documents added to the training set training iterations passed. Do not read it as prediction this is a fake depiction only. ** b . Model quality limit* * Last but not the least law to remember and always communicate to customer is that model quality will never be 100%. "},{"version":"10.0","date":"Aug-06-2019","title":"ml","name":"ML","fullPath":"iac/core/delivery-manager/implementation/ml","content":" This section describes the peculiarities of implementing an ML project, including prerequisites and typical SOW of ML project, life cycle, measuring results, conducting final test and dealing with the issue of low stats. Below you can find a case study of ML project that demonstrates the problems and solutions discussed in this section. {height=\"250\"} "},{"version":"10.0","date":"Aug-06-2019","title":"rpa-project-implementation","name":"RPA Project Implementation","fullPath":"iac/core/delivery-manager/implementation/rpa-project-implementation","content":" Some of the typical cases and solutions encountered during implementation of RPA project include: Constant monitoring for incoming data Concurrent access to shared resources Advanced Split Join Maker Checker type of flow Non Blocking Bot Execution Attended Automation Best practices for RPA projects can be found at RPA Best Practices "},{"version":"10.0","date":"Aug-06-2019","title":"rpa-project-results","name":"RPA Project Results","fullPath":"iac/core/delivery-manager/implementation/rpa-project-results","content":" RPA results depend on target Results of RPA project or RPA part of wider projects can be measured in terms of: effort saving Effort saving is the most often sought gain, especially in the first projects where the aim is to achieve maximum benefit at lowest cost. Effort saving is typically measured in terms of FTE or % of manual work reduction. capacity increase Transaction volume processed per time unit. speed When differentiated from effort saving, speed up of time spent on a process might not necessarily mean reduction of significant number of FTEs but could lead to other benefits like customer satisfaction or meeting tighter SLA. Speed can be measured as time spent on the process itself or or some idle time elimination, e.g. faster time to response. quality reliability improvement Another reason of automating a process can be not connected to cost reduction at all, but tied with quality improvement leading, e.g. to higher customer satisfaction or compliance. Quality improvement can be measured in % of errors. Target metrics should be clarified in advance and preferably set in contract. Measuring results The way how results will be measured: must be determined and agreed in advance with customer clarifying the following aspects: How results are calculated. If a POC will measures be based only on the happy path or exceptions will also count against success measures What is the effect of other activities preceding and following the RPA flow itself on the effort reduction These questions and the like can undermine the results if they are not properly handled. For example, you might count how long the RPA flow built based on the described scope takes to run and compare this time to the time of original process. However, customer might add in the time that it takes to manually process exceptions and get a totally different figure. Acquiring actual production data in test systems what data you are testing on is it actual, copied from production system is sufficient number of records available in the test system that covers all the intended scenarios Cases are known when lack of actual data in the test system made it impossible to conduct UAT and properly demonstrate the functionality built within POC to stakeholders, ruining success of the implementation. Planning UAT phase on what data is it done what test scenarios it covers are the scenarios agreed by customer who conducts it If users, when and for how long are these resources committed Are the users trained to handle the flow How results are presented live demo, video, report with statistics from UAT phase, deck with corresponding graphs, etc "},{"version":"10.0","date":"Aug-06-2019","title":"rpa","name":"RPA","fullPath":"iac/core/delivery-manager/implementation/rpa","content":" This section deals with specific of implementing an RPA project with WorkFusion, covering the topics of design and estimation, typical cases and solutions, measuring results based on target."},{"version":"10.0","date":"Aug-06-2019","title":"stages-of-data-set-collection","name":"Stages of Data Set Collection","fullPath":"iac/core/delivery-manager/implementation/stages-of-data-set-collection","content":" 1st Stage Analysis of input documents 2nd Stage Tuning and analysis of OCR quality, analysis of layouts' representation 3rd Stage Preparation for tagging 4th Stage Tagging and Validation 5th Stage Splitting documents into Training and Test Sets 6th Stage Testing, analysis of results and improvement of ML Model NOTE All the steps in the process are totally interconnected. The output of the previous step is the input for the next one. If you don't reach good quality at any of them you won't achieve success criteria of model performance. 1st Stage Analysis of input documents Prerequisites Aim: To investigate and verify for all the next steps documents formats, layouts; fields meaning; special business logic. Input: SOW, list of fields, original documents Output: Fields logic defined Steps: Study the business logic of the documents, use case, customer's document flow Study documents, analyze quality and representativeness Study fields logic, consider answer types and rare fields Finalize fields output format Consider manual task design and logic 2nd Stage Tuning and analysis of OCR quality, analysis of layouts' representation Prerequisites This stage requires to group the documents by structures. The most optimal way is to ask the customer to group the documents and DA to verify the grouping. Aims: the best OCR configuration the understanding of final OCR quality and layouts' representation (i.e. number of documents for each layout) in order to collect good training set and representative test set in future; the understanding, the evidence and the control of risks. Input: Original documents grouped by structures Output: The best OCR configuration is found; All documents have passed trough OCR; Final report on OCR quality and quantity of each layout with all the risks identified Steps: Take samples of each structure Run OCR Make an interim report on OCR quality and main problems that should be solved Tune OCR (repeat steps 3 and 4 until OCR quality is acceptable in at least 90% of documents) Test of chosen OCR configuration on control sample Final report on OCR quality OCR for all the documents Additional RISK Unacceptable OCR* quality *(i.e. missed and corrupted fields; corrupted structures, etc.) can be the most serious reason of bad ML Model results from both training and testing aspects. Small number of documents of some particular layouts (i.e. some layouts' representation is bad) can prevent high quality of training set. CHECKPOINT Interim Report(s) on OCR quality after every round of OCR tuning .rwui id 2c43b8e4 2b8d 4234 b1d9 7df0bf977aeb {color: FFFFFF !important; background: rgb(121,201,123);}.rwui id 2c43b8e4 2b8d 4234 b1d9 7df0bf977aeb:hover {background: rgb(112,186,114);}.rwui id 2c43b8e4 2b8d 4234 b1d9 7df0bf977aeb .rwui _icon {color: FFFFFF !important;}Download the example Final report on OCR quality and quantity of each layout .rwui id e3a90847 2631 4016 b72d 6928d29a361d {color: FFFFFF !important; background: rgb(121,201,123);}.rwui id e3a90847 2631 4016 b72d 6928d29a361d:hover {background: rgb(112,186,114);}.rwui id e3a90847 2631 4016 b72d 6928d29a361d .rwui _icon {color: FFFFFF !important;}Download the example 3rd Stage Preparation for tagging Prerequisites Aim: To qualify and instruct SMEs to: minimize the number of mistakes they make organize their work in the most optimal way Input: OCRed documents Output: manual task, batches of documents, SMEs qualified for tagging Steps: Design manual task Create qualification task Provide general rules of tagging for successful model training (in brief) Proceed qualification task (SMEs), exclude SMEs, unable to tag with the required level of accuracy from further tagging process Split documents into batches Define tagging logic for each batch Create clear instructions for each batch Assign a specific manual task for each SME, provide a batch and instructions to it Create a data tagging tracker to track SMEs speed of tagging Additional RISK There are several risks that can influence the process of training: SMEs available part time and their working hours are not regular they always change and substitute one another delays in proceeding qualification, training, extra effort to explain the process to new people, so extra time can be needed unqualified and under skilled SMEs selected for tagging a huge risk of incorrect and inconsistent tagging and as a result extra effort to check data set, missing mistakes in data set can cause bad ML results * That means that to proceed with tagging can ONLY SMEs that obtained qualification either while initial qualification task or second (final). If a person was not able to proceed with qualification for the second time that person should be replaced, because extra time will be needed and the stage will be protracted too much and the person who is unable to tag known documents correctly twice, hardly will tag other documents better. all SMEs tag all the documents, not in batches the work speed slows down due to necessity to switch between documents and the risk of inconsistent tagging increases CHECKPOINT Qualification The batches and manual tasks are prepared and assigned to SMEs Report provided The check point means that SMEs are qualified and prepared enough to start tagging and that all the necessary preparation for the first tagging iteration is conducted: the manual tasks are ready and SMEs knows what task to take. The following template can be downloaded and used as a report for this stage: {.rwuibutton .rwuiinline block .rwuiiddce83d1a 3f35 467f adf1 90dd663fe80e} Download file 4th Stage Tagging and Validation Prerequisites Aim: To collect the high quality Data Set appropriate for ML training at the maximum possible size during a reasonable period. Input: OCRed documents, manual tasks, batches of the documents, qualified SMEs Output: verified Data Set, categorized documents Steps: Start Iteration of batches tagging initial and following Validate each batch once it's ready Categorize documents to exclude from Data set to retag to put into test set, etc. Provide feedback on mistakes for SMEs Track and report SMEs progress & tagging quality Assign new batches for each SMEs Provide specific instructions for the batch Additional RISK If the previous stages were not performed successfully (definition of fields logic, OCR tuning, SMEs instructing), the Data Set won't be big and good enough regardless the quality of verification. Extra effort and time will be needed to enlarge the Data Set and to clean and retag the data. The following risks should be identified on this stage: Inconsistent tagging Insufficient values normalization Critical OCR errors Rare fields document layouts (will lead to bad ML quality in due to bad representation) Complicated dependencies in extracting logic that cannot be found by OOTB model CHECKPOINT The whole Data Set is tagged and verified Tagging tracker submitted daily and finalized in the report after all tagging iterations: {.rwuibutton .rwuiinline block .rwuiid1608e03f 446b 4aa4 b26f 4529e655c23f} Download file 5th Stage Splitting documents into Training and Test Sets Prerequisites Aim: To split Data Set into training and test set so as to train the model on gold values and to evaluate and test it for possible exceptions in production. The proportion is usually 80% to 20% correspondingly. Input: verified Data Set, categorized documents Output: Test Set, Training Set Step: Split manually or with the help of bot step Data Set into test and training set While data set check all the documents are categorized, so they can be filtered by category and added to test and training set according to their quantity and status. The Data Set is split by taking 20% of each batch into test set. That includes 100% correct documents to count statistics on them and demonstrate how the model was trained, and documents categorized as \"inconvenient\" as well. \"Inconvenient\" documents are those that have corrupted or missing values or layout changed after OCR and included into test set to reproduce production flow and demonstrate how model will perform in real life. Additional TIP Inconvenient documents should be added to test set to reproduce production flow. Statistics should be counted on 100% correct documents and 100% correct inconvenient to demonstrate model performance in production and to compare the results. 6th Stage Testing, analysis of results and improvement of ML Model Prerequisites Aim: To analyze results of ML training and propose ways of their improvement Input: Test set, trained model Output: ML results, post processing rules, report with statistics Steps: Run extraction on the test set Normalize gold data, if required Calculate the statistics Analyze ML results and define reasons for FP, FP FN Develop rules for post processing and other options for improving ML results Compile the report Additional TIP In case of multiple re trainings for any ML model, it's better to count statistics after each iteration of ML training and the delta with previous results to show the ML improvement. All cases of possible post processing should be defined during the results analysis (mostly by DA) and developed by MLE. DM should make sure that offered options are developed by MLE. All the defined options should be applied in statistics to make it's correct. "},{"version":"10.0","date":"Aug-06-2019","title":"faq","name":"FAQ","fullPath":"iac/core/user-guide/faq","content":" Business Processes (BPs)) Tasks Qualifications User Interface User Experience Workforces and Crowds Managing Workers How to communicate with the Workers How do I tell Workers about a change in Instructions How to reject work Business Processes (BPs) 1 How do I kick off a Business Process to run regularly You can schedule a Business Process to run at a specific interval using Scheduled Campaigns. You should upload input csv file and configuration file (.ccf) on an external server (S3, FTP or SFTP) and trigger the Task or BP run at specific time. 2 Can I set up a Business Process to loop through tasks We recommend not doing so. Instead you can use Scheduled Campaigns to kick off the BP at regular intervals and use Data Stores to use updated data in every run. 3 I want to review the results of my process manually. Can I make the review step a part of my Business Process You can add a Moderation Step to your Business process to do so. 4 How do I view results of my Business Process You can use one of the following actions: Go to View All BPs, generate and download a Snapshot. Go to BP > View Results > Data and select Final results from the dropdown. See detailed info here: Run a BP and View Results. Tasks 1 In Task Advanced Options > Availability, the Limit Number of Tasks parameter was set to \"20 for a Run\", but some Workers managed to complete more than 20 Tasks. Adjudication Rule is \"3 0\". Solution: Go to Task Advanced Options > Adjudication. Set the Min of Assignments from \"3\" to \"1\". This means one Worker can work on one task at one time, once it is completed it is returned to queue for another Worker to complete it and then again until the 3rd worker completes it. Explanation: You need to do this because if Min of Assignments is set higher ( > 1), an Assignment would not be considered completed for a worker until the minimal point is reached and calculation begins. In this example: Adjudication = \"3 0\" Min of Assignments = \"3\" Limit Number of Tasks = \"20\" As a result, Worker submitted 20 Tasks and he can continue doing those until 2 more Workers come and do the same 20 that he did and those Tasks are completed. And some workers apparently manage to submit more than 20 Tasks until that happens. Qualifications 1 The Task result data quality is low, or Task Time Line and Effort is high, or too many Records with the Confidence not found status. You should use only qualified Workers: When creating a Task, use System Qualifications and Availability settings. OR Create your Custom Qualification and assign Qualification manually or by creating a Qualification Task. 2 Why do I get the error “Qualification Task requires one and only one custom accuracy based Qualification in requirements list“ When you create a Qualification Task, it requires one Accuracy Based Qualification. For a Qualification Task, Workers will be assigned a score for that particular Accuracy Based Qualification. In Advanced Options > Qualification popup, remove all other Accuracy Based Qualifications from the requirements list. 3 How to assign Qualification to Workers You can manually assign Qualifications to Workers in the following places: Workers > Assign Qualification Workers > View All Workers Task BP Results > Workers See the Create and Assign Qualifications to Workers topic. 4 How to educate Workers and improve their skills You can create a Training Task – create a Qualification Task with the Enable Training Mode checked in Advanced Options > Qualification. Workers are provided explanation messages after they submit Training Tasks. The Gold Data file should include a column with explanation messages. See the details here. 5 What is an Accuracy Based Qualification Accuracy Based Qualifications are used to score Workers based on their performance against Gold Data (in a Qualification Task or even in a production run). 6 What is a private Qualification Private Qualifications are for internal use. They help you give private Qualifications for your own reference to Workers. They are not reflected at the Crowd. 7 How much Gold Data do I need We recommend having at least 3 times the number of Tasks you want Workers to take for the Qualification Task. User Interface User Experience 1 I cannot see some Tasks BPs Records Check your filter settings. You can click the Show button in the top right corner. Somebody might have deleted Tasks or BPs – contact your system administrator for activity logs. 2 Why do I get the message \"Warning! The use case you have selected requires the following column names in the uploaded file\" You should map column headers of your input file in the Upload Data tab to the column header the Task BP requires or upload another input data file with correct column headers. 3 I cannot see some characters in my input or output file. WorkFusion maintains character encoding, but sometimes opening the file in Excel can strip the encoding. We recommend using Open Office and open the file using UTF 8 encoding. Workforces and Crowds 1 How do I create my own Crowd You can learn how to create your own Crowds and Workforces here. 2 How do I create a Crowd for my internal employees SMEs You can give your internal Workers a Qualification specifically for them and attach that Qualification to the Crowd. 3 How do multiple Crowds work You can add Crowds to your Workforce while creating them. WorkFusion will distribute Tasks between Crowds according to priority given to each Crowd. Managing Workers How to communicate with the Workers Dashboard > Worker Feedback widget. You can see new messages and reply to Workers. Workers > View All Workers. Check the Worker(s) you want to send the message to and click on Send Message. Worker Profile > Communication Center. You can see all the messaging history and send messages. Task Results > Workers. Check the Worker(s) you want to send the message to and click on Send Message. How do I tell Workers about a change in Instructions You can either do this via a message or actually \"Add an Instruction\" in the Task Design on the fly. Learn more about Instructions and FAQs here. How to reject work While the Task is still running you can go to View Results > Workers, select the Worker(s) and click the Reject Answers button. As an alternative, you can use the Moderation Flow. "},{"version":"10.0","date":"Aug-23-2019","title":"roles-intro","name":"Project roles","fullPath":"iac/core/user-guide/roles-intro","content":" Every use case, either Information Extraction or Classification, POC Pilot or Production, implies involvement of a team of: Solutions Consultants, Business Analysts, Account Managers, Delivery Managers, and more. Let's define the roles for those who work closely to go live with the Use Case: Delivery Manager, ML Engineer, Automation Engineer Specialist, Data Scientist, and Data Analyst. Delivery Manager — DM A Delivery Manager coordinates the project during its whole lifecycle. Their responsibilities include the following: Validate business process design — data flows, rules, RPA & ML steps Define which models will be needed to train Consult customer about WorkFusion products and offerings Justify a solution to the customer and explain required resources, for example: explain statistics, reasons, methods, resources needed to improve Communicate and explain key risks to the customer Prepare project planning of the WorkFusion implementation Estimate each phase of the implementation project — tag data, prepare data set, train model, develop bots, test whole process Plan project team capacity — number of SMEs, DA, MLE and other roles Staff a project team — conduct interviews, ask the right questions and make decisions (with help of experts in areas where needed) Present the project plan to management and gain approval Continuous monitoring of project status Over time, identify and eliminate the risks arising in the project Quickly find solutions for project related issues like: installation issues, improving data quality, improving OCR quality, deployment issues, etc. For more details, refer to the DM responsibilities page. ML Engineer — MLE MLE's responsibilities include: Launch training and tests of IE Classification models (along with DA) Configure ML environment, makes sure that ML related components are up and running Help DA get ML statistics and model results analysis Define post processing logic in cooperation with DA and implement it Fix OCR errors, to achieve the best P R scores (compliant with customer's requirements) Create custom models using AutoML SDK Automation Engineer — AE AES develops the RPA part of the business process by creating software robots. They also launch training and tests of IE Classification models (along with DA) using out of the box models. Data Scientist — DS Data Scientists handle the most complex cases with deepest model customization, create non standard additional model components, if necessary. Data Analyst — DA Data Analyst builds data models and reporting packages, analyzes large data sets. Part of their responsibilities is writing comprehensive reports and developing problem solving approach. "},{"version":"10.0","date":"Aug-06-2019","title":"answer-types","name":"Answer Types","fullPath":"iac/core/user-guide/answer-types/answer-types","content":" Single value, multivalue, combined Answer Types Most common Answer Types All Answer Types When adding editing an Answer, you need to set an Answer Type. This parameter defines an input field type and format. Depending on the Answer Type selected, you need to set different options, shuffle options, or Data Store input. Single value, multivalue, combined Answer Types Answer Type can have the following types: Single value: the result value is represented by a single string (text, number, date, etc.). Multivalue: the result value is represented by a single string containing delimited values (e.g. \"1,2\", \"2 3 4\", where 1, 2, 3, 4 are atomic values). Currently, there are 2 possible types of standard delimiters: comma (\",\") and pipe (\" \").For Multivalue Answers, you can enable disable the Calculate majority for all atomic values option in Answer Advanced Options. Click here to expand... Option unchecked If this option isn't selected majority calculation will work in the following way for rule 2 1: Answer Combined: the answer is represented by a set of values with different answer codes. Most common Answer Types Answer Type Description Field example Select one A drop down list that allows selection of a single option Classification use case Price A combined input field used to enter monetary value total, price Number Allows entering integers only quantity, bank account number Group of Answers Allows tagging multiple similar combinations of values that are supposed to be grouped together product name, price Free Text Allows entering a single line text string entity type, company name Date Allows entering a date in mm dd yyyy format date Country A drop down list of countries to select (To speed up the selection, you can start typing the country name.) country Check one A radio button group that allows selection of a single option Classification use case All Answer Types Field Type Value Type Separator Can be N A Free text Single Yes Long text Single Yes Rich text Single Yes Check One Single No Check Multi Multi Pipe No Select One Single No Select Multi Multi Pipe No Number Single Yes Price Combined Yes Date Single Yes Time Single Yes Phone Single Yes E mail Single Yes URL Single Yes Address Combined Yes Country Single Yes Map Single No Calendar Combined Pipe No Masked Text Single Yes Tags Multi Comma Yes Slider Single Yes Limitation text Single Yes File upload Single Yes Label Single Yes Select Date Single Yes Correlated Fields Multi Pipe No Information Extraction Group Multi Pipe No Taxonomy Multi Yes Grid Single (JSON) Yes Invoice Answer Types Yes Answer Types Description "},{"version":"10.0","date":"Aug-06-2019","title":"correlated-fields","name":"Correlated Fields","fullPath":"iac/core/user-guide/answer-types/correlated-fields","content":" This Answer Type allows you to repeat a block of answer fields, so Workers can dynamically add Answers. This feature is useful when the number of Task Answers is unknown. For example: \"Find all the Board of Directors from an annual report\". Repeatable Block Creation To create a repeatable answer block, select the Answer Type: Correlated Fields when adding a new Answer. Add Corellated Answer Correlated Fields Options Custom parameters: 6 Add more Stack 6 max block size; you can input any integer value. Add more Name of the block creation button. Stack one of three variants(tabs, stack, horizontal) for tabs location. Default parameters: Max block size unlimited Button name Add Tabs location Tabs Each Correlated Fields Answer should have child answers with \"Do Not Use in Adjudication (SA)\" option enabled, also child answers should be not required. Worker's answer will be processed as a pair of values divided by :: Usage Used as a parent container for child answer types. Child answers should be non adjudicated and in common case. Currently supported child answers are as follows: FREETEXT, LONGTEXT, CHECKONE, CHECKMULTI, SELECTONE, SELECTMULTI, NUMBER, PRICE, DATE, TIME, PHONE, EMAIL, MASKEDTEXT, SLIDER, TEXTLIMITATIONAREA, UPCCODE, SELECT_DATE Example for setting parameters in the options: 3 Add stack \"3\" First value max block size \"Add\" Button name \"Stack\" one of the three variants (tabs, stack, horizontal) for tabs location Default parameters for (no specified value): button name \"Add\" tabs location \"Tabs\" Default value JSON object {\"cf\": {\"t1\":\"b\",\"t2\":\"b\"},{\"t1\":\"a\",\"t2\":\"a\"} } where \"cf\" correlated _field answer code, t1 first subanswer, t2 second subanswer, a and b values Example V1: 3333::555 1234::456 (2 blocks with 2 fields each) V2: {\"cf\": {\"t1\":\"b\",\"t2\":\"b\"},{\"t1\":\"a\",\"t2\":\"a\"} } JSON object Where \"cf\" Correlated Input answer code that should be used in conjunction with CorrelatedMultivalueComparator. Various modes tab support You can choose from three types of Tab Display. STACK Correlated Answer Stack Mode TABS Correlated Answer Tabs Mode HORIZONTAL Correlated Answer Horizontal Mode Sub Answers Several levels of sub Answers (2, 3) are supported in Correlated Fields. List of available answer types: FREE _TEXT LONG _TEXT CHECK _ONE CHECK _MULTI SELECT _ONE SELECT _MULTI NUMBER PRICE DATE TIME PHONE EMAIL MASKED _TEXT SLIDER TEXT LIMITATION AREA UPC _CODE SELECT _DATE "},{"version":"10.0","date":"Aug-06-2019","title":"grid","name":"Grid","fullPath":"iac/core/user-guide/answer-types/grid","content":" This Answer Type allows Workers to enter answers in the grid with a defined column structure and a dynamic number of rows. This feature is useful when the exact number of Answers is unknown. Grid Answer Type :::tip Task: \"Find all events with their parameters (event name, date, time, place, cost) from news articles\". Each event corresponds to a new row; columns are named as event parameters (event name, date, time, place, cost). ::: The Grid Answer can be substituted with another dynamic Answer Type – Correlated Fields. How to Use Click a cell in a corresponding column and enter the answer. If you need to pick a date from calendar, double click the cell. If the row you click is the last one, a new row will be appended automatically. When you hover over a cell, a tooltip with answer description is displayed. Grid Context Menu Context Menu (right click a cell): insert row remove row undo redo n a mark question as \"Not available\" \"Excel\" functionality: fill cells (fill data) select an area, then drag the bottom right corner down. As a result, the selected pattern will be copied. Fill Cells in Grid copy (CTRL C), cut (CTRL X), paste (CTRL V) table navigation using arrow keys (and also Home, End, Page Up, Page Down) SHIFT click (SHIFT arrow keys) for selection How to Set up Add a new Answer with Answer Type = \"Grid\" Grid Answer Type (Optional) You can rotate grid (display columns as rows) by entering \"vertical\" into the Answer Category field. Add Sub Answers to this Answer (see the list of supported Answer Types below). These Sub Answers will be displayed as grid column headers. Sub Answer description will be shown as a tooltip on hover over a cell. Grid Answer Structure Supported Sub Answers for Grid FREE _TEXT MASKED _TEXT NUMBER SELECT _ONE SELECT _MULTI (supported for \"Answer Category = Vertical\" only) DATE (normalized to mm dd yyyy) TIME (normalized to hh:mmtt) DAYSOFWEEK (normalized to 2 letters format) Output Format The Grid Answer provides output value in JSON format and records it in one column: Grid Output Example { \"grid\": { \"event_name\":\"Expo\", \"date\":\"11 25 15\", \"place\":\"Wildwood\", \"cost\":\"100\", \"comment\":\"\" }, { \"event_name\":\"Swimming\", \"date\":\"11 27 15\", \"place\":\"Tampa\", \"cost\":\"0\", \"comment\":\"\" } } Limitations and Notes One Grid answer type per Human Task All empty cells are normalized to \"n a\" on submit, Sub answers are not submitted. Option \"Do Not Use in Adjudication (SA)\" is not used for sub answers (because they aren't submitted). If a Grid is restored from default value then all edited cells will be marked with a red triangle in the top right corner. Training Setup Input file should contain only gold value and gold message for Grid answer type. Gold grid value should be normalized. Explain message (gold message) will be displayed under the Grid. Correct cells will have green color, incorrect red color. Every cell will have tooltip with correct answer. !Cells Tooltip "},{"version":"10.0","date":"Aug-06-2019","title":"information-extraction","name":"Information extraction","fullPath":"iac/core/user-guide/answer-types/information-extraction","content":" Information Extraction (IE) is special kind of answer used for extracting structured content from raw unstructured text. Information Extraction results are utilized by machine learning. This answer type displays input data as non editable text where user can select text chunks and tag them. Information Extraction Answer How to Execute IE Tasks Read the Task Instructions and review all Tags from the side panel. Start reading and analyzing the text sentence by sentence. When you meet a text chunk that corresponds to one of the Tags: select the text chunk click the corresponding colored Tag icon with letter OR press the corresponding keyboard key (hotkey). Your selection will be highlighted in Tag color. Repeat the steps 2 and 3 till the end of text. Review the results and submit the Task. You can optionally perform the following actions with the tagged items: Edit a tagged item by clicking the pencil icon. This option is helpful when you have found a misprint or incomplete info. Delete a tagged item by clicking the (x) icon. Scroll to tag by clicking the magnifier icon. Detailed Instructions Advanced text tagging Tagging tabular text using Automatic Suggestions (Online Learning) How to Create IE Answers Before reading this material, see the following topics: Answers, Answer Types. Add Answer :::note TOD Manual Task requires specific answer tuning, see TOD Manual Task Configuration. ::: Unique Code for Information Extraction must contain column name from Input Data .CSV file. The mapped column can contain raw text or a link to a webpage (http or https). For Extract to work, IE unique code should be documentxmllink (default) or mapped to a column containing xml link to xml. It is recommended to store text data files (HTML, XML, or TXT) in S3 bucket. Then generate list of URLs which can be used in Input Data .CSV file. Add a new Answer with the Information Extraction Answer Type. Select the Content Source: Input Data. In this case, the Answer text will be loaded from the input Data file column mapped to this Answer. OR Answer Code of the Answer with URL type, if available. In this case, Worker should enter a valid URL into the corresponding URL Answer field and click the Load Content button in the IE Answer text field. Supported IE answer source formats: TXT, XML, HTML TOD data: For TOD (Enriched) mode, TOD data field should contain metainfojson value (default) or column with json from OCR. For non TOD IE tasks, TOD data field should be left empty. (Optional) Tick the Use blocks checkbox, if you want Workers to tag several blocks with similar structure (e.g. tables) in a document. (See full block description) In Advanced Options, select the Extractor. IE answers use boilerpipe library to retrieve and filter content when it gets URL as an input. You should select boilerpipe's extractor which will be used for filtering content. More details about extractors you can find via the link: boilerpipe The following Extractor should be used for IE answers: Original Appearance – with this option you preserve the structure and all tags of the original document. Default value should be always same as Unique Code plus \" tagged\" suffix, for example: {question.data 'newslink_tagged' } Mapping The IE Answer must be mapped to an input file column. There are two ways to do this: You can name the column the same as the corresponding IE Answer Code. OR You can map any column by clicking the column header and selecting the corresponding IE Answer Code. Adding Sub Answers (Tags) The IE Answer must include one or several Tags or Tag Groups; each Tag Group must include one or several Tags. These Tags and Tag Groups are created as sub Answers for the IE Answer. Add IE Sub Answer The Answer hierarchy (IE > Tag Group > Tag or IE > Tag) with corresponding Answer Types is displayed on the following picture: IE Sub Answer Hierarchy IE sub Answers (Tags) have the following distinctive features: Available Answer Types Currency: A dropdown to set the currency or override the values from a data store, i. e. USD US Dollar. Date: Any field that is a full date or a part of date such as date, year, month, day. Has option to set the format. E mail: An input field for entering an email address in the following format: username@domain_name. Number: Input field used to enter integers and floats. Person Name: Free text format, accepts any value. Has Check one and Select one options. For tagging Names and Surnames. Company Name: Any field that is a full name of a company (in any language). Has Check one and Select one options. Text: Free text. Has Check one and Select one options. Any free text field, no NERs are applied and only default FEs are used. Line Item: Any field that is a group. Unique identifiers Free text format, accepts any value, spaces are deleted automatically. Identifier: Custom Identifier. Used to tag any unique identifier (invoice number, account number, etc.) CUSIP: A field to enter CUSIP identifier with a built in validation (9 characters). IBAN: A field to enter IBAN identifier with a built in validation (34 alphanumerical character, validation by 2 control digits). SWIFT: A field to enter SWIFT identifier with a built in validation (8 or 11 alphanumerical characters). UPC Code: A field to enter UPC Code with a built in validation (12 numeric digits). Address Address Line: Free text, i. e. 795 E DRAGRAM. City: Free text, i. e.TUCSON. State Province: Free text, i. e. AZ. ZIP Postal Code: Any field that is a ZIP code, any format of any country. Free text, i. e. 85705. Country: Drop down list of countries to select. Full Address: Free text. Hotkey Hotkey (a unique char). This key can be used for quick text tagging. Tag color is assigned automatically. If you skip this parameter, a tag button will be shown without hotkey assigned to it: Tag without hotkey Multi value. Tick this checkbox to allow Workers to tag multiple text chunks with the same Tag. Value may be constant. This parameter is used for Tag Groups and allows to use one tagged text chunk for all Tag Groups. Non taggable. If this option is checked, the Answer will not have a tag button and can be used for providing additional information about documents being tagged. Non taggable answer Answer Category Answers are visually grouped together based on their common category. You can expand or collapse a category while tagging. This feature is helpful when an IE Answer a lot of Sub Answers: you can set the same Category for related Sub Answers to improve navigation and search. Answer Category :::note Do not forget to check the Allow N A option, because some information can be missing in the source text. ::: Result Data IE Answers record the following columns to the output csv file: Tagged text. Column for each sub Answer. Multi value Answers are recorded into one cell with a pipe delimiter. Tag Group answers are recorded as a JSON to a separate column. Confidence and Accuracy are calculated for each column. IE Answer Features Tag Groups Often we have situation when a Worker should tag repeatable information blocks together with the static values. For example, bank transaction reports, bio information, dividend researches, etc. For this purpose, a new answer type, Group of Answers, has been created. This answer is displayed as a multi tab panel containing its sub answers. Tag groups How to use IE Answers with Tag Groups Select text and tag it like in an ordinary Information Extraction answer. When all tags for the first group are set, create a new tab by clicking the plus icon. Select text and tag it for other groups. You can switch between tabs or delete them. The latest tab deletion can be undone. Text selections from inactive groups are highlighted with gray and have a group number label. How to Create Tag Groups Add a new sub Answer to the IE Answer. This sub Answer should have the Answer Type: Group of Answers. The Options parameter defines the tab name. Add Tags as sub Answers to the Group of Answers. Tick the Value may be constant checkbox to allow using one tagged text chunk for all Tag Groups. Modifying Tags When it is necessary to modify or edit a pre recognized tag( s), you should select a tag, then it appears on the right panel, so that you can quickly edit or remove it. You can select a tag in one of these ways: Right click an existing tag to select this single tag. Ctrl (or Cmd on a Mac)* Left click* an existing tag (one tag is selected) or a few tags (multiple tags are selected) :::note Holding Ctrl (or Cmd) and clicking on an already selected tag will clear the selection. ::: Ctrl (or Cmd on a Mac)* Drag* existing tags (multiple tags are selected) For multiple selection you can right click an existing tag and choose one of the Expand Selection options from the context menu appeared: Rest of the column to select all tags below the selected tag till the end of the column Same tags in Column to select all tags of the same type in the column below the selected tag. Once you have selected one or more tags, they will appear on the right. All other tags will disappear. You can click anywhere in the document or the Clear Selection button to remove selection from these tags and show the entire list of tags again. Clear selection Bulk Tag Type Change If you have a number of tags selected, you can re tag all of them with a new type by clicking the respective type in the Change Selected Tags To section of the context menu at the bottom of the right panel. :::note Click Show All Tags to see the entire list of available tag types. ::: IE with Blocks Some documents have repeatable sections with the same structure (e.g. a set of tables). Each section can be tagged with a separate IE answer Block. Each block has its own set of Rows (Tag Groups) that can be added dynamically. IE Blocks When the root IE Answer has the Use blocks option enabled, the following rules are applied: Sub Answers become tags within a block Tag Groups (row1, row2, rowN... in the screenshot above) are created inside of each block as sub tabs, i.e. each Block has its own set of Groups. Blocks and Groups inside them are created removed dynamically and can be switched by clicking appropriate tabs. Example: \"Line\" and \"Box Code\" tags describe a block (a separate table). Block 2 is selected. This Block has two rows (\"row1\" and \"row2\") which represent a Tag Group. Auto Corrections When a Worker has selected only a part of a word, number, or phrase, WorkFusion engine analyses this situation and automatically suggests corrections depending on the context. WorkFusion engine detects mistakes in Workers' selection and proposes corrections: Autocorrections Supported correctors: Whitespace Punctuation Word Margin Sentence Splitter Named Entity Recognition (i.e. \"Princeton University\"). To enable this corrector, add a Sub Answer with Free Text type to an existing IE Answer and set Specify type = Reason, Organization, or Location. Named Entity Recognition Sentence Splitter and Named Entity Recognition correctors must be configured by setting the NLP service (Configuration > System Preferences > Account Settings > Preferences > NLP Service URL). See more All correctors are disabled when IE Task uses Online Learning feature (tag auto suggestions). In this case, correctors are provided by selected ML model. Table with Results You can view all the tagged text chunks in a convenient table format. Table format When you click a table row, the appropriate tag group is highlighted. You can optionally enable the full screen view for the table to see all table columns without scrolling. Answer highlighted Original Document Preview Workers can preview original documents to make sure that the extracted text is valid or to view original graphics, formatting, etc. This preview is available for PDF files. PDF Preview To enable the preview function, provide links to PDF files in the respective column of the input data file. Link PDF The original document is displayed in the bottom panel. You can drag the splitter bar to resize the document preview. Full Screen View While working with big chunks of text or long sentences, it is useful to utilize all available space on the page. Full Screen View You can enable the full screen mode if needed and disable it anytime. Document Zooming Some documents can contain text with different font height. IE Answers provide functionality to zoom in and out the source document. You can zoom in if the font is too small, or zoom out to see the whole page structure. Document zooming Undo Redo Actions User can undo or redo changes made in Information Extraction answers (adding removing tags, accepting rejecting auto suggestions, marking N A or constant, etc.). User can also use common keyboard shortcuts: Undo (CTRL Z), Redo (CTRL SHIFT Z). Undo Redo Set Tag Value You can set a tag value without selecting a text chunk and clicking a tag button. This feature is useful when you tag a low quality document with OCR errors and you don't want to train ML model on faulty data. Just click a set value link under an appropriate tag name and enter your tag value. Set Tag Value "},{"version":"10.0","date":"Aug-06-2019","title":"taxonomy","name":"Classification: Taxonomy","fullPath":"iac/core/user-guide/answer-types/taxonomy","content":" The Taxonomy Answer Type can be used for classification of products, businesses, and articles. In WorkFusion, you can use this answer type along with Data Stores. How Taxonomy Answer Looks Task with Taxonomy Answer Type In this example, Worker is classifying a business into an industry category. There are 4 Levels. Worker is displayed the corresponding sub levels after choosing the parent category.This helps narrow down choices and increases accuracy and quality of work. Setting up a Data Store The Data Store containing the taxonomy needs the following columns: ID (0 through number of levels 1): this value will be submitted in the Answer For above example we need id0, id1, id2, id3 Category(0 through number of levels 1): this value will be displayed for Workers.For above example we need category0, category1, category2, category3 Description(0 through number of levels 1): this optional value will be displayed for Workers and is intended for describing the Category.For above example we need description0, description1, description2, description3 Snapshot of the Datastore used for above example: Data Store for Taxonomy It contains the following columns: id0, id1, id2, id3, category0; category1, category2, category3; description0, description1, description2, description3 Category Columns: Contains the name of the category, category0 should contain the top level category; category1 – the sub level category and so on. Description Columns (optional): Contains a description for the category. description0 should contain a description for the top level category (in this example top level category is \"Energy\"). description1 should contain description for the first sub level and so on. ID Columns: id0 contains an id for the top level category. Notice in this example, id0 is the same for all Energy records. id1 appends the id for the sub level. \"Energy > Fossil Fuels\" for example is \"5010\" and so on. id3 (the last id column in this example corresponding to the 4th level in task) will have all unique values. ID Column does NOT need to be numbers. If your taxonomy does not contain corresponding internal ids, you can just use names of the category instead. For example, category0 and id0 can both be \"Energy\" etc. Setting up a Task with Taxonomy The answer tree in the human task will look like this for the given example: Taxonomy Task Design Let's take a look at how both answer types are set up. Taxonomy Answer Select answer type Taxonomy Select the desired Data Store from the Options field that will pop up. Notice that default answer uses the code of this answer type. Only Data Stores with \"category0\" column are shown in the list. Thus, you can quickly scan all valid Data Stores and select the one you need. Code of the answer type can vary. It does not have to be \"category\" as shown in the screenshot. Add Answer Select Item Answer For this example we need 4 Select Item answers, all sub answers of the previous answer for the 4 levels used in this example. The point of importance here is the Unique Code. The unique code needs a digit at the end: \"level1\" corresponds to first level of the taxonomy (id0, category0, description0); \"level2\" corresponds to the second level of the taxonomy (id1,category1, description1) and so on. Select item answer Output The output of a taxonomy answer will have two parts. You will have two columns in the output: one that provides the ID that corresponds to the category (this will be the value of last id column). If you want to setup default value for this answer then you need to use this ID. another that provides a textual value of the category. For this example, we will get the following: In this example we get something like \"50102034\" and \"Industrials > Industrial & Commercial Services > Diversified Trading & Distributing > Diversified Trading & Distributing\". "},{"version":"10.0","date":"Aug-06-2019","title":"bp-use-case","name":"BP Use Case","fullPath":"iac/core/user-guide/create-use-case/bp-use-case","content":" To manage Use Cases, go to Configuration > Use Cases > Business Process tab. Use Cases for BP On the BP tab, you can perform the following bulk actions: create edit Use Case copy Use Case delete Use Case export import Use Case in XML format enable disable Use Case Use Case for BP Уdit When you are creating or editing a Use Case for BP, you should set the following parameters: Field Description Name A unique Use Case name. Category Select a Category that fits your Use Case. Campaign Select a BP that will be a source for the Use Case. After you have selected a BP, you can preview or edit it by clicking the Edit link. Custom Attributes See the Automation Engineer Manual. Description Common information about the Use Case. Preview Image Upload an image to distinguish the Use Case among others and give the user initial information about the look of the Task. "},{"version":"10.0","date":"Aug-06-2019","title":"create-use-case-from-task-bp","name":"Create a Use Case from Task and BP","fullPath":"iac/core/user-guide/create-use-case/create-use-case-from-task-bp","content":" Use Cases serve as the foundation for both Tasks and Bot Configurations. They describe the workflow of a particular Business Process and thus serve as a template to create either Manual or Bot Tasks based on that workflow. For Manual Tasks, Use Cases simplify task creation by having predefined questions, answers, instructions, and sample input file and gold files. For Bot Tasks, Use Cases simplify development by providing a baseline of the configuration code. When you create a Task or BP from a Use Case, all its properties are copied. When you edit a Task, the changes do not affect its Use Case. When you edit a Use Case, the changes do not affect Tasks or BPs created from this Use Case. Use Case Flow Preparation Create a Task (or BP). Test the Task (or BP) and make necessary editing. Create a Use Case from the Task (BP) and define a Use Case Category for it. Usage Create a Task (or BP) from this Use Case (as a separate Task or within a BP). Make sure you have added a valid input Data file. See the following topics: Task Use Case BP Use Case "},{"version":"10.0","date":"Aug-06-2019","title":"task-use-case","name":"Task Use Case","fullPath":"iac/core/user-guide/create-use-case/task-use-case","content":" This topic cover the creation ad editing of Manual Tasks Use Cases. Use Cases for Bot Tasks are described here. Use Case Categories Each Use Case must belong to a Use Case Category. To manage Categories, go to Configuration > Use Case Categories. Use Case Categories On the Use Case Categories page, you can perform the following bulk actions: create edit Category delete Category export import in XML format enable disable Category Edit Use Case Category When you are creating or editing a Category, you should set the following parameters: Field Description Name A unique Category name. Description Common information about Category items. Type (Manual or Bot). Type modification is disabled if category already contains some Use Cases. Icon Upload an icon to distinguish the Category among others. Private If unchecked, WorkSpace Workers will see this Category on the Tasks page. Use Cases To manage Use Cases, go to Configuration > Use Cases. Use Cases Menu On the Use Cases page, you can perform the following bulk actions: create edit Use Case delete Use Case copy Use Case export import Use Case in XML format enable disable Use Case Edit Use Case When you are creating or editing a Use Case for Manual Tasks, you should set the following parameters: Field Description Name A unique Use Case name. Category Select a Category that fits your Use Case. To edit the Category, click the appropriate link. Task Select a Task that will be a source for the Use Case. After you have selected a Task, the Template will be loaded automatically. You can preview or edit the Template. Custom Attributes See the Automation Engineer Manual. Description Common information about the Use Case. Private If unchecked, WorkSpace Workers will see this Use Case on the Tasks page. Preview Image Upload an image to distinguish the Use Case among others and give the user initial information about the look of the Task. Advanced Options Click this link to display the Task Options and Task Designer. We recommend you to create Use Cases only from previously tested Tasks and not to edit the Use Case Advanced Options. "},{"version":"10.0","date":"Aug-06-2019","title":"basic-concepts-for-task-implementation","name":"Basic Concepts for Task Implementation","fullPath":"iac/core/user-guide/best-practices/basic-concepts-for-task-implementation","content":" This topic contains some basic concepts to keep in mind when creating a Human Task. Understand the Task Design Is the Task size optimal If Task is too big: split it up into smaller Tasks. If Task is too small: use Block Size > 1. What is the goal of the Task What type of Workers will be suited for the Task How long should the Task take to complete Are there many variables to finding an answer Is the Task subjective Is there a right or wrong answer Will the Task require Adjudication How is the Task being completed currently See the Results tab. How to determine correct (valid) Answers Confidence was found (defined by the Adjudication Rule), e.g. 2 out of 3. See the answers of Workers with high Qualification and good performance results. Work through the Task on a High Level Use one or two data points. Define your input data (what do you already know ) Define your output data (what are you trying to achieve ) Design the Task using the basic sections. Decide if the Task requires a Training Test and or Qualification Task. Estimate the costs, determine the Adjudication rules, and Workforce. See the details here. Sentiment analysis, surveys, or lengthy subjective Tasks do not work well with Gold Data. If you wish to qualify Workers for these types of Tasks – it is best to review their work manually, without the aid of Gold Data. "},{"version":"10.0","date":"Aug-06-2019","title":"business-processes-development-best-practices","name":"Business Processes Development Best Practices","fullPath":"iac/core/user-guide/best-practices/business-processes-development-best-practices","content":" Introduction Naming Conventions Business Processes components (Steps, DataStores, Bot Configs and Manual Tasks)) Bot Code Business Process Design Guidelines Common mistakes found in business process codes Logging Common performance related guidelines Code Review Introduction Here you can find Business process from webinar Business Processes Best Practices from Jul 18, 2017. BP package BP best practices _package.zip required DS to start demo BP : Invoice _storage.csv To launch demo BP you need to create DataStore \"Invoice _storage\" from attached csv file. After that open BP, check option \"No Data\" on Data tab and launch BP. Common best practices to follow while writing code for any business process. Naming Conventions Business Processes components (Steps, DataStores, Bot Configs and Manual Tasks) Name the business process and bot steps with proper naming conventions, avoid using default name for Business Process Business Process 05 13 2019 06:21:52 Keep consistency in naming the datastores, preferably use the lowercase with underscore between words. Datastore naming convention Bot Code Input parameters and webharvest defined variables are usually lower case with underscore separated words. Keep consistency in naming the variables, ex. Use camelCase for methods and variable names. Business Process Design Guidelines There are 3 main approaches on BP design and execution: Process executed on scheduled basis with specific input (Schedule Tasks and BPs) Process executed through the API (API Documentation) Continuous monitoring of the input source Common mistakes found in business process codes Don't use the thread.sleep for any retry(asynchronous calls), try using the retry attempts and retry delay option in http extended plugin Another option is to use the release plugin Avoid chained method invocations, split the chain into variables and handle exceptions to help proper debugging. Chained invocation instead do as below try{ App app = AppExample.init(binding).get(); Transaction processedTransaction = app.processTransaction(TransactionEncryptionProcessor.class, transactionId); }catch(Exception ex){ logger.error(ex.getMessage()); } Add proper exception handling with try catch, avoid empty catch block. log the exception message to help debugging. Exception handling instead do as below try{ App app = AppExample.init(binding).get(); Transaction processedTransaction = app.processTransaction(TransactionEncryptionProcessor.class, transactionId); }catch(Exception ex){ logger.error(ex.getMessage()); } Avoid duplicate queries to same datastore in multiple bot steps. instead query once and export the object to subsequent steps. Use secret vault for sensitive data. Secret vault plugin Avoid procedural coding style, instead group functionality into methods. Create classes for related properties, create methods to manipulate instances, share classes between steps using include config plugin Give meaningful names to non primitive variables. sys.defineVariable should not be inside any conditional or method. Avoid overwriting inside blocks instead use local properties and define variable outside the block. sys object Using sys.defineVariable instead do as below String requestId= defaultTaskRequestId; String accNumber= defaultAccNumber; if(resultStatus.equals(\"SUCCESS\")) { requestId= result.taskRequestId; accNumber= result.accNumber; } sys.defineVariable(\"requestId\", defaultTaskRequestId); sys.defineVariable(\"accNumber\", defaultAccNumber); Use explicit types for variable declaration, avoid def keyword for variable declaration. explicit type declaration avoid def for variable declarations. always give explicit data types. def accountNumber = 1l; Use* *sys.isVariableDefined to ensure variable exists.sys object Do null checks before comparisons. Logging Logging helps in debugging the problems in business process, follow proper formatting of log messages, only log useful information. Never log sensitive information. Add as much logging as possible. Make sure to use log level as debug to avoid unnecessary logging. log plugin and object Common performance related guidelines Its generally a good idea to configure the bot sources to optimally utilize the underlying hardware. Bot sources are deprecated in SPA 10.0. If you configure multiple threads for your bot steps without checking whether the server has enough cores memory then this will lead to performance degradation. Its also a good idea to keep bot steps as stateless. Stateless execution will add boost to overall performance. Stateless execution Avoid unnecessary calls to datastores or restful apis, if a single call can suffice for entire bp then result of that single call can be exported to later bot steps. Use SQL joins, its a good approach to join over multiple tables instead of calling each datastore separately and then doing joins with groovy java. Sql joins Use the export plugin judiciously :export plugin Export plugin misuse Avoid unnecessary use of multi column plugin, it will help in running the bots stateless. Avoid unnecessary split joins, again the split data rules are not supported in stateless execution. Workfusion tools techniques for analyzing the performance of instance are here SPA performance analysis Code Review For performing code review following template can be used (relevant for 8.x and 9.x SPA versions) Useful links: Tips and Tricks with Examples pool release 9.x Bot Sources Bot sources are deprecated in SPA 10.0. Manage Global Variables Managing Custom Attributes export "},{"version":"10.0","date":"Aug-06-2019","title":"how-to-upload-files-to-s3","name":"How to upload files to S3","fullPath":"iac/core/user-guide/best-practices/how-to-upload-files-to-s3","content":" This article describes how to upload files to Amazon AWS S3 which is used as main file storage system by WorkFusion Platform components. It is recommended to install and use one of following 3rd party S3 clients: S3 Browser for managing files on Amazon cloud Dragondisk for managing files on On premise S3 emulator recommended for MacOS and Unix users if Dragondisk doesn't work you also can use S3 Browser Training File Storage Account Pay Attention For all Training instances we share one S3 folder and provide single account for all trainees this is bad practice to provide single account in real projects, but we accept it for education reasons. So please use this policy: Do not share any personal, client and other strict data in this PUBLIC bucket. Use your Academy ID as a name for your personal folder (please take this article to see how to find the ID) Subscribe to this page to receive S3 keys updates notifications (click on 'Watch' button). New Training Instances Bucket If you are using training instances: watt volt ampere ohm your bots have WRITE permissions only on bucket wf aa students Also Do not use access key secret key in s3 put on instances. Configuration done on application level! In case your company has internal training instance, use your own S3 keys (no need to set credentials in the code, as they are configured on the instance). Download and install S3 browser Go to S3 Browser site and download the latest software version. Install S3 browser on your Windows PC (.NET Framework 4.5 or higher is required). The following setting should be used for the trainee's account in S3 Browser: Account name: you can use any name here Account type: Amazon S3 Storage Access Key ID: AKIATAQAIACOKYO45NFF Secret Access Key: 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 (The s3 keys were changed on May 24) After pressing \"Save changes\", S3 browser will ask you to add external bucket, because your keys are not allowed to list all buckets, answer \"Yes\", see screenshot below: Provide the appropriate name for this bucket wf aa students Setting up WF Studio Go to Window → Preferencies → WorkFusion Studio and enter URL and keys as you entered into S3 Browser On Prem S3 Emulator Disable multipart upload download as it is not supported in S3 Emulator: External Buckets Configuration If you want to change options for External buckets... Go to Tools > Options > General and uncheck \"Enable multipart uploads with part size\" After connection to S3 bucket, you can browse the bucket and download upload files. Get WEB URL links This chapter explains how to use S3 Browser UI to fetch list of links pointed to files stored in same folder. You can select any number of files and generate corresponding HTTP(s) links. Right click on the target file and choose the Generate Web URL option. In the Web URLs Generator window, click the Copy to clipboard button. Paste this link where you want. Sometimes you might need to change the generated URL to correctly display the document in browser and in the 9.x Information Extraction IE task, for example: Generated URL https: bucket name.s3 server name folder ...file.pdf Change to https: s3 server name bucket name folder ...file.pdf Configure Public access to a file or directory Select target file or directory. Go to Permissions tab. Tick the Read checkbox for All Users. Copy file public link as described below and open it in browser to ensure that file is available. Account setup in Dragondisk client Click here to expand... Dragondisk is alternative to S3 Browser use one of those. Download and install Dragon Disk Go to Dragondisk site and download the latest version. Dragondisk settings Create new dragondisk account: go ot File > Accounts.. > New Provider: * * Service Endpoint: Account name: Comment: Access Key: Secret Key: HTTP Port: 80 HTTPS Port: 443 Also select \"Connect using SSL HTTPS\" checkbox You are now connected to on premise S3 emulator and you can browse bucket and download upload files Get WEB URL links Select target file, right click it, and choice Properties. Go to Web URL tab. Select in Protocol combobox \"Http\" value. And press \"Copy clipboard\" button. Paste this link where you want. Make public access to file Select target file, right click it, and choice Properties. Go to Security tab, click the Add button, select All users and set Download checkbox for All Users. Copy file public link as described above and open it in browser to ensure that the file is available. Make public access to all files in folder Select target folder, right click it, and choice Properties. Open Child objects tab. Set the Replace permissions checkbox. Go to Security tab,click the Add button, select All users and set Download checkbox for All Users. Copy file public link as described above and open it in browser to ensure that the file is available. Please note that we will update access keys once a week, so we recommend you to subscribe for this KB page updates in order to always be aware of any change made. Just click on 'Watch' button ↑↑↑ Training File Storage Account Setting up WF Studio On Prem S3 Emulator External Buckets Configuration Get WEB URL links Configure Public access to a file or directory Account setup in Dragondisk client "},{"version":"10.0","date":"Aug-06-2019","title":"checklist-for-creating-task","name":"Checklist for Creating a Task","fullPath":"iac/core/user-guide/best-practices/checklist-for-creating-task","content":" id: version 10.0 checklist for creating a task original_id: checklist for creating a task Upload File Is your input file formatted correctly (.csv) Check the separator and encoding. Does it contain pertinent fields If a warning is displayed, map the input file columns to the Task variables. Does it contain gold _data (training) headers for Qualification Task Design Task Are the Instructions clear and concise Are examples provided if needed Are corner cases explained if needed Are all necessary Data Elements visible to Worker Do all hyperlinks function properly Are all Answer Types formatted correctly Are Answer Types most efficient for the Task Generally, all Required Answers should have N A allowed. In case no information is available for a particular required field, worker will have to tick the N A checkbox to submit the Task since it can't be left blank. If the N A checkbox is not present, worker won't be able to submit the task. Check if there are any hidden required fields that prevent submitting the task. Hidden answer fields which appear only if a particular option for another answer field is checked are called Sub answers. Look through the list of answers to find the sub answers with 'required' option enabled and disable it. Sub answers may not be used in every task and thus should not be required. Save Task periodically while designing – it will NOT auto save. Run Task Is your Workforce set to your specifications Is your Block Size set to your specifications Is your Reward per Task sufficient Advanced Options Properties Verify Task Expires in: (7 14 days is recommended). A shorter expiration date may entice Workers. Verify Time Allotted per Task: add about 15 minutes to estimated time, to allow Workers extra time. Verify Max Number of Pending Assignments per Task: this parameter shows the number of Worker Tasks a Worker can hold. Do your Keywords fit your Task (searchable terms) Does your Description explain your Task succinctly (one or two sentence headline) Adjudication Verify your Min Max of Assignments (the minimum number of Workers to complete each Record before it is moved on). NOTE: you cannot see results until the min number is met. Set your Adjudication Rule (i.e. a majority decision 2 1, sentiment analysis 3 0). Qualification Add all desired Qualifications. This can include – number of Tasks completed, percent of accuracy, location, custom Qualifications. Set Training Mode if needed (for Qualification Task only). Set the Number of Qualification Tasks (for Qualification Task only, i.e. number of Tasks Worker needs to complete to pass the Qualification test). Availability Verify the Limit Number of Tasks: the maximum number of Tasks a Worker can complete. Other Add Tags for easy access to Task (using filters, saving filters can help you locate your Tasks easier, grouping them together). Preview Task to be sure it functions as intended. Run in Sandbox – Test with small sample set. "},{"version":"10.0","date":"Aug-06-2019","title":"basic-concepts-for-bp-implementation","name":"Basic Concepts for BP Implementation","fullPath":"iac/core/user-guide/best-practices/basic-concepts-for-bp-implementation","content":" Determine how the work will flow through the process Will the input data of one Task be determined by the output data of another Task Is there a moderation step involved in the process If so who will be moderating What happens if the work is NOT approved – back to the same Worker – a different Worker – or to an expert How many Tasks will make up the processes Determine if there will be Manual Tasks and Bot Tasks Bot Tasks are best used for scraping, manipulating data, locating information that is always in the same place – forms, PDF standard files, SEC website, .org websites. Understand how the output of one Task will look because it may become the input of the next Task. Determine the threshold streaming of the BP – whether one entire Task needs to finish before the data moves on. Use only tested and valid Tasks Add the following Tasks on the BP Design tab: from Use Cases from already created Tasks Monitor the BP execution Watch the BP Diagram. Watch the Summary page. Analyze the Data and Worker statistics. Pause or stop the BP if some issues come out. "},{"version":"10.0","date":"Aug-06-2019","title":"importing-exporting-utf-8-data","name":"Importing and Exporting UTF-8 Data","fullPath":"iac/core/user-guide/best-practices/importing-exporting-utf-8-data","content":" Many tasks involve data requiring UTF 8 encoding, especially for foreign languages (Chinese, Hebrew, Arabic, etc.). This topic describes importing and exporting UTF 8 data to WorkFusion while creating Tasks, BPs or downloading snapshots. Importing Problem Sometimes while trying to upload data with Spanish, German, Chinese characters (¿, ñ, ë, 木) to the platform, WorkFusion will not upload a .xls or .xlsx file. Saving as .txt or .csv eliminates the Chinese characters. Saving as Unicode text preserves the characters but the file will not upload. Solution The easiest work around is to copy paste from Excel into a text editor, and save the text file from there. For example, from Excel to Notepad: Save the Excel file as Unicode Text. Open this file in Notepad (Sublime or Notepad ).Or just copy all the data from Excel and paste it to a new Notepad document. Save the notepad file as a TXT file, being sure to update the encoding to UTF 8. Importing UTF 8 Data Upload the file in WorkFusion, and it will adjust the upload to UTF 8 and tab delimited. Detailed instructions can be found here. Exporting Problem You uploaded a file to WorkFusion as TEXT (.txt) in UTF 8, but it is exporting as CSV and the characters are jumbled. Solution In this case, the data is fine but Excel does not display it correctly. If you want to view it in Excel: Create a new blank book in Excel. On the top ribbon, select Data > From Text. Change the File Origin to Unicode (UTF 8) and finish the import process. "},{"version":"10.0","date":"Aug-06-2019","title":"manual-task-for-ml","name":"Manual Task for ML","fullPath":"iac/core/user-guide/best-practices/manual-task-for-ml","content":" Use WorkSpace Preview to find appropriate task(record) on WorkSpace. Also records can be sorted on WorkSpace. Do not use adjudication 2 1 and higher in Human tasks in Production Flow due to: Due incompatibility with Workspace Preview If 2 records' results are not same then 3rd worker should review it. If 3rd answer is different to first 2 then random answer will be chosen Build Creation → Business Rule Based validation logic → Review flow to produce results with higher quality "},{"version":"10.0","date":"Aug-06-2019","title":"task-and-process-development-guidelines","name":"Task and Process Development Guidelines","fullPath":"iac/core/user-guide/best-practices/task-and-process-development-guidelines","content":" id: version 10.0 task process development guidelines original_id: task process development guidelines Key Disclaimer: as with any IT heavy project, it is best to design – at least at a high level – the overall process to minimize iterations in the development phase. This doesn't mean a full \"Waterfall\" approach, but it does mean that at least some analysis design is put in before building. Typical Development Steps: Initial Task A Task sent to Sandbox, the Task creator takes the Task as a Worker with the goal of testing the instructions and generating gold data. Internal Qualification Task After incorporating feedback and gold data from the Initial Task, this allows the Task creator to test that the gold data works correctly, as well as perform any fine tuning before sending the Task to a Worker platform. Qualification Task Sent to Production, this allows the Workers to prove that they can complete the Task at high quality; they will also ask questions and provide feedback on the Task. Initial Task – Small Batch Sent to only the high quality Production Workers from the Qualification, this small batch (100 500 records) allows the Task creator to collect production data, while also checking for trends where Workers are having issues. Internal Business Process – Tiny Batch After incorporating feedback trends from Workers, each Task should be relatively stable at this point. The Tasks can now be joined in a Business Process, which can be tested with (approx. 5 10 records) internally before sending to a Worker platform. Business Process – Small Batch Incorporate changes, feedback, and findings into a complete end to end solution, looking for additional trends in the individual Tasks but also any issues with the overall business process. Production Batches Finalize any additional feedback, but the process should now be ready for production, with the caveat that Worker quality must constantly be monitored. If automation or some level of Bot Tasks are needed, additional iterations may be required. "},{"version":"10.0","date":"Aug-06-2019","title":"task-instructions-for-workers","name":"Task Instructions for Workers","fullPath":"iac/core/user-guide/best-practices/task-instructions-for-workers","content":" The following items are the recommended sections of instructions for crowd sourcing Tasks Title – clear, clean, publicly accessible title to easily identify the Task. See more details here. Overview – brief explanation of Task objective, often in the format: \"Given , provide \". Background – provide some context or background that may be necessary for the Task. Does not need to be its own section – may be best formatted as extra paragraphs between the Overview and the Steps. Steps – clear, step by step instructions to follow. Data Definitions – a table listing the label, definition, and example of each data element the Worker should provide (useful for complex Tasks). Guidance – explanations of the steps where you provide more clarity or insight needed to complete the Task (could also be called \"Useful Tips\" or merged with \"Common Mistakes\" to for a \"Do's and Don'ts\" section). Common Mistakes – explanations of things you DON'T want to be completed (Do's and Don'ts can also be used here). Exceptions – items that go against the rest of the logic, potentially just a part of a previous section. Examples – specific examples of correct responses. FAQ (Frequently Asked Questions) – clear explanations of specific questions examples encountered by iterations of a Task. References – supplemental information to help complete the Task or provide more context (could also be \"Links\"). Not all Tasks require all sections; however, at a minimum, the Worker should be provided a Title, Overview, and Steps. "},{"version":"10.0","date":"Aug-06-2019","title":"model-migration-on-premise","name":"Model migration on-premise","fullPath":"iac/core/user-guide/best-practices/model-migration-on-premise","content":" Using S3 Browser double check that multipart upload and download are turned off(disabled). Read \"On Prem S3 Emulator\" topic at Download hypermodel content from source s3 vds models bucket Download trained model content from source s3 vds resources bucket Upload hypermodel content on destination s3 vds models bucket Upload trained model content on destination s3 vds resources bucket Update model flat.prod file on destination s3 vds models bucket with hyper model name:version. Dont forget to keep one last line empty Wait about 3 5 min and check {CONTROL TOWER HOST} gateway service listHyperModelCheck untill model with version appeared in JSON On destination Control Tower conigure Manual Task to link new Hypermodel trained model Automate Manual Task Launch testing extraction on Automation BP During extraction check logs {CONTROL TOWER HOST} gateway servide manage log"},{"version":"10.0","date":"Aug-06-2019","title":"bp-migration-notes","name":"BP Migration Notes","fullPath":"iac/core/user-guide/migrate-to-another-ct/bp-migration-notes","content":" WorkFusion SPA executes multiple Business Processes (BPs) created by 3rd parties. During upgrade from one SPA version to another it is important to make sure all BPs are still working. Parts of BPs are migrated automatically by WorkFusion's embedded tooling. However in the instances where it's not possible to do it automatically, refer to the applicable migration guide referenced below for your version of Workfusion SPA. Deprecation notices should also be taken into account. Make sure you reviewed and applied notes for all SPA versions between source and target version. E.g. to upgrade from 9.0 to 10, you need to read notes for 9.2 Migration Notes for 10.0 DRAFT Migration Notes for 9.3 Migration Notes for 9.2 "},{"version":"10.0","date":"Aug-06-2019","title":"migrate-packages-of-tasks-bps","name":"Migrate Packages of Tasks and BPs","fullPath":"iac/core/user-guide/migrate-to-another-ct/migrate-packages-of-tasks-bps","content":" This article is valid for SPA 10.0. The best way to migrate a Process or a Task from one WorkFusion environment to another (e.g. from DEV to UAT or PROD) is to create a Package and import it. Let’s assume source is a DEV environment and destination is a UAT environment. A Package is a ZIP archive containing: XML file with Task or BP export information (design, settings) All Data Stores that are used in a Task or BP steps meta information User must have the Import Export permission to view, import, and export Packages. Prerequisites BCB Configuration AutoML Use Case Copy ML artifacts Download the Package Import the Package AutoML Conflicts Apply ML model Import BCB project Version Labels Prerequisites BCB Configuration Skip this step if no BCB is used for the Business Process. Bot Config Bundle (BCB) configuration—source code located in Java project—should be deployed to GIT SVN. Use maven command to deploy the latest code to DEV environment: Update BCB from Control Tower: Advanced > Bot Configurations > Import from Repository. AutoML Use Case Skip this step if no automation is applied to Manual Tasks. If automation is applied to Manual Tasks, it is required to apply AutoML Automation Use Case to existing Business Process. If the latest version of Business Process contains AutoML Use Case: Create a new Business Process copy with the new name version. Replace all AutoML Use Cases with appropriate Manual Tasks. Save Business Process without any AutoML use cases inside. If any Manual Task contains AutoML settings, automation can be applied on destination environment for already imported Business Process. Note that Version 9.2 and newer have Search Engine 2.0 with all parameters already optimized. Custom AutoML Use Case If AutoML Use Case is a custom one and has completed changes on a source environment: Create new Business Process using as an AutoML Use Case template in the Cognitive Bot Use Cases section: Open Workflow for this Business Process Drag and drop the original Manual Task from the left pane to replace the empty task by it. Save the Business Process. There should be no validation errors. Prepare Package for export to the destination environment. Import the package to the destination environment. Open the imported Business Process and select Workflow diagram. Replace Manual Task inside it by the empty Manual Task: Save the BP Create a new Auto ML Use Case from Use Cases menu on the Business Process tab. To avoid conflicts during import export, create a new version name for the source Business Process. Copy ML artifacts If Business Process has a Machine Learning model linked to Manual Task, please use the following steps to migrate binary model and pretrained model: Migrate Trained Model. Migration is required for each model linked to Manual Task. Create a Package Go to View all Business Processes (or Tasks) > Packages tab (the last one) Click the New Package button. Enter a meaningful Name and Description for the Package. Choose Data Stores to include in the Package by ticking the appropriate checkboxes. The list contains all Data Stores used in BP Tasks (Manual or Bot). You can uncheck a Data Store if you are sure that the target WorkFusion environment contains one with the same name and appropriate data. Choose the Target Version for current Package. This feature helps move processes from instances with older WF version installed (e.g. WF 7.4.1) to new instances (e.g. 7.6.0) and vice versa. Click the Save button. Download the Package To download a Package to your local machine, click the appropriate Download button in the Actions grid column. As a result, the Package will be downloaded as a ZIP file. You can delete a Package if it is not needed anymore – click an arrow icon near the Download button and select Delete action. Import the Package Log in to the target WorkFusion environment (e.g. PROD or UAT). Go to Business Processes (Tasks) and click Import Package in the top right corner. Upload the Package ZIP archive downloaded from the source environment and click the Preview Package button. Package Import page will be shown. Here you need to resolve all possible conflicts: You could see the following warnings: \" business processes of version are active\" – in this case you need to decide whether you want to overwrite active processes or not. You can optionally pause or stop active processes. \"The package was exported from WF version . This environment has WF version (YYY)\" – in this case import can potentially cause some issues. You should check its results in the generated log file. * AutoML Conflicts* If the Machine Learning model (same name, model version,and trained model id) from the package is not available on your production environment, you will get one of the following errors: Note that \"Trained model ID is not available\" error message is displayed only if the model version is correct. You have to acknowledge that AutoML configuration will be not set (1) to import your package (2). If you are importing the same Package more than once, you need to create a new version or overwrite a particular version of this Task (BP). When overwriting an existing process version, you can have a diff view for process structure and each process step. This function can help make a decision whether to rewrite a process or not. After applying overwrite in import, the following changes are applied: Completed tasks and processes will have an old version Draft tasks and processes will have a new version Processing tasks and processes without significant changes (steps added, deleted, or substituted with other steps) will have a new version Schedules are switched to a new version If the imported and existing process versions have significant differences (steps added, deleted, or substituted with other steps), you will need a special \"Advanced Package Import\" permission to make such potentially dangerous overwrite – see the 9.2 Role Management topic. Click to see detailed description... This import type can be required when there is a need to update active process runs, avoid schedule re creation, or reuse already configured plugins (by UUID). When importing a Package with a modified process structure, the following notifications are shown: amount of added removed process steps and rules amount of active process runs When you overwrite an active process run, its result data can be inconsistent because of added removed changed steps and rules. If your package contains Data Stores and or included Bot Configs, you can get the following* Resolve Import Conflict* section(s): Choose one of the options (Replace, Keep, or Rename) for each conflict section. Data Stores are compared by structure and number of records. For included Bot configs, you can toggle a diff view. When all import conflicts are resolved, click the Import Package button in the bottom left corner. If the Package has been successfully imported, you will see the import result message. You can download a detailed import Report by clicking the View import log file link. This log can be useful in case some issues are found after import. When a Package has been imported, WorkFusion checks its structure and displays useful notes and action items to be done before running the imported Task or Process (e.g. \"Define global variables\") View the imported BP (Task) or navigate elsewhere using main menu. The imported BP or Task will create a Run in a Draft state. When a new version of a Task is imported to a target WorkFusion environment, Automation settings, Workforce, and Prices parameters will be copied from the previous version that already exists on this environment. Apply ML model If a Business Process was copied with the AutoML Use Case and model, follow these steps to apply AutoML settings: Apply Automation. Apply Auto ML settings using the AutoML Use Case described here: Custom AutoML Use Case. Import BCB project If a source Business Process was developed using BCB project, deploy and import it according to this section: BCB Configuration. Create a new Business Process copy with the same name to update all BCB configs, if the BCB is used for Business Process. Version Labels Let's assume that you have a BP and: created the 1st Package for this BP and imported this Package to PROD environment; edited BP and save it; created the 2nd Package for this BP and imported this Package to PROD environment As a result, each new imported Package creates a BP or Task Run that is automatically labeled by a timestamp. The latest version is marked with green color, other versions have grey color. The following Task and BP settings are NOT included in a Package: Workforce for each Manual task (when importing, these settings will be taken from target instance defaults) Global variable values (when importing, respective columns are created in the \"Global Variables\" datastore, but with empty values) Bonus Rules Custom Attributes Priority Permanent open tasks Forcefully complete option "},{"version":"10.0","date":"Aug-06-2019","title":"migrate-use-case","name":"Migrate Use Case","fullPath":"iac/core/user-guide/migrate-to-another-ct/migrate-use-case","content":" If you need to migrate a Use Case from one Control Tower to another (for example, from the development to the production environment), the workflow depends on whether the source and target Control Tower have the same SPA version. If version is the same, use the Simple Migration procedure. If the versions are different, opt for a Workaround. Simple Migration Simple migration is only possible when the source and target Control Tower have the same SPA version. Export from Source Version At your source (development) instance, go to System Settings > Use Cases. Select your use case. Select Export Use Case(s) to generate the xml file with the use case. Import to Target Instance On your target (production) instance, go to System Settings > Use Cases. Select the tab corresponding to the type of the use case you have exported on the step 3 (Manual, Bot or Business Process). Select Import Use Cases(s) and upload the xml file from step Note that the target Control Tower should have the same SPA version as the source Control Tower, otherwise the xml file will not be recognized: If the version is correct, preview and select components to Import: Workaround This workaround is should be used when cross version import of a use case is required. Export from Source Version On the System Settings > Use Cases tab, find your use case. Select your use case and find the business process it is based on: Edit Use Case > Campaign. Go to Business Processes and find this business process. Open it and go to the Packages tab, select New Package. Select the target version (SPA version of your target instance) and save your BP. After your BP package is generated, download the zip file. Import to Target Instance In the target instance of Control Tower, select Business Processes > Import Package. Select Add and specify your zip package Select Preview Package. After the package is checked for compatibility, select Import Package. Go to System Settings > Use Cases and select Create Use Case. In Campaign section, select the BP you imported on step 4, and save your use case. "},{"version":"10.0","date":"Aug-06-2019","title":"migrate-task-bp-use-case-to-another-ct-instance","name":"Migrate Task, BP, Use Case to Another CT Instance","fullPath":"iac/core/user-guide/migrate-to-another-ct/migrate-task-bp-use-case-to-another-ct-instance","content":" WorkFusion platform provides capability to copy (or deploy or migrate) manual tasks, business processes (BPs), and* *use cases from one Control Tower instance to another. To migrate your work, export BP or task to single XML file or ZIP package on a source instance and import these files to the target instance. With this approach you can organize solution promotion from development environment to production (e.g. from DEV server to PROD). There are two migration options: Migrate Package of Process or Task the preferred way, when a bundle of BP (or manual task), data stores associated with it and relevant metadata is created and migrated. Migrate Use Case. "},{"version":"10.0","date":"Aug-06-2019","title":"manage-workforces-and-crowds","name":"Manage Workforces and Crowds","fullPath":"iac/core/user-guide/manage-workers/manage-workforces-and-crowds","content":" Prior to reading this topic, see the Dynamic Task Distribution among Crowds topic. Crowds Crowd – a group of Workers from one Worker Portal (WorkSpace, MTurk, Elance, etc) that suits a set of pre defined requirements and Qualifications. Requirements can be of the following types: country region, available working hours, earnings. For example: you can create a Crowd named \"WS US 98%\" which consists of WorkSpace workers who have 98% approval rate and are located in the U.S. To access and manage Crowds in WorkFusion platform, navigate to one of the following pages: Workers > Workforces > Manage Crowds OR Workers > Workforces > Create Edit Workforce > Add Crowd The Manage Crowd popup displays the following Crowd parameters (if they are set): Endpoint Required Qualifications Allowed Countries Working Hours Min Max Workload You can perform the following actions with a Crowd: Create Edit Copy Delete Create Edit a Crowd To create a new Crowd, navigate to the Manage Crowds popup and click the Create New Crowd button. To edit an existing Crowd, navigate to the Manage Crowds popup and select Actions > Edit Crowd in the appropriate Crowd block. Field Description Name Unique Crowd name. Description Describe some special parameters of the Crowd to quickly distinguish the Crowd among others. End Point Choose the crowd sourcing platform where the Workers come from and submit Tasks. Set of Qualifications Select a Qualification Set that a Worker must have in order to be included in this Crowd. Alternatively, you can switch to the Advanced Mode and select up to 5 required Qualifications. Agreement Select or create a new Agreement that a Worker must accept in order to be included in this Crowd. Alternatively, you can select the No agreement required option. Allowed Countries Only Workers from the listed countries will be included in this Crowd. Text input field with country look up. You can add several countries or delete them if needed. Working Hours The time during which WorkFusion engine will post Tasks for this Crowd. If you want to show the Task during business hours: 8 17 Sun,Mon,Tue,Wed,Thu,Fri,Sat Time is set in the UTC Timezone. More examples: 1) 9 17 Fri UTC 5:30 task is available on Friday from 9:00 till 17:59 India Standard Time 2) 0 24 Mon UTC; 16 24 Sat,Sun UTC task is available on 2 periods of Coordinated Universal Time Notification Emails These email addresses will get notifications about Tasks available to the crowd, if the Crowd Notification Message is set in Task &gt; Advanced Options &gt; Notifications. This option is commonly used for internal Crowds, you can enter group alias emails, i.e. translators@my company.com. Max Hours per Day When the total Worker Task activities exceed the specified hours amount, WorkFusion engine stops posting Tasks to this Crowd till the end of the day. Minimum Commitment in Hours per Day You can set this parameter to guarantee that this Crowd will be fully loaded with Tasks. Distribution algorithm will process Crowds with the Minimum Commitment in Hours per Day parameter configured first of all. Max Earn Out per Calendar Year The maximum amount of money that all Crowd Workers can earn during the calendar year. When this limit is reached, Tasks cannot be posted to this Crowd. Workforces Workforce – a grouping entity that specifies who is working on the Task. A Workforce can contain one or several Crowds. For example: you can create a Workforce named \"US 98%\", consisting of \"WS US 98%\" and \"MTurk US 98%\" (WorkSpace and MTurk workers who have 98% approval rate and are located in the U.S.) To access and manage Workforces in WorkFusion platform, navigate to Workers > Workforces. You can perform the following actions with Workforces: Create Edit Copy Delete Create Edit a Workforce To be able to create Tasks, you must have at least one Workforce. A Workforce must contain at least one Crowd. Field Description Name Unique Workforce name. Description Describe some special parameters of the Workforce to quickly distinguish the Workforce among others. Workload Distribution This Rule defines how to distribute Tasks among the Crowds according to their parameters: Priority Working Hours Max Hours per Day Minimum Commitment in Hours per Day You can click Edit Rule and modify the code. See the Rules and Rule Templates topic for detailed info. Crowds Add at least one Crowd to your Workforce by clicking the Add Crowd button and selecting a Crowd from popup. Set the following parameters for each Crowd: Priority. This option defines the priority in workload distribution among Crowds. Depending on your needs, you can prioritize the Crowds or set an equal priority. Crowd. You can change the Crowd by selecting or typing its name in the dropdown. Enable. If you want to temporary disable a Crowd in your Workforce, uncheck this checkbox. In case you do not need a Crowd in your Workforce, click the appropriate Remove link or just change the Crowd to the other one. To have a quick look at the Crowd parameters, click the appropriate View link. "},{"version":"10.0","date":"Aug-06-2019","title":"create-assign-qualifications-to-workers","name":"Create and Assign Qualifications to Workers","fullPath":"iac/core/user-guide/manage-workers/create-assign-qualifications-to-workers","content":" Qualifications are a means that the WorkFusion platform provides to ensure the quality of Crowd Workers and narrow your Workforce for optimal performance. Qualifications can be assigned to Workers manually, automatically by Qualification Rules, or Workers can request some Qualifications. For more information, see the Qualifications topic. Create Qualification To view and manage all Qualifications, go to Workers > Qualifications page. On the Qualifications page, you can: create new Qualifications or delete existing Qualification( s); filter the grid and configure the displayed columns by clicking the cogwheel; edit existing Qualification. To create a new Qualification, click the Create Qualification button, the following screen will appear: Field Description Qualification Name Give your Qualification a unique and meaningful name. Description Provide some details to distinguish the Qualification. Accuracy Based This parameter defines whether the Qualification has Score. Private Private Qualifications – Qualifications intended to mark some Workers internally in WorkFusion. It is not visible to Workers. Private Qualification is not included in qualification requirements for the Crowd. If you want to start a Qualification Task, you should add Accuracy Based Qualification in addition to a Private one. Private Qualification parameter cannot be changed after Qualification is created. Keywords A list of terms used by the search mechanism that helps Crowd Workers find the Task when performing a search. UUID A unique Id that will be given to a Qualification after creating. Automatically Granted This option allows a Worker to request this Qualification and get it without submitting any test. If this option is selected, the Grant Score field appears (it defines initial Score to be assigned to Workers). Test Duration Time allotted for the test (in seconds). Available only if Auto Granted is unchecked. Test means that a Worker can request Qualification, but must complete a test to get it. Questions XML To create a Custom Qualification, enter the contents of the Qualification in this editor. Refer to the following link for more information: http: docs.amazonwebservices.com AWSMechTurk latest AWSMturkAPI ApiReferenceQuestionAnswerDataArticle.html Answers XML Editor for creating answer keys for Custom Qualification. Refer to the following link for more information: http: docs.amazonwebservices.com AWSMechTurk latest AWSMturkAPI ApiReferenceQuestionAnswerDataArticle.html Workers The Workers tab displays all Workers with current Qualification and their parameters. On the Workers tab, you can perform the following actions: In Worker ID column: view Workers' Qualifications navigate to Profile (see the Workers topic) Give Bonus to Workers: select a Worker, input the Bonus amount into the appropriate grid column, and click the Give Bonus button. Change Score Send Message. See the Messaging with Workers topic. History The History tab displays who edited and saved the Qualification and when. To compare Qualification versions: Tick 2 versions in the grid. Click the Compare button. The diff view is displayed under the grid. To view the Description of a version, click the appropriate Name. The Description is displayed under the grid. Assign Qualification You can manually assign Qualifications to Workers in any of the following places: Workers > Assign Qualification Select a Qualification. Set a Default Score. Provide Worker's ids separated by comma. Workers > View All Workers OR Task BP Results > Workers Select Worker(s). Click the Assign Qualification button. Select a Qualification. Set the Score. Workers > Worker Profile add new edit Qualification. Qualification Sets Qualification Sets are used to group different Qualifications and determine values for them. On the Qualification Sets page, you can: create edit Sets delete Sets export import Sets using XML format Each Qualification Set has the following fields: Field Description Name Unique name. We recommend to give a meaningful unambiguous name. Description Provide some details on all Qualifications if needed. Qualifications You can add up to 5 Qualification in a set. Select the following parameters from dropdowns: Qualification Condition (only for Accuracy Based) Value (only for Accuracy Based) "},{"version":"10.0","date":"Aug-06-2019","title":"worker-profile","name":"Worker Profile","fullPath":"iac/core/user-guide/manage-workers/worker-profile","content":" Prior to reading this topic, see the Workers topic. View All Workers Column Heading Description Worker Profile Performance Charts Worker Statistics Worker Compensation Worker Gold Performance Qualifications Worker Qualification(s) ChartChart) Communication Center Recent Tasks View All Workers To view and manage Workers in WorkFusion platform, navigate to Workers > View All Workers. This grid contains all Workers that have ever submitted Tasks or sent some feedback on a Task including Bot Workers. You can perform the following bulk actions with Workers: Give Bonus. This function is intended to stimulate Workers with high performance to accept more Tasks. Send Message. You can Answer Workers' messages regarding Tasks or offer top priority Tasks to some Workers. To send a mass message to Workers, select the appropriate Workers, click the Send Message button, and enter your message with subject. Assign Qualification. You can assign an Accuracy Based Qualification to Worker(s). Export to Excel. View All Workers The grid can contain the following columns: Column Heading Description Worker ID System generated unique identification number assigned to each Worker. Accuracy The percentage of all Worker Answers that coincided with Answers provided by majority. Approval Rate The percentage of Tasks completed and approved for the Worker. Calculated by dividing the amount of correct Answers submitted by the amount of total Answers associated with the Task. Tasks The number of overall Tasks performed by the Worker. Gold Accuracy The accuracy of the Worker in terms of Answering Gold questions. Calculated by dividing the amount of correct Answers submitted by the amount of total gold questions associated with the Task. Bonus The total amount of bonus funds issued to this Worker. Reward The total amount of compensation paid to the Worker. Rank The rank associated with the Worker compared to all Workers in the system. Pay Rate The average pay rate per hour for this Worker. Hours day The total amount of hours and days spent by this Worker on Tasks. Avg. Time per Task The average time used per Task by the Worker. Added The date and time the Worker was added to the system. Last Activity The date of the last recorded Worker action (submission, feedback) Country The country of origin associated with the Worker. Bonus Entry field used to apply a one time bonus to a particular Worker. See the description of the Filtering and Grid functions in the Task Results > Workers topic. Worker Profile To view a Worker Profile, click the appropriate record in Worker ID column. You can also navigate to Worker Profile from the Task Results > Workers grid. Worker Profile The top panel contains Worker ID, Name, Country, and Notes Comments. To edit Worker Name, Email, or Notes, hover over his her Name and click the pencil icon. Performance The Performance tab contains overall Worker statistics displayed in tables and pie charts. You can filter the data by the following parameters: Campaign (Task, BP) OR Use Case; All or one particular (Task, BP, or Use Case); Timeline; Charts Answer Distribution. This chart displays the amount of Accepted Task versus the number of Rejected Tasks. Worker Profile Answer Distribution Campaign Distribution. This chart displays how many Worker Tasks the Worker has submitted in each Campaign (Tasks or BP). This allows you to gauge, among other factors, what kind of work this Worker likes to perform. Worker Profile Campaign Distribution Worker Statistics This grid contains key Worker parameters that are calculated based on Worker's activity. Worker Compensation This grid displays current Worker earnings: Total Wages, Total Bonuses, and Pay Rate. Worker Gold Performance This section lists the gold performance of the Worker. Gold Performance is determined both by Qualification Runs and by seeding Normal Runs with Gold Questions. Gold Performance lists the percentage of correct answers (calculated by dividing the amount of correct answers submitted by the amount of total gold questions associated with the Task). Gold Performance is calculated across all the Task runs. Qualifications The Qualifications tab provides information about the Qualifications of a specific Worker. In addition, it also allows you to grant Accuracy Based Qualifications to that Worker. You can perform the following actions with the Worker's Qualifications: Add. To add a Qualification to a Worker: click the Add New button; select a Qualification from the dropdown under the grid; enter the Score; click the Save button. Edit. You can change the score for a Qualification. Remove. Be careful when performing this action, the Worker can feel offended and can stop taking your Tasks. Worker Qualification(s) Chart This chart shows you how the Worker has performed on the Qualifications assigned to him her with the course of time. Communication Center The Communications Center tab allows you to send messages to Workers and grant bonuses to them. You can also view all the message history in the Worker Messages grid. To find specific messages, use the filters above the grid. Worker Profile Communication Center Recent Tasks The Recent Tasks tab provides information about the Tasks performed by a specific Worker. You can perform the following actions: Export to Excel. Filter the Tasks. Navigate to a specific Task by clicking the Task ID. Worker Profile Recent Tasks "},{"version":"10.0","date":"Aug-06-2019","title":"business-processes-synchronization","name":"Business Processes Synchronization","fullPath":"iac/core/user-guide/sample-task-bp/business-processes-synchronization","content":" Problem Summary While designing solution using WorkFusion BPM it's useful to use synchronization design pattern. Assume you have 2 Business Processes and is BP1 is in run, you can not run BP2 in parallel. Both BPs could run on schedule and BP2 should automatically detect if BP1 is running and if it is, BP2 flow is simply goes to it's end. BP1 Set Sync Flag syncbp syncbpflag BUSY update @this set {syncvariable}=' {syncvariableset_value}' Long lasting Task Drop Sync Flag syncbp syncbpflag AVAILABLE update @this set {syncvariable}=' {syncvariableset_value}' BP2 Read Sync Flag syncbp syncbp_flag select * from @this Concurrent Long lasting Task Simple Synchronization DataStore Test Execution If you put both BPs into WF instance and create this simple DataStore, sync could be tested as following: Execute BP1 it will set flag and 'sleep' 1 minute on 2nd Bot Task Right away execute BP2 notice that flow transitioned into end without executing 'Concurrent Long lasting Task' After minute execute BP2 again notice this time it goes to 'Concurrent Long lasting Task' "},{"version":"10.0","date":"Aug-06-2019","title":"data-flow-in-business-process","name":"Data Flow in Business Process","fullPath":"iac/core/user-guide/sample-task-bp/data-flow-in-business-process","content":" Manual Tasks and Composite Rule Input File Manual Step 1 Detect Language Composite Rule Manual Step 2 Extract Data Results Bot Tasks and Export Plugin Input File Manual Step 1 Find Website by Company Name Bot Step 2 Check URL Composite Rule Bot Step 3 Write Results to Data Store Bot Step 4 Error Handling Send Email Execution and Results This section describes main examples and technics on how to pass data between Business Process (BP) steps, both Manual and Bot. The key concept of Control Tower BPM is the data flow between steps while processing big amounts of uniform data. Input data for Business Processes is divided into items with a defined structure which are called records. These records can be listed in a CSV file or can be dynamically grabbed from a Data Store, search result, parsed from a list, etc. Each step can do the following: add new columns include or exclude data from previous steps split or merge data (increase or decrease the number of records) Manual Tasks and Composite Rule This simple BP is intended to detect document language, and extract special fields for all the English invoices. You can download these files, import, and run on your Control Tower: Input File _image and text.txt BP Package Manual Language Detection Extraction 23 10 2017.zip Process Details Input File The input file contains 6 records (documents) with 2 columns: document _image – links to png scans of invoices document _text – links to html code of invoices These links will be used in Manual Tasks as input data. They are accessible in Manual Tasks using the following syntax: {question.data 'column _name' } : {question.data 'document _image' } {question.data 'document _text' } Manual Step 1 Detect Language The first step uses these two columns' data as input image address and link address. The 2 answers Unique codes (language, proforma) will serve as output data provided by workers on the Task submit. This code sample shows how to use variables in text and in html attributes (for images and links): Manual Task Code Document Text {question.data 'document_text' } Composite Rule The composite rule serves to route all English invoices to the 2nd Manual Step by comparing the answer from the 1st step (language) to the \"English\" string. Alternatively, the \"proforma\" answer could be used in the rule condition. Manual Step 2 Extract Data This step uses the following columns as input data: language (from the1st Step) proforma (from the 1st Step) document _text (from the CSV input file) – as source text for the Information Extraction answer. To use a data column as source text for the Information Extraction answer, enter its name into the Unique Code field: On the Task Preview, you cannot view the data from the 1st step (language, proforma) until at least one record is submitted on the 1st step. After you run the BP and submit several Worker Tasks from Step 1, this data is populated on Task Preview. And the tasks from Step 2 are available for accepting on WorkSpace: Results You can view data for each step in the View Results > Data page. The Final Results option contains columns at the process end (after the last step is completed): Input Data columns: document _text document _image Step 1 (Detect Language) output: proforma language Step 2 (Extract Data) output: document tagged text – contains IE text with tags created by Worker number – date – items name price Note that Control Tower adds calculated confidence and accuracy columns for each Manual Task answer, which can be disabled by scrolling a table to the right and clicking the cogwheel icon. Bot Tasks and Export Plugin The following process is intended to collect and validate official websites for a given list of companies. Valid websites are saved into a Data Store, invalid websites are emailed to a defined email address. You can download these files, import, and run on your Control Tower: Input File company_name.csv BP Package Website Validation Example.zip Secure Storage Create Secure Storage record with your email credentials and my keys alias. Process Details Input File The input file contains 5 records (company profiles) with a single column: company _name – names of companies to search official website for. These links will be used in Manual Tasks as input data. Manual Step 1 Find Website by Company Name The first Manual step is designed to show company name and link to a default Google search for this company. A worker is expected to analyze the google search, pick up a right website URL from it, and submit this URL. Note that the same input data column is used 3 times here: as text to display a company name: as a part of link href attribute and link text: Task output is provided by the answer with the website _url unique code. Task Preview looks like the following: This Manual Task also has a hidden answer with worker _name unique code which will capture Worker first name and last name: Get Worker Details Code Snippet See more information here Templates TemplateVariables Bot Step 2 Check URL The second process step is a Bot Task which is intended to: Make an http GET request to the website url (submitted by Worker on the previous step). The variable value is accessed by using the following syntax: {website url} Check the response code using the http.statusCode object property. If response code equals 200, set the validity variable to true, otherwise false. Export the validity variable value to a new column with the same name. This column value will be used in the next Composite Rule to decide where to route the record – to an Exception Handling step or to Data Store logging step. Checking website response using http plugin {http.statusCode == 200} false Composite Rule This rule checks the validity column and routes records to an Exception Handling step or to Data Store logging step. Bot Step 3 Write Results to Data Store This step takes all data columns from input file and previous steps and writes their values into the websites Data Store using Datastore Plugins. If this Data Store does not exist, it will be created automatically. Note that column values are accessible in the block by their names: website _url.toString(). Writing results to Data Store The step results look like this one: Note that not all the records are saved to the Data Store because the rest records had invalid website _url and were routed to the Error Handling step. Bot Step 4 Error Handling Send Email This step is intended to send an email to a specific address ( in the example) containing the following info: company name {company _name} worker name, who submitted invalid URL {worker _name} invalid website URL {website _url} link to a particular Business Process instance for quick navigation {bp _link} variable. See more details in Bot Tasks Context Sending email in a Bot Task Website not found for: {companyname} User {workername} submitted this invalid URL: {websiteurl} Business Process link: {bplink} The following example shows real emails sent by Control Tower with variables substituted by real data: The {username} and {password} variable values are taken from the Secrets Vault using the Deprecated Secure Store Plugins – this is a safe way not to expose sensitive data in the Bot Task code. You need to create a record in Secure Storage with the my keys name. The current step contains variables with sensitive data, therefore it is better not to pass its variables to the export plugin. Note that the export plugin has include original data=\"false\" attribute. It means that all information from previous steps will be deleted (will not be present in final process results). Execution and Results When you run the Business Process and submit all Manual Tasks, all Bot Steps will be executed automatically. As you can see on the screenshot below, 2 records had invalid URLs and were routed to the Error Handling step. Other 3 records were logged to the Data Store. Results of a Manual Step: The Check URL Bot Step validated the URLs and added the check result to the validity column: As you can see from the Final Results, no data were saved for 2 records that went to Error Handling step their cells are just blank because of the include original data=\"false\" attribute and empty export section. "},{"version":"10.0","date":"Aug-06-2019","title":"grouping-records-by-unique-column","name":"Grouping Records by Unique Column","fullPath":"iac/core/user-guide/sample-task-bp/grouping-records-by-unique-column","content":" This topic describes how to set up a Human Task with denormalized input data using the Unique Column Name functionality. This information can be useful, for example, when you want Workers to provide a \"True False\" answer, but each question will have different answer options (variants). Moreover, each Task can have different number of answer options. Initial Input File In the following example, Worker should pick up a match \"action\" with \"tool\". seq action tool 1 Hit Nail Shovel 2 Hit Nail Rake 3 Hit Nail Saw 4 Hit Nail Hammer 5 Hit Nail Hose 6 Cut lawn Edger 7 Cut lawn Rake 8 Cut lawn Mower 9 Cut lawn Hammer 10 Cut lawn Screwdriver And the Human Task should look like this: Task Example Initial file has 10 Records, but we need only 2 Records with 5 different answer options. To implement this Task using the standard column answer mapping, we need to create a new Task for each record, which is not the right approach. Therefore, we need to: Group (merge) Records by the \"action\" column. Go to Run Task > Advanced Options > Properties Enter the appropriate column name into the Unique Column Name field. Advanced Options Unique Column Name. On Design Task tab, add an Answer with \"check one\" type. Depending on your task, the answer type can be: Check Multi, Select One, Select Multi, etc. Edit the Task Code to load the answer options from the \"tool\" column. To do this, you need basic CSS, HTML, and JavaScript (jQuery) knowledge. File after Grouping Records are merged by the Unique Column, the strings are concatenated using the following delimiter: ♮ music natural sign (U 266E) Some text editors can incorrectly display this symbol. Do not forget to use the UTF 8 encoding. seq action tool 1♮2♮3♮4♮5 Hit Nail Shovel♮Rake♮Saw♮Hammer♮Hose 6♮7♮8♮9♮10 Cut lawn Edger♮Rake♮Mower♮Hammer♮Screwdriver Task Design Add a new Answer with \"Check One\" type and one option (e.g. \"One=1\"). This option is needed as a prototype for copying, and it will be removed with JavaScript on the next step. Adding Answer Editing Task Code To load the string from the \"tool\" column to answer options, you need to write the following JS code: Script Example Algorithm Splitting the string into an array (variants) using the (♮) delimiter. Copying the answer id into the (id) variable. Cloning and removing the answer option. Pasting new answer options with appropriate name, text and value attributes. Answer HTML Code{width=\"923\" height=\"472\"} Running the Task To start testing, run the Task. The merged Records will be correctly displayed only after the Run Task action. Results "},{"version":"10.0","date":"Oct-04-2019","title":"moderation-flow","name":"Moderation Flow","fullPath":"iac/core/user-guide/sample-task-bp/moderation-flow","content":" Diagrams for Moderation Flow Creation Step and Adjudication Rules in Tasks Composite Rules in Moderation Flow Known Issues and Limitations Prior to reading this material, see the Work with Business Process BP topic. Moderation workflow is a BP that usually consists of two main steps: Creation step step where* creator* worker creates some info (writes review, translates text, transcribes audio fragment, etc.) Moderation step step where moderator worker checks the creator's work, and can accept or reject it (with different options). See How to create a Moderation Task for details. BP Examples: Translation Proofreading, Find Headquarters Address Moderate Address, etc. You can also use the Moderation Flow together with Qualification Tasks to educate your Workers and improve their proficiency by giving them feedback and correcting their answers. Diagrams for Moderation Flow All Tasks are moderated, without Reworking option. Moderation Flow All Tasks, without Reworking All Tasks are moderated, with Reworking option. Moderation Flow All Tasks, with Reworking Creation Step and Adjudication Rules in Tasks In the Creation step you should pick Adjudication Rule: Moderation Adjudication (Open task→ Task Properties → Advanced Options → Adjudication Tab) NOTE! No need to create new Rule, just use existing rule named Moderation Adjudication (see below) Moderation Adjudication Expand source Composite Rules in Moderation Flow The 1st Composite Rule (To moderation) defines whether the Moderation step can be optional (based on Worker's Qualification, Statistics, Confidence, etc.) or not. Enable the MODERATION _RULE in the Advanced Options. NOTE! If the Rework is needed (See cases: 2, 4), also enable the SPLIT _DATA option. The 2nd Composite Rule (Decision) defines possible Task flows depending on the Moderator decision. You can add the following Outcomes: Accept – an Outcome defining a case when Moderator accepts the Creator Task. Add a Condition with the Approve initial worker's task Advanced Option. Reject – an Outcome defining a case when Moderator rejects the Creator Task. Add a Condition with the Reject initial worker's task Advanced Option. Rework – an Outcome defining a case when Moderator wants the Creator to redo the Task. Enable the SPLIT_DATA in the Advanced Options and tick one of the following options: Rework task by the same worker. The Worker will not get Reward for reworking. Assign task to the same worker. The Worker will get Reward for each reworking iteration. Prohibit current worker to work on this task. The Creator will get Reward, but the Task will be assigned to another Worker. Known Issues and Limitations Currently \"Prohibit current worker to work on this task\" outcome option is not supported N A option is not restored correctly for Free Text, Number and other text fields "},{"version":"10.0","date":"Aug-06-2019","title":"qualification-task-with-ie-answers","name":"Qualification Task with IE Answers","fullPath":"iac/core/user-guide/sample-task-bp/qualification-task-with-ie-answers","content":" In WorkFusion you can create a Qualification Task with 9.x Information Extraction IE (https: kb.workfusion.com display WF %5B9.x%5D Information Extraction IE) Answers. You can also enable the Training Mode for this Task and provide explanatory messages in the Gold file. How to Obtain the Gold Data Create a regular Task with an IE Answer and run it. Complete this Task in Sandbox. Generate a Snapshot of this Task or download the Result Data file. Copy your Task without Data Load the Snapshot as Gold Data for the new Task. If the Training Mode is enabled, add appropriate columns with gold messages. Set an Accuracy Based Qualification for this Task. IE Answer Settings Check the Do not use in Adjudication option: for the IE answer; for all Sub Answers in the Group of Answers. Uncheck the Do not use in Adjudication option: for Group of Answers. for IE Sub Answers that are not in the Group of Answers. IE Answer Structure Column Mapping Map columns in the Gold file to all Answers with the unchecked Do not use in Adjudication option. If the Training Mode is enabled, add appropriate columns with gold messages. Group of Answers columns must contain JSON where key is answer _code* *and value is array of elements where keys are code of Sub Answers and values are corresponding gold Answers. Group of Answers Gold Expand source You do not need to create separate gold _ and gold ... message columns for each Sub Answer in the Group of Answers, because this gold data is already included in the Group of Answers JSON. You will see mapping Warnings for Sub Answers in the Group of Answers – ignore them. Training Task with IE Answers After submitting a Training Task with IE Answer, Worker will see the following explanatory messages and correct answers: Training IE Task Qualification Score Calculation Group of Answers is considered correct only when all Group Sub Answers are answered correctly. "},{"version":"10.0","date":"Aug-06-2019","title":"requiring-workers-to-click-specific-link","name":"Requiring Workers to Click Specific Link","fullPath":"iac/core/user-guide/sample-task-bp/requiring-workers-to-click-specific-link","content":" This example describes how to create a Task requiring Workers to click on the url provided before submitting a Task. This feature is helpful when you want to at least guarantee the link was clicked and prevent the situation when Workers weren’t going to the page, but rather answering questions without clicking the link. We want the following message appear, if a Worker clicks the Submit button without checking the target URL: Required Link This feature can be done in the Design Task > Code Editor by adding an HTML attribute and a JavaScript function: Locate link you would like to monitor in a Task Code and put a class inside it (for example, prim input url) The whole link tag will have the following code: Put this JavaScript piece after the @hit macro (after <@hit …. line): Save the Task. Required Link Code Editor To check the script work in WorkFusion: Open the Task Preview. Click the External URL link at the top – the Task will be displayed with the Submit button. Click the Submit button – you should get the alert message \"You have to check each URL provided\". Click the required URL, the click the Submit button – this alert message should not appear. Alternatively, you can check the script work directly in the Crowd Virtualizer sandbox after the Task is run. "},{"version":"10.0","date":"Aug-06-2019","title":"rss-feed-monitoring","name":"RSS Feed Monitoring","fullPath":"iac/core/user-guide/sample-task-bp/rss-feed-monitoring","content":" The RSS Feed Monitoring BP is intended for aggregating the latest and unique financial news from different RSS feeds, extracting ETF Tickers and Names from these articles, and sending emails to customer with all extracted data and source links. Input Data Data Store with RSS feeds Data Stores with Keywords Data Store with unique news URLs Goal Extract and validate all links from RSS feeds Extract text from all valid URLs Find specific Keywords in the extracted texts If a text contains target keywords, internal Worker should read this text and find ETF Tickers and ETF Names All the found info (RSS feed, link, ETF Tickers, and Names) is sent on the client's email Examples RSS feed: News article link: Filling Detail: RSS Feed Monitoring Workflow Proсess Steps Description This process contains Split rules for adding new Records (split one record into multiple). We need to split Records because one Data Store contains multiple RSS feeds, and each RSS feed contains multiple links to financial news articles. 1. Extract Monitored Resources This step reads the following options from the Data Store: RSS feed URLs Last check dates xPath selectors for feeds, links, and text email address and names 2. Extract Latest RSS Text This step extracts all links from the RSS feed using xPath form the Data Store and checks if the link is unique. Records with non unique links go to the process End. All unique news article links are stored in a Data Store. 3. URL Validator This step checks the availability of links extracted from the RSS feed on the previous step. Records with invalid links go to the process End. 4. Boilerpipe Text Extractor This step extracts text from html news article content and saves it to Amazon S3 bucket. This Bot step also extracts article headers and ETF links and appends these columns to the snapshot. 5. Find Keywords Steps These 2 steps (Select N Datastore Name, Match with Header Keys) are intended for searching specific keywords in the news headers and descriptions. In this process, the keywords are searched consequently in three predefined Data Stores. If no keyword match was found, the Record goes to the process End. Records with keywords are routed to the Manual step. 6. Review RSS Information Manual On this step, human Worker needs to review the news article and extract the following fields, if available: ETF Name ETF Ticker There can be multiple ETF names and Tickers. Review RSS Human Task If an article does not contain information about new ETF launch, a human Worker answers \"No\" and the Record goes to the process End. Otherwise, an automatic email is sent to an address taken from the Data Store. 7. Send Email with RSS News Email contains the following information: ETF Tickers ETF Names RSS feed article URL and header "},{"version":"10.0","date":"Aug-06-2019","title":"schedule-business-process","name":"Schedule a Business Process","fullPath":"iac/core/user-guide/schedule-task-bp/schedule-business-process","content":" Besides running your business processes manually, you can start them according to a schedule. To create a schedule, go to Schedules tab and click Create. * * In the Schedule editor: Select the business process you need to run. Note Using the scheduler, you can run not only business processes, but also standalone bot tasks, so be careful when selecting the required item from the dropdown list. Attention When you delete a business process from the list of business processes, its instance is kept in Control Tower. Be careful not to create multiple processes with the same name as it might cause confusion when scheduling. If you need to use Input data, upload the data file. Otherwise, select Empty (in this case the scheduler will use the data from the recorder variables). Set the schedule period. Note The server time of the Scheduler is set to GMT, and all schedules are triggered according to GMT. Set the run parameters, if required. Add the title of the schedule, description and tags (if required). Set schedule frequency (type in or select the values from the dropdown menus). You can set the process to start at a particular time or in certain time intervals. Note The schedule will be triggered according to the schedule frequency only within the schedule period. Click Save. Once the schedule is saved, you cannot edit it. If you need to edit a saved schedule, copy it and edit the copy. You can pause and start a running schedule, and delete a schedule at any stage. When the Scheduler starts a business process, it creates a separate instance of this process. You can see all instances in the View all business processes tab. "},{"version":"10.0","date":"Aug-06-2019","title":"schedule-business-process-via-bot-task","name":"Schedule Business Process via Bot Task","fullPath":"iac/core/user-guide/schedule-task-bp/schedule-business-process-via-bot-task","content":" Package Scheduler v2 10 11 2017.zip Description The attached Business Process is used to emulate functionality of the Scheduler with the help of a machine config. You can add this custom logic and use it, if you want to stop a Business Process which has not been completed and re run it again over a time interval interval. The Business Process workflow is as follows: Stop Run for the record in datastore (with status IN _PROGRESS and campaign uuid from the ETL form) via the REST API call and set the STOPPED status. Run the Business Process with campaign uuid from ETL form using the start task plugin. The plugin returns the Run uuid. The machine config inserts a new record to datastore with status IN _PROGRESS for this campaign uuid. Using the release plugin the machine config is turned to Sleep for a certain time and then executes this step again. Optionally, you can add the code below to the machine config to update the record status in datastore to COMPLETED in the Business Process: RunDto run = (RunDto) ((WebHarvestTaskItem) item.getWrappedObject()).getRun(); String runuuid= run.rootRunUuid; > update @this set status = 'COMPLETED' where runuuid= {run_uuid}; Preparation Add the secure storage variable: Attribute Value alias basic.hash.for.rest key basic.hash.for.rest value Create datastore (SchedulerDatastore) for monitoring. Example of the record in datastore: Import the Business Process with Scheduler to the instance. Procedure Start the imported Business Process with Scheduler and provide campaign uuid on the ETL form for the Business Process, which you want to run. To find Campaign uuid go to Campaigns → View all, set the filter to \"Composite\" and find your Business Process by title. Open the Business Process, go to the Enter Property tab and see the UUID in the Campaign UUID field: "},{"version":"10.0","date":"Aug-06-2019","title":"start-task-bp-from-data-source","name":"Start Task, BP from Data Source","fullPath":"iac/core/user-guide/schedule-task-bp/start-task-bp-from-data-source","content":" Versioning Info For 8.0 and older versions see this article 8.0 Starting Task, BP from Data Source For version 8.4 see this article 8.4 Starting Task, BP from Data Source Data Source is a mechanism providing input data and configuration for your BPs and Tasks. This input format is best suited for those Use Cases where the data resides on an external source such as SFTP Server or Amazon S3 Cloud. To start a Task BP from a Data Source you need to have: UUID of existing Task BP (Definition UUID) Business Processes List > Open definition in new tab, switch to the opened tab and click Definition UUID to copy UUID to the clipboard Input Data file (.csv) uploaded to external server Configuration file (.ccf) uploaded to external server Data Source WorkFusion object containing external server credentials and path to the configuration file Schedule number of occurrences per each Data Source update Creating a Configuration File The campaign configuration file (.ccf) contains all task BP parameters needed to start a new run. This file contains parameter=value pairs on each new line. Download an example. Parameter Value Description ACTIVE TRUE default parameter DATAFILE inputfile.csv relative path to the input data .csv file (server folder with .ccf file is a root) for s3 it's full path inside your bucket DATAFILESIZE 1 default parameter FILESEPARATOR COMMA; PIPE; SEMICOLON; TAB separator that has been used in input file ENV SANDBOX; PRODUCTION environment BLOCKSIZE 1 block size TASKDISPLAYPRIORITY MAX; NORMAL; MIN task priority (tasks with the max priority will be displayed first if task type is the same) CAMPAIGNUUID 79eea42a 127e 4613 83a2 7fbe80ba0a38 create a Task BP (Campaign) in WorkFusion and copy campaign uuid (or copy uuid of the existing Campaign) GOLDRUN false; true indicates if the run is gold USEGOLDBUCKET false; true indicates if gold bucket should be used tags datasource, test, list of tags SNAPSHOTTYPE XLSX; CSV format for the snapshot OUTPUTDIR results relative path for storing the Task BP snapshot (server folder with .ccf file is a root) STREAMTYPE Immediately; PermanentOpenTask Immediately all hits will be created on endpoint; PermanentOpenTask specific quantity will be displayed; if value not specified immediately by default STREAMVALUE 10 works if STREAMTYPE=PermanentOpenTask; default value 10 STREAMTHRESHOLD 1 (if absolute value); 25% (if percentage) 100% by default if value not specified Uploading Files to Server Upload input file (csv) and configuration file (ccf) to an external server (FTPS, S3, SFTP). Remember to check the file and folder READ and WRITE permissions on the external server. Creating a Data Source To start managing Data Sources, go to Configuration > Campaign Data Sources in the main menu. Data Sources On this page, you can create, copy, edit, and delete Data Sources. To create a new Data Source: Click the Create Data Source button. Data Source on S3 Enter the appropriate Name and Description for your Data Source. WorkFusion will automatically generate a UUID for the new Data Source. You can use this UUID in API calls or Schedules. Select a Type from the dropdown: S3 (Amazon's Cloud Server) FTPS (File Transfer Protocol SSL encryption) SFTP (Secure File Transfer Protocol based on SSH) Enter the Configuration Parameters, which depend on the connection type: S3 accessKey – Access Key ID secretKey – Secret Access Key bucket – destination bucket in S3 keyPrefix – remote folder Note: keyPrefix should have the slash symbol \" \" at the end SFTP host– host name to establish connection over SFTP protocol port – port number to connect to on the SFTP server login – SFTP Server account user name password – SFTP Server account password remote _folder – a directory on the SFTP server to start the session private _key – key stored on your local machine to enable authentication without password public _key – key stored on the SFTP server to enable authentication without password FTPS host – host name to establish connection over FTPS protocol* * port – port number to connect to on the FTPS server login – FTPS Server account user name password – FTPS Server account password remote _folder – a directory on the FTPS server to start the session Click the Save button. Creating a Schedule Go to Campaigns > Schedules Select your Campaign. Select the Data Source you've created. After the Schedule has been created, the specified Task or BP will start automatically. WorkFusion will poll this Data Source for any input files at a specified interval (every 10 minutes) and if a file is present it will be run. Result To verify that your Task BP has started, log in to the external server and open the folder with the .ccf file. Verify that {name}.ccf.uploaded file has been created. When a Task BP is completed: a {name}.ccf.done file appears. a result file will be created in the appropriate folder (OUTPUT _DIR parameter of the .ccf file). If a Bot Task hasn't started, check that Bot config for this step doesn't contain Source value with Availability options. To reuse an existing Data Source: delete the .ccf.uploaded and .ccf.done files from the external server folder; update the input .csv and .ccf files if needed. "},{"version":"10.0","date":"Aug-06-2019","title":"add-bot-task","name":"Add Bot Task","fullPath":"iac/core/user-guide/work-with-bp/add-bot-task","content":" Bot Tasks contain work to be done by WorkFusion's Bot. These Bots perform the steps in your Business Process which are more suited for automation than human labor. Bot Tasks can exist only as a Business Processes step. For example: address parsing, determining the width and height of an image, interacting with your API or repository, or filtering content that do not meet certain criteria. You can add a Bot Task to a BP using one of the following methods: from Toolbar or Canvas Context Menu. A blank Task is created – you should select a Use Case and design the Task. from Sidebar or Element Context Menu. Task design is copied from the source Task. You can edit the Task if needed. If you duplicate a Task BP, all subsequent changes in source or duplicated Task BP will be synchronized. If you copy a Task BP, you will be prompted to edit its name, and there will be no synchronization between the source and the copy. Select Use Case Selecting a Use Case for a Bot Task is similar to the appropriate step for Manual Task. Tip If you are not a professional user, we recommend you to select the ETL Use Case category. Bot Configs can be converted to ETL forms to provide safe reusing and parameter setting. See description here. Design Task Enter the Task Name. Enter the input column name(s) (or answer codes). Some ETL Bot Tasks do not require these parameters. Enter output column name(s) (answer codes). Some ETL Bot Tasks do not require these parameters. Set other options, if needed. Do not forget to save the Task before closing it. After saving the Task, save the BP. "},{"version":"10.0","date":"Aug-06-2019","title":"add-manual-task","name":"Add Manual Task","fullPath":"iac/core/user-guide/work-with-bp/add-manual-task","content":" You can add a Task to a BP using one of the following methods: from Toolbar or Canvas Context Menu. A blank Task is created – you should select Use Case, upload Gold Data (if needed), design, and set the Task properties. from Sidebar or Element Context Menu. In this case, all Task properties and design will be copied from the source Task. You can edit the Task if needed. If you duplicate a Task BP, all subsequent changes in the source or duplicated Task BP will be synchronized. If you copy a Task BP, you will be prompted to edit its name, and there will be no synchronization between the source and the copy. Select Use Case See Draft Select Use Case for more details. Upload Data When a Task is a part of a BP, you can upload Gold Data only. The data can be used for monitoring Workers' Accuracy. Attention The option is available in WorkFusion SPA. Design Task See Draft Design Manual Task for more details. There are only two differences from an individual Task Design: Data Elements. You can insert Data Elements from Input file data and Data from previous steps. The data is taken from all the previous Tasks Answers. Preview. The preview may be unavailable if data from the previous steps is used in the Question section. Task Properties See Set Run Parameters for more details. You cannot run a Task from the tab as the BP engine manages the Task run. Do not forget to save the Task before closing it. After saving the Task, save the BP. "},{"version":"10.0","date":"Aug-06-2019","title":"business-process-validation","name":"Business Process Validation","fullPath":"iac/core/user-guide/work-with-bp/business-process-validation","content":" Rules to Create Valid Business Process Error Messages Task Data Rules to Create Valid Business Process BP must contain the Start and End elements. BP must contain at least one Task element. If all Records should move from one step to another without any conditions, you can connect these steps directly, without a Rule. All elements must be identified (have name and parameters set). The Start element must have an outgoing connection. The End element must contain at least one incoming connection. Task, Rule, or BP should have at least one incoming and one outgoing connection. Error Messages If a BP validation detects some errors, explanation messages will be displayed above the Toolbar. Click the error messages one by one, and appropriate BP steps or connections will be highlighted in red: You can perform one of the following actions with faulty red BP elements, depending on the error shown: Reconnect (drag incoming or outgoing connection – see the Connections topic) Define (give a name to the appropriate Task, set the design, parameters, and save this Task) Delete Task Data Ensure that all the BP Tasks use valid column values from input Data file or Answer Codes from the previous Tasks. "},{"version":"10.0","date":"Aug-06-2019","title":"business-process-run-results","name":"Business Process Run Results","fullPath":"iac/core/user-guide/work-with-bp/business-process-run-results","content":" How To Run Business Process When you have uploaded the Data, completed and validated the BP Design, and set the Run options, perform the following actions: Run the BP in a Sandbox environment by clicking the Run This Process button on the Run Business Process tab. If the BP is valid and has started successfully, you will see the following screen: BP Successful Run You can view the results of the BP, or view the BP you have just created via the BP List. Log in as a Worker (in WorkSpace), search the BP Tasks by name or Tags, and complete all the Worker Tasks. Depending on the Task Adjudication Rules, you should have more than one Worker account. Go back to the Control Tower) and edit the BP options, Design, or Data if needed. Copy the BP with Data. You can optionally stop or delete the original BP if you do not need it. Run the BP in a Production environment by checking the appropriate checkbox above the Run This Process* *button. The following sections describe how to watch the BP progress, analyze Worker answers and activity, and get the final Result Data. Workflow Diagram You can watch the BP progress and get real time statistics using the BP scheme on Design tab in View Mode and on the Run tab. BP Design View Mode Each Task has an indicator with percent and number of Records handled in this Task. Example: 67% (10 15). Each Rule Outcome has an indicator with number of Records that passed through a specific rule outcome based on conditions defined. Example: 8 → Yes and* 2 → No*. These metrics are updated when new Records are submitted from different process steps by human workers or bots. Summary The Summary page gives a BP overview with key parameters and links. In the Choose Task drop down, you can select an individual Task, Process, All BP Tasks, or Final Results. M – Bot Task H – Manual Task P – Process Here you can view the Submission Statistics, Costs, and Worker Statistics for the whole process or for a selected step. You can expand the BP Task Details by clicking the corresponding plus ( ) icon or selecting a Task figure on the diagram. See the detailed description of Task Summary here. View Results Data Available from 9.0 The View Results > Data page combines all the data gathered in the BP: Worker answers from Manual steps – example: \"website _url\" Data from Bot steps (provided in the export plugin) – example: \"validity\" Columns from the Input Data file – example: \"company _name\" In the Choose Task drop down, you can select an option to display Data for an individual Task, Process, All BP Tasks, or Final Results. You can tweak the data view using the following filters: Record Status (In Progress, Not Started, Completed, Completed Confidence Low) Process Step (any Manual or Bot step, or Final Results) Column name and value Worker ID Task ID Statistics block By clicking on the block you may find basic statistics information about the Business Process instance. Final Results The results can be exported to a CSV or Excel file and saved to your computer. Some value may be empty (e.g. 1st and 2nd rows in the \"validity\" column). It happened because these records were not routed to the Bot step which automatically validates the website and adds \"true\" or \"false\" to the \"validity\" column. These records were routed directly to the process end after the 1st step, because their \"website _url\" was submitted as \"n a\". See the detailed description of Task Result Data here. If you have problems with viewing exported UTF 8 data in Excel, see this topic . Workers In the Choose Task dropdown, you can select an option to display Workers for an individual Task, Process, All BP Tasks, or Final Results. See the detailed description of Task Workers here. For Bot Workers, the following actions are NOT available: Pay for Rejected Work Assign Qualifications Retract Worker Answers Send Message Give Bonus Analytics In the Choose Task drop down, you can select an option to display Workers for an individual Task, Process, All BP Tasks, or Final Results. See the detailed description of Task Analytics here. Messages See the detailed description of Task Messages here. View All BPs To view all BPs with their details, go to Business Processes > View All Business Processes. See the description: Business Processes List "},{"version":"10.0","date":"Oct-16-2019","title":"collaborative-bp-editing","name":"Collaborative Business Process (BP) editing","fullPath":"iac/core/user-guide/work-with-bp/collaborative-bp-editing","content":" title: Collaborative Business Process (BP) editing When two or more users are editing the same Business Process (BP) or different Runs of the same BP, WorkFusion engine detects this situation and performs the following actions: displays all users that are viewing or editing current BP adds indication for each Task or Rule being edited by other users automatically updates BP steps when they are saved by other users notifies users about changes in BP structure prompts for conflict resolving if needed: update process or ignore changes made by other users displays changes made by other users and shows differences This feature helps to understand who is editing the same process, preserve all useful changes, and resolve conflicts when several users have changed the same BP step or BP structure (rules, outcomes, added deleted steps). Displaying all BP editors In the top right corner, you can see the number of users that are currently editing the process. Click the following icon to see all user names in a dropdown list. Collaborative Users Other users editing BP steps WorkFusion shows an indication when any other user is editing: Manual steps (in the BP designer or in a separate Task designer) Bot steps (in the BP designer or in a separate Task designer) Other Users edit BP steps Composite Rules Rule Collaborative Editing The step is outlined with a color corresponding to a user. When you hover over a step, you can see the list of users currently editing this step. Run time update Case 1 Manual and Bot Tasks When other user: edited step \"A\" (changed Name, Design, Options, etc.) and saved it; OR replaced step \"A\" with other step \"B\" from library by dragging it on step \"A\"; saved the BP. Then step \"A\" is automatically updated (or replaced by step \"B\") for all users that are not editing step \"A\" at the moment. Case 2 Rules When other user: edited Rule \"X\" (changed Name, Outcome name, or Condition, but did NOT add remove Outcomes) and saved it; saved the BP. Then Rule \"X\" is automatically updated for all users that are not editing Rule \"X\" at the moment. Recently edited steps indication If you have edited a BP step and saved this step, but haven't saved the BP yet, this particular step will be marked with a pencil icon. Real time notification about process updates If other user modified BP structure (added removed moved step,rule, or outcome) and saved the BP, you will see the following action panel: Other user modified BP This action panel displays the name of the user who modified BP and contains 3 actions: View the changes – open a modified BP version in view mode in a new browser window. BP changes preview Update my view – all changes made by other user will be applied to the BP. Ignore changes – keep working on your BP version without applying any changes from other users. You can keep editing BP without taking any action. In this case you will need to resolve conflicts when you save the BP. Conflict resolving Conflict resolving dialog is shown in two cases: when you save a step or rule (instant conflict); when you save a BP (deferred conflict). When you have edited BP structure, step, or rule and tried to save it, you can get the following Conflict messages: Another user has edited this BP. You have the following options: view the changes – this step will be opened in view mode in a new browser window; discard my changes – keep other user's changes and save the BP; overwrite other user changes – ignore other user's changes and save your version of the BP. Another user has edited this Manual task (Bot task, or business rule). In this case you have the same options as for BP structure conflict, and a new one: Show comparison – view differences in your task rule and other user's version to make a decision, which version needs to be saved. There can be one conflict for BP structure, and multiple conflicts for several BP steps or rules. You can switch from one conflict to another by clicking arrows on the left and right side. To save a BP having conflicts, you need to do the following: Resolve all conflicts either by overwriting or by discarding. Click the Save Business Process button again. "},{"version":"10.0","date":"Aug-06-2019","title":"composite-rules","name":"Composite Rules","fullPath":"iac/core/user-guide/work-with-bp/composite-rules","content":" Composite Rules are used to define the variant of BP flow depending on the Record column values. Example: If the website _status of a Record equals to \"VALID\", then execute the \"Rename the Company Contact Information\" Task with this Record. Otherwise, go to End. You can add a Composite Rule to a BP using one of the following methods: from the Toolbar or Canvas Context Menu. A blank Rule is created. from the Sidebar or Element Context Menu. In this case, the Rule will be copied from the source Rule. You can edit the Rule if needed. Editing a Rule To edit a Rule, double click on the Rule figure or right click and select Edit from the context menu. Enter a unique name for your Rule. Click Add Outcome if there is not any. Each Rule must have at least one Outcome. Click Add Condition to add a condition to an outcome. You can add multiple conditions to Outcome. If you leave Outcome without any conditions, this will mean all other Records. The logic of Rule elements is the following: AND between Conditions OR between Outcomes Set up Conditions for your Outcomes. Each Condition contains three required fields. These are as follows. Answer Code (or the name of column in Data file). The field has an autocomplete. Logical Operator (i.e. equals, less than). Value (or regular expression – matches regexp, not matches regexp). RegExp allows you to use OR logic in Condition values by adding the pipe \" \" symbol, e.g.: 1 2 Save the Rule by pressing Save. Connect the newly created Outcomes to the target Tasks or Processes. To learn how to use the Code Editor and Advanced Options for Rules, see the Professional User Manual. You need to have programming background and Java, Drools knowledge. "},{"version":"10.0","date":"Oct-16-2019","title":"business-processes-list","name":"Business Processes list","fullPath":"iac/core/user-guide/work-with-bp/business-processes-list","content":" The Business Processes List (Business Processes > View All) is aimed to fulfill the following functions: give a quick overview of statuses, details, progress, possible issues for all Business Process instances; enable access to all the instances of a Business Process with a possibility to manage them, including changing the state (start, pause, resume or stop), creation of another instance (copy) and deletion of an instance. provide possibility to perform Business Process Actions including the creation of new Business Process or importing a Business Process created in another Control Tower. Overview The Business Processes List consists of the following areas: List of Business Processes – in the list all BP instances are grouped under respective BP definition to enable a compact hierarchical view and quick recognition of their statuses. Actions – main operations with a Business Process, such as Create New and Import. Filters – using the filters you can narrow the overview to your needs with the help of predefined and custom criteria. List of Business Processes The list of Business Processes displays all processes available in Control Tower at the moment. A BP definition is represented as a box, where the name and date are displayed along with the states of all instances of Business Process. It provides a compact hierarchical view, especially for processes with a lot of instances. The BP date denotes when the latest instance has been created. The icons display the state of all instances of the Business Process: Draft Processing Paused Completed With Errors With Messages The icon is hidden if there are no instances with the respective status. Click the Open in new tab icon to open the specific Business Process definition in a new tab. This function can be useful when you are working with one BP and want to view only it instances. If a Business Process contains a Manual Task, which can be automated with a Machine Learning Model, its name will be indicated with the \"A\" sign: BP Instances To expand a Business Process and view all its instances click the arrow on the left side of the box. Definition UUID: The Definition ID, where the Business Process is executed. This field can be useful when using WorkFusion REST API. Status Tabs: Click on a tab to view the instances with the respective status. Bulk Actions: Tick the box to select specific instances to perform bulk actions, for example, Start, Stop, Delete or Repair. You can remove the selection from an instance or instances to exclude it them from the bulk action. Sorting: Arrange Competed Started instances in ascending descending order. BP Instance Steps Click the arrow to expand an instance to view its steps: Instance details: The following details are displayed: who and when started the execution of the BP instance status of the execution, Worker Platform (for instances containing Manual steps) and environment. Draft Processing Paused Completed With Errors With Messages WorkSpace Started by Schedule. Click the icon to view the Schedule Click the BP instance name to start editing this instance: changing process steps, scheme, input data, options, etc. Instance Progress Bar: Progress bar displays the percentage of completion of the instance execution. Click the Event log icon 🛈 near the progress bar to view the events logged during the execution of the instance. If some errors occur, this icon will have a red background. Generate Snapshot: You can generate a Snapshot for the BP instance (for non draft BPs). Actions Dropdown: A list of available Actions that can be performed on the instance: Copy Edit and Run Start Stop, Pause Resume View data Repair Download original data Delete Note that available actions depend on the BP instance status (draft, processing, completed, with errors) BP Steps (Tasks) and Sub Processes: Under each BP Instance, you can see a hierarchical list of its Steps (Tasks) and Sub Processes. This list contains only steps which have at least one active record, i.e. process execution reached these steps. Task Progress Bar: Progress bar displays the percentage of the task execution completion and ratio of input output records (documents). Generate Snapshot: You can generate a Task Snapshot – CSV or XLSX file containing step execution results. Actions Dropdown: View output data resulting from the task execution. Click the arrow near the Task's progress bar to view Task details. Task details contain: Launched on – date of the first execution of the current step. Completed – date when the current step was completed. Input records – number of records that reached the current step. Spent – amount spent on a Manual Task. (displayed for SmartCrowd product only) Workers – number of unique Workers who submitted answers to the particular Manual Task. Bot Configuration – link to the Bot configuration. Automation Available If Automation with a ML model can be applied to a Manual Task in the Business Process, the instance is labelled with the Automation Available sign and the respective step is indicated with the \"A\" sign: For more information about Applying Automation refer to VDS Workflow Part 2 > > Step 4 Apply Automation. Actions With the Action button you can create a new Business Process or import a Business Process Package. Filters Using the filters you can select the Business Processes to be displayed in the list. The filters are aimed to facilitate your work with the Business Processes List, as you have a rich set of criteria to define what instances are included to and excluded from the list. With the filters you can focus on the Business Processes, whose instances are in your scope at the moment. Filter state – shows how many records are filtered. With this option you always know the exact number of items matching criteria of the filter applied. Filter types There are two types of the filters: predefined – the filter uses criteria for selection from a built in set; advanced – in advanced filter you can set a number of criteria picked up from a list of options, such as state, time span and others. Predefined filter The predefined filter uses built in set of criteria to select what Business Processes will be shown. In the predefined filter you can select Business Processes containing instances created by you, with a certain state, executed in one or another environment and created within the defined period of time. Advanced filter In advanced filter you can use the following options: Text filter for Tags, Author, Title and Process UUID. In the text filter you can define the match of results such as contains does not contain or equals to not equals to. Filter for date when the instance Created, Started, Ended or Due. In the date filter you can define the match of results such as equals to not equals to, grater than less than, grater than or equals to* less than or equals to* and* period*, Filter for status – you can select the following states of instances in Business Processes: Completed, Active, Running, Answered, Paused, Errors, Draft, Has New Messages. The filter criterion is true. Filter for environment – Production or Sandbox. Click Add Parameter to include and define an option for the filter. You can choose as many filter parameters as needed to create the required filter. With the Match all parameters option you can define, if the filter should match to all the criteria specified or any of the parameters. Save filter You can save the filter for future use. Click the arrows and select Save Filter in the drop down. Key in the name for your filter and choose, if the filter is your private or it's available for the other users. Clear or Refine Filter If a filter is applied the yellow box is displayed in the filter area You can remove or edit the filter by clicking Clear filter or Refine filter there respectively. "},{"version":"10.0","date":"Aug-06-2019","title":"connections","name":"Connections","fullPath":"iac/core/user-guide/work-with-bp/connections","content":" All BP elements must be linked by connections which define the flow of Data file Records. The arrow displays the connection direction. How to create an outgoing connection Deselect element by clicking on Canvas empty space. Hover over the element. An arrow icon will be displayed to the right of the element: BP Design Connection Arrow Drag the arrow icon on another element. A colored dashed line will be displayed. If the line is green, the action is allowed. BP Design Connection (Allowed) If the line is red, the action is not allowed. An error message will be displayed. BP Design Connection (Not allowed) Each connection start and end need to have blue squares, otherwise it's likely not connected and the WorkFusion engine will throw an error message. Editing Connections You can edit connections or delete them. To edit a connection: Select a connection by clicking it. Drag and drop the beginning or ending point to another element If the line is green, the action is allowed. If the line is red, the action is not allowed. You can also place a connection label by dragging a yellow square in the middle of the label. BP Design Connection You can rename or delete a connection using context menu. Outcomes for Rules Outcome is a Rule outgoing connection. You can edit the Outcome Name and Conditions by double clicking it or from context menu. "},{"version":"10.0","date":"Aug-06-2019","title":"create-bp-from-bot-task","name":"Create BP from Bot Task","fullPath":"iac/core/user-guide/work-with-bp/create-bp-from-bot-task","content":" Video and Materials Publishing Recording to Control Tower Publishing Bot Task to Control Tower Designing Business Process Creating Workflow Adding Decision Rule Adding Input Data Adding Manual Task to BP Video and Materials In this guide, we will create the BR Rating BP. You can see the process of creating the BR Rating BP in Control Tower in the video. Downloadable materials Here are the materials you can use to implement the BP Rating BP. The raiting website validation recording that can be opened in WorkFusion Studio and published to Control Tower The raiting website scraping recording that can be opened in WorkFusion Studio and published to Control Tower The BP Rating package that can be imported into Control Tower as a ready business process (BP) The Input data file The recording has been created using RPA Express 2.0.3. Thus, you will need to have version 2.0.3 or later installed on your machine to open the recordings. Below, you can find the detailed guide on how to publish your recordings to Control Tower and create a BP step by step. Publishing Recording to Control Tower To publish a recording to Control Tower, do as follows. Click the Publish to Control Tower button on the menu. Enter the BP name. Click Finish. If the recording has been published successfully, the following dialogue window appears. To open the BP draft in Control Tower for editing, click Open in Control Tower. Note Each recording is published both as a ready one step BP that can be run in Control Tower and as a BP step (or a Bot Task) that can be used in other business processes. To update a recording that has already been published, publish it again using the Publish to Control Tower button. You will be prompted with the following message. Click Finish to update both the BP and the Bot Task in Control Tower. It will also update all BPs that include this Bot Task. To publish a recording as a different BP, use the As new business process option. Publishing the recording as a new BP will remove connection between the recording and the previously published BP, and will create a connection to the new BP. If the recording is published again, only the new BP will be updated. Publishing Bot Task to Control Tower To publish a Bot Task to Control Tower from the Code perspective in WorkFusion Studio, follow the scenario below. Open the Configs folder of your project. Right click on the Bot Task you need to publish. Select Publish Bot Task. * * Enter Instance URL (the URL of the Control Tower) and the BP name. Click Finish.* * To update the already published Bot Task, publish it again. To publish without updating the previously published Bot Task, select the Publish Bot Task to new BP option. Designing Business Process Besides running the one step BPs published from WorkFusion Studio, RPA Express Control Tower allows you to create complex BPs with multiple steps (bot tasks and manual tasks) and various workflows. To create a new BP in Control Tower, do as follows. Go to Business Processes > Create You can also click the Create New button on the business processes page. Press Select Use Case. Go to Other > Miscellaneous Tasks > Select This Use Case. * * The Workflow tab opens for you to define the BP workflow. About BP As an example, we will use the BP Rating BP that collects information about a number of companies online. The BP consists of two bot tasks and a manual task. These are as follows. The raiting website validation Bot task that checks if the provided web page URL is valid. The raiting website scraping Bot task that extracts information from the web page. A manual task, in which a human is able to review the URL that was marked as wrong and provide the correct URL for the second Bot task. In the manual task, you see which webpages return the Not found status (saved in the is _valid variable) in Bot Task 1, find the correct url manually, and pass it to Bot Task 2. You can find the video demonstrating how to create a BP in Control Tower, and download used recordings and input data at the end of this guide. Creating Workflow On the Workflow tab, perform the following steps. Step 1: Enter the BP title and tags if required. Step 2: Add the BP elements to the canvas. Available BP elements in RPA Express are as follows: Start, End, Manual Task, Bot Task, Decision Rule. A process must contain start and end elements and at least one bot or manual task. Attention Using BPs as elements of other BPs is not available in RPA Express. The option is available in WorkFusion SPA. You can add the elements to the canvas by: adding them from the toolbar or via the context menu (invoked by right clicking on the canvas). In this case, an empty element is created that needs to be defined. To define the element, you need to drag the required bot task, manual task or rule on the respective element icon from the side panel. dragging them from the side panel (only bot tasks, manual tasks, and rules). In this case, the already existing element is added to the workflow and doesn’t need to be defined. To edit a step in the workflow, double click on its element in the canvas. Step 3:* *Create connections between the elements by dragging the arrow iconfrom one element to the other. If the line is green, the connection is allowed. If the line is red, the connection is not allowed (an error message is displayed in this case). Rules for creating connections The Start element must have 1 outgoing connection. The End element must have at least 1 incoming connection. All tasks and rules must have at least 1 incoming and 1 outgoing connection Step 4: Make sure all the BP steps use valid data from the previous tasks, i.e.: column headers from Input Data Answer codes from manual tasks variable values from the published recordings Step 5: Validate and save your BP by pressing the corresponding buttons. In case you have any error messages, click them one by one, and appropriate steps or connections will be highlighted in red. Adding Decision Rule You can define several flows for a BP, depending on certain conditions with the help of Rules. Rules are added to a BP by: adding a new rule from the toolbar or using the context menu adding the existing rule from the sidebar To define a new or edit existing rule, double click on the element's icon. The following actions are performed in the Rule Editor. Enter the rule name. Click Add Outcome. Outcomes define different BP workflows based on certain conditions. Click Add Condition to add a condition to the outcome. Thus, you can add multiple conditions. If Outcome has no conditions, it means for all other records. Click Save. ** The logic of Rule elements is the following: OR between Outcomes AND between Conditions Each Condition contains three fields: Variable (Answer code from a manual task, Column header from input data, variable from a bot task) Logical Operator (i.e. equals, less than) Value or regular expression, if required. Outcomes are connected to respective elements in the BP. ** Validate the BP to check that the workflow is valid. Adding Input Data Using Input Data in a BP is optional. If you need to use input data, do as follows. Open the Data tab of the BP. Uncheck the No Data box and click Upload Data. * * Click Add, select the data file, and click Upload. Click Clear to remove the selected file and select another one. The input data file has to be a .csv file. The column headers in the file should be the same as the variables used in the BP. The default data file settings are as follows. File separator – Comma File encoding – UTF 8 You can change the settings by clicking the Show options link. Input data file Input data can contain values for different types of variables. Follow these rules to create the correct input data file. The List variables values need to be written in this form: \"1\", \"2\", \"3\" . The Table variables values need to be written in this form: \"1\", \"2\", \"3\" , \"a\", \"b\", \"c\" , where \"1\", \"2\", \"3\" and \"a\", \"b\", \"c\" are table rows. The Boolean variables values have to be started with an apostrophe: 'true, 'false. Download a sample data file. Upon uploading, the contents of the input file is displayed along with the file's existing columns headers. You can remove the uploaded data by clicking the Remove Data button. In case the headers do not correspond to the variables used in the BP, the following notification appears. Click Map columns to see the error and re map the columns. Click Next to proceed to the Workflow tab. Adding Manual Task to BP To create a manual task in a BP, double click on the manual task element in the workflow. Note You can also create a standalone manual task not connected to any BP, by clicking Create Manual Task on the manual tasks list, or by selecting Manual tasks > Create. To add the existing manual task to the BP workflow later, drag it from the side panel. Design the manual task as follows. Click Select Use Case. If you create the manual task from the dashboard, you can upload Input Data that will be used in it (read about data file format here). If the manual task receives data from the previous BP steps, no input data is required. Click Next. Add Title and instructions if required, and click Save. Reopen the editor by double clicking on the manual task icon. Select data elements that the manual worker will need to use to complete the task from the side panel. Data elements are received from the input data file and from the previous BP steps. Add Answer, that is the data that will be provided by the manual worker to the next step in the BP, or as its final result. Fill in Unique Code, that is the variable in which the result of the Manual task is saved. Enter the answer name into the Answer field, that is the name the worker sees on opening the task. Select Answer Type. Add Description (optional). Check whether the answer is required. Click* Save Answer*. You can add several answers to one manual task. Press Validate and Save to complete the BP. "},{"version":"10.0","date":"Oct-16-2019","title":"copy-bp","name":"Copy a BP","fullPath":"iac/core/user-guide/work-with-bp/copy-bp","content":" BPs with the same name have a common template (Definition) – if you modify the design or parameters of one BP, all other process instances with the same name will be affected. Tags are unique for each BP, hence you can use different Tags to distinguish BPs among the BPs with the same name. You can copy a BP using the Actions dropdown. Common Rules for Copying Copy a BP means create a new process instance which is linked to the source BP and belongs to the same Definition. If you have copied a BP with Create an independent process definition – a new object is created in WorkFusion, and it will not be linked to its source BP. Copy Business Process Copy Business Process allows creating a new process instance to be run in the same Definition with the same or other Input data. Using options available in the Copy function, you can separate the copied process instance from the current Definition, change its structure and even the Steps. Copy BP A new copy of source BP is created, changes to this process instance will affect all process instances with the same name. Copy BP with Data This option should be selected, when you need to run your new process instance with the same Input data as in the source BP. The option can be used in both cases, when you copy a process instance or create a new one, which is not linked to the source BP. Create an independent process definition A new process instance is created, changes to the instance do not affect the source BP. Having created a new process instance with this option, you can change the structure of your new instance without affect to the source BP. For example, you can add or delete steps, change the transition directions and so on. At the same time, modification of the steps will affect all process instances, where the steps are used! Create deep copy (new process definitions to all steps) The Deep Copy option is used, when you should create a completely unlinked process instance with all steps separated from that ones in the source BP. It means, the Deep Copy enables modification not only of the structure in your new process instance without affect to the source BP, but of each step, as they are created as separate instances and not linked to the source steps anymore. Enter the postfix for your steps in Postfix for step names to distinguish the new steps from the source ones in the Bot library. BP Export Import See the detailed description: Migrating Task, BP, Use Case to Another Instance. "},{"version":"10.0","date":"Aug-06-2019","title":"create-new-bp","name":"Create a New BP","fullPath":"iac/core/user-guide/work-with-bp/create-new-bp","content":" There are 2 ways to create a new Business Process: Click Dashboard > Business Processes > Create Business Process. Create New Business Process or click the Create New Business Process* button from the View All Tasks* menu. Create New Business Process Select a Use Case Use Cases serve as the foundation for both Tasks and Bot Configurations. They describe the workflow of a particular Business Process and thus serve as a template to create either Manual or Bot Tasks based on that workflow. For Manual Tasks, Use Cases simplify task creation by having predefined questions, answers, instructions, and sample input file and gold files. For Bot Tasks, Use Cases simplify development by providing a baseline of the configuration code. Selecting a BP Use Case is the same as for Tasks – see the Select a Task Use Case topic. Upload Data On this step, you should upload an initial Data file that will be used by the BP Tasks. Uploading Data for BP is similar to uploading data for Tasks – see the Upload Data for Task topic. There are only several differences: You can tick the No Data option for Business Processes that do not require an input data file. For example, this comes in handy when the data is generated later on or downloaded from a Data Store or an external data source at runtime. Sample file cannot be downloaded. Gold Data for a BP cannot be uploaded. But you can upload Gold Data for individual Human Tasks of a BP. If you have problems with UTF 8 data upload, see this topic. Click Next to proceed to the Design BP tab, if you are satisfied with the uploaded data. "},{"version":"10.0","date":"Aug-06-2019","title":"design-business-process","name":"Design Business Process","fullPath":"iac/core/user-guide/work-with-bp/design-business-process","content":" On the Workflow tab, you can create a logic schema of your BP by connecting and editing Manual Tasks, Bot Tasks, BPs, and Rules. To save and run a BP, you should upload initial Data file and validate the BP schema. When creating a BP, we recommend to use tested Tasks and BPs only. Make sure that all Tasks within the BP have valid input Data and generate valid Answers. Layout Title, Tags, Date. Enter a unique BP name and and Tags for better indexing. See the naming recommendations. The date is updated automatically depending on when you created updated your BP. View Process , Edit Process .* In the View Process mode, you can watch the progress of handled Records at each step or double click Tasks and Rules to view their settings. In this mode, the BP cannot be modified.* * In the Edit Process* mode, any modifications are done. Toolbar. The Toolbar is used to add elements to the canvas and manage viewing options. Click actions: Select, Pan. The Pan tool is used to shift the canvas by dragging in the empty space area. BP Scheme elements: Start, End, Manual Task, Bot Task, Rule, BP. You can add elements to the canvas either by dragging them or by selecting the element icon in Toolbar and clicking the empty space of the canvas. Actions: Undo, Redo, Delete. You can undo or redo the previous actions. To delete an element, select it and click the Delete icon or press the Delete key. View options: Fit screen, Zoom in, Zoom out, Actual size, Percent zoom, Outline. The set of options is intended for comfortable viewing of big BP schemes. Useful Tip To navigate between different parts of the BP scheme no matter how big and complicated it is, move the blue frame on the scheme miniature in the right bottom corner. Validate, Save. You should validate a BP before saving it. Side Panel. The side panel on the right contains all the existing Manual Tasks, Bot Tasks, BP use cases (used as nested processes), and Rules. To use these elements in your BP, drag them to the canvas. To hide the panel, click the left panel border. You can also copy Tasks and Rules by hovering over the element and clicking the Copy button. Canvas. The canvas has a checkered background and is intended for placing the BP elements and connections between them. Context Menu. To call a context menu, right click on an element. If you duplicate a Task BP, all subsequent changes in the source or duplicated Task BP are synchronized. If you copy a Task BP, you are prompted to edit its name, and there is no synchronization between the source and the copy. To improve scheme readability and logically group similar steps, you can add custom colors to any process step or sub process. Available since WF 8.2 If you right click on an empty space, the following context menu is displayed. Bottom Panel. The panel is intended to view and edit BP Tasks. To call the bottom panel, double click a Task, or right click it and select Edit from the context menu. You can change the panel size by dragging its top border. How to Design BP Switch to the Edit Process mode. Add the Start and End elements. Add Tasks and or Processes. Edit and save them. Add Rules between Tasks and or Processes. Add Outcomes with Conditions and save them. Add connections between all elements in sequence. Validate the BP. Read error messages and warnings and edit the BP elements if needed. Save the BP and switch to the Run tab. "},{"version":"10.0","date":"Aug-06-2019","title":"etl-bot-configuration","name":"ETL Bot Configuration","fullPath":"iac/core/user-guide/work-with-bp/etl-bot-configuration","content":" Goals of ETL Bot Feature Setup Guide ETL Bot Examples Goals of ETL Bot Feature In some cases from time to time you should make minor adjustments in configurations of Business Process steps. You may need such easy configurable options for every launch or every week or month. It's not a problem for a developer to open a Bot and change a few variable values there. But it's another story when an in production Business Process is managed by Operations SME or similar non technical persons. Such people do not dive too deep in Bot structure, can get scared to modify the code or can modify it in a wrong way. To avoid such inconveniences WorkFusion provides the ETL Bot feature. Once a Bot s converted to an ETL Bot, the end users of Business Process get an editable form with HTML like fields instead of Groovy XML code. So, an easy and error proof configurability is achieved. Rather than enabling simplicity of configuration, an ETL Bot can encapsulate a portion of variables transformation and or additional calculations. It means, a user provides the required data and then they are converted into required variables for . So, an ETL Bot consists of 2 parts: UI form with easy to configure parameters; Additional Groovy XML code you require to prepare variables for next steps. Setup Guide You can start either with an existing Bot or a new one. Let's consider a simple example: we need to add ETL Bot with 3 editable fields: If a Bot is already created, just copy the Bot source code to the clipboard. Go to Campaigns > Templates and Create a new template. Enter Template Name. For Template Type select Bot Base. Paste your existing Bot code from the clipboard. If the template is new as in our example, use the following code: {{etlcolumnname_var1}} {{etlcolumnname_var2}} {{etlcolumnname_var3}} Make sure you have each UI field (e.g. {{etl column name _var3}} ) defined in the section. Otherwise such field will not appear on the UI form of the ETL Bot Go to Configuration > Use Cases. Select Bot tab. Create Use Case. Enter Name, select Category where your Use Case will appear for reuse. In Base Template select template created in 2 above. In Type select ETL. At this point we need to configure all variables we want to have as fields in the UI. To do so, use the right panel. Click* Add Answer* to add field by field. In our example we are adding 3 Free Text variables: {width=\"658\" height=\"400\"} Once Use Case is created, you can use it in any Business Process. Open the Business Process from Business Processes list to edit or create a new one. In Design tab just drag a new Bot into canvas and double click the Bot square bar. Control Tower will ask you to choose Use Case to start from. Use the recently created Use Case from 4 See that an ETL Bot has 2 modes: BASIC EDITOR (opens by default) and ADVANCED EDITOR. Make sure you see 3 input fields for our example in BASIC mode: {width=\"660\" height=\"435\"} ETL Bot Examples Now, we modify our initial example and add a piece of custom logic to the ETL Bot we've created. Let's assume that our task is to ask a user to provide variable names and save those variable values to Control Tower's Data Store. Once done, we need to output the variables to the next Business Process step. The modified ETL Bot is as below: {{etlcolumnname_var1}} {{etlcolumnname_var2}} {{etlcolumnname_var3}} var1 var2 var3 CustomDashboardData uuid bp_name As a result for a Business Process user the ETL Bot in BASIC mode looks exactly as shown on the screenshot from 6 above. In addition, in run time this Bot applies the user configuration to the custom code we've added, i.e. the variables typed in by the user are stored in DataStore and output into . "},{"version":"10.0","date":"Aug-06-2019","title":"manage-global-variables","name":"Manage Global Variables","fullPath":"iac/core/user-guide/work-with-bp/manage-global-variables","content":" Global variables are intended to store common values used in different BPs and not hard code them in Bot Tasks. Global variables can be used to store the following information: URLs of services (OCR rest service, web app login page) parameters (minimal DPI for document recognition, output file extension) modes (test or production) strings (names, messages, translations) timings (schedules, timeouts) To insert a global variable value, include a `` plugin into your Bot config. By default, all global variables are stored in Data Stores > Global Variables. Variable names correspond to column headers, values are taken from the first row (if other rows exist, they are ignored). Read more about Data Stores. You can use and create Global variables right inside the BP Workflow without manually creating Data Stores and adding columns to the Field Schemes. How to Use Global Variables Insert a `` plugin into your Bot steps with a meaningful name attribute. Global Variables usage in Machine Configs To name a global variable, use only lower case alphanumeric characters and underscores, e.g., ocr_parameters. Save the step and the BP. You will get a warning: Global variables used in Step _Name Bot step are missing N values. Click the warning message or go to Run > Advanced Options > Global Variables. Click the Edit Values button. Data Stores > Global Variables are opened in the edit mode in a new tab. Global Variables is a default configuration within Data Stores and cannot be deleted. Set values for all variables that are missing values and click Save. Return to the Global Variables tab and click the Refresh button. Click OK and Save the Business Process. If global variables are set in the Data Stores > Global Variables, you can use them in Bot steps of other processes. "},{"version":"10.0","date":"Aug-06-2019","title":"nested-business-processes","name":"Nested Business Processes","fullPath":"iac/core/user-guide/work-with-bp/nested-business-processes","content":" When creating a BP you can add (include) one BP inside another. This functionality provides better re using, testing, monitoring, and visualization of processes. BP with Sub Process Sub Process Expanded Preparation To use a BP as a Sub Process, complete the following steps: Create a Business Process (from an existing or empty Use Case). Test this BP in Sandbox environment and fix if needed. Create a Use Case from this BP. As a result, this BP Use Case will appear on the BP Designer right panel and will be available to be copied as a Sub Process. BP Use Cases on BP Designer Adding a BP Use Case as a Sub Process Let's assume that we have created an \"OCR\" business process, tested it, and created a Use Case. Our aim is to include this \"OCR\" Use Case as a Sub Process into the \"Invoice Data Extraction\" BP. OCR as a Sub Process We need to perform the following actions: In BP Designer, drag the \"OCR\" from the right panel (Processes tab) to the canvas. Connect the \"OCR\" Sub Process with other BP steps. Verify that the BP input file column names match the \"OCR\" sub process Data Elements (variables). Editing a Sub Process When you have dragged (duplicated) the \"OCR\" Sub Process into the \"Invoice Data Extraction\" BP, the following inheritance rules exist: if you edit Sub Process step (parameters, name, etc), the Use Case will be accordingly updated and vice versa. if you change the connections inside the Sub Process, the Use Case will NOT be updated and vice versa. In case you want to change an \"OCR\" Sub Process step without affecting the source Use Case: Expand the Sub Process by clicking the ( ) icon in its top left corner. Right click the target step and select the Copy action. Delete the target step. Drag the copied step inside the \"OCR\" Sub Process and connect this step to others. You can also delete other Sub Process steps or drag new steps inside the \"OCR\" if needed. Drag a Step inside Sub Process WorkFusion supports multiple levels of BP nesting. The Sub Processes do not have their own BP Options (Streaming Threshold, Due Date) and input data file. They receive data from the previous BP step and pass the result data to the next BP step. Alternatives to Sub Processes You can configure a Bot step that creates updates a specific folder on an external server registered in WF Start Task, BP from Data Source, and this event will start a new BP or Task. You can use the plugin to start another BP or Task depending on the current BP results. "},{"version":"10.0","date":"Aug-06-2019","title":"manage-custom-attributes","name":"Manage Custom Attributes","fullPath":"iac/core/user-guide/work-with-bp/manage-custom-attributes","content":" How to Set Custom Attributes Create a Field Scheme Select the Field Scheme in Advanced Settings Define Values for Custom Attributes How to Use Custom Attribute Values Next Steps Custom Attributes are intended to set parameters specific to a particular Run of Business Process or Task. Using Custom Attributes, you do not need to create a new Campaign for a Task or BP to set new parameters. For example, you can define the following Custom Attributes for two Runs of the same BP: 1st Run: node _id=\"10067\", capabilities=\"MAC\", host=\"192.168.101.100\" 2nd Run: node _id=\"10068\", capabilities=\"WINDOWS\", host=\"127.0.0.1\" Otherwise, if you need to set common values used in different processes, use Global Variables. How to Set Custom Attributes Create a Field Scheme Go to Configuration > Field Schemes. Create a new Field Scheme Enter Name and Description. Add Answers to the Field Scheme and define their Unique Codes and Answer Types. Creating a Field Scheme for Custom Attributes Click Save. Select the Field Scheme in Advanced Settings Open your Business Process > Advanced Options. Select the Field Scheme in the Custom Attributes drop down. Advanced Options Custom Attributes Click Save. Define Values for Custom Attributes In the top left corner of the Run BP tab, hover over the green Attributes labels. Click the pencil icon. Custom Attributes on Run Tab Enter values for each field in the Custom Attributes popup. Set Custom Attributes Click Save Save the Business Process. There is no ability to run Task BP when one of required Custom Attributes is not provided. A warning message is displayed. How to Use Custom Attribute Values You can use Custom Attributes in Bot steps by getting their values from item.getWrappedObject().getRun().getCustomAttrMap(): Using Custom Attributes in Machine Config Expand source =0) { attrs = attrs.substring(attrs.indexOf(\"'\") 1); values.add(attrs.substring(0, attrs.indexOf(\"'\"))); attrs = attrs.substring(attrs.indexOf(\"'\") 1); } java.util.Iterator listIterator = values.iterator(); while(listIterator.hasNext()) { attrsMap.put(listIterator.next(), listIterator.next()); } } if (sys.isVariableDefined(\"item\")) { java.util.Map customRunAttrMap = item.getWrappedObject().getRun().getCustomAttrMap(); for (Object key: customRunAttrMap.keySet()) { if (key.toString().equals(\"mvel_attributes\")) { extractMvelAttributes(customRunAttrMap.get(key), attrsMap); } else { attrsMap.put(key, customRunAttrMap.get(key)); } } } > {attrsMap.get(\"capabilities\")} Not Found Here is a result of this sample Bot Task: Custom Attribute Values in Results Next Steps Copy your BP. Set new Custom Attribute values if needed – the source BP Run will not be affected. By default, Custom Attribute values are copied from the previous Run. "},{"version":"10.0","date":"Oct-16-2019","title":"run-bp-and-view-results","name":"Run a Business Process (BP) and view results","fullPath":"iac/core/user-guide/work-with-bp/run-bp-and-view-results","content":" id: version 10.0 run bp view results title: Run a Business Process (BP) and view results original_id: run bp view results When you have uploaded the Data, completed and validated the BP Design, and set the Run options, perform the following actions: Run the BP in a Sandbox environment by clicking the Run This Process button on the Run Business Process tab. If the BP is valid and has started successfully, you will see the following screen: BP Successful Run You can view the results of the BP, or view the BP you have just created via the BP List. Log in as a Worker in WorkSpace, search the BP Tasks by name or Tags, and complete all the Worker Tasks. Depending on the Task Adjudication Rules, you should have more than one Worker account. Go back to the Control Tower and edit the BP options, Design, or Data if needed. Copy the BP with Data. You can optionally stop or delete the original BP if you do not need it. Run the BP in a Production environment by checking the appropriate checkbox above the Run This Process** button. The following sections describe how to watch the BP progress, analyze Worker answers and activity, and get the final Results Data. Workflow Diagram You can watch the BP progress and get real time statistics using the BP scheme on Design tab in View Mode and on the Run tab. BP Design View Mode Each Task has an indicator with percent and number of Records handled in this Task. Example: 67% (10 15) . Each Rule Outcome has an indicator with number of Records that passed through a specific rule outcome based on conditions defined. Example: 8 → Yes and * 2 → No* . These metrics are updated when new Records are submitted from different process steps by human workers or bots. View All BPs To view all BPs with their details, go to Business Processes > View All Business Processes. Learn more about Business Processes List Results Tab The Results tab provides a comprehensive overview of business process (BP) and its elements. It shows: Structure of BP Information about instances Statistics for every element of BP Failed elements in BP and errors that caused failure Execution results of individual tasks inside BP Workers' names, answers and feedback for individual manual tasks Appearance of some tabs may differ depending on user permissions and license. Results tab has 2 areas: Information header Tab view. Tab view may have different sections depending on selected BP element. Information Header 1. Instance Menu Instance drop down menu provides information about BP structure. Choose individual instance from the tree structure to display its Results tab. Instances are displayed in order of execution, with BP as the first option. The current instance is displayed in bold. Icon to the left of instance name denotes its type: ** business process or subprocess ** machine task manual task Instance execution failed with error. To expand the components of subprocess, select the arrow sign ( >) near its title. 2. Instance Metadata See more Instance metadata shows: Version Author and creation date Execution status Event log Automation status Business process tags Unread workers messages Your layout is defined by selected instance type and user licenses. Select to open full event log. Tab View Your layout depends on selected instance type. Only Summary is available for subprocesses, bot tasks and BP with bot tasks only. For BP with both human and bot tasks: Summary, Workers, Feedback. For human tasks: Summary, Workers , Assignments, Feedback. The Workers tab is displayed if user has* MANAGE _WORKERS* permission. Select one of the tabs to view specific information. Summary Summary tab displays up to 3 blocks: Statistics, Steps, Data Final Results. Select the header of each block to expand or collapse it. 1. Statistics Statistics is shown for any instance type. The look of the Statistics tab depends on the type of the selected instance. Sample statistics chart for bot task: Sample statistics chart for human task: Refresh button updates this view. 2. Steps The Steps block is shown for compound instances: BP or subprocesses. It displays instance structure. You can manage each instance: view statistics and full event log, generate snapshot of the process, evaluate cognitive bot etc. The actions use can select depend on user instance type and user license. Refresh button updates this view. 3. Data Final Results The Data block is displayed for human or bot step, and the Final Results block is displayed for BP, so you see either Data or Final Results depending on the type of instance you've selected. The Final Results block is shown for BP if at least one record passed through all the steps. This block shows each record in selected instance. If no record passed through BP yet, you will see the message below: Refresh button updates this view. Use Table tools to customize the look of this table. Workers The Workers tab provides an overview of all human workers for manual task (or BP if selected). Assignments The Assignments tab displays information about answers of workers related to specific task. See example Feedback The Feedback tab is available for manual tasks or BP with manual tasks. It contains all messages from workers for the selected element. See more Link to the message author page Message date Select multiple messages to delete them Message subject and text preview Unread messages have blue background. Select message to read full text, reply to or delete it . Select to go back to list view of all messages. Select to delete current message. Actions Select to update all information on the Results tab (info header, expanded blocks or table). Select to generate and download snapshots of the selected instance. Select to open list of actions. These actions are related to the overall business process (not the selected instance!). Depending on the execution status of the BP this list may include the following actions: Copy Pause Resume Stop Repair Download Original Data Delete Table Tools Table tools apply to all tables in the Results tab: Data, Final Results, Workers, Assignments. Table Settings Select at the right corner of the table to open table settings. You can select columns you want to see to customize your table. Export Select to export any table in the format of your choice. Only columns selected in Table Settings will be exported. Filter Filter through table columns using one or multiple parameters. "},{"version":"10.0","date":"Aug-06-2019","title":"plugins-execution-logging","name":"Plugins Execution Logging","fullPath":"iac/core/user-guide/work-with-bp/plugins-execution-logging","content":" Since SPA 9.1 plugins exection logging was introduced to enable logging time of execution of some plugins. This enables to gather analytics of bot tasks processing. Currently following plugins data is recorded: robotics flow selenium flow ocr automation extract automation get eval result automation start eval General recording rules A separate record is created for each plugin, for each processed HitSubmissionDataItem. Record contains: Plugin name id of HitSubmissionDataItem The total number of plugin executions in one bot config The total time of plugin execution for one data item Number of retry attempt. Table structure All data is stored in PluginExecutionLog table: Column name Filed description id Sequential number pluginname Name of plugin like it appears in bot config (e.g. robotics flow, automation extract) dataitemid id of HitSubmissionDataItem which was processed by plugin executiontime Total time of plugin execution in one bot config per one HitSubmissionDataItem (more than one plugin appearance can be in the bot config) executioncount Total number of plugin executions in one bot config per one HitSubmissionDataItem (more than one plugin appearance can be in the bot config) retryattempt Taken from AwsHit table, column processesAttemptCount is_successul Flag that indicates exception during plugin execution. False if any exception was thrown during execution, otherwise true. Each record can be connected with Tracker (used for transactions tracking) via table DataItemTrackerHitLink (via column hit _id). Technical details In order to make save of record to db internal REST service was introduced. It's path is v1 internal analytics bot execution log. Table PluginExecutionLog was also added to DataPurge procedure as the size of table is equivalent to size of HitSubmissionDataItem. Some details on possible use cases. Records which related to the same data item id. But all records show that plugin executed without exception. Reason: Plugin works correctly but bot config throws exception after plugin execution. Example of OOTB bot config (Generic) Extract Information 8.4.0 "},{"version":"10.0","date":"Oct-16-2019","title":"set-bp-run-options","name":"Set Business Process (BP) run options","fullPath":"iac/core/user-guide/work-with-bp/set-bp-run-options","content":" title: Set Business Process (BP) run options BP Operations On the Run Business Process tab, you can: view Process Details and Estimated Costs edit BP Tags save the BP download Input Data perform BP Actions call the Advanced Options run the BP in the Sandbox or Production environment To view the BP scheme in full size, double click it. BP Advanced Options The options are as follows: Global Variables – see this article. Due Date – the date until all BP Tasks should be completed. BP Tasks become unavailable after the Due Date. Forcefully complete Tasks – the feature allows you to forcefully complete individual Tasks and Business Processes, when completion criteria are reached, these are % of Worker Tasks completion and idle time. Task is completed when both conditions are met. Streaming between the steps configuration – minimum completion % required for streaming between the steps. Streaming is a mechanism for limiting number of Records available on the current BP step depending on number of Records completed on the previous BP step(s). Example Streaming = 5 Tasks. For the 2nd BP step it means: 5 Records must be completed on the 1st BP step, only after that they are routed to the 2nd step. Streaming defines only the lower limit, i.e. if 12 records are completed on the 1st step, all these records will be routed to the 2nd step at once. Try to repair failed tasks up to – set the number of Bot steps execution retries for each Record in current BP. If a Bot step fails during execution (external server not available, RPA node disconnect, etc.), it will automatically restart until successful execution or until the limit of retries is reached. Afterwards, you can repair failed tasks manually (Actions > Repair). Custom Attributes – if you want to add Custom Attributes to your BP, select the Field Scheme upon which they will be based. Copy results to DataStore – when checked, BP result Data is saved to DataStore. There are two options: add results to new DataStore for each BP. DataStore name: {mm dd YYYY hh mm ss} {BP name} append results to single DataStore for all BPs. DataStore name: {BP name} Track if transactions meet service level agreement (SLA): Enabling this feature allows to see the Service Level Agreement widgets in the Analytics Dashboards. "},{"version":"10.0","date":"Aug-06-2019","title":"run-business-process","name":"Run a Business Process","fullPath":"iac/core/user-guide/work-with-bp/run-business-process","content":" Running a business process Processing a manual task How to re run a business process Importing exporting a business process Business processes backup Running a business process To run the business process: Go to Run tab in the business process Click Run This Process. Before running the process, you can set the following Advanced options for the business process: Set Threshold – minimum completed tasks (or % of processes records) required to go to the next step in the process. Select whether to save the results of the process to a Datastore. Set the number of times Control Tower will attempt to repair the process in case an exception occurs. Note If you have more than 1 bot working at the time of the BP execution, the BP will run simultaneously on the maximum number of available bots required for it. You can limit the number of bots each task can use with the help of Bot Sources. If you have multiple records in the Input data, the data records will be distributed among available bots. Results page The Results page contains all information about the process with key parameters and links: Summary with the general information about the business process (status, event log, etc.) Statistics The list of all steps in the process and their statuses and event log for each step Output data from completed steps On the steps tab, you can see if you need to process any manual tasks. The final results tab contains the output data that can be exported in a .csv or .xlsx file. Additionally, you can see the output data from each step in the business process by clicking on the step’s name in Steps tab. Event log Results page contains event logs for the whole business process and each of its steps. To view the log, click the icon In there are errors in the event log, a user can see the whole error log by clicking the arrow on the blue field. The Hit ID is displayed in a separate column for quick reference. In case the business process failed due to reasons not related to its configuration (external server not available, RPA node disconnect, etc.), you can try to repair it Actions > Repair. Processing a manual task Open the Steps tab on the Results page to see if you have any pending manual tasks in the business process. To process the manual task: Click on the name of the task. You will be redirected to the task's page. Open the Statistics tab Click Workspace link to open the task. Log in using your credentials. Accept the task. The page with the manual task will open. Provide the Answer for the task and click Submit. The data from the Answer will be sent to the next step in the business process, if there are any. Update the process's Results page the status of the manual task and the business process will be updated. How to re run a business process Attention If you want to run your Business Process once again, you need to copy it. For more details refer to the video and the description below. In order to run a business process again you need to: Copy the business process instance Select Include the input data to run it with the original input data. Run the Business Process or create a Schedule to start the execution at the specified time. Importing exporting a business process There is an option to import export business processes to from Control Tower. To do this, you need to create a business process package: Go to the business process page, open Packages tab, and click New Package button. Add the title, description (if required), and click Save. To save the package to your machine, click Download. To import a created package to Control Tower: go to Business processes, click Import package * * click Add and select the archive with the business process package. You can preview the business process package before importing it. If a business process with such name already exists on Control Tower, the system will suggest two options: Create a new version of the process and leave the existing one unchanged (default) Overwrite the existing business process. After selecting the required option, click Import Package. The new business process will appear on the Business processes page. Business processes backup To create a backup of all your business processes, click Backup all button in the User's drop down menu: It will export all your processes and tasks in the business process package format. It means you can copy and import your processes to another machine, where RPA Express is installed. Backup folder is configured during RPA Express installation. The default location is: c: Users %YOURUSERNAME% ControlTower Backup Note The backup service will clean up the backup folder before creating the backup of all business processes. It helps saving only the latest versions of your processes, as by default during the first start of Control Tower all packages from the backup folder are imported. "},{"version":"10.0","date":"Oct-16-2019","title":"transactions-tracking","name":"Transactions Tracking","fullPath":"iac/core/user-guide/work-with-bp/transactions-tracking","content":" This article describes how to enable some features of Process Analytics and start tracking Service Level Agreement and Straight Through Processing; metrics which are an essential part of the Intelligent Automation Cloud Analytics. Reading this article is essential before users start using Analytics. The usage of this features is also advised after reading article about how to Work with Business Process BP. The Transactions Tracking functionality is available from SPA 9.1 version and allows in depth Analytics of the Business Processes. This feature was formerly known as Dashboards of Control Tower. New Analytics Dashboards can be found in Control Tower side bar menu: Definitions Transaction a business entity like a document, an email or an invoice. Transactions Tracking is a functionality that allows to identify business entities within a process. It's enabled on a specific step in a process and becomes a part of a process design. Available from 9.1. Service Level Agreement (SLA) Metric is a metric which shows how the transactions processed in a process within set limits. Service Level Agreement (SLA) Feature is a functionality to track the SLA metric available from SPA 9.1. An SLA contract can be set for the count of processed transactions per day or the time needed to process a single transaction. SLA is available only if transaction tracking is activated. Straight Through Processing (STP) STP shows how transactions move through business processes and if they were processed automatically (i.e. Straight Through), or with involvement of human workers. It's used to calculate the automation rate. Available from 9.1 STP shows the ratio of Transactions that fully processed by Bots (each of Business Process is processed by Bot) to the total amount of Transactions (Transactions that processed by both Bot and Human). The higher STP percentage is, the better Business Processes are automated. Difference between records and transactions In the example below the processes are the same. With tracking In case Transaction tracking is activated: transactions as business entities are tracked despite of any split join steps, so processing time from end to end is calculated properly and SLA by time can be applied. only transactions as business entities are counted to measure volume of processed work so SLA by volume can be applied. Without tracking In case Transaction tracking is not activated, then just records are counted and it doesn't correspond with business entities and reflect more technical information than business related. What can be tracked SLA settings can be applied only to a Process Execution. Transactions Tracking is applied at Business Process with the ability to track and exclude from tracking steps. How to use Enable Transactions Tracking To enable Transaction Tracking for STP calculation and SLA metrics for the Business Process, user shall follow the steps in Control Tower: Go to Control Tower Choose required Business Process in draft state and open Workflow tab Right click on step and select Transaction Tracking > Start tracking transactions sub menu. The Step will be marked Transactions: Tracking started After transactions tracking is enabled, every input record to this step will be counted as an independent transaction and it's movement through a process will be saved to database. If tracked record steps are split, all its child steps will be tracked as parts of parent record as described above. The transactions may not appear in the first step of the process as a input data might contain a link to a storage with documents to process, so only on the next step a system can retrieve documents to be processed and here's transaction tracking should be started. we recommend avoiding such designs of BP unless one is certain that this behavior is needed To mark the document as a transaction that should be tracked, you should start tracking from the step where every record input corresponds to the single document. Multiple steps with \"Start tracking transactions\". Note! It is possible to enable tracking on multiple steps. It will not affect records that already tracked, but will start to track any new independent records, that will pass through this step. Constraints Transaction Tracking and Stateless Execution Transactions tracking doesn't work the Stateless Execution feature is enabled! When the Stateless Execution feature is enabled, the Control Tower does not collect intermediate data into the database, and transaction info will not available in the analytics dashboard. Read more about Stateless Execution of Bot Tasks. Behavior on split rules When a tracked transaction passes split rule, all records from rule outcomes will be tracked as part of the original record. There is slightly different behavior for different split rules when two records, that were created from one parent, meet in one of the next steps. If records were produced by split rule and were joined by join rule, then only one record for business entity will be created in log. In case of 'split _data' rule, after two parts meet in some step, log record will be created for each part of the original record. More about Split Join Rule Usage. Tracking sub processes Tracking on sub processes works as it was part of original process: activate tracking on any step inside sub process and transactions will be tracked until the end of parent process, or you can activate it before sub BP and transactions will be tracked inside sub BP as usual. To activate tracking for incoming records you should expand business process and activate tracking on the first step. How to Exclude step from Straight Through Processing (STP) You can exclude some steps on a business process from STP rate calculation. Exclusion of step will not affect how the entity is logged to database every step will still be recorded. That option will only affect the statistics calculation excluded steps will be not counted in STP related metrics. To exclude a step from the STP: Go to required Business Process in draft state > Workflow tab within Control Tower Right click on the step Select Transaction Tracking >Exclude Step from STP from sub menu. Sample process Below process has following steps: Bot step Check input data provided for the process with enabled Transactions Tracking Manul step to add information which is Excluded from STP Manul step to add information which is Excluded from STP Bot step Add data to the table with no Transactions Tracking enabled. This BP Package process and it's data is attached at the bottom of the page. Service Level Agreement Service Level Agreement is used to define whether the transactions processing meets the set time thresholds for completion. SLA available only if Transaction Tracking is enabled otherwise all transactions are calculated based on HitSubmissionDataItem.originalItemId and are considered to meet the SLA by default. 2 New SLA types added: Transaction Cycle time defines that transaction processing time. Cycle time of a transaction is defined as max(AwsHit.completionDate) min(AwsHit.creationTime) with all joins used for defining transactions (ink). If a transaction cycle time is = SLA value then all transactions meet SLA else all of transactions below SLA value are marked as not meeting the SLA. How to get SLA info for each BP execution (example) SELECT r.rootRunUUID , r.sla_type , r.sla_value FROM Run r where r.sla_type is not null and r.status'DRAFT' and r.status'DELETED' How to enable SLA tracking Prerequisite: SLA can be only configured for processes with enabled Transactions Tracking only. SLA can be enabled and configured for Process execution. Go to required Business Process in Control Tower Open Run Tab > Advanced Options popup. Choose from two options for SLA agreement: Cycle time or Number of transactions. Cycle time agreement will verify if every separate transaction is processed per defined amount of seconds. Number of transactions agreement means that all transactions that were processed per day will meet SLA if certain amount of transactions was processed by this process execution. Scheduling Business Process with SLA SLA can be applied to the process executions that are launched by scheduler. How to schedule Business Process with SLA enabled Go to Control Tower Advanced > Schedules menu item Create new Schedule Choose required business process with activated Transaction Tracking Select SLA checkbox Other settings are the same as in Advanced Options. Note: this SLA will apply to all process executions launched by scheduler, there is no way to edit scheduler, only to replace it with a new one. Constraints Exporting a BP Package During the export of the BP package with the configured SLA, these settings will not be exported and needs to be reset after importing. Copying a Process Execution: After copying a Process Execution, the SLA settings need to be configured again as it is not saved in the Business Process package. It will not affect Process Executions that are already running. Migrations of Business Process Transaction Tracking is available since SPA 9.1 version and is not portable to older versions of SPA. Note! Transactions tracking is not compatible with versions prior to 9.1 Transactions tracking becomes a part of the BP Package so can be migrated between environments (e. g. from Dev to UAT). SLA settings are not saved during creation of the BP Package. Performance No any evidence of the performance degradation was found during testing with enabled Transaction Tracking. "},{"version":"10.0","date":"Oct-16-2019","title":"work-with-business-process","name":"Work with Business Process (BP)","fullPath":"iac/core/user-guide/work-with-bp/work-with-business-process","content":" title: Work with Business Process (BP) Business Process (BP) is a combination of connected Manual Tasks, Bot Tasks, and or other BPs handled by a user defined flow. The BP Designer allows you to control the flow of the Manual and Bot Tasks. A BP has one input Data file which is enriched and or modified by each Task results. Tasks in BP are executed successively or in parallel depending on the Composite Rules and their Conditions. A BP Task can utilize data generated by previously completed Tasks or data from the input file. Prior to reading this material, see the Create a Manual Task and Copy a Task topics. When creating and editing a BP, some steps are similar to a regular Task creation. Watch the videos: Simple Business Process Business Process with Bot Steps BP Flow Create Tasks and test them separately. You can create Use Case from Manual and Bot Tasks to re use them in your BP. Create a BP and start designing and configuring it. Prepare an input Data file, add the previously created Tasks to your BP and connect them. Run the BP in a Sandbox environment. You can log in as a Worker, find and complete the Manual Tasks of the BP. Check the results and edit the BP and or Task parameters if needed. Copy the BP with Data. Run the copied BP in a Production environment and monitor its execution. You can optionally pause resume or stop the BP execution. When the BP is no longer needed you can delete it. If your BP is tested and you want to re use it (create a copy or include in another BP as a sub process), create a Use Case from the BP. Data Flow A BP should have one input Data file which is enriched and or modified by each Task results. You can view and export data for each Task in the BP, this data contains input columns and columns added or modified by this Task. A BP Task can utilize data generated by previously completed Tasks or data from the input file. Each completed BP has a file with Final Results which combines input Data file and all the Tasks results. To see the intermediate BP results, you can generate a BP Snapshot. See the following topics: Create a new Business Process Design Business Process Set Business Process run options Run Business Process and view results Business Processes list Copy a Business Process View Business Process event logs Transactions tracking Collaborative Business Process editing "},{"version":"10.0","date":"Aug-06-2019","title":"view-bp-event-logs","name":"View BP Event Logs","fullPath":"iac/core/user-guide/work-with-bp/view-bp-event-logs","content":" WorkFusion provides a capability to view Task and BP logs containing system events, error messages, and stack traces. If a BP has some processing issues, the following icon is displayed in the BP list: When the processing issues icon is displayed, it means: at least one record in a process has exhausted the maximal number of retries and failed; other records continue execution on different process steps until: their maximal number of retries is exhausted OR they reach the final step the business process with processing issues is not stopped automatically. You need to do one of the following actions: fix the process steps code, save, and click Actions > Repair. OR stop the current process instance by using Actions > Stop. Afterwards, you can copy this process instance, fix code errors, and run it with the same or new data. To view the detailed information, go to BP > View Results > Summary. From the Choose Task dropdown, select a BP step or Sub Process. Click the (info)\"){.emoticon .emoticon information} Show Run's events icon. The Events popup will be displayed. You can click the* Export To Excel* button to read, filter, and analyze logs using all power of the Excel functionality. The Hit ID is displayed in a separate column for quick reference. You can expand descriptions to view the whole stack trace by clicking the arrow icon in the Description column. If some Bot steps have errors that are not connected with their configuration (external server not available, RPA node disconnect, etc.), re run them by clicking Actions > Repair: "},{"version":"10.0","date":"Aug-06-2019","title":"answers","name":"Answers","fullPath":"iac/core/user-guide/work-with-tasks/answers","content":" Managing Answers To add an Answer, complete the following actions: On the Design Task tab, click inside the Answer section. On the Side Panel, you will see Answers list. Click the Add Answer button. The Add Answer window will pop up as shown below. Fill the Answer fields and click the Save button. Add Answer Popup When the Answer is saved, you can preview it and edit if needed. Answer Input Fields Code * This field provides a reference point to be used by associated Adjudication Rules. It is also used as the header for the Output Data file generated at the completion of the campaign. Enter a unique text string (only letters, numbers, and underscores). Answer * This field is a label above the Answer field. Workers see it as a definition of an Answer. Answer Type * A drop down menu for selecting UI data input control. See the Answer Types topic for detailed information on the design and logic of each control. Options * Options for multi variant answers in key=value format. (Option _Name1=Value1). You can use dynamic expressions in options. To enter a new option, click Enter or TAB. You can also reorder the option items by dragging them, or delete them. Options can also contain different information depending on the Answer Type: for Long Text – number of rows for Masked Text – ability to define the mask for answer for Map – enable disable street view, etc. Shuffle Options Tick this checkbox to shuffle the order in which the options are presented to Workers. Input form Data Store Tick this checkbox to import Answer options from a Data Store. Description This field allows you to describe in detail what's required by your Worker and is displayed underneath the Answer field. It supports Dynamic HTML and HTML snippets. Required Setting this option engages automatic validation checks ensuring that the Worker can not submit the task without providing the Answer. Allow n a Select this checkbox to add an additional alternative answer option like \"I do not know\". CSS Rules To customize the Answer look and feel, enter a name for your custom CSS class. After saving the Answer, switch to the Code Editor and add your CSS code for this class in the section. For Sub Answers, this field should contain expand:on(Parent Answer Option _Value). See the CSS Rules for Answers and Sub Answers topic containing CSS Rules examples. Default Value This value is inserted by default and can be modified by Workers. Default value can be taken from Input Data file by using the following expression format: {question.data.column _name} Do Not Use in Adjudication (SA) Tick this checkbox to omit the Answer from being considered when calculating adjudication (majority determination). This option is useful when the answer is being used for comments or similar free form type answers. You can add multiple answers or even no Answers at all, if your Task does not require it. To modify the order of Answers, drag them up and down. To delete an Answer, click the close icon (x) to the right of the respective element. Dynamic Expressions in Answer Options Select One, Select Multi, Check One, and Check Multi answer types support dynamic expressions in options of the following type: {question.data.A}= {question.data.B} {question.data.A}=1; 2= {question.data.A} PREFIX {question.data.A}SUFFIX {question.data.B}= {question.data.C} PREFIX, SUFFIX any strings In the example above, A and B are the column names from input Data file. On Preview and Endpoints these expressions are substituted by values from corresponding columns. Dynamic expressions are useful in case these answers should have multiple values – no need to enter them manually. For example: Input File Input File or Data Store What Worker sees What Worker selects Result Title: Yahoo, Facebook; URL: http: yahoo.com , http: google.com Yahoo, Facebook Yahoo http: yahoo.com To use these columns in Answers, add the following dynamic expressions to the Options list: Dynamic Expressions in Answer Options The following figure displays dynamic expressions in Design Task and Preview: Preview with Dynamic Expressions Sub Answers Answers with multiple options can have Sub Answers. Sub Answer is displayed to a Worker only when a particular option from an Answer was selected. Example: Question: \"Count the objects and select their amount from the list\" Parent Answer options: 1=1 2=2 3=3 4=4 Other=5 Sub Answer: If 5 was selected from Main Question, then display a text input (Sub Answer). To add a Sub Answer to an existing Answer, perform the following steps: On the Design Task tab, click inside the Answer section. On the Side Panel, you will see the Answers list. Move the mouse over the Answer's corresponding row. Click the Plus icon \" \" as shown below. The Answer input fields are displayed. a . Expand the Show Advanced Options section. b . Insert the following string into the CSS Rule field: expand:on(Parent Answer Option _Value) Parent Answer Option _Value – any option of the Main Answer Example: The parent Answer has the Check One type with the following Options: black=1 white=2 other=3 To display the Sub Answer when the third option (other) is selected in the parent Answer, insert the following string into the CSS Rule field of the Sub Answer: expand:on(3) Sub Answer Display Click Save when all pertinent fields are completed. Each Answer field added will be displayed in a table format under the parent answer node. You can edit, reorder, and delete Sub Answers. There can be multiple levels of Sub Answers. Validation Rules Format Validation in Answers is active by default, and user can not enter incorrect values. For example, email, date, number, etc. When the validate parameter is set to false, user can enter any data in any field. To disable validation, add the following parameter in the Code Editor: Save the Task to apply the changes. "},{"version":"10.0","date":"Aug-06-2019","title":"copy-task","name":"Copy a Task","fullPath":"iac/core/user-guide/work-with-tasks/copy-task","content":" In WorkFusion platform, you can save time and effort by re using previously created Tasks and Business Processes. Tasks with the same name have a common template (Definition) – if you modify the design or parameters of one Task and save it, all other Tasks with the same name will be affected. Tags are unique for each Task, hence you can use different Tags to distinguish Tasks among the Tasks with the same name (of the same Definition). You can copy a Task by selecting Copy in the Actions dropdown. Common Rules for Copying If you have copied a Task with the Create an independent task option selected – a new Task instance is created in WorkFusion, and it will not be linked to its source Task. Copy Task To run the same Task with new Data, select the Copy Task action. As a result, a new Task instance with the same name as original is created. The Data file is not copied – you will need to upload your Data. All changes to this new Task will affect Tasks with the same name. If you want to \"unlink\" this Task, change its name on the Task Design tab and save the Task. Include the Input Data To re run the same Task with the same Data, select the Include Task the input data option. This action is helpful if some issues occurred. As a result, a new Task instance with the same name and Data as original is created. All changes to this new Task will affect Tasks with the same name. If you want to \"unlink\" this Task, change its name on the Task Design tab and save the Task. Create an Independent Task Select the Create an independent task option to: avoid affecting all Task instances with the same name create a new Task instance based on an existing one You will be prompted to enter a new Task name in the Definition name field. This Task will not be linked to its original. Task Export Import See the detailed description: Migrate Task, BP, Use Case to Another CT Instance. "},{"version":"10.0","date":"Aug-06-2019","title":"adjudication","name":"Adjudication","fullPath":"iac/core/user-guide/work-with-tasks/adjudication","content":" Adjudication is a process that provides the logic on how correct answers are derived, what determines the accuracy of the work, and whether it should be accepted once completed. Adjudication rules use a majority rule algorithm that you can set to determine how many Workers you want to have to agree in order to reach a consensus. Adjudication in the WorkFusion platform is a process that ensures the quality of work performed by crowd Workers. There are several such Rules which are created by default on your instance of the platform. In addition, you can also create your own rules. Every Task that you create has an Adjudication Rule associated with it. You can view or edit the Rule associated with your Task in the Advanced Options of the Run Task tab. Field Description Min of Assignments The minimum number of Workers required to complete each Worker Task. The value set determines the Minimum number of workers REQUIRED to complete a record before it is considered completed. This means you will NOT see results from this record in your snapshot until the minimum number of assignments is met. Example: if you want 80 different Workers to complete the same Worker Task (e.g. questionnaire), create a Task with one Record as input data and set the \"Min of Assignments\" = 80. Max of Assignments If the Task reaches this number of Workers, no new Worker Tasks will be issued. Adjudication Rule Select the Adjudication Rule with the logic schema that best suits your Task. You can select one of the included Adjudication Rules from the drop down menu or create your own one. Price Strategy This strategy is intended to update Task price automatically (for example, every day). It is based on specific rules with Price type. See the Price Rules topic. Enable Manual Review If you wish to qualify Workers without Gold Data or other Workers, review their work manually. To understand the Adjudication Rules concept, see the following description of two Rules loaded by default: Start with 1 assignment 2 1 rule (3 assignments max, pay all) This rule means that we need 2 identical answers to reach the majority. If their answers are different from each other, then utilize a third Worker to establish a majority based answer. Once that majority is reached, pay all of them. If there is still no match, then since the Max of Assignments field was set to 3, no further Worker Tasks are posted. The output file (snapshot) will indicate the answers provided by the 3 workers and that Confidence is \"1 out of 3\". Start with 1 assignment 3 2 rule (5 assignments max, pay all) This rule means that three Workers are set to work on this Worker Task. If their answers are different from each other, then utilize an additional two Workers to establish a majority based answer. Once that majority is reached, pay all of them. Edit Rule To set Rule parameters: Click the Edit Rule link to the right of the Adjudication Rule dropdown. Depending on the Adjudication Rule selected, there will be a specific set of Rule Variable inputs (i.e. Check every i th gold question, Majority value, Worst accuracy level upper limit). Select the Adjudication Strategy. Set the Rule Variables. Click the Save button. The Rule you are saving might be used in other Tasks. You can also edit the Rule code by clicking the Switch to Advanced View link. To get detailed info, see the Rules and Rule Templates topic. You can optionally edit the Price Strategy the same way as the Adjudication Rule. "},{"version":"10.0","date":"Aug-06-2019","title":"availability","name":"Availability","fullPath":"iac/core/user-guide/work-with-tasks/availability","content":" On the Availability tab, you can restrict access to the Task depending on number of already completed Tasks, current time, country, and geolocation of Workers. Field Description Limit Number of Tasks The maximum number of Tasks a Worker can complete. Set to 0 for unlimited number of Tasks. You can set the exact number of Tasks or Percent of the total Endpoint Task amount. The limit number can be applied to: Run. All Worker Tasks of the current Task are affected. Campaign. All Tasks with the same name are affected. Use Case. All Tasks based on the Use Case of the current Task are affected. Task Availability The time during which the Task will be available to Workers on target Endpoints. If you want to show the Task during business hours: 8 17 Sun,Mon,Tue,Wed,Thu,Fri,Sat By default, time is set in the UTC Timezone. You can specify multiple time frames and your Timezone: 8 17 Mon,Tue,Wed,Thu,Fri UTC 4; 16 20 Sat,Sun UTC 7 Task Availability Template This message is displayed to Workers when they try to view or accept the unavailable Task. Allowed Countries Text input field with country look up. Start typing the country name and select a suitable item from autocomplete suggestion. You can add several countries or delete data if needed. Enabling Geolocation Enabling this option will cause Worker to see only Tasks that are geographically close to his her current location. Workers will be asked for their location by browser. Field Description Use common coordinates for all tasks When this option is selected, input the Latitude and Longitude values that will be used for all Task Records. Use column to define per task coordinates When this option is selected, map the Latitude and Longitude fields to the corresponding columns of the Input Data file. In this case, each Task Record will have unique coordinates. Task Submission Area Select one of the following options: Exact Location (Within 30 Meters). Custom Area. Enter the Area Radius in meters. "},{"version":"10.0","date":"Oct-04-2019","title":"create-moderation-task","name":"Create a Moderation Task","fullPath":"iac/core/user-guide/work-with-tasks/create-moderation-task","content":" A Moderation Task is intended for reviewing results of completed Tasks or Processes by accepting rejecting and optionally commenting each Record. Moderation Task Example All these actions can be done by downloading a Snapshot and manually editing each Record, but creating a Moderation Task has the following benefits: You can choose which Records need a review: how many Records and with what Confidence. You can assign a Moderation Task to a special group of reviewers – Moderation Workforce. All actions are done in WorkFusion platform – there is no need to download, edit, and upload Snapshots. Moderation Tasks can be a part of Moderation Flow. Start Creating a Moderation Task You can create a Moderation Task: from View All Tasks Actions Create a Moderation Task or from View All Business Processes or from BP > Design BP > right click any BP step. You need to switch to the View Mode. BP Diagram Create a Moderation Task Applying Moderation Settings After you have selected to create a Moderation Task, a Moderate Result popup appears: Moderation for Manual Task Moderation for Bot Task You need to set the following parameters: Sample Size (All, Number of Records, Percentage of Records) – how many Records are taken as an input from the original Task results. Confidence (All, High, Low) – only records with the Confidence specified will be included into the Moderation Task input Editable – if selected, all original Task output data will be available for editing to Moderators. Example A Worker provided a Company Website answer: \"https: workfusion.com\" If the Editable option was selected, a Moderator can edit the Worker's answer to \"https: workfusion.com\" and choose the Edit & Accept Moderation Decision. When all Moderation Settings are done, click the Apply button. Editing a Moderation Task After applying Moderation Settings, a new Task is created with the \"Moderate:\" prefix in the title. This task design and settings are copied from the original Task, with the following modifications: Input Data is taken from original Task output Data filtered according to Sample Size and Confidence set on the previous step. Task Design: 2 new answers are added after Worker answers: Task Moderation Decision and Task Moderation Comments. Confidence is displayed for each Worker answer. Moderation Task Preview Workforce. If workforce with \"Moderate Workforce\" name exists on your WorkFusion instance, this Workforce will be selected for Moderation Task. Otherwise, Workforce will be copied from the original Task. Adjudication: Rule = \"Moderation 1 0\"; Min and Max № Assignments = 1 Running the Moderation Task When all Task options are set and the Task Preview looks fine, run the Moderation Task as a regular Task and check the results when users from Moderation Workforce start submitting. Using Moderation for Information Extraction Tasks After creating a Moderation task out of Information Extraction (IE) one, you need to perform the following steps to display Worker answers correctly: Find out the column name with text tagged by Worker – it has the following name pattern: In the example below, the IE unique code is \"invoice\", therefore the column name is \"invoice _tagged\": In the Moderation task, open the IE answer: In Advanced Options > Default Value, add the column name obtained on the STEP 1 using the following pattern: For current example, you need to add {question.data.invoice _tagged} "},{"version":"10.0","date":"Aug-06-2019","title":"create-new-task","name":"Create a New Task","fullPath":"iac/core/user-guide/work-with-tasks/create-new-task","content":" Create a New Task There are 2 ways to create a new Task: Click Manual Tasks > Create Manual Task or click the Create Manual Task button from the View All Manual Tasks menu. See the Selecting a Use Case topic for further instructions. Select a Use Case Use Cases serve as the foundation for both Tasks and Bot Configurations. They describe the workflow of a particular Business Process and thus serve as a template to create either Manual or Bot Tasks based on that workflow. For Manual Tasks, Use Cases simplify task creation by having predefined questions, answers, instructions, and sample input file and gold files. For Bot Tasks, Use Cases simplify development by providing a baseline of the configuration code. To select a Use Case, complete the following steps: Select one of the categories and you will see all the Use Cases available in that category. To create a Task from scratch, select Other > Blank. Select a specific Use Case and click the Select This Use Case button in the bottom right corner. Use Cases have a preview image on the right side, if available. :::warning You will not be able to change the Use Case of the Task on the next steps. To do so, you should create a new Task (the current Task will not be saved). ::: "},{"version":"10.0","date":"Aug-06-2019","title":"create-private-qualification-task","name":"Create a Private Qualification Task","fullPath":"iac/core/user-guide/work-with-tasks/create-private-qualification-task","content":" This run book describes how to setup a qualification process where initial access to qualification tasks is limited to a group of named workers. This approach is typically useful when there are data access restrictions within qualification process. Glossary Trusted workers group of worker that should have access to the qualificaiotn task Steps Create team for Trusted Workers in WorkSpace by combining them into a team. See Managing Teams for how to and details. NOTE: Team should be created by the same requester as which has been used to setup WorkSpace license in Control Tower. Create Qualification Task in Control Tower. See how to Create a Qualification Task for details. Add team qualification from 1 above to the Qualification Task created above. Check \"Require All Qualifications to Preview the Task\" checkbox. Please refer to the image below: The setup will result in workers having no ability to view the qualification task unless they belong to the team. "},{"version":"10.0","date":"Aug-06-2019","title":"create-qualification-task","name":"Create a Qualification Task","fullPath":"iac/core/user-guide/work-with-tasks/create-qualification-task","content":" Qualification Task objective is to get a set of qualified Workers that will be permitted to accept and submit Tasks. Prior to reading this material, see the Create and Assign Qualifications to Workers and Create a Manual Task topics. When creating a Qualification Task, all the steps are similar to a regular Task creation, except the following aspects. Create a Qualification Task You can start creating a Qualification Task: Like an regular Task, see details. OR Task Actions > Create Qualification Task When using this option, you will not be able to change the Task type to non Qualification afterwards. Upload Gold Data Gold Data is a file containing correct Answers for all the Questions that you intend to qualify the Workers on. A Gold Data file has an additional column with a \"gold _\" header prefix for each Answer in the Task. For example, if your manual task contains Answer Unique Codes \"invoice number\" and \"vendor name\", Gold Data should have additional columns with \"gold invoice number\" and \"gold vendor name\" headers. These columns should have the correct Answers. If necessary, Gold Data should have messages containing explanations for Workers to train them on the Task. The message column for each Answer should have headers with a \"gold \" prefix and a \" message\" postfix. For example,\"gold vendor name\" and \"gold vendor name _message\". To download gold data click on Upload Gold data in human task Gold data for the manual task illustrated above should have the following structure: Gold Data should be 100% accurate and tested before sending to a Crowd. Gold Data should have a mix of Easy, Moderate, and Difficult examples to ensure a Worker will be exposed to a variety of scenarios, without the Qualification being too difficult or too easy to earn. After uploading the Gold Data file, you will see all Records in the grid. For a Qualification Task, you should upload only a Gold Data file, no initial Data file is needed. Multivalue Answer Separators in Gold File Each multivalue Answer Type has it's own separator for Gold file. It means that WorkFusion user should provide correct separator in Gold File. You can add AND OR conditions for Gold multivalue Answers. && – AND; – OR Examples: If Worker should select all the answers > 1&&2&&3 If Worker should select one of the answers > 1 2 3 It is possible to combine AND OR values: If Worker should answer 1 2 or 3 4 > 1&&2 3&&4 Task Design Qualification Task Design is similar to a regular Task Design. Useful tips: Add the \"Qualification\" or \"Test\" to the Task Title and Tags. This will help Workers find Qualifications. You can optionally add Qualification Instructions. Preview the Task to make sure that all data is correctly displayed. Task Options Set the Run Task Parameters, as for a regular Task. Open Advanced Options > Qualification tab. Add an Accuracy Based Qualification. Tick the Qualification Task check box. Select the Qualification Rule. In most cases, you can use the \"Generic Qualification Rule; Pay All\". Click the Edit Rule link. All the Rule Variables will be displayed. Edit Qualification Rule topic. (optional) select Adjudication Strategy or edit the Rule in Advanced View. Set the Max Assignments parameter – maximum number of unique Workers who can complete the same Task. It is useful to recycle the same Tasks when there is not enough Gold Records. Click the Save button. Note that this Rule might be used in other Tasks. Set the Number of Qualification Tasks – number of Tasks Worker needs to complete to pass the Qualification test. (optional) Enable Training Mode. This option is used to train Workers by providing explanation messages after they submit Task. Each column with Gold Answers should have a corresponding column with Gold Message. The message column header should have a \"gold \" prefix and a \" message\" postfix. (optional) Maintain Constant Number of Qualified Workers. If you need a specific number of Workers to get the Qualification, tick this option and set the number. The Task will be paused when this number is reached. Run Task To run a Qualification Task, perform the following actions: Run the Task in a Sandbox environment. View and analyze the results. Copy the Qualification Task with Data. Edit Gold Data and Task Options if needed. Run the Task in a Production environment. "},{"version":"10.0","date":"Aug-06-2019","title":"create-manual-task","name":"Create a Manual Task","fullPath":"iac/core/user-guide/work-with-tasks/create-manual-task","content":" Task – unit of work assigned to Workers, who can be public Workers or private internal Workers (contractors, SMEs, employees). The main goal of a Task is to take initial raw data (csv file) and to structure, enrich, or check this data according to some rules. Manual Task Task completed by a human, either by a cloud Worker or an internal employee. Bot Task Task completed by some type of computer automation (data parsing, extraction, transformation, loading). Qualification Task a test Task intended for screening your Workers and filtering out Workers who are unqualified to take your specific Task. The Tasks area of the Control Tower allows you to create new Tasks, manage existing Tasks, monitor the progress of active Tasks, and view the results of completed Tasks. Basic Task Workflow Create a Task and start designing and configuring it. To prevent losing your data, occasionally save the Task (Task Status = Draft). Run the Task in a Sandbox environment (Task Status = Processing). After you have posted the Task, log in as a Worker (Workforce is set in the Task Run parameters) and find and complete the created Task. See the WorkSpace Worker manual{.unresolved}. Check the results and edit the Task parameters if needed. Copy the Task with Data. Run the copied Task in a Production environment and monitor its execution. You can optionally pause resume or stop the Task execution. When the Task is no longer needed you can delete it. See the Task Actions table and statechart. Task Data Flow The main idea of a Task is to: Prepare the initial Data file. Enrich the data or transform it according to some rules during the Task execution. These actions are done by human Workers or Bot Configs. (optional) Get intermediate results by generating Snapshots. Get the final Result Data file. This file can be used as an input file for another Task in a Business Process. "},{"version":"10.0","date":"Aug-06-2019","title":"create-qualification-task-with-group-of-answers-type-task","name":"Create a Qualification Task with Group of Answers Type Task","fullPath":"iac/core/user-guide/work-with-tasks/create-qualification-task-with-group-of-answers-type-task","content":" The topic describes a procedure to create a Qualification task with the Group of answers type task. Step by step Guide Create Manual Task with the required fields including the group of answer type fields. For more information refer to How to create Manual Task{.toc link} Complete the Manual Task in Workspace to create Gold Data. Generate a snapshot from the completed Manual Task Create Gold Data from the generated snapshot. Gold Data is a file containing correct Answers for Questions that you intend to qualify the Workers on. A Gold Data file has columns with the \"gold _\" header prefix for each Answer in the Task. For example, if your Answer Unique Code for group of answer is \"order\", Gold Data should have an additional column with the \"gold _order\" header. This column should have the correct Answers from generated snapshot with the following pattern where: order – is a Unique Code for group of answer; qt, total, price – unique codes for answers in the group; 70, 70.00, 1.00 – Gold Data for each answer. If necessary, Gold Data should have messages containing explanations for Workers to train them on the Task. The message column for each Answer should have headers with the \"gold _ \" prefix and the \" _message\" postfix. For example, gold order message Create a new Manual task, click Upload Gold Data to upload the data. Select the Qualification task check box. You can also select a qualification rule, a number of documents for qualification etc. Useful Links on How to Setup Qualification Task Please refer to Intelligent Automation Cloud documentation to get additional information about Tasks: Qualifications Create a Qualification Task View Qualification Task Results Create Tasks with Ongoing Qualifications "},{"version":"10.0","date":"Aug-06-2019","title":"design-task","name":"Design a Task","fullPath":"iac/core/user-guide/work-with-tasks/design-task","content":" The Design Task tab provides the means to edit the Task structure, description, look and feel, and answers. The Main Panel (left side of the tab) is composed of a WYSIWYG editor, and the Side Panel (right panel) is reserved for adding and configuring relevant Task Section. Task Data and Design Task Design Schema The following image displays the sections of the Task Designer. Design Task Schema How to edit the Task Design: Place cursor into the target Section by clicking inside it (Title, Instructions, Question, or Answer). Perform an action (from Main Panel or Side Panel) see the list of available actions in the table below. To check the updated Task Design, click the Preview button. (optional) Save the Task. Section Main Panel Actions Side Panel Actions Common per Record* 1. Title type Rich Text help text (no actions) common 2. Instructions type Rich Text show, hide Instructions show, hide Tests add edit, delete, reorder Instructions add edit, delete, reorder Tests add edit, delete, reorder FAQs common 3. Question type Rich Text type Data Elements {question.data 'element name' } delete Data Elements insert Data Elements Text – common Data Elements – per Record 4. Answer (no actions) add edit, delete, reorder Answers add edit, delete, reorder Sub Answers Answer Types – common Answers – per Record * The last column indicates whether the section content is common for all Records in the Task or the section content is unique per Record. Try not to copy and paste formatted text directly from MS Word or other advanced text editors, because these insertions can significantly increase the Task size and slow down its loading. When you want to copy and paste text, do it from simple text editors (NotePad , Sublime Text) and format it in WorkFusion. The Task Design Schema can significantly vary depending on the Use Case selected. Possible variations: section order changed some sections missing new sections created styles changed To modify styles and sections, see the Manual Tasks topic in Professional User Manual. WYSIWYG Editor You can use the WYSIWYG editor (TyniMCE) to format text, insert links, images, tables, lists into Instructions or Question sections. TinyMCE Editor A detailed editor description can be found here: har _editorhelp.pdf Inserting Images Place the cursor where you want the image to go. Click the Insert Edit Image. An Image dialog will pop up. Insert Image To choose an image location, you can use one of the following options: Paste a link to your image into the Image URL field. Example: https: yourserver.domain images your _image.jpeg If your input file contains a column with image URLs, enter the appropriate Data Element into the Image URL field. Example: {question.data 'img _url' } To upload an image file from your local PC, click the icon to the right of the Image URL field.Click the Choose File button and then the Upload button. Finally, select the uploaded image from the gallery. After uploading image or entering the image URL, you can edit the image formatting on the Appearance tab. Code Editor By default, the WYSIWYG editor is presented in Task Designer Mode which allows you to easily format all aspects of your question including links and images. Switching to the Code Editor mode allows you to edit the underlying source code (Javascript, HTML, CSS, and FreeMarker) used in rendering the information. Preview Once input data has been uploaded, you can preview how it will appear to workers via clicking the Preview button in the top right corner. See an example of Task Preview below: To switch among Previews for different Task Records, use side arrow buttons or bottom navigation control. Save Note that a Task is not created until the first saving. When the Task Design is finished, click the Save button. While designing a new Task – SAVE after each new element is added – Tasks are NOT saved automatically. Save as Use Case To make the current Task re usable, you can create a Use Case out of this Task by clicking the Save as Use Case button. Then follow these instructions – Task Use Case. Next Step When you are finished with the Task design, click the Next button in the bottom right corner or select the Run Task tab. "},{"version":"10.0","date":"Aug-06-2019","title":"create-tasks-with-ongoing-qualifications","name":"Create Tasks with Ongoing Qualifications","fullPath":"iac/core/user-guide/work-with-tasks/create-tasks-with-ongoing-qualifications","content":" In WorkFusion platform, you can mix regular Worker Tasks with Qualification Worker Tasks from Gold file to drop Worker Qualification score after a certain amount of errors. To implement an ongoing Qualification: Create a regular Task and provide a Gold Data file along with Input Data. Upload Gold Data In Advanced Options > Properties, set the Percentage of Gold Records parameter greater than zero. Percentage of Gold Records If you enter 10% and you have 1,000 records in your INPUT file, you will run 1,100 records: 1,000 input records and 100 gold data records. Open the Advanced Options > Adjudication tab > Advanced View. Under the Check every i th gold question make this Number the number of gold records you would like a Worker to complete before their score is checked. Save the Adjudication Rule. If this is set at 10, once a Worker completes 10 gold records their score will be checked. If they have a percentage lower than 90% their score will be dropped. It will be checked again after they complete another 10 gold records (in any run you add gold records to). We recommend a number at least as high as 10 to be sure there is enough of a sample set for the Worker, 5 may be too low, 10 or more would work well. Also, you will get emails from Workers once their score is dropped, so you may want to send the Workers a message before implementing this Gold Data, that their score may be dropped if the quality of their work diminishes, to preempt some blowback. Ongoing Qualifications in Business Processes You can also monitor and update Worker Qualification Score for the Tasks in a BP: Open your BP > Design Business Process tab. Enable the Edit Mode. Double click the Human Task in your BP. Navigate to the Upload Gold Data tab and click the appropriate button. Set all other options like for a separate Task (see the topic beginning). Save the Task and save your BP. "},{"version":"10.0","date":"Aug-06-2019","title":"download-https-without-java-certificate","name":"Download HTTPS without Java certificate","fullPath":"iac/core/user-guide/work-with-tasks/download-https-without-java-certificate","content":" Problem When you download content from HTTPS without Java certificate, you get the following response: or the following exception: javax.net.ssl.SSLHandshakeException: java.security.cert.CertificateException: Illegal given domain name: https: test.domain.com file.test Solution You need to provide documentLink to your specific file. If the file you are trying to download is accesible via browser, you can implement the following code (see comments in the code for concrete steps): "},{"version":"10.0","date":"Aug-06-2019","title":"messages","name":"Messages","fullPath":"iac/core/user-guide/work-with-tasks/messages","content":" Workers can send feedback while executing the Task. All the Messages from Workers are displayed on the Messages page. When there are some unread messages, they are displayed on the All Tasks page. You can perform the following actions with Messages: answer a Message create a FAQ from Message delete a Message After receiving and answering a Message, you can improve other Workers' performance by updating the Task Instructions and or Design. When you reply on message, it is available in WorkSpace dashboard of the respective Worker. In case a Worker wants to continue the communication (replies from Dashboard), this message will be linked to this Task in WorkFusion (as a new one). All communication with Worker is displayed at Worker Details > Communication center. Answer Message To answer the message: Answer the Message. Click the Reply button. Creating FAQs To create a FAQ: Tick the Create FAQ Item checkbox. Answer the Message. Click the Reply button. Delete Message( s) To delete a message or messages: Tick the checkbox( es) on the message( s) you want to delete. Click the Delete message button. "},{"version":"10.0","date":"Aug-06-2019","title":"instructions-and-faqs","name":"Instructions and FAQs","fullPath":"iac/core/user-guide/work-with-tasks/instructions-and-faqs","content":" Managing FAQs FAQ (Frequently Asked Questions) – a Task section comprising some common questions that the Workers can ask about the Task and respective answers. FAQs, like Instructions can be added in the Design Task tab dynamically while the Task is still running. There are two ways to add FAQs: Through Worker Messages. While replying to Worker's messages, you can tick the Create FAQ Item checkbox before hitting \"reply\". This will automatically create an FAQ for this Task. Through the Design Task tab: On the Design Task tab, click inside the Instructions section. On the Side Panel, you will see Instruction and FAQ tabs. Select the FAQ tab and click the Add FAQ Item button. Add FAQ Item An FAQ window will pop up as shown below. Enter your Question and Answer and click the Save FAQ Item button. Add FAQ Popup An FAQ item is appended to the Instructions section and is displayed on the Preview. To edit an existing FAQ item, click the respective item on the FAQ tab (Side Panel) and edit Question or Answer fields in the popup window. Task FAQ add, edit, delete To modify the order of FAQ or Instruction elements, drag them up and down. To delete a FAQ or Instruction elements, click the close icon (x) to the right of the respective element. Managing Instructions Instructions – explanatory part of a Task intended to provide initial data, Task goal, steps to complete, examples. Proper Instructions should contain the following parts: Overview – brief explanation of task objective. Background – some context or background that may be necessary. Steps – clear, step by step instructions to follow. Common Mistakes – explanations of things you DO NOT want to be completed. Examples – specific examples of correct responses. Instruction item can be published or marked as Test. Each time the Instructions are updated, the Worker should acknowledge the changes and mark them as read. Adding Instructions and FAQs To add Instructions, complete the following actions: On the Design Task tab, click inside the Instructions section. On the Side Panel, you will see Instruction and FAQ tabs. Select the Instruction tab and click the Add Instruction button. Add Instruction window will pop up as shown below. Enter Name and Description and click the Save button. Add Instructions Popup Condition – information about condition of Task that is displayed in Instruction body (When). Actions to be taken – information on how to handle the condition that is displayed in Instruction body (What to do). Use Instructions as Tests When Instructions are used as Tests, Workers are granted access to the Task only after answering the Test question correctly. If the worker answers are incorrect, test is refreshed and access to the Task is not provided. Workers are told to try answering the question again. If the Use as Test checkbox is marked, the following additional fields are displayed in the form. Add Instruction as Test Question Data – field names are parsed from Input file Data. Correct Answer – field names are parsed from Answer Codes. Enter the Question and Correct Answer and click the Save button. To show hide Instruction or Tests, click the appropriate link shown below. Task Design with Instructions and Tests How Instructions are Displayed Workers need to tick the I have read the instructions checkbox to work on the task. Task Rendering When a Task contains Instructions as Test, Workers see the test question. If Workers answer the question incorrectly, an error message is displayed. When the Worker's answer is correct, the Worker can access The Task and a Test Passed instruction is displayed. "},{"version":"10.0","date":"Aug-06-2019","title":"notifications","name":"Notifications","fullPath":"iac/core/user-guide/work-with-tasks/notifications","content":" On the Notifications tab, you can set: templates sent to Crowds and Workers notification repetition period Field Description Worker Answer Accepted Message This message is automatically sent to Workers when their answers are accepted. Worker Answer Rejected Message This message is automatically sent to Workers when their answers are rejected. Task Published Message This message is automatically sent to all Workers from the Crowd when this Task becomes available. Send Notification Notification repetition period (Once per day, For every Run). Worker Bonus Message This message is automatically sent to Workers when a bonus is awarded. Crowd Notification Message This message is automatically sent to specific Crowd workers when this Task becomes available. The list of workers who will get email notifications is set in Workers &gt; Workforces &gt; Manage Crowds &gt; Edit Crowd. For detailed info, see the Manage Workforces and Crowds topic. To preview a Notification Template, click the corresponding preview icon on the right. To get information on creating or editing Templates, see the Templates topic. "},{"version":"10.0","date":"Aug-06-2019","title":"properties","name":"Properties","fullPath":"iac/core/user-guide/work-with-tasks/properties","content":" On the Properties tab, you can set different options for your Task. If you are new to the WorkFusion platform, just edit the Keywords and Description fields – all other options have default values set. Field Description Keywords Provide keywords that will help Workers identify your Task while searching. Description Briefly describe what Workers are expected to do in order to successfully complete the Task and get paid. Custom Attributes If you want to add Custom Attributes to your Task, select the Field Scheme upon which they will be based. Time Allotted per Task Maximum amount of time Workers are allowed to spend while completing the Task. If a Worker exceeds this time limit, the Task becomes available for other Workers. Task Expires in A time period after which a Worker Task expires if nobody accepts it. After expiration, the Worker Task will not be available to Crowd Workers. Due Date Date until all Worker Tasks should be completed. The Task becomes unavailable after the Due Date. Max Number of Pending Assignments per Task Maximal number of Tasks user can accept and keep in accepted state (not submitted). To accept new Worker Tasks, Worker should start submitting the accepted ones. Value 1 for unlimited Worker Tasks per Worker. Frame Height The height of the frame element which contains the Worker Task. Useful Tip: make the frame height equal to the height of the Task Design (Preview). External Form URL Displays the URL of the Task. This field is automatically populated based on the Crowd settings. After the Task has been started, this URL is used to build a Task preview link with public access. Unique Column Name Used for grouping Records from the uploaded Data file. See the Grouping Records by Unique Column topic. Permanent Open Tasks The number of Worker Tasks available to Workers simultaneously. For example, a Task contains 100 Records, Permanent Open Tasks = 10. It means each Worker in WorkSpace can see that only 10 Tasks are available in this Task Type. Percentage of Gold Records This field is used to create Tasks with Gold Data for some Records. These Tasks are intended to imperceptibly check the Qualification of Workers. See the Create Tasks with Ongoing Qualifications topic for detailed description. Turn On Recommendations for Automation Check to enable continuous monitoring and Automation Recommendation Engine calculations for this Task. Loopback This function is used to automatically assign a Worker Task to a new Worker if the majority is not reached. The Loopback number defines how many times the Task will be launched if the Task results contain Records without Confidence. Default value = 1, it means that no new Worker answer is needed. This option is useful for Tasks with Block Size &gt; 1, in other cases use a different Adjudication Rule. Loopback value shouldn't be applied to Qualification Tasks. Enable Dynamic Task Rendering Enable dynamic Task sorting. This functionality is intended to sort Worker Tasks by type, to make it possible to complete Tasks in a custom order. See the Dynamic Task Rendering and Task Comparator (Manual tasks sorting) topics for detailed description. Forcefully Complete Tasks This feature allows user to forcefully complete individual Tasks and Business Processes, when completion criteria is reached: % of Records completion idle time Task is completed when both conditions are met. "},{"version":"10.0","date":"Aug-06-2019","title":"questions","name":"Questions","fullPath":"iac/core/user-guide/work-with-tasks/questions","content":" Questions section can contain question text together with Data Elements. Data Elements are taken from column names of the Data File. If the column headers do not match the variables within the Task design, a warning message is displayed alerting you which column names need to be remapped. See the Upload Data topic for details. Data Elements have the following template: {question.data 'data element name' } Questions and Data Elements In the Question section, you can perform the following actions: type Rich Text type Data Elements delete Data Elements In the Side Panel, you can only insert Data Elements at the current cursor position. Do not type or paste Data Elements into Title or Instructions sections – the Preview will not be accessible. "},{"version":"10.0","date":"Aug-06-2019","title":"result-data-and-snapshots","name":"Result Data and Snapshots","fullPath":"iac/core/user-guide/work-with-tasks/result-data-and-snapshots","content":" The Data page is intended for viewing the Task results. The results are displayed in the grid with the following columns: columns from the original Data file 3 columns for each Answer: Answer Code. see the description here. Answer Confidence. Answer Accuracy. Confidence – a qualitative parameter showing the number of Workers with coincident Answers per Record. Example: Confidence = \"2 out of 3\". This means: 3 Workers have completed this Record; 2 of them provided coincident Answers. Accuracy – a parameter determining the probability that the majority Answer is correct (calculation is based on previous Worker statistics in similar Tasks). Worker Accuracy – the percentage of all Worker Answers that coincided with Answers provided by other Workers (with majority). Low Confidence When the Confidence is low (1 of N), the Accuracy for that Record is not calculated. We recommend one of the following solutions for this situation: Copy this Task and upload a Data file containing all Records with low Confidence. Same as the previous, and edit the Set of Worker Qualifications required to accept this Task. Same as the previous, and edit the target Crowd for the Task. For example, you can use your internal qualified Workers. Create a Moderation Task to verify the Answers View The grid with Records has a horizontal scrolling, and you can manage the number of displayed columns by clicking the cogwheel in the top right corner. If you do not see some Records, check the applied filters by clicking the Show button in the top right corner or check the **Choose Status** dropdown. To view the Gold Results, select the appropriate element in the Choose View dropdown. Export To export the Task Data, complete the following operations: Select necessary filter or create a new one and apply the filter. Click the Export to Excel or Export to Csv button depending on the format needed. Snapshots You can save intermediate or final Task results in Excel format by clicking the Generate Snapshot button. No filters are applied to Snapshots. If you have problems with viewing exported UTF 8 data in Excel, see this topic (https: kb.workfusion.com display SD .Importing and Exporting UTF 8 Data v6.1). The following rules apply to Snapshots: Snapshot for Task will be generated automatically after Task completion; if a Task is not completed, Snapshot will contain only completed records; for an expired Task, Snapshot contains answers for completed records and also input data for records without answers; Snapshot for Qualification Task will be empty, as we already know correct answers. Gold Data You can use provided Answers to create Qualification Tasks – convert Records to Gold Data by selecting appropriate Records and clicking the Convert to Gold button. To undo this operation: In the Choose View dropdown, select Gold Results. Tick the appropriate Records and click the Delete Gold Questions button. Preview Answers To get the detailed info on a Record, click the appropriate eye icon (3rd column). The following popup screen will appear: You can also preview a Record in a new window by clicking the External URL link at the top. Worker Answers To compare Answers from all Workers, click the Worker Answers tab. To display the additional Answer and Record parameters, click the Show Extra Columns and or Show Original Data links. Worker Accuracy is a dynamic parameter and might have changed since Worker completed a Task. You can Recalculate Worker Accuracies, to use the actual Worker Accuracy when calculating the Record Accuracy. Audit Trail On Audit Trail tab, you can view: all Worker Task events event duration and time payment details Workers who submitted answers "},{"version":"10.0","date":"Aug-06-2019","title":"results-analytics","name":"Results Analytics","fullPath":"iac/core/user-guide/work-with-tasks/results-analytics","content":" The Analytics page displays qualitative and quantitative factors of a Task. The results are based on completed Worker Tasks and are recalculated over time. Match Quality This chart displays the Confidence distribution against all Answers in Records. Answer Distribution This chart is available for multi select Answers. Assignment Distribution You can edit the Task Availability to suit the best time for Approved Tasks. Results This grid contains all submitted Worker Tasks with their data. You can click the Task ID or Worker ID and navigate to the respective Worker Task or Worker. "},{"version":"10.0","date":"Aug-06-2019","title":"qualification","name":"Qualification","fullPath":"iac/core/user-guide/work-with-tasks/qualification","content":" On the Qualification tab, you can narrow or filter your Workforce for optimal performance. Basic Mode Select a Set of Qualifications from appropriate dropdown. See more information about Qualifications in the Create and Assign Qualifications to Workers topic. If Workers need to have all the Qualifications in the Set, tick the Require All Qualifications to Preview the Task checkbox. By default, Workers with any of Qualification in the Set are allowed to preview the Task. To limit the number of the Workers with Qualification, tick the Maintain Constant Number of Qualified Workers checkbox and enter the necessary number. To create a Qualification Task, tick the corresponding checkbox. See the detailed info on Qualifications in the Create a Qualification Task topic. Advanced Mode To switch to the advanced mode, click the appropriate link. In the advanced mode, you can: add up to 5 Qualifications by clicking the Add button set Condition and Value for each Qualification remove Qualifications by clicking the (x) icon. "},{"version":"10.0","date":"Aug-06-2019","title":"review-pdf-in-manual-task","name":"Review PDF in Manual task","fullPath":"iac/core/user-guide/work-with-tasks/review-pdf-in-manual-task","content":" Use case: human worker need review content of a .pdf file and fill some field in a manual task At current moment Workfusion platform doesn't provide functionality to review pdf files \"from the box\", but if it still necessary here is a solution which can help. Create a Manual task Go to design tab and select code editor of the task Find the following part of code Replace it by the following code Expected result (example): PDF viewer itself is the following code:* * Iframe also can be used istead of object&embed It uses input field “pdf _link” and it’s width and height also can be changed, but please keep in mind that it works for pdf files only. "},{"version":"10.0","date":"Aug-06-2019","title":"set-run-parameters","name":"Set Run Parameters","fullPath":"iac/core/user-guide/work-with-tasks/set-run-parameters","content":" On the Run Task tab, you can: view Task summary details edit Task Tags set a Task Workforce preview the Task save the Task perform Task Actions call the Advanced Options run the Task in the Sandbox or Production You should always test your Tasks or Business Processes on Sandbox to ensure that the instructions make sense and that everything is configured correctly. Viewing Task Details The Task Details block displays the Number of Records and Gold Records together with the Workforce selected for this Task. To see a detailed field description, place your cursor over a (question){.emoticon .emoticon question} icon. {width=\"936\" height=\"519\"} Editing Tags You can optionally add Task Tags to categorize it and make it easier for Business Users to find. Setting a Workforce Workforce – a grouping entity that specifies who is working on the Task. A Workforce can contain one or several Crowds.For example: you can create a Workforce named \"US 98%\", consisting of \"WS US 98%\" and \"MTurk US 98%\" (WorkSpace and MTurk workers who have 98% approval rate and are located in the U.S.) To select or change a Workforce for a Task: Click the Select link. In the popup window, tick the necessary Workforce and click the Select button. To view the selected Workforce and its Crowds with parameters, click the Workforce name link in the Task Details. The following popup appears: For detailed info on Workforces, see the Manage Workforces and Crowds topic. Task Settings In the Task Settings block, you can edit the Block Size and Task Priority fields. Block Size – the number of Records a Worker will get in each Task. Default value = 1. If the Block Size = 3, a Worker should complete 3 Records to get paid for one Task. Set the Task Priority to change the Task ranking in search results of target Endpoints. Tasks with High priority will be displayed at the top. Example: You have 2 similar Tasks with different input Data files. And you want Workers to complete the 2nd Task primarily. For this purpose, set the Priority = Max for the 2nd Task. When a Worker finds this type of Task and clicks the View link, the Worker will see the Tasks with the highest priority on top. Task Actions (Draft Status) On the Run Task tab, you can select one of the following Task Actions: Copy Task Copy Task with Data Copy to New Task (see the Copy a Task topic) Create Qualification Task (see the Create a Qualification Task topic) View Data. For Draft Tasks – show the Upload Data tab. For Processing Completed Tasks – show the View Results tab. Delete Task Download Original Data. Input Data file will be downloaded. Export. The Task is exported in the XML format. You can import this file Task actions vary depending on the Task Status (Draft, Processing, Paused, Completed). Advanced Options Reward and Estimated Costs Properties Adjudication Qualification Availability WorkSpace Preview Bonus Notifications "},{"version":"10.0","date":"Aug-06-2019","title":"run-task-and-view-results","name":"Run a Task and View Results","fullPath":"iac/core/user-guide/work-with-tasks/run-task-and-view-results","content":" When you have uploaded the Data, completed the Task Design, and set the Run options, perform the following actions: Run the Task in a Sandbox environment by clicking the Run This Task button on the Task Run tab. If the Task has started successfully, you will see the following screen: You can view the results of the Task or view the Task you have just created via the Tasks List. Log in as a Worker (in WorkSpace, Amazon MTurk, Elance, etc.), search the Task by name or keywords, and try to complete several Worker Tasks. Go back to the WorkFusion platform and edit the Task options, Design, or Data if needed. Copy the Task with Data. You can optionally stop or delete the original Task if you do not need it. Run the Task in a Production environment by checking the appropriate checkbox above the Run This Task button. Videos ▶ Run Tasks and Processes Pause, Resume, Stop Tasks and Processes Copy Tasks and Processes The following topics describe how to watch the Task progress, analyze Worker answers and activity, and get the final Result Data: Taking a Task TestingTask SummaryWorkersResults AnalyticsResult Data and SnapshotsMessagesView All Tasks "},{"version":"10.0","date":"Aug-06-2019","title":"taking-task-testing","name":"Taking a Task - Testing","fullPath":"iac/core/user-guide/work-with-tasks/taking-task-testing","content":" Prior to post your Task to Production environment and pay real Workers for their submissions, we recommend you to test your Task on Sandbox environment. Endpoints Endpoint (or Cloud Worker Platform) is a web service where Workers log in, search, and complete your Tasks. To view the Task on the Cloud platform, click the appropriate icon (WorkSpace, Mturk, etc.) on the Task Summary page. You can also search the Task by its name on the Endpoint, if you have a Worker account. If you do not have a Worker account, you will be redirected to the login register page. Register a new Worker account (or several accounts to test the Adjudication Rule). In this topic, all examples are described for WorkSpace (WS). WorkSpace is the basic Endpoint for internal Workers and is developed by WorkFusion. Environments All Endpoints provide two separate environments: Sandbox and Production. To view your Task on both Sandbox and Production, you should create two Worker accounts on the appropriate Environments. Sandbox Sandbox environments usually have a \"sandbox\" substring in their url: https: worker provider name.workfusion.com workspace sandbox Production Production usually does not have the \"sandbox\" substring in the url: https: worker provider name.workfusion.com workspace To run your Task in Production, remember to perform the following actions: Test it on Sandbox. Change Task Options, Design, or Input Data, if needed. Copy your Task with Data. Check the Run in Production checkbox on the Run Task tab. Click the Run This Task button. Log in to Production as a Worker and check the Task availability. How to Submit a Task For full instructions on taking tasks in WorkSpace, see WorkSpace Worker Manual and Training Presentations. Search for Tasks Type the keywords (e.g. Classification, Translation, Information Extraction) in the search field and review the search results. Or drill down to Task categories and sub categories that fit your skills and interests. If you haven't found available Tasks, try to obtain different Qualifications. Workers with Qualifications can access the bigger number of Tasks with higher reward. Work on Tasks If you have found your Tasks, perform the following actions: Carefully check all the Task instructions. Accept the Task and start working on it. Watch the timer to complete the Task in time. Submit the Task when you are done. You can accept and submit other Tasks of the same group in succession, no need to search them again. Modify Task Options While testing your Task on Sandbox, you can find some \"bugs\": information or instructions displayed incorrectly, task is unavailable to Workers, etc. To fix these situations, edit the Task Design and Input Data, or play with the following options: Adjudication Rule Start from 1 0 Rule just to check that everything looks good and Workers can submit Tasks. If you need a 2 1 or other Rule, create several Worker accounts and submit the Task using multiple accounts. To test the Adjudication, try to provide the following answer combinations on the same Task: all the same answers Result Confidence 2 of 2 (depending on the Adjudication Rule) all different answers Result No Confidence found some answers coincide, other do not coincide Result the combination of previous results Qualification If your Task requires a Qualification Set: manually grant all the required Qualifications to your Worker account or obtain Qualifications by completing Tasks. If you just want to check the Task Design, you can temporary disable the Qualification requirements. Availability The Task can be unavailable to you because of the following parameters on the Availability tab: Time Country Geolocation "},{"version":"10.0","date":"Aug-06-2019","title":"task-summary","name":"Task Summary","fullPath":"iac/core/user-guide/work-with-tasks/task-summary","content":" The Summary page gives a Task overview with key parameters and links. Run Details Field Description Status This field indicates whether the Task is completed, paused, or still processing. To view all the Task events, click the 🛈 icon. Priority The Task can have normal, high or low priority. Environment This field displays two pieces of information about the environment. In the screenshot above you can see an WS icon indicating that the Task was posted to WorkSpace. To view the Task on the Worker platform, click the appropriate icon (WS). You should have a Worker account with required Qualifications to access the Task. Block Size This is the number of Records a Worker will get in each Worker Task. In this case it is set to 1. You can change the Block Size before running the Task on the Run Task tab > Task Settings. Created By The Task author's username. Started The date the Task was run. Finished If the Task is completed, the date it was finished. Submission Statistics Field Description Progress A progress bar showing the percent of Records that have been successfully submitted. Successful Submissions The total number of successful submission. A Record is considered to be successfully submitted once the Adjudication for that Record is complete. Number of Records The Total number of Records in the input Data file. Number of Processed Records The total amount of Records with processed data. Number of Gold Records If you have used any Gold Records, the number of gold Records used will be displayed here. Gold Ratio The Gold Ratio chosen, if any, is shown here. This parameter is set before running the Task on Run Task Tab > Advanced Options > Properties. Times Field Description Average Time per Record The average time Workers spent on a Record. Average Time per Task The average time Workers spent on a Worker Task. Effort This is the total time Workers spent on this Task combined. Time Line Total time spent since the Task start. "},{"version":"10.0","date":"Aug-06-2019","title":"upload-data","name":"Upload Data","fullPath":"iac/core/user-guide/work-with-tasks/upload-data","content":" Task Data is a .csv file with source input data needed to run the task. The column headers in the file should be the same as the variables used within the task design but can be remapped if necessary. Each row of the Task Data file defines a Record. You can download a template file for either the main or gold data in .csv format via the Download Sample File button. The column headers within each of these templates correspond to the variables (Data Elements) configured within the Task Design. Important notes How To Create Data File Before you begin creating your data file, please revise your knowledge about the CSV format and find some tips how to create a .csv file. Default Settings By default, the items (fields) should be delimited by the comma (\",\"), the file should be encoded in UTF 8. If you want to use another delimiter or another encoding, you should specify it later in the options. Header line Your data file should contain a header line, where you define names for the columns in the Input data. The column names should contain alphabetical characters in lower case, numerical characters and underscore only. Any other characters may violate creation of the Input data. Links to files The files you are going to use in your task should be placed in an online or cloud storage supporting access over HTTP protocol, for example S3, Dropbox or Google Drive. Local files can not be used! Complete the following steps to upload and set a Data file: Click the Upload Data button or click anywhere within the center position of the tab. Upload Data Step If you want to create a Qualification Task or mix Questions with known and unknown Answers, click the Upload Gold Data button. See Create a Qualification Task topic to get advanced information about Qualifications and Gold Data. In the popup window, click the Add button, select a main input Data file, and click Upload. Upload Data Popup WorkFusion platform automatically determines the File Separator and File Encoding parameters, which can be changed if needed. Click Show options and select the File Separator and the File Encoding from the respective drop down menu. If you have problems with UTF 8 data upload, see this topic. Click Clear to remove the selected file and select another one to upload. The contents of the input file is displayed along with the file's existing column headers. Click the Preview eye icon associated with any given row of data, to preview the Task using the corresponding data. When both the main Data and the Gold Data files were uploaded, you can switch between data sets by using the Choose View drop down in the top right corner. If the column headers do not match the variables within the Task design, a warning message is displayed alerting you which column names need to be remapped. To map remap a column to a variable, click the column header found in the table and select an item from a drop down. After you have mapped a column header to a Task variable (e.g. **url > picture _address), you cannot use this column name to create a new variable ( {question.data 'url' } **will be an invalid variable). You can optionally remove the uploaded data by clicking the Remove Data button. Click Next to proceed to the Design Task tab, if you are satisfied with the uploaded data. You can go back to this step from Design Task tab and upload new data or remap column headers. "},{"version":"10.0","date":"Oct-04-2019","title":"view-all-tasks","name":"View All Tasks","fullPath":"iac/core/user-guide/work-with-tasks/view-all-tasks","content":" The View All Manual Tasks page provides the following functionality: contains all the Tasks created and their links gives a quick overview of Task statuses, details, progress, possible issues provides possibility to perform Task Actions including the Creation of New Task. Before taking some actions, set the appropriate sorting (Sort by dropdown) and filtering (Filter dropdown or Show button) at the top of the grid. The displaying of Task parameters and functions varies depending on the Task Status. Task Name. Click this link to: continue creating a Task (for Draft Tasks) view Task results (for other Tasks) If several Tasks have the same name, you can distinguish them by defining unique Tags. You can edit Tags by hovering over the Tags label and clicking the pencil icon on the right side. Progress Bar. Progress Bar displays the number of completed Worker Tasks against the total number of Worker Tasks. The bar is clickable for all non Draft Tasks, the link redirects you to Task results. Indication Icons Business Process Task (if any). This icon informs you that current task is a step of a Business Process. Workforce: WorkSpace Task Status: Draft Processing Paused Resuming Completed with Expired Tasks indication (if any) Processing Issues (if any) Use Case New Messages with the indication of their quantity Actions Dropdown. A list of available Task Actions. Expanded Task Details. Additional Task information. Processing Task Name. Tasks with a Processing Status have names highlighted with blue. Business Process Name. If a Task is a part of a Business Process, the name of respective BP is displayed under the Task name. Generate Snapshot. You can generate a Task Snapshot (for non Draft Tasks). For Draft Tasks, this button is replaced by the Edit & Run button."},{"version":"10.0","date":"Aug-06-2019","title":"workers","name":"Workers","fullPath":"iac/core/user-guide/work-with-tasks/workers","content":" After you have posted a Task to the Crowd, Workers start accepting and completing this Worker Task. To watch Worker statistics, answers, and manage Workers participating in your Task, open the Task and navigate to View Results tab > Workers. Filtering To narrow down the number of Workers displayed in the grid, you can apply filters in the dropdown list or create your custom filter. The Workers grid has a horizontal scrolling, and you can manage the number of displayed columns by clicking the cogwheel in the top right corner. To create and apply a custom filter: Select the Advanced checkbox. The following Filter panel will appear: Select available options, logical operators (contains, less than, etc.), and enter your values for each option. You can optionally save this filter and make it reusable by giving it a Filter Name and clicking the Save button. The saved filter will appear in the Filter dropdown. Click the Apply button. Grid with Workers Each grid row represents a Worker with his her key parameters (Accuracy, Approval Rate, Performance, etc.) In the grid, you can click the user name to navigate to Workers Profile. Bulk Actions To perform a bulk action, tick checkbox( es) opposite Worker( s) and click one of the following buttons: Give Bonus. You can optionally set the Bonus for each Worker in the appropriate grid column before clicking this button. Send Message. See the Messages topic. Assign Qualifications. You can assign a Worker one of the existing Qualifications and set the Score for it. Retract Worker Answers. This function is intended to exclude answers from the final result (for example, if a Worker is a cheater). If a Task contains retracted Answers, the Majority and Confidence will be recalculated. Retract button is available only for human non qualification Tasks.Worker with retracted Answers can continue accepting and submitting these Worker Tasks. Answers given by this Worker after manual retract are not retracted automatically. Answers can also be retracted by the Rule. See Workers Answer. The Data page will be loaded with a filter by the Worker ID of the selected Workers. Pay for Rejected Work. This function allows to approve rejected Worker Tasks. For example if worker's answer has been manually rejected on a Moderation step.Crowd Worker will receive money and the status of his her Answer will be changed to Approved. Export All to Excel. All enabled filters are applied. Charts Country Distribution Time Distribution Payment Distribution (Hourly Rate) "},{"version":"10.0","date":"Aug-06-2019","title":"title","name":"Title","fullPath":"iac/core/user-guide/work-with-tasks/title","content":" We recommend that you modify the title of the task so that once the task is ran, it will have a unique name that is easy to find. Task Title Recommendations Use informative unique wording to distinguish the Task among others on crowd sites and in your WorkFusion \"View All Tasks\" page; to clearly state the Task goal for Workers. Do not insert line breaks and rare ASCII characters. Use heading style with big font. Save to apply changes. "},{"version":"10.0","date":"Aug-06-2019","title":"view-qualification-task-results","name":"View Qualification Task Results","fullPath":"iac/core/user-guide/work-with-tasks/view-qualification-task-results","content":" Qualification Task results is similar to a regular Task results. To understand the differences, see the following info. Workers The Retract Worker Answers function is not available for Qualification Tasks. You can Reset Gold Accuracy for each Worker instead. This functionality is intended to clear Worker's gold statistics in this Task (for example, if instructions have been updated). After re qualifying, Worker can continue executing the Qualification Task. The Accuracy parameter is not available for Qualification Tasks. The main Worker parameters in Qualification Tasks are: Gold Accuracy Gold Performance Score All these parameters are calculated by the Qualification Rule which compares Worker Answers with Gold Data. WorkFusion will automatically give a Qualification Score to a Worker that has completed the Required number of Worker Tasks. If there is not enough Worker Tasks to qualify the required number of Workers, create an additional Qualification Task by copying the source Task. Data The Convert to Gold function is not available for Qualification Tasks. Useful Tips: You can detect Record with low amount of correct Answers and check the Preview and Data for some errors. You can see all the Answers of a given Worker and decide whether to give him her a Qualification or not. Qualification Task Data "},{"version":"10.0","date":"Aug-06-2019","title":"workspace-preview","name":"WorkSpace Preview","fullPath":"iac/core/user-guide/work-with-tasks/workspace-preview","content":" By default, Dynamic Task Rendering is enabled for all Tasks, i.e. Worker cannot pick a particular Worker Task from a Task type in the WorkSpace{.unresolved} application. As a workaround, you can open a Task View and click Skip multiple times to get a Worker Task needed. A better way to solve this problem is using WorkSpace Preview feature in WorkFusion (Run Task > Advanced Options > WorkSpace Preview). Enabling WorkSpace Preview Dynamic Task Rendering will not work with WorkSpace Preview enabled. Use only \"1 0\" Adjudication Rule. If a Task is part of a business process, WorkSpace Preview can be configured only inside the business process. If the HT is opened as a separate task, the Data Elements list is empty. When WorkSpace Preview is set, a Worker can click the Show Tasks button and pick a particular Worker task from a grid: To enable WorkSpace Preview, complete the following actions: Set up your Human Task design. Go to Run Task > Advanced Options > **WorkSpace Preview. ** Select a Field Schema from a dropdown or create a new one by clicking the New Scheme button. The Field Schema defines the grid column names (with format) shown to Workers in WorkSpace. See more details about Field Schema here. Click a field on the left panel and then click a corresponding Data Element on the right panel. Repeat this step for all fields. This action will map Task input columns with grid columns shown in WorkSpace. Pay attention to the Answer Types in Field Schema – they should correspond to the Data Element values (e.g. URL, Number, Email, etc.) Fields with Answer Type = \"URL\" are rendered as hyperlinks. Click OK to save these settings. Result After posting the Task to WorkSpace, Workers will see the following grid: Workers can do the following actions: expand collapse grid by clicking the Show Tasks button sort grid rows by clicking corresponding column headers search particular Worker Tasks by entering their values into the Search field refresh grid to display only currently available Worker Tasks "},{"version":"10.0","date":"Aug-19-2019","title":"analytics-components-and-licensing","name":"Analytics Components and Licensing","fullPath":"iac/core/control-tower/analytics/analytics-components-and-licensing","content":" Components WorkFusion Analytics products have 2 components: Desktop dashboard development, configuration and publish on Server Server dashboard viewing and filtering, data refresh Licensing Description Desktop License Key is available for 2 activations 1 activation per PC and could be used only for WorkFusion Analytics (not Tableau) Desktop. Server License Key is available for 3 activations 1 activation per server (estimated to use the same key on Dev, UAT, Prod environments) and could be used only for WorkFusion Analytics (not Tableau) Server. Each activation will provide access to WF Analytics Server web application for 5 WF Analytics Server users. 1 of the 5 users will be used for Analytics Control Tower integration. If there is no necessity to view and interact with dashboards using WF Analytics Server web application for more than 5 WF Analytics Server users 1 Server License Key will be enough. How many Analytics Server licenses do I need If it is needed to view dashboards in Control Tower only 1 Server License Key is required. If Analytics is enabled in Control Tower dashboards could be viewed by any CT user regardless their number. Scheme OOTB dashboards usage OOTB dashboards usage with option for development and customization Glossary Activation a license validation procedure that prevents unlimited free use of software. Connection user WF Analytics Server user whose credentials are used for Analytics Control Tower integration. 1 of the 5 WF Analytics Server users. WorkFusion Analytics Server user a user that has access to WorkFusion Analytics Server web application. "},{"version":"10.0","date":"Aug-19-2019","title":"analytics-faq","name":"Analytics FAQ","fullPath":"iac/core/control-tower/analytics/analytics-faq","content":" Licences How many Analytics Server licenses will I need if I want 100 users to view dashboards in Control Tower Only 1 license key is required. For more information please refer to Analytics Components and Licensing article. How many Analytics Server users do I need It depends on the Analytics Server usage purposes. The first license for Analytics Server provides access to 5 users. If you interact with dashboards using only Control Tower, you need only one user. So the initial license is enough. If you want users to view dashboards on Analytics Server, you need as many available server users as will login. Analytics Server uses user password authentication. So you may need additional server license per user in case the number of available users is exceeded. If you want to customize dashboards or create them from scratch, you will be required to add an Analytics Desktop license. For more information about licensing please refer to the Analytics Components and Licensing article. Can customers re use licenses from contract with WF for their own Tableau Server No, license keys for WorkFusion Analytics Server can't be re used for Tableau Server and vice versa. Are there any concerns for the customers who use the core based license It doesn't matter what license type to use. Installation and Integration with Control Tower How can I deploy OOTB workbooks There are 2 ways of workbook deployment: manual using Tableau Desktop automated using deployment scripts Second way is preferable. Can I use my own Tableau Server And what should I do to make it work with WorkFusion Application Yes, you can use your own Tableau Server. In case OOTB dashboards are used, it is preferable to use Tableau Server compatible with dashboards. If version of Tableau Server differs from preferable please check Dashboards FAQ section for possible solutions. To configure integration between Tableau Server and WorkFusion APP Server you need to: have correctly created Tableau Sites and Projects add WorkFusion APP server to Tableau Server trusted hosts Analytics is enabled in Control Tower out of the box. What kind of authentication is used to show Analytics on Control Tower Control Tower connects to Analytics Server using user password authentication. CT sends the credentials to Tableau Server (regardless Tableau Server authentication type). The credentials are stored in Secrets Vault properties file. Dashboards What reporting functionality do you have You can choose different options: using OOTB dashboards customization OOTB dashboards creating dashboards from scratch share dashboards using Control Tower, WorkFusion Analytics Server or Tableau Reader. What are possible options if customer uses newer version of Tableau Server than WF is tested to use It is still possible to get OOTB dashboards in such case. The most significant faults could be from UI side. Also Tableau 10.5 supports dashboards created in previous versions beginning with 10.2. Can I display data store data in Tableau as raw data without bar charts etc. and can you show examples Yes, you can. Just drag and drop required dimensions to rows or columns. You can use 'text tables' viz in 'Show me' hint. My data is in multiple data stores can Tableau handle joining it Yes, it can. Depending on Tableau Desktop version multiple data sources could be blended (you will have separate data source in Tableau for each data store) or joined (all data stores will be 'combined' in one Tableau data source). As a CT user, can I save my filters for each dashboard You can save filters, if CT version is less then 8.4.4. Since 8.4.4 for providing better dashboard performance ability to save filters personally was removed. Can I separate dashboard views by role management In Control Tower there is no possibility to separate different dashboards according to the roles. For now only possible to change CT user role permissions regarding dashboards: \"Manage Dashboard\" provides ability to choose dashboards for displaying on Dashboard page. \"View Dashboard\" determines the presence of Dashboard page. "},{"version":"10.0","date":"Oct-16-2019","title":"analytics","name":"Analytics","fullPath":"iac/core/control-tower/analytics/analytics","content":" WorkFusion Analytics provides a view on the performance and health of your business processes. Analytics is based on a data visualization tool that builds dashboards and provides visual graphical analysis and filtering control on dashboards. From high level components standpoint WorkFusion Analytics consists of: Server – dashboards are published there and where Control Tower takes them to display at Dashboard Page. Desktop – helps to create new dashboards within self service BI and customize out of the box dashboards. To get more info Analytics Components and Licensing. Dashboard is a reporting mechanism that aggregates and displays metrics and key performance indicators (KPIs). Dashboards help improve decision making by revealing and communicating in context insight into business performance. A dashboard consists of one or more charts, tables, and numeric metrics. Dashboards are displayed at main page of Control Tower Dashboard Page. In addition to out of the box Dashboards, WorkFusion Intelligent Automation Cloud team or WorkFusion partners can also provide on demand. WorkFusion also provides training for customer's users which teaches how to create additional Custom Dashboards. Refer to \"Business Intelligence Analyst\" course in WorkFusion Automation Academy https: automationacademy.com. If you have any questions regarding working with analytics in WorkFusion Intelligent Automation Cloud or any other topics you see Analytics FAQ and can reach out to us on the WorkFusion Forum https: forum.workfusion.com. "},{"version":"10.0","date":"Oct-14-2019","title":"automl","name":"AutoML","fullPath":"iac/core/control-tower/analytics/automl","content":" The AutoML dashboard provides an insight into the following data: AutoML tasks performance failed documents during AutoML processing forecast for AutoML in production statistics of AutoML fields processing AutoML dashboard presents the following key metrics: documents processed by AutoML models total Manual Efforts spent on document processing Manual Efforts saved per process Straight Through Processing (STP): the proportion of documents processed by bots without manual processing Filters The AutoML dashboard has the following filters: Process Name: all Process Names are selected by default. Process Execution allows to select a specific process execution. The filter row represents the following two values combined: Process Execution Start Date UUID of Process Execution Time Range: users can filter a time range based on the Process Execution Start Date. The data is displayed for the last seven days by default, and can be displayed for up to the last 30 days. Process Status can be one of the following: Completed (for completed or stopped process executions). Processing Paused Draft and deleted process executions are excluded from the dashboards. Model Type can be Extraction or Classification. Field Group can have the following values: not in a group: a field is not a part of any group (e.g. 'order') Field Name: all Field Names are selected by default. Show Values. Filter fields to show the following answers: Correct only Incorrect only Result Type. Filter fields by the following result types: Extracted with Errors Failed to Extract Correctly Extracted Should not be Extracted Confidence is a variable representing estimated accuracy for the given threshold returned by model (liblinear, Vowpal Wabbit). Charts STP (Straight Through Processing) This chart provides data on the Straight Through Processing: the rate of documents processed entirely by bots without manual effort. STP rate is calculated the following way: STP Rate = (1 DPM DO) * 100, where: DPM — Documents processed manually, both fully manual and bot assisted processing DO — Documents overall {width=\"600\"} Volume The Volume chart shows how many documents were processed daily. {width=\"600\"} The processed volume is split by the execution type: bot assisted: fields were partly extracted by a model automated: a document was fully processed by a model fully manual: a document was handled manually without a model. Manual Work Reduction Manual Work Reduction chart shows how much time was saved in relation to the baseline. {width=\"600\"} It's calculated based on the related metrics: Manual Efforts (Hours): real time spent on performing a task. The time spent manually extracting the fields the model didn't extract. Baseline Hours: total time required to complete a task if all fields were to be extracted manually. The manual efforts for baseline hours are calculated based on the historical data in the system and the number of processed documents. It's time spent on fully manual or bot assisted processing of documents. In case there are only fully automated documents, manual work reduction is 100%. Saved Hours: the difference between Baseline Hours and Manual Efforts. Manual Efforts This chart shows the total amount of manual efforts spent on the document processing, both fully manual and bot assisted processing. Manual Effort (in hours) equals to the real time spent to perform a task: the time spent manually extracting the fields the model didn't extract. Manual work reduction is a relative representation of saved time in hours. {width=\"600\"} AutoML Production Performance Forecast Show how AutoML Production Performance Forecast works {width=\"1234\" height=\"344\"} The area above the Score filter shows the number of fields extracted with a corresponding score. The AutoML Production Performance Forecast presents information about model performance and quality results. It consists of the following areas: Quality description metrics Overall automation rate You can filter quality description metrics using Score filter and Accuracy Threshold parameter. The area above Score filter shows the number of fields extracted with a corresponding score. Quality description metrics are as follows: Not Learned—a ratio of fields that should be extracted but were not extracted by a model. Low Confidence—a ratio of fields that should not be extracted but were weren't extracted with a model score lower than the selected Accuracy Threshold. Mistake—a ratio of fields that should not be extracted but were weren't extracted with a model score equal or greater than the selected Accuracy Threshold. Correct—a ratio of the correctly extracted fields. Automation Rate chart has the following metrics: Automated—a ratio of documents that were successfully processed by a model, with or without the human assistance. Fully Manual—a ratio of documents that were processed by humans. Chart Explainer contains metrics description. AutoML Statistics by Field The AutoML Statistics by Field chart shows the model quality results per each field over all the documents. {width=\"800\"} Set the model type via the Model Type filter: Classification or Extraction. {width=\"300\" height=\"141\"} For Extration (Information Extraction) model type, the Group by column displays the name of the group if a field contains a group of answers. For Classification, the Group by column displays gold values. The Documents column shows the number of documents that contains a displayed field. Model Metrics Automation Accuracy Information Extraction For Information Extraction, Automation Accuracy equals Precision in data science terminology. It reflects how precise the extraction is, and hence is calculated as the ratio of correctly extracted objects, i.e., TP (meaning True Positive), to all extracted objects, i.e., TP FP (meaning False Positive). {.image center width=\"700\"} Classification For Classification problems, Automation Accuracy equals Accuracy in data science terminology. It reflects how accurate the classification is, and hence is calculated as the ratio of correctly classified objects to all classified objects. {.image center width=\"550\"} Automation Rate In data science terminology: recall. Automation Rate reflects how many correct results the Machine found from ALL the correct results (mean Gold). Automation Rate is calculated as the ratio of correctly classified or extracted objects (Correct or TP) to all the objects of this type existing in the data set (Gold or TP FN). {.image center width=\"550\"} Rework If Automation Accuracy is 100%, Automation Rate would reflect Automation Efficiency. However, Machine does make errors that need to be corrected by Person. Rework reflects the amount of effort required from Person to correct Machine Errors. This metric is calculated as the ratio of the values that were extracted incorrectly (Precision mistakes or FP) to all the correct values that are present in the data set (Gold). {.image center width=\"550\"} Rework may be more than 100% in case of too many FPs. Automation Efficiency Automation Efficiency indicates the total amount of work that was automated by Machine and is calculated as the amount of useful work done by the Machine, i.e., Automation Rate, minus the amount of errors made by the Machine, i.e., Rework: Automation Efficiency may be negative, in case Rework is more than Automation Rate. In a general case, the Business Process contains multiple steps and each step requires multiple object types to be extracted classified. Efficiency for the whole automation step or the whole BP can be calculated as the ratio of completed effective work (the sum of useful work minus the rework at each sub step field document type) to all the work that should have been automated (the sum of Gold values at each sub step field document type). Model Results Not Learned—a ratio of fields that should be extracted but were not extracted by a model. Low Confidence—a ratio of fields that should not be extracted but were weren't extracted with a model score lower than the selected Accuracy Threshold. Mistake—a ratio of fields that should not be extracted but were weren't extracted with a model score equal or greater than the selected Accuracy Threshold. Correct—a ratio of the correctly extracted fields. Chart Explainer has metrics description. Note: quality metrics are only available if gold data is used. Gold vs Extracted Statistics The Gold vs Extracted Statistics table shows detailed extraction classification information per field and has an additional filter panel. {width=\"800\"} You can compare gold and extracted data for each field and see the aggregated number of documents where the exact pair of gold and extracted value appears. Confidence is a variable representing estimated accuracy for the given threshold returned by model. Result type helps to filter fields and their values to understand what kind of post processing is required in a particular case. Result type is decided according to the following conditions: if sum(FP FN) >0 or sum(FP) >0, then 'Extracted with Errors' if sum(FN) >0, then 'Failed to Extract' if sum(TP) >0, then 'Correctly Extracted' otherwise the result type is 'Should not be Extracted'. Expired Tasks list displays the number of expired tasks and the percentage from total number of tasks. Expired Tasks are not included in the charts calculations above. Additional filters are added to make interaction with this table more convenient."},{"version":"10.0","date":"Oct-16-2019","title":"capacity","name":"Capacity","fullPath":"iac/core/control-tower/analytics/capacity","content":" Capacity dashboard provides quick and comprehensive overview of Intelligent Automation Cloud components utilization. It helps to understand how the infrastructure resources are utilized, showing CPU, Memory and Disk utilization over time. This helps to determine is there are enough resources to dynamically scale to process peaks in Intelligent Automation Cloud workload. Bot Execution Platform Agent charts visualize how workers with different profiles (Control Tower and AutoML) share CPU and Memory resources. Filters Component Available components are as follows: Control Tower Bot Execution Platform Master Bot Execution Platform Agent RPA Server OCR Server MS SQL Database Server Analytics Server Integration Services Server Host Contains host names for all components. Time Range Users can filter time range based on Process Execution Start Date. The data is displayed for the last seven days by default, and can be displayed for up to the last 30 days. CPU Utilization CPU Utilization is tracked every minute for each component installed. See the example dashboard below: {width=\"800\"} Red means that CPU utilization has reached 100%. Gray means that CPU utilization is below 100%. Memory Utilization Memory Utilization is tracked every minute for each component installed. See the example dashboard below: {width=\"800\"} Gray indicates the amount of Memory Used. Light Gray indicates the amount of Memory Total Disk Utilization Disk Utilization (the percentage of time the disk processes read or write requests) is tracked every minute for each component installed. See the example dashboard below: {width=\"800\"} Bot Execution Platform Agent {width=\"800\"} Worker profiles are marked with colors. Tooltips show the following metrics: number of cores number of reserved cores memory reserved out of total worker profile report date "},{"version":"10.0","date":"Oct-16-2019","title":"dashboards","name":"Dashboards","fullPath":"iac/core/control-tower/analytics/dashboards","content":" Reporting API The new reporting API represents operational metrics of Control Tower, Workspace, AutoML, RPA, Bot Execution and Infrastructure instead of the raw data flowing through the system. You can now leverage automation metrics without spending additional efforts on their calculations via embedded dashboards or connecting with your own BI tools. Reporting API empowers you with a data language to describe automation processes and help you track progress as well as find bottlenecks. Capacity Dashboard The new Capacity dashboard provides a quick and comprehensive overview of system components utilization for business operations in a way that helps show how infrastructure resources are utilized. There are four charts explaining resource utilization data: CPU, Memory and Disk utilization over time answer whether there are enough resources to dynamically scale depending on the workload. Bot Execution Platform Agent chart visualizes how workers with different profiles (Control Tower and AutoML) share CPU and Memory resources. {width=\"600\"} RPA Dashboard RPA dashboard provides an overview of bot accuracy, performance, and utilization, thus suggesting ways to improve at the process or bot levels. Here are the key metrics: Volume, RPA Accuracy Rate, Bot Availability, Fleets Utilization, Bot Units Workload. The following Bot Units Comparison chart, for example, shows the average execution time of a number of Bot Units that have been executed during 200 seconds with the average waiting time of up to 3 seconds for each of them, with cluster utilization being close to maximum. {width=\"600\"} For a full list of dashboards with descriptions, see the Dashboards page. For a full list of Analytics terms, refer to Analytics glossary (https: kb.workfusion.com display WF Glossary Glossary Analytics). (list of dashboards goes here) "},{"version":"10.0","date":"Oct-14-2019","title":"manual","name":"Manual","fullPath":"iac/core/control-tower/analytics/manual","content":" Manual dashboard gives you the ability to see the human tasks processing information within WorkFusion SPA. Find the worker who executed tasks of the particular business process, see the volume of tasks processed during period of time by a person. Check how many people took the part in the business process and particular task. {width=\"699\"} 1 Tasks 2 Workers 3 Workers Details A tooltip is displayed on mouse hover over an element (point or bar). It contains additional information about a specific element. Show Field Tracking in tooltip means * Transaction Tracking (https: kb.workfusion.com display WF Transactions Tracking).* There are 3 values: Activated For all Process executions transaction on this day transaction tracking is enabled. Transactions count by documents. Not Activated Transaction tracking NOT enabled for all process executions on this day. Transactions count by input records automatically. *means multiple values. Only in some process executions transaction tracking is enabled. In the others fields, also If there is more than one possible value, it returns the ( *) in tooltip (e.g. SLA values). Filters How filters works Select the data that will be displayed on the dashboard. Users can interact with filters the following way: Make a different selection in the checkboxes. {height=\"201\"} Uncheck the All checkbox and start typing (e.g. process name) {height=\"150\"} Process name Filter values by name of process. Default value check all processes. {height=\"200\"} Process Execution Allows to choose specific process execution. The row in a filter is combined by 2 values: Process Execution Start Date. UUID of Process Execution. {width=\"320\"} Time range Users can filter time range (Process Execution Start Date). Default value 7 last days. Dashboards have data for last 30 days. {height=\"150\"} Process Status There are 3 types of process status: Completed Processing Paused {height=\"137\"} Task Name Users can filter all tasks. {height=\"152\"} Actions on charts allow to filter data on dashboard by single clicking on the element. Show {width=\"640\" height=\"441\"} Tasks {width=\"450\"} Scatter plot **shows each task in 2 dimensions ** X axis number of human workers which worked on this task Y axis average Cycle Time in minutes Tooltip: Process Name Task name Cycle time in special format hh:mm:ss Volume Workers {width=\"450\"} Scatterplot **shows each worker in 2 dimensions ** X axis number of completed tasks which this worker worked on Y axis average Cycle Time in minutes Tooltip: Worker Name Cycle time in special format hh:mm:ss Volume Workers Details {width=\"900\"} Table show details information: Worker name Process name Task name Daily Effort Cycle time Assignments "},{"version":"10.0","date":"Oct-14-2019","title":"process","name":"Process","fullPath":"iac/core/control-tower/analytics/process","content":" Process dashboard allows for the in depth drilldown into the following entities: process execution volume by status quantity of completed and processing transactions SLA Violation slices by time volume and number or processes without set SLA process executions with issues {width=\"800\"} 1 Process Execution 2 Transactions 3 Issue Contribution 4 SLA A tooltip is displayed on mouse hover over an element (point or bar). It contains additional information about a specific element. Show Field Tracking in tooltip means * Transaction Tracking (https: kb.workfusion.com display WF Transactions Tracking).* There are 3 values: Activated For all Process executions transaction on this day transaction tracking is enabled. Transactions count by documents. Not Activated Transaction tracking NOT enabled for all process executions on this day. Transactions count by input records automatically. *means multiple values. Only in some process executions transaction tracking is enabled. In the others fields, also If there is more than one possible value, it returns the ( *) in tooltip (e.g. SLA values). Filters How filters works Select the data that will be displayed on the dashboard. Users can interact with filters the following way: Make a different selection in the checkboxes. {height=\"201\"} Uncheck the All checkbox and start typing (e.g. process name) {height=\"150\"} Process name Filter values by name of process. Default value check all processes. {height=\"200\"} Process Execution Allows to choose specific process execution. The row in a filter is combined by 2 values: Process Execution Start Date. UUID of Process Execution. {width=\"320\"} Time range Users can filter time range (Process Execution Start Date). Default value 7 last days. Dashboards have data for last 30 days. {height=\"150\"} Process Status There are 3 types of process status: Completed Processing Paused {height=\"137\"} Task Name Users can filter all tasks. {width=\"350\"} Process Execution {width=\"350\"} Chart shows number of Process Executions by process status. Red color shows the number of Process Executions with issues. X axis number of Process Executions Y axis Process Statuses Tooltip: Process Status Number of Process Executions Transaction tracking Transactions {width=\"350\"} Chart shows number of Transactions by transaction status. Red color shows the number of Transactions with issues. X axis number of Transactions Y axis Process Statuses Tooltip: Process Status Issues Number of transactions Transaction tracking Issue Contribution {width=\"600\"} Chart shows tasks with issues. X axis number of Issues. Y axis list of Tasks in descending order. Tooltip has information: Process Name Task Name Number of issues Transactions tracking SLA {width=\"600\"} Chart shows number of transactions within SLA and SLA violations on a daily basis. Color means: gray transactions within SLA (met SLA) red SLA violations (missed SLA) SLA (Service Level Agreement) is used to define whether transactions processing meets business expectations. There are 3 possible SLA settings: SLA by Time Cycle time of a transaction must be lower than X seconds. If a transaction cycle time is less than or equal to SLA value than a transaction meets SLA, else missed. SLA by Volume Volume of transactions must be higher than X transactions per day. If volume of transactions for a day is greater than or equal to SLA value, then all transactions meet SLA else all of transactions below SLA value are marked as missed m SLA. No SLA set all transactions considered to meet SLA by default. X axis Date (Process Execution Start Date). Y axis divided into 3 part by setting SLA. Axis shows number of transactions in this day. Tooltip contains addition information Date (Process Execution Start Day). Number of SLA violations or Transactions within SLA (depends on mouse hover of an element). SLA value: seconds for SLA by Time number of transactions for SLA by Volume null for \"No SLA set\" Average Cycle time for aggregated transactions in this element Transactions tracking"},{"version":"10.0","date":"Oct-16-2019","title":"reporting-api","name":"Reporting API","fullPath":"iac/core/control-tower/analytics/reporting-api","content":" Reporting API represents operational metrics of Control Tower, Workspace, AutoML, RPA, Bot Execution and Infrastructure. You can leverage automation metrics of Intelligent Automation Cloud without additional efforts on their calculations via embedded dashboards or connecting with your own BI tools. This gives you the data language to describe automation processes in order to track progress and find bottlenecks for further improvement. Using Reporting API This section explains how to connect to Reporting API with WF Analytics Desktop. This requires a license, for more details refer to Analytics Components and Licensing. Connection Details Microsoft SQL Server Database is used for connection. Default port is 1433. If you use a non default port it should be pointed out using the , format. Mind the following pre requisites: Database name (default: workfusion). Schema name (default: dm). SQL credentials for connection. Username is rapi. Connection Example Start WF Analytics Desktop. In the Connect section, click More..., and then select Microsoft SQL Server. Enter the Server you want to connect to and the default Database name. Enter your Username and Password. Select Require SSL, if needed. Optional: Select Initial SQL to specify an SQL command to run at the beginning of every connection. For example, when you open a workbook, refresh an extract, sign in to WF Analytics Server, or publish to WF Analytics Server. Click Sign In. On the data source page, do the following: Select Table View or Stored Procedure. Use the text box to search for a certain name if needed. You can use several Tables Views Stored Procedures and SQL queries as well. Drag the Table or Stored Procedure to the canvas, and then select the Sheet tab to start your analysis. Optional: Initial SQL may be changed or added. Optional: You can also add a New Data Source. Reporting API Reference Area Table Order Field Description Value AutoML automldocument 1 Id Primary key for a table. Auto generated integer number. AutoML automldocument 2 Process Execution Id Foreign key to process execution table. AutoML automldocument 3 Execution Type Shows how a document was processed. bot assisted means that not all fields were extracted by a model. fully automated means that a document was processed by a model only. fully manual means that a document was handled manually without a model. expired means that a manual task for information extraction is expired and this documents should be excluded from calculations. AutoML automldocument 4 Is Automated Shows if all fields in a document are extracted by model only. Value: 0 no, 1 yes. AutoML automldocument 5 Gold This field contains the number of fields that should be extracted (sum of FP and TN). Value: integer. AutoML automldocument 6 Extracted This field contains the number of fields that are extracted (sum of TP and FP). Value: integer. AutoML automldocument 7 ML Processing Time Shows the processing time of the automation extraction plugin (post to ML service ML extract etc). Value: milliseconds. AutoML automldocument 8 Manual Processing Time Shows the manual processing time of a document. Value: milliseconds. AutoML automlfield 1 Id Primary key for a table. Auto generated integer number. AutoML automlfield 2 Document Id Foreign key to document table. AutoML automlfield 3 Group Name Field group contains several fields (the equivalent of group answers). Value: can be 'null'. AutoML automlfield 4 Name Field name an element of a document to be processed by AutoML model. AutoML automlfield 5 Gold Value Gold value' contains a correct value of a field. This value is provided in advance or during AutoQC. Value: can be 'null'. AutoML automlfield 6 Extracted Value Extracted Value' means a correct value of a field. Value: can be 'null'. AutoML automlfield 7 TP True Positive (correct). Value should be extracted, and it is extracted by the model. Value: 0 or 1. AutoML automlfield 8 TN True Negative (correct). Value should not be extracted and nothing has been extracted by the model. Value: 0 or 1. AutoML automlfield 9 FP False Positive (mistake). No value should be extracted, but the model has extracted something. Value: 0 or 1. AutoML automlfield 10 FN False Negative (mistake). Value should be extracted but the model has extracted nothing. Value: 0 or 1. AutoML automlfield 11 Confidence Confidence is a dependent variable representing estimated accuracy for the given threshold returned by model. Value: 0 or 1. AutoML automlfield 12 Score Score is normalized score, an independent variable which represents threshold for model confidence score for a given field rounded to the nth decimal point , returned by model. Value: 0 or 1. AutoML automlfield 13 Result Type This field stores the result of a field extraction which is calculated based on the next conditions if sum of (FP FN) is greater then 0 'Extracted with Errors', if sum(FP)&gt;0 then 'Extracted with Errors' if sum(FN)&gt;0 then 'Failed to Extract' if sum(TP)&gt;0 then 'Correctly Extracted' otherwise result type is 'Should not be Extracted'. Value: Extracted with Errors, Extracted with Errors, Failed to Extract, Correctly Extracted, Should not be Extracted AutoML automlfield 14 Report Date This field is populated when a record containing data for AutoML processing went through 'Analytics' step in a process. Format: YYYY mm dd hh:mm:ss. AutoML automlfield 2 Process Id Foreign key to process table. Bot Execution Platform bepagent 1 Id Primary key for a table. Auto generated integer number. Bot Execution Platform bepagent 2 Client Appid This field provides what client application uses a BEP worker. &lt;Control Tower host&gt;, AutoML model service. Bot Execution Platform bepagent 3 BEP Agent Host Name IP address of a BEP Agent. Bot Execution Platform bepagent 4 Worker Id BEP worker ID. text Bot Execution Platform bepagent 5 Worker Profile Control Tower, AutoML execution, AutoML training Bot Execution Platform bepagent 6 CPU Consumed Each BEP worker reserves CPU for execution. Bot Execution Platform bepagent 7 Memory Consumed Each BEP worker reserves Memory for execution. bytes Bot Execution Platform bepagent 8 Report Date This field is populated automatically every minute to store latest value for BEP workers metrics. Format: YYYY mm dd hh:mm:ss. Bot Execution Platform bepmetric 1 Id Auto generated integer number. Bot Execution Platform bepmetric 2 BEP Task Id Primary key for a table. Auto generated integer number. Can be null if a BEP task comes from AutoML. Bot Execution Platform bepmetric 3 Task Id Foreign key to task table. Bot Execution Platform bepmetric 4 Client Application Control Tower, AutoML. Bot Execution Platform bepmetric 5 Input Queue Date Timestamp when a task message got into Rabbit MQ. Bot Execution Platform bepmetric 6 Worker Queue Date Timestamp when a task message moved from Rabbit MQ to a worker queue. Bot Execution Platform bepmetric 7 Worker Processing Start Date Timestamp when a task message started processing by a worker, it's the same moment when a task message leaves a worker queue. Bot Execution Platform bepmetric 8 Worker Processing End Date Timestamp when a task message completed processing by a worker, it's the same moment when a task message gets into a result queue. Bot Execution Platform bepmetric 9 Result Received Date Timestamp when a task message result was processed by client application. Bot Execution Platform bepmetric 10 Input Queue Wait Time Msec Time spent in Rabbit MQ. Bot Execution Platform bepmetric 11 Worker Queue Wait Time Msec Time spent in a worker queue. Bot Execution Platform bepmetric 12 Worker Processing Time Msec Time spent on processing a task message by a worker. RPA botunit 1 Id Primary key for a table. Auto generated integer number. RPA botunit 2 Bot Relay Port RPA bot relays perform work orchestration. Bot relay names consist of &lt;RPA Instance name&gt;:&lt;Port name&gt;. RPA botunit 3 Bot Port RPA Bot Port. RPA botunit 4 Bot Agent Port RPA Bot Agent Port. RPA botunit 6 Host RPA bot relay hostname. RPA botunit 7 Fleet A group of bot relays. Required for proper bot tasks distribution. RPA botunit 5 Worker Port RPA Worker Port. RPA botunitsession 1 Id Primary key of a table. Auto generated integer number. RPA botunitsession 2 Task Id Foreign key of a task table. RPA botunitsession 3 Bot Unit Id Foreign key of a bot unit table. RPA botunitsession 4 Start Date This field is populated when a user or a schedule runs a process and the first step is started. Format: YYYY mm dd hh:mm:ss. RPA botunitsession 5 End Date This field is populated when a process was successfully completed or a process was stopped. 'Null' value is valid in case a process is still in progress or paused. Format: YYYY mm dd hh:mm:ss. RPA botunitsession 6 Status Shows if a process is available for running or deleted. active, deleted. RPA botunitsession 7 Driver Type Shows the type of a used driver like chrome, firefox, desktop, universal, etc. RPA botunitsession 8 Error Message Contains the description of the reason why a session fails if it's not successful, otherwise it's 'null'. RPA botunitsession 9 Wait Time Msec Shows how long does a session wait to be processed by bot. ms. RPA botunitsession 10 Processing Time Msec Shows how long does it take to process a session by bot. ms. RPA botunitsession 11 Total Time Msec This field is calculated as the sum of processing and wait time. ms. RPA botunitstatus 1 Id Primary key for a table. Auto generated integer number. RPA botunitstatus 2 Bot Unit Id Foreign key to bot unit table RPA botunitstatus 3 Bot Unit Status This field is populated when bot status changes. active, idle, unavailable RPA botunitstatus 4 Date from This field is populated when bot status changes. Format: YYYY mm dd hh:mm:ss RPA botunitstatus 5 Date to This field is populated when bot status changes. Format: YYYY mm dd hh:mm:ss RPA botunitstatus 6 Status Duration This field is calculated based on status start date and end date. seconds. Infrastructure component 1 Id Primary key for a table. Auto generated integer number. Infrastructure component 2 Name SPA components names. App, OCR, BEP Master, BEP Agent, Bot Relay. Infrastructure component 3 Work Type Defines what type of work a component supports, where 'Tower' work type includes all machine tasks, not related to RPA, OCR. RPA, OCR, Control Tower, BEP. tbd Infrastructure component 4 Description Additional information about Intelligent Automation Cloud component (for example, Application Server is Control Tower for 'App'). Infrastructure componentmetric 1 Id Primary key for a table. Auto generated integer number. Infrastructure componentmetric 2 Component id Foreign key to component table. Infrastructure componentmetric 3 Host Component host name Infrastructure componentmetric 4 CPU Shows the burden on a processor across all cores for a given moment (every minute) of time in terms of percentage that indicates if any changes are to be made in the system otherwise it may get exhausted of capacity. from 0 to 100%. Infrastructure componentmetric 5 Cores Shows the number of cores for Intelligent Automation Cloud components. Infrastructure componentmetric 6 Disk Percentage of time the disk is processing read or write requests for a given moment of time (every minute). Infrastructure componentmetric 7 Memory Available Memory available for a given moment of time (every minute). GB Infrastructure componentmetric 8 Memory Total Memory total for a given moment of time (every minute). GB Infrastructure componentmetric 9 Report Date This field is populated automatically every minute to store latest value for component metrics. Format: YYYY mm dd hh:mm:ss Control Tower process 1 Id Primary key for a table. Auto generated integer number. Control Tower process 2 Name Process name, also known as business process name, defines the flow of manual and bot tasks. Control Tower process 3 Status Shows if a process is available for running or deleted. active, deleted. Control Tower process 4 Execution Type Shows if a process is a composition of tasks or it's a single task out of a process. single, composite. Control Tower process 5 Creation Date Shows when a process was created. Format: YYYY mm dd hh:mm:ss Control Tower process 6 Last Modified Date Shows when a process was modified. Updated every time after 'Save' button is clicked. Format: YYYY mm dd hh:mm:ss Control Tower processexecution 1 Id Primary key for a table. Auto generated integer number. Control Tower processexecution 2 Process Id Foreign key to process table. Control Tower processexecution 3 UUID UUID universal unique ID for a process execution, https: &lt;Control Tower instance name&gt; workfusion secure business process edit &lt;UUID&gt; helps to find a given process execution from Control Tower UI. Control Tower processexecution 4 Start Date This filed is populated when a user or a schedule runs a process and the first step is started. Format: YYYY mm dd hh:mm:ss Control Tower processexecution 5 End Date This field is populated when a process was successfully completed or a process was stopped. 'Null' value is valid in case a process is still in progress or paused. Format: YYYY mm dd hh:mm:ss Control Tower processexecution 6 Status This field is updated according to a process execution status changes, deleted process executions are not stored in this table, for all statuses except completed the corresponding process execution end date is 'null'. completed, paused, processing. Control Tower processexecution 7 Tracking Activated Shows that transactions are tracked that helps to identify business entities within a process at a certain step of a business process. Activated, Not activated. Control Tower processexecution 8 SLA Type Type of service level agreement (SLA), applied at the level of a process execution. time, volume. Control Tower processexecution 9 SLA Value Value for service level agreement (SLA). seconds or number of transactions. Control Tower processexecution 10 Issues Shows if a process execution has issues, that means at least one record in a process has exhausted the maximal number of retries and failed. with issues, without issues. RPA processexecutionbotutilization 1 Process Execution Id Foreign key to processexecution table RPA processexecutionbotutilization 2 Report Date This filed is populated automatically on an hourly basis. Format: YYYY mm dd hh:mm:ss. RPA processexecutionbotutilization 3 Utilization Rate Shows the percentage of bot active time relatively to available bot time. Bots availability is calculated per 24 7. from 0 to 100%. RPA processexecutionbotutilization 4 Active Time Shows the duration of a period when bots have active sessions for a given process execution. seconds. RPA processexecutionbotutilization 5 Bot Relay Port RPA bot relays perform work orchestration. Bot relay names consist of &lt;RPA Instance name&gt;:&lt;Port name&gt;. AutoML processexecutionml 2 Baseline Hours Shows total expected time to perform a task as if all fields were extracted manually. hours. AutoML processexecutionml 3 Saved Hours Shows the difference between expected spent time and real manual effort. hours. AutoML processexecutionml 4 Manual Effort Shows real spent time to perform a task: how long did it take to extracted fields manually which a model didn't extract. hours. AutoML processexecutionml 5 Manual Work Reduction Shows how much time AutoML models save relatively to expected spent time. from 0 to 100%. AutoML processexecutionml 6 STP Rate Shows the rate of documents processed by bots only. from 0 to 100%. AutoML processexecutionml Process Execution Id Foreign key to processexecution table Control Tower task 1 Id Primary key for a table. Auto generated integer number. Control Tower task 2 Name Task Name, a step of a process Control Tower task 3 UUID UUID universal unique ID for a task execution, https: &lt;Control Tower instance name&gt; workfusion secure business process edit &lt;UUID&gt; subTaskUuid=&lt;taskUUID&gt; helps to find a given task execution from Contol Tower UI Control Tower task 4 Process Execution Id Foreign key to processexecution table Control Tower task 5 Task Position Shows the position of a task in a process. integer. Control Tower task 6 Task Type Shows if a task is in a process or out of a process, like single Manual tasks. single task, task in process. Control Tower task 7 Start Date This field is populated when at least one record reaches a task in a process. Format: YYYY mm dd hh:mm:ss Control Tower task 8 End Date This field is populated when a task was successfully completed or a process was stopped. 'Null' value is valid in case a task is still in progress or paused. Format: YYYY mm dd hh:mm:ss Control Tower task 9 Status This field is updated according to a task status changes, deleted and draft tasks are not stored in this table, for all statuses except completed the corresponding task end date is 'null'. completed, bot processing, paused, processing. Control Tower task 10 In Out of Tracking Shows if a task is included in a tracking of a transaction. in tracking, out of tracking. Control Tower task 11 Excluded from STP Shows if it's needed to exclude this task from STP calculation, usually used for mandatory manual tasks. 0 no, 1 yes. Control Tower task 12 Execution Type Shows a type of a task. bot, manual. Control Tower task 13 Stateless Execution Shows if stateless execution is enabled. In this case no operational data for a task is stored in a database. Data is stored only for the first marked task and for Final Result. 0 no, 1 yes. Control Tower task 14 Has Processing Issues Shows if a task execution has issues, that means at least one record in a process has exhausted the maximal number of retries and failed. 0 no, 1 yes. Control Tower task UUID Process execution UUID. Control Tower transactione2e 1 Id Primary key for a table. Auto generated integer number. Control Tower transactione2e 4 Creation Date This field is populated in 2 cases: 1) when transaction tracking is activated then this field shows when tracking of a transaction started, 2) when transaction tracking is not activated then this field shows when a transaction defined as a record from input file reaches the first task. Format: YYYY mm dd hh:mm:ss Control Tower transactione2e 5 Completion Date This field is populated when a transaction was successfully completed or a process was stopped. 'Null' value is valid in case a transaction is still in progress. Format: YYYY mm dd hh:mm:ss Control Tower transactione2e 6 Status This field is calculated basing on 'start date' and 'end date', while 'end date' is 'null' a status is marked as 'processing'. completed or processing. Control Tower transactione2e 7 SLA result Shows if a transaction is within SLA or violates SLA. 'Within SLA' or 'SLA violation' Control Tower transactione2e 8 STP Shows if a transaction goes straight through process, without any manual touch. 0 no, 1 yes. Control Tower transactione2e 9 Retries Shows the number of plugin attempts to process a transaction. integer. Control Tower transactione2e 10 Processing Time Shows processing time for a transaction during a process execution, end to end. seconds. Control Tower transactione2e 11 Wait Time Shows overhead time for a transaction during a process execution, end to end. seconds. Control Tower transactione2e 12 Transaction Cycle Time Shows end to end processing time, the sum of processing and wait time. seconds. Control Tower transactione2e 13 Activated Tracking Shows that transactions are tracked that helps to identify business entities within a process at a certain step of a business process. Activated, Not activated. Control Tower transactionitem 1 Id Primary key for a table. Auto generated integer number. Control Tower transactionitem 2 Transaction E2E Id Foreign key to transaction table. Control Tower transactionitem 3 Task Id Foreign key to task table. Control Tower transactionitem 4 Creation Date This field is populated when a transaction item is created in a process. Format: YYYY mm dd hh:mm:ss. Control Tower transactionitem 5 Start Date This field is populated when a transaction item reaches the next task in a process. Format: YYYY mm dd hh:mm:ss. Control Tower transactionitem 6 End Date This field is populated when a transaction item was successfully completed or a process was stopped. 'Null' value is valid in case a transaction item is still in progress. Format: YYYY mm dd hh:mm:ss. Control Tower transactionitem 7 Completion Date This field is populated when a result of transaction processing is received. Format: YYYY mm dd hh:mm:ss. Control Tower transactionitem 8 Wait Time Shows how long does a transaction item wait to be processed. seconds. Control Tower transactionitem 9 Processing Time Shows how long does it take to process a transaction item. seconds. Control Tower transactionitem 10 Total Time This field is calculated as the sum of processing and wait time. seconds. Control Tower transactionitem 11 Work Type Defines what type of work applied to a transaction item, where 'Tower' work type includes all machine tasks, not related to RPA, OCR (if OCR plugin is used) or AutoML. AutoML, RPA, OCR (if OCR plugin is used), Manual, Tower. Control Tower transactionitem 12 Work Type Count This field shows how many work types were used for processing a transaction. Control Tower transactionitem 13 Worker Id Foreign key to worker table, populated in case transaction item is processed manually, otherwise the value is null. Control Tower transaction_item 14 Retries Shows the number of plugin attempts to process a transaction item. integer. Manual worker 1 Id Primary key of a table. Auto generated integer number. Manual worker 2 UUID Universal unique ID for a worker, also known as a native ID. Manual worker 3 Name First name and last name of a worker. Manual worker 4 Type Shows a type of a worker. bot, manual. Manual worker 5 Nickname Nickname, used instead of Worker name when Worker Name is not filled. Manual worker 6 Email Contains a worker email. can be 'null' Manual worker 7 Country Contains a country name where is a worker from. can be 'null' "},{"version":"10.0","date":"Oct-14-2019","title":"overview-dashboard","name":"Overview","fullPath":"iac/core/control-tower/analytics/overview-dashboard","content":" Overview dashboard is a starting point of the powerful analytics for your automation journey. It allows for a complete picture of a business processing state within Intelligent Automation Cloud. This high level information allows to drill down to a particular process execution details. The Overview dashboard aggregates the following data: business processes transactions volume Service Level Agreement (SLA) violations volume SLA violations percentage Using this data, the Overview dashboard provides the following performance throughput metrics: Volume Speed SLA Filters The Overview dashboard has the following filters: Process Name All Process Names are selected by default. Process Execution Allows to select a specific process execution. The filter row represents the following two values combined: Process Execution Start Date UUID of Process Execution Time Range Users can filter a time range based on the Process Execution Start Date. The data is displayed for the last seven days by default, and can be displayed for up to the last 30 days. Process Status Process Status can be one of the following: Completed (for completed or stopped process executions). Processing Paused Draft and deleted process executions are excluded from the dashboards. Metrics Volume The Volume chart tracks the number of completed transactions for the day these process executions where started. {width=\"600\"} Volume tooltip contains the following data: Process Execution Start Date Number of the completed transactions started on the selected day. Transaction tracking: Activated Not Activated * both options track process executions started on the selected day. Speed The Speed chart provides data on the average Wait and Processing time per completed transaction daily. Wait time means is the time a transaction was in queues. Total Processing time includes both Wait and Processing Time. Speed tooltip contains the following details: Process Execution Start Date Average Processing Time Average Wait Time Service Level Agreement (SLA) Service Level Agreement (SLA) is used to define whether transactions processing meets business expectations. SLA can be set only if the transaction tracking is enabled. For more details, refer to Transactions Tracking (https: kb.workfusion.com display WF Transactions Tracking TransactionsTracking ServiceLevelAgreement). The SLA chart shows the percentage of transactions within SLA in respect to all daily completed transactions. Note: If SLA is not set for process execution, then all transactions are considered to be within SLA by default. {width=\"600\"} Transactions within SLA are highlighted in grey, SLA violations are marked with red. SLA tooltip contains the following details: Process Execution Start Date % of Transactions within SLA or % of SLA violations depending on the bar region you a hovering over. "},{"version":"10.0","date":"Oct-14-2019","title":"rpa","name":"RPA","fullPath":"iac/core/control-tower/analytics/rpa","content":" The RPA dashboard provides an overview for bots accuracy, performance and utilization, gathering insights for bot process level improvements. RPA dashboard presents the following key metrics: Volume Bot availability RPA accuracy rate Tasks comparison Fleets utilization Bot Units workload RPA dashboard filters The RPA dashboard has the following filters: Process Name All Process Names are selected by default. Process Execution Allows to select a specific process execution. The filter row represents the following two values combined: Process Execution Start Date UUID of Process Execution Time Range Users can filter a time range based on the Process Execution Start Date. The data is displayed for the last seven days by default, and can be displayed for up to the last 30 days. Process Status Process Status can be one of the following: Completed (for completed or stopped process executions). Processing Paused Draft and deleted process executions are excluded from the dashboards. Task name: All Task Names are selected by default. Bot Relay Host: All Bot Relay hosts are selected by default. Bot Relay is a proxy that gives Bot its Task to perform. Fleet: All Fleets are selected by default. Learn more about Fleets: Task Distribution (https: kb.workfusion.com display RPA Task Distribution). Bot Session Status: can be either Successful or Failed. Volume Number of RPA sessions is tracked for each day when the process executions where started. {width=\"400\"} Successfully completed sessions are highlighted in gray, failed sessions are marked with red. Bot Availability Uptime Percentage shows the percentage of time when Bot Units were active or idle. Aggregated view is enabled by default. Bot status indication is as follows: Gray — Active Light Gray — Idle Red — Unavailable Detailed view shows bot statuses per Fleet and Bot Relay Host. It's possible to drill down to a separate process as shown in the example below: {width=\"600\"} RPA Accuracy Rate RPA Accuracy Rate shows the percentage of successfully completed sessions for a selected period per Process Name. Aggregated view is enabled by default. It's possible to drill down to a separate process as shown in the example below: Tasks Comparison Tasks Comparison chart compares entities by average execution and wait time (in seconds). Available entity types are as follows: Tasks Processes Fleets Bot Units {width=\"400\"} The circle size represents the number of sessions with a given average execution and wait time. The closer an entity is to the left bottom the better: it means low wait time and fast execution. The closer circles are to each other the more stable execution is, otherwise the sessions may vary by a work accomplished within these sessions, or it is an indication of an execution bottleneck. Switch between entities using a drop down: Fleets Capacity Utilization Fleets Capacity Utilization is calculated based on the percentage of the time Bot Units with a given Fleet were active: {width=\"600\"} Volume per Fleet The Volume per Fleet chart shows the amount of sessions processed by Bot Units with a given Fleet: {height=\"250\"} Bot Unit Sessions The Bot Unit Sessions chart visualizes session flow per bot unit grouped by fleets: {width=\"600\"} Session Status can be Gray Successful or Red Failed. Failed session tooltip will also contain its error message. "},{"version":"10.0","date":"Oct-14-2019","title":"speed","name":"Speed","fullPath":"iac/core/control-tower/analytics/speed","content":" Speed dashboard helps find any bottlenecks of the process executions. Process summary graph makes process execution speed monitoring possible, while also providing a quick view of the SLA violation and cycle time. Breakdown summary will helps to highlight which type of task took the most of processing time, how long it took and the volume being processed within the process execution. Speed dashboard provides detailed information on transaction processing within process execution, work type and task levels. Its main metrics are as follows: Processing Time Wait Time Volume Filters The Speed dashboard has the following filters: Process Name All Process Names are selected by default. Process Execution Allows to select a specific process execution. The filter row represents the following two values combined: Process Execution Start Date UUID of Process Execution Time Range Users can filter a time range based on the Process Execution Start Date. The data is displayed for the last seven days by default, and can be displayed for up to the last 30 days. Process Status Process Status can be one of the following: Completed (for completed or stopped process executions). Processing Paused Draft and deleted process executions are excluded from the dashboards. Task name: All Task Names are selected by default. Task Status. Process Status can be one of the following: Completed (for completed or stopped tasks) Processing (for manually processed tasks) Bot Processing Paused Work Type. Each task uses a specific plugin or requires a human engagement. Available work types are as follows: Person — task is completed by a human worker. Bot Task — machine task is completed by some type of computer automation within RPA module (data parsing, extraction, transformation, loading, data input etc.). OCR Task — tasks that conve imagines into the machine readable formats (e.g. JPG to HTML) completed by Optical Character Recognition module. AutoML — tasks that are performed for automation of the manual tasks by cognitive bots. Tower — other machine tasks done by the WorkFusion Control Tower application required for Process Execution. Process Summary This chart provides data on end to end processing and wait time per transaction along with a number of transactions per process which met or missed SLA. {width=\"800\"} Transactions within SLA are highlighted in grey, SLA violations are marked with red. Breakdown Summary This chart provides data on end to end processing and wait time per transaction along with a number of transactions per work type. {width=\"800\"} Tasks This chart provides data on end to end processing and wait time per transaction along with a number of transactions per task. {width=\"800\"} Task Execution Details This tables provides a detailed distribution on transaction cycle time per task in seconds. All tasks are split into two groups: tasks included in tracking and tasks out а tracking (excluded or in a moment before the tracking begins). {width=\"800\"} "},{"version":"10.0","date":"Oct-16-2019","title":"css-rules-for-answers-and-sub-answers","name":"CSS Rules for Answers and Sub-Answers","fullPath":"iac/core/control-tower/manual-tasks/css-rules-for-answers-and-sub-answers","content":" To customize the Answer style, perform the following actions: While adding editing the Answer, enter a CSS class name in the CSS Rules field. For example, myAnswerClass Save the Answer. In the Code Editor, add a section. Inside the style section, write your CSS code for this class. For example, .myAnswerClass {color:red;} To set a correct Sub Answer behavior and style, perform the following actions: While adding editing the Sub Answer, enter a CSS class name in the CSS Rules field together with expand:on(Parent Answer Option Value). If the Parent Answer Option Value is selected in the Main (Parent) Answer, this Sub Answer will be shown. Otherwise, the Sub Answer is not displayed. For example, mySubAnswerClass* *expand:on(1) Complete steps 2 4 from the previous procedure. The Sub Answer inherits all styles from its Main (Parent) Answer by default. Without the expand:on(Parent Answer Option _Value) in CSS Rules, the Sub Answer is always displayed and is handled like an ordinary Answer. Main Answer (Option 2 is selected) with custom styling Answer with Custom CSS Main Answer (Option 1 is selected) and Sub Answer with custom styling Sub Answer with Custom CSS .myAnswerClass { color: red; } .myAnswerClass .answerInput { color: black; } .myAnswerClass .na check { outline: 2px solid green; } .myAnswerClass .instructions { color: blue; } .mySubAnswerClass { outline: 2px dashed violet; } "},{"version":"10.0","date":"Aug-06-2019","title":"embedding-media-content-video-audio-pdf","name":"Embedding Media Content - Video, Audio, PDF","fullPath":"iac/core/control-tower/manual-tasks/embedding-media-content-video-audio-pdf","content":" PDF For some Tasks, it may make the most sense to create PowerPoint slides or Word documents to provide task's instructions or display the source data. To do this, convert the PowerPoint presentation or Word document to a PDF, then add a PDF viewer via the Code Editor. Code Example HTML5 Video The following example contains an HTML5 video player – Flowplayer (). Code Example HTML5 Audio The following example contains an HTML5 audio player – Audio.js (). Code Example Image Grid You can insert an image or video grid using the Bootstrap Thumbnails component. Code Example "},{"version":"10.0","date":"Oct-16-2019","title":"manual-tasks","name":"Manual Tasks","fullPath":"iac/core/control-tower/manual-tasks/manual-tasks","content":" Prior to reading this material, see the Design a Task topic. To understand and effectively use this topic, we recommend to get familiar with HTML, CSS, JavaScript, FreeMarker languages and Bootstrap and jQuery frameworks. In simple cases use the WYSIWYG editor to design Tasks. To add some extra functionality, open the Code Editor on the Design Task tab and carefully modify the source code: Copy the original code and save it on your local machine or in the cloud. Start making necessary changes. Preview the Task after significant edits. Save the Task or undo the changes (ctrl z in the Editor) depending on the result. You can periodically save your code to a new file or file version on your local machine. See a typical Task code below: Task code example .cc decorate:nth child(even){margin left:5px} .cc decorate{float:left;} .cc input.text{width:60px !important} .thumbnail{display:table !important} .place{padding:0px} .imgblock{width:580px;float:left;} .imgwrap{width:570px;height:180px;background color:lightGrey;text align:center} .imgwrap img{max width:600px;max height:180px} .answrap{overflow: auto;width:460px;height:280px;background:white} Identify an appropriate age and gender combination\"> The goal of this project is to correctly identify an appropriate age and gender combination for each person depicted in the images below. {question.data 'url' } FTL Description Task code is written using FTL (FreeMarker Template Language). A template is a mix of the following sections: Section Example Text html or regular text Interpolations (variables) {question.data 'element_name' } FTL tags Comments See the official FTL documentation here. Macro Templates You can use the include directive to insert another FreeMarker template file (specified by the path parameter) into your template. The output from the included template is inserted at the point where the include tag occurs. The included file shares the variables with the including template, similarly like if it was copy pasted into it. In WorkFusion, all FTL files are managed in Campaigns > Templates > Macro (Type). By default, the following templates are included in the Task code: html.ftl answers.ftl extras.ftl Do not delete the default templates. You can make your custom Macro Templates with .ftl extension and include them in your Task FTL. Directives Directives are instructions to FreeMarker used in FTL templates . You use FTL tags to call directives. Syntactically this is done with two tags: and . This is similar to HTML or XML syntax, except that the tag name starts with . If the directive doesn't have nested content (content between the start tag and the end tag), you must use the start tag with no end tag. There are two types of directives: predefined directives and user defined directives. Predefined Directives The full list of the FTL predefined directives is here. The following directives are often used in Task code: Directive Description Usage description Checking if input data exists: description Repeat the nested code for each item in sequence. This case is useful when a Task has Block Size > 1: description Creating or replacing a variable. In the following example, the feedback is enabled for Workers: User Defined Directives For user defined directives you use @ instead of , for example .... See more info here. The following table lists WorkFusion specific directives. These directives are defined in Macro Templates (html.ftl, answers.ftl, extras.ftl). Directive Description Usage &lt;@hit&gt; This is the main mandatory directive that defines the Task html code. All other directives and text must be written inside the &lt;@hit&gt;&lt; @hit&gt; section. &lt;@instructions&gt; This section contains the following blocks: Title attribute. This attribute defines the Task name and can contain html tags. @editable directive(s). Add this directive to enable a WYSIWYG editing area. Instructions, Instructions as Test, FAQ. These blocks can be added only from UI (see detailed info). Instructions are collapsible and are shown when changed or unread. The macro defining Instructions and FAQ blocks is situated in the html.ftl file. &lt;@instructions title=\"&lt;h2&gt;Identify the people&lt; h2&gt;\"&gt; &lt;@editable&gt; ...WYSIWYG editor... Some html code &lt;@editable&gt; This section adds a TinyMCE WYSIWYG editor to the Task Designer and must contain the unique id attribute. When you create content using the WYSIWYG editor, this content is recorded as html code inside the @editable section. You can insert several @editable sections with different identifiers. &lt;@editable id=\"INSTRUCTIONS\" preview=PREVIEW!false&gt; HTML code automatically created by WYSIWYG editor &lt; @editable&gt; &lt;@form&gt; This directive defines a form with its attributes and inputs. You can set the following attributes: validation. Default value: true formMethod. Default value: \"POST\" action. Default value: \" {mturkExternalSubmit!}\" The @form macro is situated in the html.ftl file and contains some hidden inputs (workerId, questionId, etc.), validation logic, and the form html code. There must be only one &lt;@form&gt; directive, even if the Block Size is more than 1. The &lt;@submit&gt; directive must be inside the &lt;@form&gt;. Expand source &lt;@form formMethod=&quot;POST&quot; action=&quot; {mturkExternalSubmit!}&quot; validate=false&gt; &lt; if questions &gt; &lt;div class=&quot;bwizard&quot;&gt; &lt; list questions as question&gt; &lt;@report question=question includeAll=true &gt; &lt;div class=&quot;questions&quot;&gt; &lt;@editable id=&quot;DATA&quot;&gt; &lt;p&gt;some html and interpolations {questions N .data & 39;elementname& 39; }&lt; p&gt; &lt; @editable&gt; &lt; div&gt; &lt;div class=&quot;answers&quot;&gt; &lt;@answers question=question &gt; &lt; div&gt; &lt; list&gt; &lt; div&gt; &lt; if&gt; &lt;@submit text=&quot;Submit&quot; &gt; &lt; @form&gt; &lt;@report&gt; This directive is intended for including all or some columns from the Input Data file into the Result Data file. includeAll = true. All columns from the Input Data file will be included. include = \"column1, column2\". Only the specified columns will be included. The @report macro is situated in the extras.ftl file. &lt;@report question=question include=\"mediaid,url\" &gt; OR &lt;@report question=\"question\" includeall=\"true\"&gt; &lt;@answers &gt; This directive adds all the Answers created in Task Designer to the form. To add styles for Answers, see the next topic. The @answers macro is situated in the answers .ftl file. &lt;@answers question=\"question\"&gt; &lt;@submit &gt; This directive adds a submit button to the form and triggers the form submission and validation. You can alter the submit button caption by adding the text attribute. &lt;@submit text=\"Submit Answers\"&gt; &lt;@script &gt; This directive is intended for including scripts in the FTL. Use this directive instead of the html &lt;script&gt; tag, as the &lt;@script&gt; directive checks whether this script has already been included before and optimizes the code re usage. &lt;@script src=\" https: yourcdn.js \" &gt; Interpolations (Variables) Interpolations will be replaced with a calculated value in the output and allow you to use unique content from Input Data file in each Worker Task. {question.data 'element _name' }** element name – any column header in the Input Data file. If element name does not correspond to any column header, you must map it on the Upload Data tab. To use interpolations outside the list directive, use the following notation: {questions N .data 'element _name' } where N = (Record in Block Size) 1. :::tip When Block Size > 1, you can display the Record by including the following interpolation: {question _index 1} ::: FreeMarker engine special variables: Including Resource Files You can optimize the Task FTL file size and readability by including: FTL files : Some frameworks and fonts are already included in the html.ftl jQuery, Bootstrap, Font Awesome. To inquire framework version, see the FTL code (Campaigns > Templates > Macro). Scripts: `` CSS: `` :::note Use your CDN links with https: protocol only. Check the HTTP Headers (especially Content Type) for all the resource files. ::: You can also include public available docs with instructions in the code. Upload your docs on Google Drive, Dropbox, Box, Amazon s3, OneDrive, or other cloud storage and share the public access. Make sure that your link has the HTTPS protocol, and the cloud service provides a large and stable bandwidth. "},{"version":"10.0","date":"Aug-06-2019","title":"dynamic-task-distribution-among-crowds","name":"Dynamic Task Distribution among Crowds","fullPath":"iac/core/control-tower/manual-tasks/dynamic-task-distribution-among-crowds","content":" This feature was created to make possible to complete the Tasks in a custom order. It does not matter where Tasks are posted – WorkFusion decides in which order to show Tasks to Worker right after the Task has been accepted. How it works: Initially, WorkFusion does not post specific Tasks to the Crowd. It posts Tasks placeholders. Tasks are stored in WorkFusion in an ordered pool. By default they are ordered by their creation time and priority. Task Distribution Until a Task is accepted, Workers will see the Task placeholder. Usually it is the first Task in the Task pool, but in case if a lot of Workers are working, Task shown before the accepting might be not the same as the Task shown after accepting. When a Worker accepts the Task, WorkFusion will show the first free Task from the ordered pool. This feature allows you to complete the most important Tasks first, change the Task order on the fly, and use custom Tasks comparator independently of target Endpoint. Max Hours per Day and Minimum Commitment in Hours per Day parameters allow you to setup guaranteed minimum and maximum workload for the Crowd. This is useful for private Crowd configuration. You can also use the Working Hours parameter to setup Tasks availability. Workforce is a «Workers pool» which contains Workers from several Crowds. The main parameters available for Workforce configurations are Crowds itself and Crowds Priority. Task Distribution Examples This section contains an overview of several widespread business cases. 1. Complete Tasks as soon as possible We have 2000 Tasks to be done as soon as possible using all available resources. In this case all Crowds in the Workforce should have the same priority: Crowds with the same Priority How the distribution works: If this is the first Task run all Crowds will have no velocity statistics available. Therefore, 2000 Tasks will be posted to all Crowds in equal shares 1000 1000. Note, we are not posting real Tasks – we are posting Task placeholders. Every 2 minutes the distribution engine calculates completed Tasks count, velocity and average time per Task for each Crowd. By default, 7 days period is used for velocity calculation and 30 days period is used for average time per Task calculation. Velocity and average time are calculated per Crowd. It does not matter which Tasks are posted. Now there are several cases: No Tasks were completed at all. No action required. 50 Tasks were completed by Elance Workers and 0 Tasks were completed by MTurk Workers. In this case 50 Tasks will be moved from MTurk to Elance. We are not moving all the Tasks because we want to give a change to MTurk Workers to do the work. As the result we’ll have 1000 Tasks available in Elance and 950 on MTurk. 100 Tasks were completed on Elance and 200 Tasks were completed on MTurk. 1700 Tasks left and distribution engine has to redistribute them. Total velocity for all Crowds is 100 200=300 Tasks in the unit of time. To complete all Tasks in the same time with the maximum Crowd utilization, we should have 100 *1700 300=566.6(rounded to 567) Tasks on Elance and 1700 567=1133 Tasks on MTurk. Therefore, 900 567=333 Tasks will be moved from Elance to MTurk. As the result we have 1133 Tasks available on MTurk and 567 Tasks available on Elance. Let’s say that in the next unit of time only 50 Tasks were completed on MTurk and 350 Tasks were completed on Elance. There are 1300 Tasks left and distribution engine has to redistribute them. Total velocity for all Crowds is 50 350=400 Tasks in the unit of time. To complete all Tasks with the maximum Crowds utilization we should have 350 *1300 400=1137.5 (rounded to 1138) Tasks on Elance and 1300 1138=162 Tasks on MTurk. Engine will remove 1133 162=971 Task from MTurk and post these Tasks to Elance. As the result we’ll have 162 Tasks available on MTurk and 1138 Tasks available on Elance. 2. Complete Tasks in preferable Crowd first, then post to other Crowds We have 2000 Tasks and we want to have as much as possible Tasks to be completed by the most experienced MTurk Workers (US based, 5000 Tasks 98% accuracy). This Crowd should have the highest priority: Crowds with different Priority Initially all Tasks will be posted to the first Crowd. During the next hour no Tasks will be redistributed. After 1 hour, the distribution engine will start handle the first and the second Crowds as they have the same priority. Priority is important during the first 60 minutes only (configurable in the DB). This is like a temporary advantage over other Crowds. In the next hour all 3 Crowds will be handled as described in the 1st case (Complete Tasks as soon as possible). 3. Complete Tasks as soon as possible, but provide a guaranteed workload for private Crowd We have 2000 Tasks (time allotted per Task is 10 minutes) and we have private and public Crowds. The Private Crowd has N Workers which can do 60 hours of work per day and we want to guarantee that the private Crowd will be fully loaded. In this case we should setup Minimum Commitment in Hours per Day parameter in the Crowd. Both Crowds in the Workforce should have the same priority: Guaranteed Crowd Workload How the distribution works: If this is the first Task run, all Crowds have no average Task completion time statistics available. Maximum allotted time for assignment parameter will be used. In this case it is 10 minutes. Distribution algorithm will process Crowds with the Minimum Commitment in Hours per Day parameter configured first. We need to guarantee 100% workload. 60h=3600min. 3600min 10min=360 Tasks. Therefore, distribution algorithm should post at least 360 Tasks to the Private Crowd. We have 2000 Tasks and 2 Crowds. According to the 1 case, 2000Tasks 2Crowds=1000 Tasks per Crowd. 1000 is more than 360, so Tasks will be posted in equal shares 1000 1000. Example 2: we have 500 Tasks only. According to the workload calculation 360 Tasks will be posted to private Crowd, the rest will be posted to public Crowd. Let’s suppose that in 40 minutes 5 Tasks were completed by the Private Crowd. The distribution engine will perform the following actions: Calculate the average time per Task: 40 5=8 minutes per Task. Calculate how many Tasks should be available in the Private Crowd to guarantee full workload. (60h – 40min) 8min per Task = 445 Tasks. Now the Distribution engine will redistribute Tasks so that private Crowd will have 445 Tasks available. "},{"version":"10.0","date":"Oct-16-2019","title":"freemarker-in-manual-tasks","name":"FreeMarker in Manual Tasks","fullPath":"iac/core/control-tower/manual-tasks/freemarker-in-manual-tasks","content":" Start from Key points: Overall Template data model = output The template at a glance Expressions Built in Reference: http: freemarker.org docs ref_builtins.html WorkFusion Template Variables Review Use FreeMarker Test Tool Online FreeMarker Template Tester: http: freemarker online.kenshoo.com Complex Manual Task example Upload files Validation. AJAX calls with jQuery, show results to worker external instructions in iFrame Task Layouts and Embedding Media Content Manual Tasks Task Layout, Styling, and Controls Embedding Media Content Video, Audio, PDF Assignments Practice FreeMarker built ins Explore online FreeMarker user guide Prepare .CSV with 5 columns and fill 1 row with data In WorkFusion create Manual Task and use .CSV as input data Using FreeMarker build ins output all 5 fields as HTML table (do NOT use TASK DESIGNER, work in CODE EDITOR mode). I.e. put data into HTML table. Make HTML table headers out of UPPER CASE variable names. WorkFusion injects into FreeMarker variable with all input data: question.data Use list build in to iterate through question.data Advanced Task Design and Media Content Embedding Input: scanned invoices (pdf), html invoices, invoice id. 1st step: Detect invoice language and is it WorkFusion related or not. Input = pdf invoices Use Bootstrap styles and controls Create a custom layout: show records as tabs (block size > 1) Show PDF invoice on the left Show answers on the right (optional) show video player in Instructions section – any video file make an AJAX request to some open API (weather, Wikipedia, news) and display a response Rule: if invoice is not WF related, go to end. Otherwise, go to Step 2. 2nd step: Extract information from invoice (use groups for products table). Input = html invoices Display instructions in an iframe https: pub_demo.s3.amazonaws.com ie help index.html. Display answers from previous step. "},{"version":"10.0","date":"Oct-16-2019","title":"task-layout-styling-and-controls","name":"Task Layout, Styling, and Controls","fullPath":"iac/core/control-tower/manual-tasks/task-layout-styling-and-controls","content":" Layout When a Task has been posted to a Crowd, it is rendered in an iframe. The iframe height is set in Task Advanced Options > Properties > Frame Height. The Task content is wrapped in the elements with default max width = 985px. To modify the width, use the following CSS code: Task width .template wrapper > .center box { width:1000px !important; } To effectively manage the Task layout, wrap each section (Instructions, Question, Answers) into div with appropriate class or id and add custom styles. Basic Task Schemes See the following schemes with code examples (Bootstrap styles are used): Vertical – sections are displayed one under another. Task Design Vertical Code Example Expand source You can switch the sections order (e.g. Answer before Question). Question section can contain child divs in one or several rows. This layout is useful for image or document comparison. Horizontal – Questions and Answers are displayed in one row Task Design Horizontal Code Example Expand source You can switch the horizontal order (left, right) You can set the width of each section (50% and 50%, 30% and 70%, etc.) Tabs Layout the idea is to save the vertical space by using Bootstrap Tabs component. Task Design Tabs Code Example Expand source One of the variants: Instructions tab Questions and Answers tab Additional tabs (e.g. Terms of Service) Arrange Records as Tabs for Block Size > 1. To include all Records, use the following cycle: Loop example By default, the Question and Answers sections for all Records in a Worker Task are placed one below another. Records as List To display Each Record in a new tab, use the following code: Records as Tabs Code Example Expand source {questionindex 1} {questionindex 1} Styles and Scripts To explore default styles and scripts, see the styles defined in Macro Templates (html.ftl, answers.ftl, extras.ftl) in Campaigns > Templates > Macro. To add your custom styles, use one of the following variants: .my_class {border: 1px solid green;} To add styles for Answers, see the CSS Rules for Answers and Sub Answers topic. To add your custom scripts, use one of the following variants: (function(){}); Components and Controls To minimize the Task code and utilize tested components, you can use the following frameworks (already included in html.ftl): Bootstrap (v 2.0.2) – Grid System Table, Button, Icon Tab Control Panel, Well, Alert Collapse (Accordion) Modal Tooltip, Popover Carousel jQuery UI (v 1.9.2) –"},{"version":"10.0","date":"Aug-06-2019","title":"using-tabula-in-manual-task","name":"Using Tabula in Manual Task","fullPath":"iac/core/control-tower/manual-tasks/using-tabula-in-manual-task","content":" Prepare Manual Task Insert the below code into Manual Task Code Editor section: Manual Task > Code Editor Expand source Tabula usage instruction\"> body { padding left: 20px; padding right: 20px; } .center box, .feedbackpanel { max width: 100% !important; } .thumbnail { border: 0; padding: 0; } .place { padding: 0; border radius: 3px; margin bottom: 10px; } .table upload { width: 100%; border collapse: collapse; } .table upload td { vertical align: top; padding: 0; } .table upload .answers column { width: 310px; padding: 10px; border right: 1px solid d8d8d8; background: fff; } {questionindex 1} {question.data.blank} (document).ready(function () { (' previewFrame').load(function() { loadFile(); (' previewFrame').unbind(\"load\"); }); window.addEventListener(\"message\", function (event) { if(event.data.coordinates) { (' pdfselectionsjsonnew').val(event.data.coordinates); var url = (' previewFrame').attr('src'); var fileid = url.slice(url.lastIndexOf(' ') 1); .ajax({ url: \"https: your instance tabula.crowdcomputingsystems.com pdfs \" fileid \" tables user.json\", type: \"GET\", async: false, success: function(msg) { var res = {\"pages\": } var i = 0; msg.forEach(function(page) { res \"pages\" .push({\"page\": }) console.log(page); page.forEach(function(table) { console.log(i); console.log(res \"pages\"); res\"pages\" \"page\" .push({\"table\":table}); }); i ; }); (' pdfselections_json').val(JSON.stringify(res)); } }); } if(event.data.coordinates && JSON.parse(event.data.coordinates).length) { ('.submit btn').click(); } else if (!event.data.fileUrl){ alert('Please select at least one page in the document by clicking and dragging over the relevant area (usually table)'); } }, false); var submitBtn = ('.submit btn').hide(); var pseudoSubmit = ('Submit'); submitBtn.after( pseudoSubmit); pseudoSubmit.on('click',pseudoSubmitClick); }); function pseudoSubmitClick() { document.getElementById('previewFrame').contentWindow.postMessage(\"getCoordinates\", 'https: your instance tabula.crowdcomputingsystems.com '); } function loadFile() { .ajax({ url: \"https: your instance tabula.crowdcomputingsystems.com upload.json\", type: \"POST\", async: false, data: { fileurl: decodeURIComponent( (' documentlink').val()) }, success: function(msg) { var fileid = JSON.parse(msg) 0 .fileid; var framecurrenturl = (\" previewFrame\").attr(\"src\"); (\" previewFrame\").attr(\"src\", framecurrenturl \"pdf \" file_id); } }); } Replace `` with the current instance name. {question.data 'documentlinkpdf' } – the document link pdf column name in the input file. Working with Tabula Controls: Auto detect Tables – table areas are automatically recognized and selected in a document; Clear All Selections – delete all selected areas; Reupload File – upload another pdf file. Additional controls: Manual selection of an area – click and hold the left mouse button, then drag it over the area to be selected, release the button when finished. Click cross in the red area to delete a single area. For a multi page PDF document the Repeat Selection control is displayed near the selected area, which is highlighted with red. Click the control to select the same area on each page. "},{"version":"10.0","date":"Oct-16-2019","title":"business-process-operations","name":"Business Process Operations","fullPath":"iac/core/control-tower/business-processes/business-process-operations","content":" Learn more about working with Business Processes in our Business Process User Guide. Create a new Business Process Design Business Process Set Business Process run options Run Business Process and view results Business Processes list Copy a Business Process View Business Process event logs Transactions tracking Collaborative Business Process editing Split Data between Bot Tasks Split and merge Business Process records Split join rule usage Connection to MS SQL "},{"version":"10.0","date":"Oct-16-2019","title":"anatomy-of-business-process","name":"Anatomy of Business Process","fullPath":"iac/core/control-tower/business-processes/anatomy-of-business-process","content":" Business Process (BP) is a combination of connected Manual Tasks, Bot Tasks, and or other BPs handled by a user defined flow. The BP Designer allows you to control the flow of the Manual and Bot Tasks. A BP has one input Data file which is enriched and or modified by each Task results. Tasks in BP are executed successively or in parallel depending on the Composite Rules and their Conditions. A BP Task can utilize data generated by previously completed Tasks or data from the input file. Prior to reading this material, see the Manual Task and Copy a Task topics. When creating and editing a BP, some steps are similar to a regular Task creation. BP Flow Create Tasks and test them separately. Create a BP and start designing and configuring it. Prepare an input Data file, add the previously created Tasks to your BP and connect them. Run the BP in a Sandbox environment. You can log in as a Worker, find and complete the Manual Tasks of the BP. Check the results and edit the BP and or Task parameters if needed. Copy the BP with Data. Run the copied BP in a Production environment and monitor its execution. You can optionally pause resume or stop the BP execution. When the BP is no longer needed you can delete it. Data Flow A BP should have one input Data file which is enriched and or modified by each Task results. You can view and export data for each Task in the BP, this data contains input columns and columns added or modified by this Task. A BP Task can utilize data generated by previously completed Tasks or data from the input file. Each completed BP has a file with Final Results which combines input Data file and all the Tasks results. To see the intermediate BP results, you can generate a BP Snapshot. See the following topics: Business Process modeling Business Process operations "},{"version":"10.0","date":"Oct-16-2019","title":"business-process","name":"Business Process","fullPath":"iac/core/control-tower/business-processes/business-process","content":" Definition Business Process (BP) is a combination of connected Manual Tasks, Bot Tasks, and or other BPs handled by a user defined flow. The BP Designer allows you to control the flow of the Manual and Bot Tasks. A BP has one input Data file which is enriched and or modified by each Task results. Tasks in BP are executed successively or in parallel depending on the Composite Rules and their Conditions. A BP Task can utilize data generated by previously completed Tasks or data from the input file. Business Process Documentation Anatomy of Business Process Business Process modeling Process Automation Guide Business Process operations Collaborative editing Split Data between Bot Tasks Split and merge Business Process records Split join rule usage Connection to MS SQL "},{"version":"10.0","date":"Oct-16-2019","title":"connection-to-ms-sql","name":"Connection to MS SQL","fullPath":"iac/core/control-tower/business-processes/connection-to-ms-sql","content":" JDBC API connection to external database WorkFusion supports integration with external databases for fetching the data, executing stored procedures or executing other database queries. Connecting to an external database is done using JDBC API. There are JDBC driver implementations available for most of the modern database solutions. Microsoft SQL JBDC driver is now part of the v10.0 Control Tower configuration. To enable connectivity to an external MS SQL databases, connect to MS SQL databases from Bot Tasks to execute queries. Out of the box database plugin select first_name from actor Script block Or alternatively inside a script block: Sample Code groovy import groovy.sql.Sql url = 'jdbc:sqlserver: hostname:6501;DatabaseName=database' user = 'username' password = 'password' driver = 'com.microsoft.sqlserver.jdbc.SQLServerDriver' sql = Sql.newInstance(url, user, password, driver); rows = sql.rows('{call dbo.GetDetail}'); dataList = new ArrayList(); for (int b=0; b < rows.size(); b ) { def nRow = rows.get(b); def inputData = rowAsMap(nRow); jsonValueMap = new com.google.gson.Gson().toJson(inputData); dataList << jsonValueMap; } "},{"version":"10.0","date":"Oct-16-2019","title":"process-automation-guide","name":"Process Automation Guide","fullPath":"iac/core/control-tower/business-processes/process-automation-guide","content":" Why can you automate a Business Process Business activities are almost always distinct and specific to a given enterprise due to different systems, data fields, protocols, governance and risk factors used to conduct their business. Business’ activities are solved and organized according to a business process. All industries, all companies use processes to perform work. Processes may be standardized across teams, across functional units or across an enterprise. Processes are rarely, if ever, standardized across an industry. The level of standardization often indicates a customers’ operational maturity. Even if non standardized, processes inherently contain patterns and characteristics that may be shared across functional groups and even across competitors and industries. Once you can identify these patterns, you can create a blueprint for solving an activity with process automation. Why should you automate a Business Process Customers primarily pursue automation for their processes to reduce costs. Processes with a high degree of cost include: (1) high monthly or rapidly growing volume and subsequent large supporting workforce; (2) large financial risk (fines or fees) in the event of a handling error; (3) requirements for highly skilled or specialized knowledge due to experience or education. Cases (1) and (2), and sometimes a combination of (1) & (2) are typical starting points for automation. Case (3) often includes an efficiency gain from alleviation of transactional handwork in favor of creative headwork, and are pursued as expansion activities for future phases of automation. We’re going to focus on case (1), which often is tracked as a metric called FTE reduction. In particular, we’re going to look at processes in case (1) from a governance perspective, since layers of governance contribute to the FTE count, but still can be handled via process automation. How can we translate activities into processes: The APQC Method Another View on APQC Typical Business Process Types Basic Process Type I: Transactional Case Sub types: Creation; Update; Investigation; Routing Basic Process Type II: Error Handling and Data Validation Sub types: Validation, Reporting, Error Identification Basic Process Type III: Escalation and Manager Reviews Subtypes: Compliance, QA, fraud alerts Basic Process Type IV: Audit Review Subtypes: Compliance, QA, SQC When We Should Not Automate a Business Process This four part classification framework can cover the majority of automation portfolios across banking, insurance, retail and operations. It has been battle tested in over 200 automation use cases globally. Despite its extreme practicality and robustness, it is important to understand when and where process automation may not be an appropriate tool to use when reducing business process costs. We have found the following flags apply: Non standard processes: Processes which are not written, do not have explicit instructions or vary significantly from person to person often have a high degree of maintenance during BAU, unless you approach these cases in an agile way, and agree on that the “correct processing path” is the automated one. Often we recommend “leaning out” these processes first. Seconds or minute SLA: Processes which require strict SLAs often require a long tuning and optimization period before launched and maintained as part of BAU. Preparation steps or parallel processing may be used if there is a large backlog or work in queue. Technology constraints: Processes may contain fundamental hurdles that require workarounds for end to end process automation to proceed Image based Captcha: No enterprise grade machine vision solution exists to overcome this limitation. Typically these steps must be routed to an analyst for handling. Handwriting: Although handwriting detection may be used with exception routing, enterprise grade handwriting extraction does not exist without significant tuning and questionable accuracy returns. We recommend redesigning or re engineering these cases first, before automating Customer experience impacts: High tough interactions such as outbound phone calls, follow up communications or retention processes often require a broader conversation around customer strategy, and which might not be solved with process automation."},{"version":"10.0","date":"Oct-16-2019","title":"split-and-merge-business-process-records","name":"Split and Merge Business Process Records","fullPath":"iac/core/control-tower/business-processes/split-and-merge-business-process-records","content":" Split and Merge Records Split and Merge algorithm is very helpful to accelerate execution of a Business Process by running tasks in parallel rather than sequentially. For example, if a document consists of several pages, we can split them, process and then merge back to work with a single document from this point. There is a few options to help us accomplishing this task: Option 1: Via Rule Few details to consider in this approach: Business Process streaming option should be switched off (Run Business Process > Advanced Options > Threshold = 100% ). This means each step of BP will 'wait' until all tasks will be processed, only after that next step starts to process. Grouping by Rule contains hard coded value for column name, see example below. Flow can work with any amount of initial data, no restrictions (Option 2 works with 1 record on Input). Usage: We need to merge (group) results after join rule. Results should be joined by columns and in each column values (all values from all records) should be separated by pipes ( ). Package to import: Split and join sample TEST 06 26 2019.zip Explicit grouping by rule: new subm accum split rule.txt Option 2: Via DataStore This sample shows how to use data stores functionality to achieve split and merge functionality. Features: Works with any column names with no extra coding Ability to filter away unwanted records Auto recovery after abrupt failures (after \"kill 9\" hard restart of the server will continue the processing without loosing data or state) Clean up of the temporal data after itself Steps with blue background the bot tasks which with business logic. These steps one would want to replace with use case specific steps Steps with purple background common re usable bot tasks. These steps contain split merge orchestration logic and can be re used across multiple business processes. Notice, how many tasks go from step to step: Initially, we have 61 record arriving to the first step. The first step splits every record into three records Split records are send to \" group Register\" step Every split record is registered in split merge state datastore Every split record is processed by the use case specific logic After that records are sent to either of two steps: \" group Unregister\" step: this bot task will mark the record as Ignored, and will not include into the result \" group Wait for all\": this bot task will wait for all the records to arrive to this or Unregister step before proceeding \" group Merge\" step. All the processed records are read from the datastore and merged back into single record (JSON array) \" group Clean up\" step. At this point it's safe to delete data from the data store, as all the required data was read on the previous step and supplied to the input. Warning: this step should not be combined with \" group Merge\" step. As otherwise we may loose the data if something goes wrong (for example server is force restarted right after we clean up the datastore). Processing of the merged batch continues. Package to import: Split and Merge using Datastore v2.1 14 1 2019.zip Variables required in the input to Register step: group size set in the step where you split records. Required to know the number or records to wait for group uuid set in the step where you split records. Required to identify records related to the same group Example: Split sample Expand source Technical details: group datastore _name auto populated by \" group Register split records\". It is not stored in the datastore itself. group item _id auto populated by \" group Register split records\" group item _status auto populated by \" group Register split records\" PROCESSING set by \" group Register split records\" IGNORED set by group Unregister split records COMPLETED set by \" group Wait for all split records\". Such records are \"secondary\" in the group. They will be merged by \"master\" record in the group READY FOR MERGE set by \" group Wait for all split records\". Master record in the group. It is responsible for merging all records. group merge datastore name set in the very end by \" group Wait for all split records\". Actual process data is stored here before merge. Nice to have (candidates for future improvements): Ability to delete datastores Handle the case when the whole group was filtered out Create Multiple Records and Split them Using Original Data Business case: We have 1 record at the start We want to keep original record from start and split it to multiple records All new records will have original data plus new data. Splitting code sample {multiple_dividends} {multiple_dividends} &lt;content&gt; {multipledivdendsjson_val}&lt; content&gt; > "},{"version":"10.0","date":"Oct-16-2019","title":"split-join-rule-usage","name":"Split-Join Rule Usage","fullPath":"iac/core/control-tower/business-processes/split-join-rule-usage","content":" Split Join Rule is used to separate each single record (or document) from Input Data into a few records to let them pass through different steps of the Business Process (either Bot or Manual Tasks) in parallel, and then combine the processed records back into a single record (or a document) to process the consolidated results from the previous steps further. When a record in Business Process is split into multiple, the new records are routed to different steps, so some of them are processed faster, while the others require more time to get processed. It happens due to different types of steps, as some of them are Bot Tasks and the others are Manual. In case of Manual Tasks availability of work forces to perform assignments is a key point to how fast the records are processed. So, the records which are processed faster reach the Join rule quicker, they should wait for the rest of records to get joined for processing at the step behind the rule. Besides, before 8.4.4 an Input Data record should wait at the Join Rule for all other records from Input Data to transit to the next step. It results in forming a queue at the rule and negatively affects the performance, since the records wait for each other to build up a complete set as in Input Data and only then are processed further. In order to avoid such situations since 8.4.4 the original record is immediately promoted to the next step after the Rule, when all the split records from this original record arrive; and gets processed in the step. The sample Business Process below explains how the Split Join works in opening an account: Image Flow Description: At this step passport details of a client are used as Input data. Using the Split Rule the details are sent to different steps to acquire and validate the information. Processing of split records: Checking the client debts to find out the current client's financial situation (Bot Task); Collecting information about active client's accounts to learn how many active accounts the client has (Bot Task); Validation of client's address to make sure, that the address provided is valid (Manual Task); Validation of client's passport authenticity to confirm, that the passport provided is real (Manual Task); Both Manual Steps are executed slower as the Bot Steps depending on availability of the workforce and how fast the Workers perform their tasks. The Join Rule is intended to manage processing of the split records. It sends processed split records to the next step immediately as they arrive to the Rule and triggers the Bot step execution after the last split record has passed through it. At the next step after the Join Rule, the algorithm waits till all split records of the original one arrive and starts the Bot step execution, in our case the Bot creates the account or rejects the creation depending on the information containing in the joined record.Before 8.4.4, the Bot Task should wait for all records from Input Data, which go through the Split Rule, to pass the Join Rule. Since 8.4.4, if the record R1 from Input Data started earlier than the record R2, but all split records of the record R2 pass sooner through the Join Rule, the Bot step will execute the record R2 and then wait for the all split records from the record R1. "},{"version":"10.0","date":"Oct-16-2019","title":"split-data-between-bot-tasks","name":"Split Data Between Bot Tasks","fullPath":"iac/core/control-tower/business-processes/split-data-between-bot-tasks","content":" In some cases, it is required to split data in the middle of the business process (BP). One of the bot tasks in BP generates data stored in the list (table), and the next bot task needs to be executed for each record in the list (table). Users cannot provide the data records as a .csv input data file as they are received during BP execution and are not available before the BP starts. One or several bot tasks need to be executed only once in BP, while other tasks need to run for each data record. Users cannot provide several records in a .csv input data file as some tasks need to be executed only once. The solution is to split data in the middle of BP using a special Bot Form (ETL task) \"Test split bot etl\". It will allow to: execute certain steps for each data record without the need to use For Each loop in the recording, run these steps in parallel on several nodes (desktops) without the need to provide the records in the input file, thus combining the benefits of the two approaches of implementing loops in Control Tower. The diagram below shows a sample workflow of a business process using a task to split data in the middle of the process so the last step can be executed by several bots in parallel. The guide below demonstrates how to use the bot task in BP based on two simple use cases Use Case 1. Splitting List Variable Description There is a list of emails stored in an Excel file. The bot needs to retrieve the emails and send the same message to all recipients. Downloadable materials You can download the archive with the source materials used for the case to see how the Data split bot task works in practice. The archive contains: two recordings made in RPA Express 2.1.1 BP package .csv input data file for BP Note that you need to provide the Excel file with email addresses and specify the file path in the input data file for BP to work correctly. BP consists of two bot tasks implemented in WorkFusion Studio. Bot task 1 gets the emails from the Excel file and saves them in a List variable. Bot task 2 sends a message to the email address received in Bot task Implementation To implement the workflow, split the data stored in the emails variable after Bot task 1 into several records before the second step is performed, so that it can be executed in parallel on all available nodes. After publishing both bot tasks (recordings) to Control Tower, do as follows. Create a new business process and add both bot tasks to the workflow. Find the Test split data etl task on the right panel on the Workflow tab. Click Copy. Rename and save the task. Add the task to the workflow before the Sending emails step. Double click the ETL task, specify the variable to split, select the List type, and specify the variable to save the split data to. Save the task. Result There are 14 email addresses in the Excel file. Thus, Bot task 1 (https: kb.workfusion.com pages viewpage.action pageId=56721732 SplitDataBetweenBotTasks 1) is executed once, while Bot task 2 (https: kb.workfusion.com pages viewpage.action pageId=56721732 SplitDataBetweenBotTasks 2) is executed 14 times (once fore each email address) in parallel, which significantly speeds up BP execution. Use Case 2. Splitting Table Variable Description There is a table with new employees details stored in an Excel file. The bot needs to retrieve the contact details and fill in an online form for each employee. Downloadable materials You can download the archive with the source materials used for the case to see how the Data split bot task works in practice. The archive contains: two recordings made in RPA Express 2.1.1 BP package .csv input data file for BP Excel file with sample data Note that you need to specify the file path in the input data file for BP to work correctly. BP consists of two bot tasks implemented in WorkFusion Studio. Bot task 1 gets the employee details from an Excel file and saves them in a Table variable. Bot task 2 fills in a web form with information about each employee (each row in the table). Implementation To implement the workflow, split the data stored in the employees table after Bot task 1 into separate lists (rows containing employee information) before the second step is performed, so that it can be executed in parallel on all available nodes. To do it, add the ETL task to the workflow as described above. Configure the Data split task as follows. Specify the variables to split and to save the data to. Set the Table type. Select whether to split data by rows or by columns. In this use case, the information about each employee is contained in a separate row in the file, so select the Row option. Result There are 50 rows in the Excel file. Thus, Bot task 1 (https: kb.workfusion.com pages viewpage.action pageId=56721732 SplitDataBetweenBotTasks 3) is executed once, while Bot task 2 (https: kb.workfusion.com pages viewpage.action pageId=56721732 SplitDataBetweenBotTasks 4) is executed 50 times (once for each row) in parallel. "},{"version":"10.0","date":"Aug-06-2019","title":"activity-log","name":"Activity Log","fullPath":"iac/core/control-tower/configuration/advanced/activity-log","content":" User Manual Activity Types Filters Predefined Filter Advanced Filter Save filter Clear or Refine Filter Database objects Versioning info To view the 9.1 and below, see 9.1 Activity Log page User Manual Control Tower provides audit functionality to track user and system actions (login sessions, creation of Tasks, editing Use Cases, etc.) To view the history of actions, go to Configuration > Activity Log. The Activity Log page displays a grid with the following information: Revision number Username – user (or System) who performed particular operation. Action – operation performed by user (e.g. create, update) Type – target operation object (e.g. User session, License, Template, Task). This field is clickable. Entity ID – unique identifier of the object Creation Date event time. IP Address IP address of the user To see the changes made in files (Rules, Bot Configs, Templates, etc.), click the appropriate link in the Type column. As a result, the file comparison dialog will be displayed under the grid. Activity Types Type Name Description Rule Changing of the composite, adjudication, qualification and other types of rules. User session Tracking sessions of the user logins and logouts. Bot Configuration Tracking changes of the Bot Configuration. Template Tracking changes of the Templates. Custom Qualification Tracking changes of the custom qualifications. Question Tracking changes of the manual task configuration. Worker Qualification Tracking changes of the qualifications assigned to worker. License Tracking changes of the WorkSpace Mturk licenses. Hit Tracking changes of the manual task internal entity. Endpoint Task Tracking changes of the manual task internal entity. Scheduled Campaign Upload Tracking changes of the schedulers. Business Process Tracking changes of the Business Process. Task Tracking changes of the Manual Task. Campaign Tracking changes of the business process definition. Campaign Structure Tracking changes of the XML structure of Business Process workflow. Task Preview Tracking of the Manual Task preview feature use. Business Process Preview Trac Preview User Tracking changes of the users in the system. (Unsuccessfull login attempts count does not supported for LDAP and SSO ) User group Since SPA 9.2 Tracking changes of the user groups in the system. User password Tracking the fact of the password change. Role Tracking changes of the role. Secrets Vault Since SPA 9.2 Tracking changes of the Secure Vault entries. Export (XML) Tracking export of the entity. Task Export (XML) Tracking export of the Manual Task. Business Process Export (XML) Tracking export of the Business Process. Campaign Export Tracking export of the Business Process definition. Data Store Tracking changes of the Data Store entries. Training Set Tracking changes of the Training Set entries. Answer Type Tracking changes of the Answer Type entries. Preferences Tracking changes of the preferences. Business Process Tab Tracking of the Business Process Designer tabs. Task Tab Tracking changes of the Manual Task. Account Tracking changes of the default workspace Mturk license. Inconsistent Assignment Tracking exception with assignment of the Manual Tasks. Filters Since there are thousands of logged records, there are two types of filters that can be applied to the list of activities to facilitate your search. Predefined Filter A predefined filter allows to apply a time frame to the list. Advanced Filter Using an advanced filter you can filter the records by Type, Entity ID, or Username. For each parameter you can specify a condition (contains or doesn't contain) for the filter criteria to include or exclude them from the result respectively. Save filter You can save the filter for future use. Click the arrows and select Save Filter in the drop down. Key in the name for your filter and choose, if the filter is your private or it's available for the other users. Clear or Refine Filter If a filter is applied the yellow box is displayed in the filter area You can remove or edit the filter by clicking Clear filter or Refine filter there respectively. Database objects All of data are stored in the table AuditData. See columns description in Activity Log page information above. Query sample: show changes for Roles and Users "},{"version":"10.0","date":"Aug-06-2019","title":"advanced-configuration","name":"Advanced Configuration","fullPath":"iac/core/control-tower/configuration/advanced/advanced-configuration","content":" This section contains all advanced settings and administrative functions that need to be set while configuring a new WF instance: System Preferences 9.x Bot Sources Activity Log Managing Answer Types Run Priorities Quartz Report Deprecated Creating an MTurk License You should have Admin permissions to access configuration settings. "},{"version":"10.0","date":"Aug-06-2019","title":"quartz-report","name":"Quartz Report","fullPath":"iac/core/control-tower/configuration/advanced/quartz-report","content":" Versioning Info The function is available since WorkFusion 8.4.4 Control Tower provides troubleshooting functional like Quartz Report. Quartz Job Report is based on Quartz Job Scheduling Library and enables extraction of information about the scheduled jobs runs without using database queries. To view the report, go to Configuration → System Preferences → Advanced then click to Download Quartz Report button to download the CSV file with report. This file contains additional information for support engineer to understand a problem with Business Process Task Report Schedule processing * * "},{"version":"10.0","date":"Aug-06-2019","title":"managing-answer-types","name":"Managing Answer Types","fullPath":"iac/core/control-tower/configuration/advanced/managing-answer-types","content":" The Answer Types screen allows you to create and manage Answer Types within the WorkFusion platform. These Answer Types are used when creating Tasks, ETL Configs, and Field Schemes and adding Answers to them. Generally, an Answer is a form input field of a special type, but some Answer Types are designed as complex blocks ( 9.x Information Extraction IE, Taxonomy). To start managing Answer Types, go to Configuration > Answer Types. Answer Types See the list of available Answer Types here. On this page, you can perform the following actions: create you must know the Answer Type Code (e.g. FREE _TEXT) edit you can change Title, Value Type (single, multi separator, combined) delete Changes made to Answer Types will affect all Tasks with appropriate Answers. Adding Editing an Answer Type Select Configuration > Answer Types from the main menu. Click Create Answer Type (or click the appropriate link in the Title column to edit). The following screen is displayed: Create an Answer Type Complete the required fields as follows: Field Name Description Answer Type Title The title of the answer type will be shown in the dropdown list of the Edit Answer dialog. Answer Type Code See the list of available Answer Types here. Answer Type Group Answer Types are divided into groups when they are displayed in a dropdown of Task Edit dialog. Answer Value Type Single value, Multi value, or Combined. See details here. Answer Value Separator Comma, or Pipe for Multivalue Answers. Answer Type Description Additional information explaining the characteristics of the answer type. Show Default Value Enable default value displaying. Options Available Enable Answer options (e.g. number of rows for Long Text) Options Description Additional information explaining the Answer Options section. Make answer type available for entities Tick the entities where this Answer Type can be used: Manual Task Bot Config Field Scheme click the Save button. Using Answer Types All the created Answer Types can be selected for Answers in Task Designer or in ETL forms. You can select any available Answer Type and fill required options, if needed. Selecting Answer Type for Answer "},{"version":"10.0","date":"Aug-06-2019","title":"run-priorities","name":"Run Priorities","fullPath":"iac/core/control-tower/configuration/advanced/run-priorities","content":" Run Priorities allow you to control the order in which Tasks are processed. For example, you are running several Tasks simultaneously on a given Endpoint and you want a certain Task to have a higher priority than other Tasks. You can assign this Task a higher Run Priority in Task Settings, and Workers will see this Task at the top of their search. To start managing priorities, go to Configuration > Run Priorities. Managing Run Priorities You can perform the following actions with Run Priorities: create edit (click the appropriate name) delete The \"Normal\" priority is default and cannot be deleted. Creating Editing a Task Priority Click the Create Run Priority button or click the appropriate priority name in the grid. Edit Run Priority Enter a unique Name for the priority. Enter a priority Value from 1 to 100. The higher this value, Task with higher priority will be processed in the first turn. Using Run Priorities All the existing priorities appear in Task Settings: Task Priority "},{"version":"10.0","date":"Aug-06-2019","title":"schedule-settings","name":"Schedule Settings","fullPath":"iac/core/control-tower/configuration/advanced/schedule-settings","content":" The present topic describes how to enable and set up the Data Purge function for information acquired during execution of Business Processes. Note The Data Purge function does not work with Data Stores. Refer to 9.x Data Stores Purging to get information about how to purge the data from Data Stores. Campaign Job Settings Automatic Calculations Data Purge Settings To start managing Schedule Settings in Control Tower, go to Configuration → System Preferences → Schedule Settings. This page is intended for setting campaign jobs, enabling disabling \"heavy\" statistical calculations, and setting data purge parameters. Campaign Job Settings Parameter Default Description dataPurgeScheduler 0 0 0 * * * Enable disable Data Purge job. See the settings here. runCampaignStatsDailyCurrentJobByScheduler 0 0 30 * * * calculate Campaign daily statistics runCampaignStatsJobByScheduler 0 2 1 * * calculate Campaign statistics on custom interval runCampaignStatsMonthlyJobByScheduler 0 59 23 L * calculate Campaign monthly statistics Job Schedule Cron format: Cron Expressions Automatic Calculations If checked, these options can significantly increase instance workload: Calculate Confidence For Worker And Answer (can be viewed in Task Results > Data grid) Automatically Generate Snapshot After Every BP step Data Purge Settings 10.0 Data Purge Settings Data Purge Settings\") "},{"version":"10.0","date":"Aug-06-2019","title":"account-settings","name":"Account Settings","fullPath":"iac/core/control-tower/configuration/system-settings/account-settings","content":" The Account Settings tab allows you to add, edit, enable, or disable the licenses for the Endpoints that the WorkFusion platform sends work to. Among these endpoints are WorkSpace and other worker portals. Account Settings To create a new License, see Creating a New Licence guide. Preferences Parameter Description Timezone Select the time zone. All times shown within the WorkFusion platform will be based on this time zone. Notification E mails Enter each email address that you would like to have notifications sent to. Use a comma to separate each listed email. Send email in case processing issues If a Task or BP has some processing issues, a notification email will be sent to all users in Notification list. This email contains the following info: BP name (with link) Name of BP step with processing issues. User name who started this task Date and time when task started. Last stack trace. Accept Amazon Notification Select this checkbox if you would like to accept notifications originating from Amazon MTurk. Snapshot Type Global setting for the snapshot format type. Select the desired format from the dropdown list: CSV (Comma Separated Value) or XLSX spreadsheet. Idle Duration Set a minimal time interval that is counted as idle. Idle Task is a Task having no Worker activity for a defined number of hours. Production Task Comparator Sort Tasks having the same template (Campaign), and make Workers complete these Tasks in a custom order. See the Task Comparator (Manual tasks sorting) topic. Sandbox Task Comparator Sort Tasks having the same template (Campaign), and make Workers complete these Tasks in a custom order. See the Task Comparator (Manual tasks sorting) topic. NLP Service URL NLP service is used for named entity recognition in 9.x Information Extraction IE. Count Of Records for Re training This parameter is used to override the appropriate ML model's parameter. To use this value, select it in Automation BP Settings. CGI Proxy URL URL of the proxy server used in 9.x Information Extraction IE. Remember user on login page If this option is checked, the \"Keep me logged in\" checkbox will be shown on the login page. When the checkbox is selected, the session is not interrupted, so the user stays logged in until he she logs out manually. Dynamic rendering enabled by default This option enables Dynamic task rendering for all newly created tasks. "},{"version":"10.0","date":"Aug-06-2019","title":"active-tasks","name":"Active Tasks","fullPath":"iac/core/control-tower/configuration/system-settings/active-tasks","content":" The Active Tasks tab shows the Tasks that are actively running on all of your Endpoints (WorkSpace, MTurk, etc). To start managing active Tasks, go to Configuration > System Preferences > Active Tasks tab. The Stop Task(s) button allows you to stop any of these active Tasks. Meaning the Task will be removed from the Endpoint. Column Heading Description Campaign Name The Name of the Campaign that this Task belongs to. Run UUID A unique identifier for this Task Run. Job Name The Name of the Batch Job executing the Task. Start Time The time the Run started. Last Updated Time The last time the Run was synchronized with the endpoint. Status The status of the Task (Run). "},{"version":"10.0","date":"Aug-06-2019","title":"advanced","name":"Advanced","fullPath":"iac/core/control-tower/configuration/system-settings/advanced","content":" The Advanced tab of the System Preferences allows you to manually launch some batch jobs and gives you a view into the Web Service keys that WorkFusion uses. System Preferences Advanced Button Description Clean Mechanical Turk Developer Sandbox Clear all the runs on the Amazon MTurk Sandbox endpoint. So clicking this button will remove any tasks that you currently have running on MTurk Sandbox. Clean Mechanical Turk Clear all the runs on the Amazon MTurk Production endpoint. So clicking this button will remove any tasks that you currently have running on MTurk Production. Force Processing Force the WorkFusion batch jobs to run immediately rather than at their scheduled intervals. NOTE: It is not recommended to use this feature on a daily basis because it could overload the endpoints. So unless there is an emergency, the batch jobs should be allowed to run on their scheduled intervals. Update Worker Qualifications WorkFusion platform will send query to all Endpoints to obtain and update Worker Qualifications (System Qualifications). Recalculate Worker Day Activity logs Force the WorkFusion Worker Day Activity Log job to run immediately. Install Macro Templates Version Refresh the code in the Macro Templates with the latest code from the WorkFusion source repository. Re Index Quick Search Data Refresh the Search Engine that WorkFusion uses with the latest indexes. "},{"version":"10.0","date":"Aug-06-2019","title":"creating-a-new-licence","name":"Creating a New Licence","fullPath":"iac/core/control-tower/configuration/system-settings/creating-a-new-licence","content":" A License is a WorkFusion object containing address and credentials used to connect and post Tasks to Cloud Worker Platforms, such as MTurk and WorkSpace. To create a new License, perform the following actions: Go to System Preferences > Account Settings. Click Add License. ! Create License Step 1 (attachments 10914647 10914644.png effects=border simple,blur border \"Create License Step 1\") Enter License Parameters: Driver Name (Mechanical Turk or WorkSpace) Domain Name (only for WorkSpace) AWS Account ID and AWS Secret Key – these keys are provided by Amazon MTurk or WorkFusion. Create License MTurk Click the Next button. ! Create New License Step 2 (attachments 10914647 10914646.png effects=border simple,blur border \"Create New License Step 2\") Enter the Licence Name. Click the Validate AWS Key button. If the combination of keys is valid, click the Save button. See the full instruction on creating an MTurk License here. "},{"version":"10.0","date":"Aug-06-2019","title":"system-preferences","name":"System Preferences","fullPath":"iac/core/control-tower/configuration/system-settings/system-preferences","content":" System Preferences page contains all WorkFusion instance parameters: Account Settings User Settings Advanced Active Tasks Schedule Settings This application page is available only for Admins. "},{"version":"10.0","date":"Aug-06-2019","title":"user-settings","name":"User Settings","fullPath":"iac/core/control-tower/configuration/system-settings/user-settings","content":" The User Settings page allows you to configure settings for your user account such as editing your first and last name, password, time zone preference as well as setting up email notifications. To start managing your User Settings, go to Configuration > System Preferences > User Settings tab or click the User icon in the top right corner and select User Settings. Account Details Enter your personal details: First Name Last Name E mail. This email will be used as User Name on login page. Change Password This section is used for changing your old password. User Time Zone Set your personal Timezone that can differ from the WorkFusion system timezone. Mail Notifications In this section, you can set the events that trigger notifications to your email and reports (daily, monthly). Reset Filters You can reset all or just specific filters (e.g. tasks, runs, qualifications). This feature can help in situations when you have created an illegal filter and cannot view a specific grid, for example task or process list. "},{"version":"10.0","date":"Oct-16-2019","title":"data-purge-settings","name":"\\[10.0\\] Data Purge Settings","fullPath":"iac/core/control-tower/configuration/system-settings/data-purge-settings","content":" title: 10.0 Data Purge Settings For Version 9.x, see this article. Data Purge Settings Role Management Space utilization Affected Tables Disk space FAQ How much data volume is generated by business processes Calculation example How to reduce data volume stored by business processes Disable snapshots generation Store big data points (for example documents) in File StorageinFileStorage) Do not pass unnecessary data points between steps Use stateless execution of tasks Enable data purge Data Purge Settings Data Purge increases load on DB server disk. The Data Purge job deletes obsolete operational data from WorkFusion Intelligent Automation Cloud instance according to the following settings: Parameter Default Description Purge Session Duration (minutes) 60 Defines how long each purge session lasts after the purge is triggered by the Scheduler. Set to 0 to disable data purge. Data Storage Period (days) 90 Defines the number of days to keep business process data after the business process completes. Only data of business process exceeded the period is purged. Batch Size (DB records) 10 000 Number of DB records to purge at once. Sleep After Each Purge (seconds) 1 Make pause after each Business Process being purged to minimize an impact to performance. Data Purge Results The Data Purge job clears the data of completed (or marked as deleted) business processes: Business process and business process steps input data Business process and business process steps output data Output data snapshots Generated events The Data Purge job does not delete the following: Any data of Active, Draft, Not started or Failed business processes Data stores S3 file storage data Activity logs Example of a Business Process after the Data Purge operation: Results Data – empty Snapshots – empty Download original data – disabled Events – empty Role Management Perform the following steps to grant a user the Manage Data Purge permission Open Configuration → Role Management (see 10.0 Role Management page for additional information). Open Role which the user belongs to (e.g. ROLESUPERADMINISTRATOR). Select the Manage Data Purge permission. Save changes. Re login to Control Tower. Go to Configuration → System Preferences → Schedule settings. Data Purge Settings should be available. Space utilization Affected Tables The following MySQL tables in the wfdb database are affected by the Data Purge function when the Purge Data function is enabled: List of Affected Tables AwsHitQuestion (contains JSON with output of a record answers processing) AwsHitAssignmentAnswer (contains raw answers given by worker to a record) HitSubmissionDataItem (contains JSON with a record input data) HitSubmissionDataItemHistory (contains snapshots of existing record input data if it is modified by step transition, e.g. 2 records are merged) HitDataItemLog (contains log of record life cycle events, e.g. posting, submission, approval) FILE (contains file description, e.g. name and type) DATA_STORE (contains binary content of files) EVENT_TRACKING (contains text description of event, e.g. message or exception stacktrace) EVENT_OBJECT (contains reference to object induced event, e.g. BP step or definition) EVENTTRACKINGEVENT_OBJECT (removed in WorkFusion 8.5, used for many to many relation of event objects and descriptions) Entity relationship diagram as per 9.0: Disk space Data Purge operation will not release the freed space to the OS file system. Instead the freed space will become available for new data inserts. FAQ How much data volume is generated by business processes Every business process step accepts input data and produced output data. Output data of one business process step is then copied to the input of the next business process step. Adjudication plays additional role in this process. The same task can be performed by several workers and then the final result derived using majority rule. To accommodate this functionality output (answers) of each worker needs to be saved before deriving final output. Generally amount of data written to the database can be estimated as the following: Parameter Description S Number of stateful steps of the business process T Number of transactions processed by the business process V Amount of data passed through the steps of the business process Estimate of data stored = S * T * V * 4 Calculation example A business process with 20 steps 100 000 transactions processed per day Every transaction has several data points: ~128kb: input document (a few page long document: greyscale pdf image; OCR result; tagging labeling result) ~4kb in total for intermediate data points generated by business process logic Estimate of data stored = S * T * V * 4 = 20 * 100000 * (128000b 4000b) * 4 = 1056 GB per day (keep reading how to decrease this number 100 times) How to reduce data volume stored by business processes Let's take our example and see how we can reduce business process's disk appetite. Disable snapshots generation As a rule there is no one who downloads it, disable snapshot generation by default (it will still be possible to generate it on demand): Schedule Settings AutomaticCalculations Store big data points (for example documents) in File Storage For example in S3 Object Storage coming with WorkFusion Intelligent Automation Cloud out of the box. Our example: now instead of passing the whole file content through all the step we can past just a link to the document. ~128kb: input document is replaced with ~200b bytes: link to the input document Estimate of data stored = S * T * V * 4 = 20 * 100000 * (200b 4000b) * 4 = 33.6 GB per day Do not pass unnecessary data points between steps There is no need to pass all the data to every step to the very end of the business process. Just pass what is actually needed on the next step. Always use `, and include only the required data points. Our example: now instead of passing ~4kb of all data points we pass let's say about ~1kb of required data points only** Estimate of data stored = S * T * V * 4 = 20 * 100000 * (200b 1000b) * 4 = 9.6 GB per day Use stateless execution of tasks When possible utilize Stateless Execution of Bot Tasks, this way the step data will be stored in memory only without writing to the database. Our example: instead of saving data for every step we mark most of the steps as stateless, so for example 5 out 20 remain stateful. Estimate of data stored = S * T * V * 4 = 5 * 100000 * (200b 1000b) * 4 = 2.4 GB per day Enable data purge When data flow is in proper order time to enable data purge. It will let us keep the disk of the constant size, without need to provision bigger disks too often. Our example: setting data storage period to 30 days, we end up with: 2.4 GB of data generated, and about the same amount of data purged every day with total amount of 72 GB of 30 days historical data stored at any given point in time. "},{"version":"10.0","date":"Aug-19-2019","title":"add-bot-task-to-bp","name":"Add Bot Task to BP","fullPath":"iac/core/control-tower/bot-tasks/add-bot-task-to-bp","content":" Before reading this topic, see the detailed Use Case description here. Add a new Bot Task from the Toolbar or from the Sidebar of the Design Business Process tab. Select your Bot Use Case as a base for new Bot Task Enter ETL parameters in the Answer fields if needed. Adding Machine Task to BP Basic Editor In basic mode, you should complete the following fields: Task Name Answers Advanced Editor Description Code Unique Identifier (generated automatically and can be used for config inclusions in plugin) Code Editor (2 modes: Advanced Editing and Preview) In Preview, all the code is processed with FreeMarker – you can view your variable names and column names. Machine Task Preview"},{"version":"10.0","date":"Oct-16-2019","title":"automl-plugins","name":"AutoML plugins","fullPath":"iac/core/control-tower/bot-tasks/automl-plugins","content":" AutoML plugins provide a capability to interact with automl services from Bot Tasks. API reference See AutoML Gateway Service API docs for a full reference: Train flow automl train start 'automl train start' plugin sends a request to start training to automl services. Attributes Name Required Description version Plugin version: '1' for Intelligent Automation Cloud 10.0 timeout Request connection read timeout in milliseconds. If not defined: request without timeout. model id Provide id that will be used as trained model id. model artifact id Model identification in the following format: ':' fields Trained fields as json. See FieldInfo for more information. parameters Additional parameters.Format: mapped as json. Result The response contains the stringified JSON with the following attributes: status – request status code body – response message 'automl train start' response example ... ... automl train status 'automl train status' plugin sends a request to get training status from automl services. Attributes Name Required Description version Plugin version: '1' for Intelligent Automation Cloud 10.0 timeout Request connection read timeout in milliseconds. If not defined: request without timeout. model id Id of the checked trained model. parameters Additional parameters.Format: mapped as json. Result The response contains the stringified JSON with the following attributes: status – request status code body – model training status body.state body.message 'automl train status' plugin ... ... Execute flow automl execute 'automl execute' plugin sends a request to execute model to automl services. Attributes Name Required Description version Plugin version: '1' for Intelligent Automation Cloud 10.0 timeout Request connection read timeout in milliseconds. If not defined: request without timeout. model id Id of the trained model which will be used for execute. document Document to extract. parameters Additional parameters. Format: &lt;parameter name, parameter value&gt; mapped as json. Result The response contains the stringified JSON with the following attributes: status – status code of request body body.result– model execute result; 'automl execute' response example ... ... Usage For an example of usage, see the out of the box Cognitive Automation Business Process section. "},{"version":"10.0","date":"Sep-03-2019","title":"bot-task-plugins","name":"Bot Task Plugins","fullPath":"iac/core/control-tower/bot-tasks/bot-task-plugins","content":" By default, you can use bot plugins included in the original Web Harvest framework. To learn more, refer to Standard Web Harvest processors. Though while writing Web Harvest XML configs, you can also use additionally developed plugins that are implemented in the Java language by WorkFusion team. XML configuration is executed for each submission. If your input CSV file contains 40 records, respectively your task consists of 40 submissions, and the Web Harvest XML configuration will be executed 40 times, once for each submission. WorkFusion puts data from the CSV file to the Web Harvest context, so you can access it using placeholders. Context There is a set of defined variables for you to use and access in Web Harvest XML configuration. See more information here. WorkFusion Bot Plugins Plugin Description alexa Obtains Alexa rank of the given website. automation Interacts with VDS Services. cache Obtains an object form the global server cache by a key or stores an object in cache for future requests. conversion Convert input data to custom format. datastore Manages Data Stores and transactions. date format Formats string representation of a date to a desired format. excel to from list Reads data from xlsx or xls formats and writes to List> row: columnName:value and vice versa. excel to csv Converts an excel file to a csv file. export Stores information collected during bot task in the output file. It requires at least one child element. http extended Backport of the plugin from the Web Harvest trunk codebase. include config Executes a specified WorkFusion bot configuration in scope of the current execution (including recursive executions). JSON manipulation Convert JSON string to object, searches in JSON, deletes and adds nodes, changes node values. language extractor Detects the language of the given website. list to csv Exports JSON formatted info to a given csv file. log Logs messages that can be seen on Run's Events and in log files. mail check Connects to a mail server and checks for new unread emails with a specified subject pattern. OCR Performs text recognition on images using OCR Service and ABBYY FREngine. pool Semaphore functionality for objects in the section. For each invocation, the plugin returns any not borrowed object from evaluated list. release Postpones the next execution of current record. required The plugin declares which column names (variables) must be passed in the input data file. robotics Clicks through desktop or web applications using commands in Web Harvest scripts. S3 Accesses and manages data on Amazon S3 storage. Secrets Vault Provides functionality to manage Secrets Vault. send message Sends messages to Workers using their IDs or to all Workers in a Crowd using the Crowd name. script var An alternative to the plugin in scripts to simplify access to resulting value. similarity score Compares two text strings and provides their similarity score. As an output, the plugin returns a double number, for example: 0.27 or 40.0. split Splits text content into chunks (sentences, words). task start Starts a task business process from a specific definition with new input data. to text Parses the content and extracts text from it using the Apache Tika toolkit. unzip Extracts content of .zip files. url validator Generic plugin that performs data validation. validate Detects availability of the given url. var global Inserts values of variables defined in the Global Variables Data Store. who is Obtains information about the stated web site using Who Is service. Run Time Libraries The following run time libraries are available in Bot configs. Library Description org.apache.commons.lang.StringUtils Operations on String that are null safe. com.google.code.gson Google Gson library to serialize and deserialize Java objects to (and from) JSON. commons httpclient Feature rich package implementing the client side of the most recent HTTP standards and recommendations. "},{"version":"10.0","date":"Aug-19-2019","title":"bot-execution-lifecycle","name":"Bot Execution Lifecycle","fullPath":"iac/core/control-tower/bot-tasks/bot-execution-lifecycle","content":" There are two generic approaches for RPA bot execution. These are as follows: multiple records per one batch processing at one transaction one record per one transaction Both approaches have their pros and cons. Refer to the image for more details on RPA bot performance in terms of time. We recommend to create one record per one transaction execution logic and only then to investigate whether it is required to use multiple records per one batch or not. Multiple records per one batch Let's see the example when three records are executed at one transaction. A login operation is not a time consuming operation, taking from 10 up to 30 seconds to perform. Major business logic is encapsulated inside the red square block, that takes from 1 up to 40 50 minutes to execute a separate record. For some reason, that block may have some issues with stability. Thus, it might fail with any RPA processing issue, or any business logic issue. In case of a failure, the whole batch with three records fails for the RPA bot to be relaunched from scratch. SLA for a single record is equal to SLA for the whole batch. The only generic benefit is that this approach consumes less time in cases when the logic is quite simple and execution is straight enough to finish processing. The approach is more unstable in cases when single bot execution depends on complex business logic implementation, or long execution time frame. One record per one batch One record per one transaction approach is more preferred in terms of SLA, as one record is being processed at one go and reaches the export plugin as soon as possible. That record is immediately ready for the next bot. In case of a failure, it is relaunched independently without interruption for all remaining records. SLA for a single record is equal to SLA for one transaction. An increase in the execution time of business logic significantly reduces the effect of login and logout operations in terms of the total execution time, and leads to a decrease in the share of these components in the total execution time. "},{"version":"10.0","date":"Oct-16-2019","title":"bot-tasks-context","name":"Bot Tasks Context","fullPath":"iac/core/control-tower/bot-tasks/bot-tasks-context","content":" Web Harvest Context Object sys Object http Object log WorkFusion Context Usage Examples In Bot configs you have access both to Web Harvest and WorkFusion context variables. Manual Task context is described here Templates TemplateVariables Web Harvest Context Every Web Harvest variable context initially contains the following help objects that can be used in any expression inside configurations: Object sys Object sys which contains some general purpose constants and methods. sys.lf Line feed character ( n) sys.cr Carriage return character ( r) sys.tab Tab character ( t) sys.space Space character ( ) sys.quot Double quote character (\") sys.apos Single quote character (') sys.backspace Backspace character ( b) sys.date() Returns current date in the yyyyMMdd format sys.time() Returns current time in the HHmmss format sys.datetime(format) Returns date time in specified format (Java date and time formatting patterns must be used). sys.escapeXml(text) Escapes characters &amp;'\"&lt;&gt; in the specified text according to XML standard. sys.fullUrl(pageUrl, link) For the specified URL of the web page and specified link (which could be relative, absolute or full URL) returns full URL. sys.defineVariable(varname, varvalue, overwrite ) Defines new variable with specified name and value in the current Web Harvest context. Parameter overwrite tells whether to overwrite existing variable with the same name. Its default value is true. It has the same meaning as var def processor, however it could be useful for value exchange between scripts and Web Harvest context. sys.isVariableDefined(varname) Tells if variable with specified name is defined in the context. sys.getVar(varname) Returns variable from scraper context. sys.xpath(xpathexpr, xml) Evaluates XPath expression on specified XML. Returns instance oforg.webharvest.runtime.variables.Variable class. For more details check Java API of this class. Object http Object http which provides access to Http client and gives information about HTTP responses: http.client Returns instance of org.apache.commons.httpclient.HttpClient class which is used as primary HTTP client during the configuration execution. For more details, see Jakarta HttpClient documentation. http.contentLength Lenght of the last HTTP response's content in bytes. http.charset Encoding of the last HTTP response, if it contain textual content. http.mimeType Mime type of the last HTTP response. http.headers Array of HTTP response header pairs. To access individual header by index, use http.headers.length http.headers index .key http.headers index .value To access first header value by name, use http.getHeader(\"headername\"). To get all headers with specified name use http.getHeaders(\"headername\"): http.getHeaders(\"Set Cookie\").length http.getHeaders(\"Set Cookie\") index http.statusCode Status code of the last HTTP response. http.statusText Status message of the last HTTP response. http.totalLength Total length in bytes of all responses returned to this HTTP client. http.totalResponses Total number of responses returned to this HTTP client. Object log Object log provides ability to print debug information, the logger object is of type org.slf4j.Logger and supports all of the methods from there. log.info(message) Prints the specified message log.error(message, exception) Prints the specified message along with the exception stacktrace log.debug(messageTemplate, argument) Formats and prints the specified message template with the passed argument. For example log.debug(\"Number of objects received: {}\", receivedObjects.size()) and many more... Refer to org.slf4j.Logger WorkFusion Context There is a set of defined variables that we can use and access in Web Harvest XML configuration: Variable Type Description Since WF Version 1 hitsubmissiondataitem Object Contains current hit submission data item. CLASS com.freedomoss.crowdcontrol.webharvest.HitSubmissionDataItemDto 2 item Object A web harvest task item. It contains necessary information about run, campaign, submission and other objects. CLASS com.freedomoss.crowdcontrol.webharvest.WebHarvestTaskItem 3 prevData Object Contains data about answers of the previous runs (only for composite campaigns). CLASS com.freedomoss.crowdcontrol.webharvest.PreviousData 4 assignment Object Current assignment. CLASS com.freedomoss.crowdcontrol.webharvest.AwsHitAssignmentDto 5 source Object Current source CLASS com.freedomoss.crowdcontrol.webharvest.SourceDto 6 dataStoreProperties Object CLASS com.freedomoss.crowdcontrol.webharvest.web.security.DatabaseProperties 7 includedConfigs Map Map of included configs. It is used in &lt;include config&gt; plugin. Map&lt;String, String&gt;: key config name, value config body 8 applicationHost String WorkFusion host name. 9 applicationContextPath String WorkFusion context path. 10 applicationResourceUrl String Root url for resources. Default: https: s3.amazonaws.com 7.2.13 11 userInternalCredentials Object Username and hashed password (hashed value fetched from database) of the user that executes current run (Run.createdBy). Starting from Intelligent Automation Cloud 10.0, contains only username of the task author and empty string password. We don't recommend using this object. 12 seleniumDriver Object 13 seleniumServer Object 14 seleniumLogger Object 15 seleniumDriverRegistry Object 16 seleniumnodeid Object 17 seleniumparentbrowsercapabilities String 18 capabilityNodeId String 19 exportResult Object 20 releaseDate Object 21 s3EndpointUrl String 22 s3AccessKey String 23 s3SecretKey String 24 s3KeyMap Map 25 secureStorePassword String 26 securityProviderMap Map Usage Examples Each WorkFusion variable is accessible in Bot config using the getWrappedObject() method see reference here: The following javaDoc describes all Classes and DTOs (data transfer objects) accessible in Bot configs https: s3.amazonaws.com workfusion docs context machine javadoc index.html Example of accessing WorkFusion context variables from machine config "},{"version":"10.0","date":"Aug-21-2019","title":"conversion-plugins","name":"Conversion Plugins","fullPath":"iac/core/control-tower/bot-tasks/conversion-plugins","content":" These plugins are used to convert input data to a custom format. Attributes Name Required Default Description output format no format converted to. Depends on the conversion plugin. on error no EXCEPTION Bot config behavior on exception during conversion: EXCEPTION throws RuntimeException EMPTYVALUE returns empty string ORIGINALVALUE returns original value DEFAULTVALUE returns default value defined in on error default value attribute on error default value no Default value for on error = \"DEFAULTVALUE\" If on error is specified, then it works like* on error=\"EXCEPTION*\" If on error=\"DEFAULT _VALUE\" is specified and on error default value is not specified, then it works like* on error=\"EXCEPTION*\" convert date This plugin is used to convert input data to a custom date format. If output format was not specified then the default format MM dd yyyy is used. Additional Attributes Name Required Default Description current date no Define the current date, to make possible converting dates like \"yesterday\" and \"Monday\". for no Comma separated field names that have to be converted. Example Input date can be without year. In this case, the current year is used. If date could not be converted it remains as is. Output Formats MM dd yy M d yy M d yyyy MM dd yyyy MMM. d yy MMM dd, yy MMMMM dd, yy MMM. dd, yy MMM. dd yy MMM. d, yy MMM d yy MMM dd yy dd MMM yy MMM d, yy yyyy MM dd MMM dd yyyy dd MMM yyyy MMMMM dd yyyy MMMMM dd, yyyy MMM. dd, yyyy MMM. dd yyyy MMM dd yyyy MMM dd, yyyy MMM. d, yyyy MMM. d yyyy MMM d, yyyy MMM d yyyy MMMMM d MMM. d MMM d M d MM dd MM dd dd MMM MMM dd MMM dd MMM. dd MMMMM dd See the legend here. ETL Template Example Answers: answer code answer name answer type options required inputdata Input Column Name Free Text yes outputdata Output Column Name Free Text yes dateformat Date Format Free Text yes onerror On Error Behaviour Select One EXCEPTION=EXCEPTION EMPTYVALUE=EMPTYVALUE ORIGINALVALUE=ORIGINALVALUE DEFAULTVALUE=DEFAULTVALUE yes onerrordefault_value On Error Default Value Free Text no convert price This plugin is used to convert input price to a custom price format. If output format was not specified then the default format 'C . .' is used. Additional Attributes Name Required Default Description default currency no USD If the input data is a number without currency, define the default currency with this attribute. for no Comma separated field names that have to be converted Example Output Formats C . C . . C . C CCC . CCC . . CCC . CCC CCCCC . CCCCC . . CCCCC . CCCCC c . c . . c . c ccccc . ccccc . . ccccc . ccccc CCC: . CCCCC: . C: . Legend: CCCCC full main currency unit name, like \"dollar\". CCC short main currency unit name, like \"USD\" C main currency unit symbol, like \" \" ccccc full fractional unit name, like \"cent\" c fractional unit symbol, like \"¢\" . number format, count of sign can be from zero to infinity. : used as a delimiter Supported Currencies American dollar, USD, , cent, ¢ Canada canadian dollar, CAD, C , cent, ¢ Euro euro, EUR, €, cent, ¢ United Kingdom pound, GBP, £, penny, pence, p, GBX Japan yen, JPY, ¥ China yen, CNY, ¥ Switzerland frank, CHF, ₣ ETL Template Example Answers answer code answer name answer type options required inputdata Input Column Name Free Text yes outputdata Output Column Name Free Text yes priceformat Price Format Free Text yes onerror On Error Behaviour Select One EXCEPTION=EXCEPTION EMPTYVALUE=EMPTYVALUE ORIGINALVALUE=ORIGINALVALUE DEFAULTVALUE=DEFAULTVALUE yes onerrordefault_value On Error Default Value Free Text no convert number This plugin is used to convert input number to a custom number format. Additional Attributes Name Required Default Description rounding no none Rounding type: none ceil floor roun for no Comma separated field names that have to be converted Rounding Types Input Number Rounring Type none ceil floor round 5.5 5.5 6 5 6 2.5 2.5 3 2 3 1.6 1.6 2 1 2 1.1 1.1 2 1 1 1.0 1.0 1 1 1 1.0 1.0 1 1 1 1.1 1.1 1 2 1 1.6 1.6 1 2 2 2.5 2.5 2 3 3 5.5 5.5 5 6 6 If the output format was not specified property, then the default format . is used. Example or 0 format means that output number would be integer only. . or 0. output always integer with point. input number 1.15 with format . will be rounded to 1.2 on output independently of rounding type. Illegal output format zero before 'number sing before' 'point' like 00 . max and min value for integer input number 9,223,372,036,854,775,807 and 9,223,372,036,854,775,808 respectively. for non integer number max and min value is infinity. Legend: . number format. (before point) any number of digits before point(if present). (afer point) one digit after point. . means exact three digits after point. 0 (before or witout point) number as is. 0 (after point) sets '0' digit at specific position. . Decimal separator or monetary decimal separator. , Grouping separator. ETL Template Example Answers answer code answer name answer type options required inputdata Input Column Name Free Text yes outputdata Output Column Name Free Text yes numberformat Number Format Free Text yes onerror On Error Behaviour Select One EXCEPTION=EXCEPTION EMPTYVALUE=EMPTYVALUE ORIGINALVALUE=ORIGINALVALUE DEFAULTVALLUE=DEFAULTVALLUE yes onerrordefault_value On Error Default Value Free Text no convert json This plugin is used to convert a JSON value of answer with type Group of Answers (GROUP). Attributes Name Required Default Description value yes JSON value (multi tab IE answer value) Additional Child Converter Attributes Name Required Default Description for yes Comma separated field names inside JSON that have to be converted Example json converted {\"code\": {\"amount\":\"100.12\",\"score\":\"10%\",\"oldprice\":\"1.03 dollar\",\"newprice\":\"1.05 dollar\",\"record_date\":\"11 12 2015\"} }\"); convert percent This plugin is used to convert an input number or string to a number with percent sign (e.g. fifty seven percents > 57%). This plugin does not have additional attributes (only on error and on error default value). Example 123 and half percent 0.7"},{"version":"10.0","date":"Aug-06-2019","title":"create-bot-use-case","name":"Create a Bot Use Case","fullPath":"iac/core/control-tower/bot-tasks/create-bot-use-case","content":" Before reading this topic, see the detailed Use Case description here. To create a Bot Use Case: Go to System Settings > Use Cases tab. Click the Create Use Case button. Enter the Use Case Details: select the Use Case Type: BASIC – the default type. SCRAPER – this Use Case automatically creates a Bot Task intended for scraping web pages. Web Scraper functionality is deprecated since WorkFusion 8.0, and you can't expect it to work with stable quality. ETL – using this type, you can add Answers to the Bot Task. These answers can define variables, parameters, or column names used in the config code. By using ETL configs, you improve the config re using and flexibility. Business users (non programmers) do not need to edit the config code directly – they only need to specify parameters in Answers while adding Bot Task to a BP. FORM Form functionality is deprecated and you can't expect it to work with stable quality. RECORDER. Recorder Use Cases can only be imported from WorkFusion Studio Record perspective. To create Recorder Use Case, make a recording in the WF Studio Record perspective, and then select Publish to Control Tower. See more on the topic: RPA Recorder. Switch to Advanced Mode and paste your valid XML code from Web Harvest IDE. (If you are creating an ETL config) Add Answer – these Answers are completed when using the Bot Step in a BP. You can set some parameters or variables (e.g. map input and output columns) Save the Use Case. How to Use Variables and Answer Values Variables – use this tag to define a new variable in your config. – returns value of defined variable inside tag. {var _name} – returns value of defined variable in tag attributes. Example to return a value of a variable from step input data inside another tag, for example \"ocr _link\", use the template processor: Answers {{answer _code}} – returns the ETL answer value. Example Column Names in Answers All the code of ETL config is processed by FreeMarker engine. To return a value from input file column name defined in Answer, you should wrap it with the ... tag. FreeMarker will not search FTL tags and interpolations and other special character sequences in the body of this directive, except the noparse end tag. Example "},{"version":"10.0","date":"Aug-06-2019","title":"datastore-plugins","name":"Datastore Plugins","fullPath":"iac/core/control-tower/bot-tasks/datastore-plugins","content":" Execution Plugins datastore create datastore insert datastore datastore connection Transaction Plugins datastore transaction commit transaction DataStore Date and Timestamp Types Adding INDEX to DataStore Column Passing Data Store records to Export DataStore plugins can be of two types: execution (perform some action, generally database queries) transaction (transaction management and configuration) Execution Plugins datastore This plugin is used to select data from Data Stores (or execute any query). Attributes Name Required Default Description name yes Data Store name. max no Max row count in result. This plugin returns a ListVariable that contains a query result from the Data Store. Example Example See how to work with result set retrieved by SQL query {query} update @this set {statusColumn} = 'NEW' where @id in ( {org.apache.commons.lang.StringUtils.join(ids, \",\")}); create datastore This plugin is used to create a Data Store (db table) Attributes Name Required Default Description name yes Data Store name. Child Elements Name Required Default Description datastore column no New column description with the following attributes: name type TEXT, INTEGER, DATE, or TIMESTAMP (default TEXT) Example insert datastore This plugin is used to insert a single record into your Data Store. Attributes Name Required Default Description datastore name yes Data Store name. json value map no java.util.Map instance converted to JSON (or com.freedomoss.crowdcontrol.webharvest.plugin.datastore.dto.DataStoreRow since version 7.0; json format like {\"values\": {\"columnName\":\"column1\",\"columnType\":\"TEXT\",\"value\":\"value1\"},{\"columnName\":\"column2\",\"columnType\":\"INTEGER\",\"value\":\"5\"} }); in case of a simple \"column name:value\" map, TEXT type is selected by default. Available types : TEXT, INTEGER, DATE, or TIMESTAMP. create no false If set to true Data Store will be automatically created if does not exist, missing columns will be automatically added to the Data Store. As a result, a new Record is inserted into specified Data Store. Map entry keys are column names and map entry values are values to be inserted into the appropriate column. Example datastore connection By default, when using the plugin, you are connecting to a database that is configured on the WF server. When running a config locally from WorkFusion Studio, data stores are searched in the datastores directory. The plugin is intended for connecting to an external database and performing queries of its child plugins (datastore, create datastore, insert datastore). Attributes Name Required Default Description url yes Database connection URL (JDBC URL) username no credentials, if needed password no The following example contains instructions to: connect to an external PostgreSQL database; find a Data Store with the \"Countries _Lookup\" name; and perform a query \"select * from @this\". Example This plugin returns a list variable that contains a query result from the Data Store. Transaction Plugins Data Store transactions can be of two types: implicit (transaction starts implicitly, by declaration in execution plugins) with defined commit part explicit (everything inside plugin body is executed inside one transaction). Data Store creation and schema update are non transactional operations. datastore transaction This plugin explicitly defines a transaction. It supports nested transactions (in this case nested transaction is executed like a separate transaction). The result of inner datastore plugin execution is committed only at the end of body execution. Attributes Name Required Default Description id no Transaction id. isolation no Transaction isolation level (possible values: READCOMMITTED, READUNCOMMITTED, REPEATABLE_READ). Example commit transaction This plugin is used in conjunction with an implicit transaction declaration. When an implicit transaction is used, it will be committed only when this plugin is used (for specified transaction). Attributes Name Required Default Description transaction id yes Transaction id. Example DataStore Date and Timestamp Types WorkFusion 7.5 introduces new Data Store column types: Date and Timestamp. Pay attention to DateTime formats provided as input, they are required to parse date and time correctly. DateTime Example Expand source Adding INDEX to DataStore Column This trick can be helpful for Data Stores containing reference data (e.g. for Select One answer type). Index names for PostgreSQL are public, so make sure that there is no duplication (suggested naming: ds DataStoreName ColumnName _idx). Name can't start from a digit. DateTime Example Expand source Passing Data Store records to Export Data Store to Export plugin Expand source select * from @this; "},{"version":"10.0","date":"Aug-14-2019","title":"bot-tasks","name":"Bot Tasks","fullPath":"iac/core/control-tower/bot-tasks/bot-tasks","content":" How to Create, Test, and Run Execution Details Results Hello World Example Related Materials Required background: XML, XPath, XQuery, Web Harvest, Groovy. Bot Tasks contain work to be done by WorkFusion's Bot. These Bots perform the steps in your Business Process which are more suited for automation than human labor. Bot Tasks can exist only as a Business Processes step. For example: address parsing, determining the width and height of an image, interacting with your API or repository, or filtering content that do not meet certain criteria. Bot Configs are written using Web Harvest library and are executed for each Record separately. For writing Web Harvest XML configs you can use pre defined Web Harvest XML elements described here or WorkFusion plugins. Bot Config Bundle is a bundled java library containing a bot config and a java code reference that can be used in this bot config. How to Create, Test, and Run Create and test a Web Harvest config using the WorkFusion Studio (Eclipse based IDE). Create a Bot Use Case in WorkFusion. Choose the Bot Use Case type (ETL is the best practice, because it better fits for reusing and adds flexibility) Create a BP and include a Bot Step based on this Use Case. Alternatively, you can use WorkFusion Repository and import a Bot Config Bundle to WorkFusion. Test the Bot Config in BP using a small input batch. Make changes, if needed: column names, use proxy, datastore connection, output column names. Update the created Bot Use Case and use it in your production BP. Alternatively you can create a Bot Task in BP (Design Process tab) from scratch (Blank Use Case) and paste your code, but this approach leads to multiple issues. Execution Details XML configuration is executing for each submission. If your input CSV file contains 40 Records, the WebHarvest XML configuration will be executed 40 times, once for each Record. If some error occurs, the Bot Config will be run 5 times. Results The results of a Bot Task are defined in the plugin settings. All the columns created by the Bot Task can be used in further BP steps (both Manual and Bot). Hello World Example The following example: gets HTTP response from URLs listed in the url to check column of the input data file; records the HTTP response into the http _response variable; exports all original columns and appends a new http column containing the http _response variable value. Bot Config Example Related Materials Bot Execution Lifecycle — applies to any type of Bot Task. Working with Bot Tasks in Control Tower Bot Tasks Context Bot Task Plugins Groovy in Bot Tasks Stateless Execution of Bot Tasks Bot Execution Lifecycle Tips and Tricks with Examples File manipulations with Samba services WorkFusion Repository "},{"version":"10.0","date":"Aug-06-2019","title":"etl-converting-bot-configs-to-forms","name":"ETL: Converting Bot Configs to Forms","fullPath":"iac/core/control-tower/bot-tasks/etl-converting-bot-configs-to-forms","content":" Convert Basic Bot Configuration to a Form How to Add to BP Form Examples RSS feed Retrieval SSI Database Lookup Cut Chapter from PDF (Split PDF)) How to skip escaping Bot Form (ETL) is a special Bot Config type intended for flexible re using by defining all initial parameters (variables) through a UI form and not through direct code editing. This feature allows business users to include Bot Configs into BPs and safely set their initial parameters and input output data **without editing the code**. The goal of this topic is to convert simple Bot config to a user friendly ETL form. Sample bot code {{{inputcolumnname}}} Pass = {{amount_limit}}}\"> Fail Sample ETL form Bot Configuration has Input Column, If statement and Output column. Form can define: input column name {{input column name}} limit for condition {{amount _limit}} output column name {{result column name}} In above case user would specify input column that contains “number”, specify the value “ 100” , output column name = “result” Convert Basic Bot Configuration to a Form Go to Advanced > Templates and use your existing Bot configuration code to create a new template with the Bot code. Make sure to select the Template Type = Bot Base Create a Use Case: Go to System Settings > Use cases Select Template and Name. Type Should be ETL Go to Advanced Mode > Add Answers Input Column Name (input column name) Condition (amount _limit) Output Column Name (result column name) Edit code for all three to go from e.g. {input column name} to: {{input column name}} to get column name or {{{input column name}}} to get the value of this column, not its name Save the ETL config. Use the ` directive to fix validation error in ` condition. See more information about here (http: freemarker.org docs refdirectivenoparse.html). How to Add to BP See full instructions here: Add Bot Task to BP Form Examples RSS feed Retrieval {{selfossurl}} {{markasread}} {selfossUrl} items items=200&amp;offset= {offset}&amp; type={{type}}&amp;topic={{topic}}&amp;tag={{topic}} RSS feed Retrieval SSI Database Lookup {{db_name}} {custodianbankswift} {custodianbankname} {currency} {correspondent_bic} false {{check_query}} SSI Database Lookup Cut Chapter from PDF (Split PDF) {{{start_page}}} {{{end_page}}} {{{pdfdocumentlink}}} {{{offset}}} = tocstart.toInt() offsetInt) && (tocend.toString().isEmpty() currentPage Cut Chapter from PDF How to skip escaping String result = org.apache.commons.lang3.StringEscapeUtils.unescapeJava(\"{{text}}\"); How to skip escaping "},{"version":"10.0","date":"Oct-16-2019","title":"etl-bot-user-friendly-configuration","name":"ETL Bot: User-friendly Configuration","fullPath":"iac/core/control-tower/bot-tasks/etl-bot-user-friendly-configuration","content":" Goals of ETL Bot Feature In some cases from time to time you should make minor adjustments in configurations of Business Process steps. You may need such easy configurable options for every launch or every week or month. It's not a problem for a developer to open a Bot and change a few variable values there. But it's another story when an in production Business Process is managed by Operations SME or similar non technical persons. Such people do not dive too deep in Bot structure, can get scared to modify the code or can modify it in a wrong way. To avoid such inconveniences WorkFusion provides the ETL Bot feature. Once a Bot s converted to an ETL Bot, the end users of Business Process get an editable form with HTML like fields instead of Groovy XML code. So, an easy and error proof configurability is achieved. Rather than enabling simplicity of configuration, an ETL Bot can encapsulate a portion of variables transformation and or additional calculations. It means, a user provides the required data and then they are converted into required variables for . So, an ETL Bot consists of 2 parts: UI form with easy to configure parameters; Additional Groovy XML code you require to prepare variables for next steps. Setup Guide You can start either with an existing Bot or a new one. Let's consider a simple example: ETL Bot with 3 editable fields If a Bot is already created, just copy the Bot source code to the clipboard. Bot Sources are deprecated in Intelligent Automation Cloud 10.0. Go to Advanced > Templates and Create a new template. Enter Template Name. For Template Type select Bot Base. Paste your existing Bot code from the clipboard. If the template is new as in our example, use the following code: {{etlcolumnnamevar1}} {{etlcolumnnamevar2}} {{etlcolumnname_var3}} Make sure you have each UI field (e.g. {{etlcolumnname_var3}} ) defined in the `` section. Otherwise such field will not appear on the UI form of the ETL Bot Go to Configuration > Use Cases. Select Bot tab. Create Use Case. Enter Name, select Category where your Use Case will appear for reuse. In Base Template select template created in 2 above. In Type select ETL. At this point we need to configure all variables we want to have as fields in the UI. To do so, use the right panel. Click* Add Answer* to add field by field. In our example we are adding 3 Free Text variables: Once Use Case is created, you can use it in any Business Process. Open the Business Process from Business Processes list to edit or create a new one. In Design tab just drag a new Bot into canvas and double click the Bot square bar. Control Tower will ask you to choose Use Case to start from. Use the recently created Use Case from 4 See that an ETL Bot has 2 modes: BASIC EDITOR (opens by default) and ADVANCED EDITOR. Make sure you see 3 input fields for our example in BASIC mode: ETL Bot Examples Now, we modify our initial example and add a piece of custom logic to the ETL Bot we've created. Let's assume that our task is to ask a user to provide variable names and save those variable values to Control Tower's Data Store. Once done, we need to output the variables to the next Business Process step. The modified ETL Bot is as below: {{etlcolumnnamevar1}} {{etlcolumnnamevar2}} {{etlcolumnnamevar3}} var1 var2 var3 CustomDashboardData uuid bpname As a result for a Business Process user the ETL Bot in BASIC mode looks exactly as shown on the screenshot from 6 above. In addition, in run time this Bot applies the user configuration to the custom code we've added, i.e. the variables typed in by the user are stored in DataStore and output into . Goals of ETL Bot Feature Setup Guide ETL Bot with 3 editable fields ETL Bot Examples "},{"version":"10.0","date":"Aug-06-2019","title":"excel-to-from-list","name":"Excel to-from List","fullPath":"iac/core/control-tower/bot-tasks/excel-to-from-list","content":" These plugins read data from xlsx or xls formats and write it to List > row: columnName:value and vice versa. excel to list This plugin must contain a child plugin (http extended or file) which returns an excel file. Do not use excel to list with a standard webharvest http plugin – use the WF http extended plugin. Webharvest http plugin has a bug that causes incorrect encoding while reading binary XLS(X) fies. excel to list example Output For this file (excel to list.xlsx), after config execution, the \"approver\" variable will have the following value: result O&amp;T D&apos;Onofrio, John Greenbaum, Richard Marcucci, Mark Nerlino, Vince O&apos;Leary, Terry Rao, Jagdish Stomberg, Sheree list to excel This plugin must contain a child node (var or template) which returns a list. Attributes Name Required Default Description format no xlsx You can also use format=\"xls\" to generate a file in excel 97 (binary) format. Output This plugin returns a byte array. list to excel example { \"firstname\":\"John\", \"lastname\":\"Smith\" }, { \"firstname\":\"Kevin\", \"lastname\":\"Johnson\", \"comment\": \"Optional comment\" } list to excel s3 example "},{"version":"10.0","date":"Aug-06-2019","title":"file-manipulations-with-samba-services","name":"File manipulations with Samba services","fullPath":"iac/core/control-tower/bot-tasks/file-manipulations-with-samba-services","content":" Current article describes typical file folder operations with Java Samba services."},{"version":"10.0","date":"Aug-06-2019","title":"groovy-in-bot-tasks","name":"Groovy in Bot Tasks","fullPath":"iac/core/control-tower/bot-tasks/groovy-in-bot-tasks","content":" Basic Level Video Lessons 65 Groovy Coding Samples Advanced Groovy: Functional Programming Comprehensive Groovy Book to learn Additional Learning Resourses Using Groovy for WorkFusion Bot Tasks Pass Objects Between Groovy Blocks Working with WebHarvest objects in Groovy Assignments 1. Practice JSON Manipulations 2. Use Groovy for parsing XML responses Basic Level Video Lessons 65 Groovy Coding Samples Advanced Groovy: Functional Programming Comprehensive Groovy Book to learn Code examples from this book Additional Learning Resourses Using Groovy for WorkFusion Bot Tasks Pass Objects Between Groovy Blocks There are several options to pass an object or variable between script blocks: Use the Web Harvest sys object: define a variable* without type* or without def: Use the putAt() method of Groovy Binding API: or simple syntax: putAt(\"newcolumn\", exportinfo); If you need to create a Web Harvest Node variable which can be passed to other plugins (except export), do not use the putAt() method. Example dynamic variables having GLOBAL scope, i.e. available between blocks and down in WebHarvest as well simpleGroovyString = \" hello\" creating a global variable using web harvest sys object def localString = \" Var from script 1\" sys.defineVariable(\"global_var\", localString); > Working with WebHarvest objects in Groovy Assignments 1. Practice JSON Manipulations Create simple BP with single Machine Task. Load JSON from external file: How to read file from HTTP Get JSON with webharvest tag into Java variable and parse it with com.google.gson.Gson library (import com.google.gson.Gson) check online documentation how to use Gson library Out of parsed data calculate: Count of available toppings for ‘Cakes' List 'Old Fashioned' available batter types Calculate sum of ppu’s for all donuts Provide 3 calculations in export data of your MT How to export data (simple way) 2. Use Groovy for parsing XML responses On input you have file with 30 companies Study Google Places API Register and obtain your googleapikey Use Google Places API to verify: Address, Phone, Company Name. Also have API verification link in result data. When calling Google API expect XML as response type. Parse responses with Groovy. "},{"version":"10.0","date":"Aug-21-2019","title":"json-manipulation-plugins","name":"JSON Manipulation Plugins","fullPath":"iac/core/control-tower/bot-tasks/json-manipulation-plugins","content":" Json plugins are intended for the following operations: convert JSON string to object search in JSON (using XPath analog – JsonPath) delete JSON nodes change node values add new JSON nodes ObjectNode class API: Use API link for updated jackson: JsonPath SDL description: Online evaluator for test JSON expression: http: jsonpath.com JSON expression manual with examples from Stefan Gössner: Common Attributes Name Required Description expression no JsonPath expression to find a key in the Json. Examples: \" .response.roles\" \" .store.book (@.author == 'John Snow') \" json plugin converts a JSON string to an ObjectNode object and can contain other json plugins inside itself. included config with store json and new json _object variables \"element1\", \"element2\", \"element_3\" { \"store\": { \"book\": { \"category\": \"reference\", \"author\": \"Nigel Rees\", \"title\": \"Sayings of the Century\", \"price\": 8.95 }, { \"category\": \"fiction\", \"author\": \"Evelyn Waugh\", \"title\": \"Sword of Honour\", \"price\": 12.99 }, { \"category\": \"fiction\", \"author\": \"Herman Melville\", \"title\": \"Moby Dick\", \"isbn\": \"0 553 21311 3\", \"price\": 8.99 }, { \"category\": \"fiction\", \"author\": \"J. R. R. Tolkien\", \"title\": \"The Lord of the Rings\", \"isbn\": \"0 395 19395 8\", \"price\": 22.99 } , \"bicycle\": { \"color\": \"red\", \"price\": 19.95 } }, \"expensive\": 10 } json simple usage example {\"color\":\"Black\", \"size\":\"Big\"} expression example 10) \"> json put This plugin creates a json node with a defined key and value. The value is set inside the plugin. Attributes Name Required Description key yes json key json put example SUCCESS {} 25 John Doe json set This plugin sets a value to the Json node found by expression. json set example 5.77 json add This plugin adds a new node to the parent json. json add example SUCCESS {} 25 John Doe ROLE_ADMIN ROLE_POWERUSER ROLE_DATASTORE Agent Smith json delete This plugin deletes nodes that match an expression of the parent plugin. json delete structure json delete example Examples Complex json example Expand source Just String Value Just String Value { \"store\": { \"book\": { \"category\": \"reference\", \"author\": \" {dynamic_author}\", \"title\": \"Sayings of the Century\", \"price\": \"8.95\" }, { \"category\": \"fiction\", \"author\": \"Evelyn Waugh\", \"title\": \"Sword of Honour\", \"price\": \"12.99\" } , \"bicycle\": { \"color\": \"red\", \"price\": \"19.95\" } } } 5.77 5.77 SUCCESS 25 Pavel Valetka ROLE_ADMIN ROLE_POWERUSER ROLE_DATASTORE {} {} "},{"version":"10.0","date":"Oct-16-2019","title":"ocr-plugin","name":"OCR Plugin","fullPath":"iac/core/control-tower/bot-tasks/ocr-plugin","content":" Important Information Be aware that OCR plugin works in a synchronous fashion, and therefore, would become a performance bottleneck, if used within BP where either OCR page volume is high, or documents are larger than 20 pages. It is suggested to use the prepared BP artifacts where OCR logic is asynchronous. The plugin is available since Intelligent Automation Cloud 8.4.0. ocr ocr image ocr pattern Examples ocr The `` plugin is used to recognize text in images using WorkFusion OCR service. The ` plugin must contain one or more plugins and zero or one plugin. and plugins need not to be nested immediately in , but can be nested into other tags, that are directly or indirectly nested in `. In secure storage, JWT parameters should be defined. They are used by `` plugin to authenticate to OCR Service. properties in secure storage jwt.secret= jwt.issuer=workfusion In CATALINA _HOME conf workfusion.properties, `` plugin parameters should be defined. workfusion.properties ... base URL of the OCR instance where to send requests ocr.api.base_url=http: your ocr host.com api for how long to wait for OCR to complete ocr.completion_polling.timeout.seconds=1800 how often to check OCR completion status ocr.completion_polling.interval.seconds=15 for how long to keep jwt authentication token before generating a new one. should be Name Default Description correct skew true See correctSkew in ocrsdk. export format TXT Specifies the set export formats. This parameter can contain up to three export formats, separated with commas (example: \"txt,pdfSearchable,xml\"). Supported formats are as follows: html – HTML page pdfSearchable – enables to search text in a file xml – file contains characters words along with their location in the original document (coordinates frames) xmlForCorrectedImage – the same as XML, except for location is taken from a processed adjusted document txt – plain text the default format The corresponding results are in the getResults() map property in the &lt;ocr&gt; plugin result, where each result is accessible by an export format name as a key. xml write recognition variants false Makes xml xmlForCorrectedImage formats to contain all variants of character word OCR considered as a possible recognition. Also see xml:writeRecognitionVariants in ocrsdk. profile documentConversion See profile in ocrsdk. language English See language in ocrsdk. engine ABBYY Currently can be only ABBYY. correct orientation true See correctOrientation in ocrsdk. custom regions Defines regions to process. Has JSON format: {\"page\":1,\"left\":0,\"top\":0,\"right\":800,\"bottom\":600, \"type\":\"BTText\" } Possible values: BTText, BTRasterPicture, BTTable, BTBarcode, BTVectorPicture, BTCheckmark, BTCheckmarkGroup. See more details here in BlockTypeEnum section. Remember that the XML syntax applies to the machine config. Thus, the above JSON should be included according to the XML escaping rules. Particularly, you can either replace \" with &amp;quot; escape sequence, or you can enclose the attribute value with single quotes ' instead of double quotes \". skip preprocessing false Can be true or false. use only custom regions false Can be true or false. If true, OCR will skip original analyzing stage extract information from custom regions only . use words from dictionary only false Can be true or false. Not supported for multiple images. dictionary A string containing a new line separated list of words to use as a dictionary to improve recognition. The set of words extends, not limits, the default dictionary. Not supported for multiple images. alphabet extension A string with extra symbols to extend already defined alphabet . allowed region types Comma separated list of allowed region types for identified blocks classification, e.g., to suppress classifying any block as picture (BTRasterPicture) specify the parameter value as BTTable,BTText,BTBarcode,BTVectorPicture. custom alphabet Specifies a set of characters to be used as an alphabet for recognition. skip text layer extraction false Text layer of the source PDF file is not used, only the image layer is recognized. original text Not supported for multiple images. remove noise models Instructs to remove noise on the image. Contains comma separated values: CorrelatedNoise, WhiteNoise. The method can be used for color and 8 bit gray images only. remove garbage size Instructs to remove garbage (excess dots that are smaller than a certain size) from the image. Should be &gt; 0; or 1 to automatically detect garbage size. debug false Can be true or false. Instructs to keep extra logging and data for debugging purposes. Do not use for text with sensitive data. use default pattern false Can be true or false. Instructs to apply default pattern (bundled with the OCR application). Cannot be used together with the &lt;ocr pattern&gt; plugin. discard color image false If you work with black and white images or the color of images is not important, set the discard color image to true. enhance local contrast false Specifies whether the local contrast of the image should be increased. Such preprocessing may increase the quality of recognition. It is effective for: photos or scans of documents with texture or pictures in the background photos or scans of documents with highly colorful background or text highlighting priority 0 An integer number, defines priority in the image processing queue. Max priority is limited to value of spring.int.messaging.priority.max (10 by default). timeout ocr.process.timeout value property in application.properties of OCR Rest Service An integer number, defines timeout in seconds. completion polling timeout seconds ocr.completionpolling.timeout.seconds property value in application.properties config of WorkFusion or 15 Defines for how long to poll OCR status while waiting completion. completion polling interval seconds ocr.completionpolling.interval.seconds property value in application.properties config of WorkFusion Defines how often to poll OCR completion status. api base url ocr.api.baseurl property value in application.properties config of WorkFusion Base URL for OCR HTTP API. For more information, refer to OCR API at docs and processImage at ocrsdk. Result In case of success, the `` plugin returns Java Bean of type OcrResult, that provides response metadata (status, task ID, etc.) map of downloaded results, i.e.: format > byte array of content (as the result can be a PDF which is not necessarily convertible to a string). Property Name Type Description results Map&lt;String, byte &gt; Maps where key is export format and value is recognition result content. id String OCR task ID. status String Describes resulting execution status. Supposedly, it will be always Completed. The &lt;ocr&gt; plugin polls OCR Service for OCR task completion. In case it encounters timeout or recognition failure, it throws an error. Possible values: Submitted, Queued, Scheduled, InProgress, Completed, ProcessingFailed, Deleted, Cancelled. registrationTime String containing date time Time of the first submitImage or processImage. processStartTime String containing date time Time of recognition process start (more precisely, when it was taken from the queue). processEndTime String containing date time Time of recognition process completion or failure. statusChangeTime String containing date time Time when OCR task status was changed: submitted, queued, started, completed, failed, cancelled. message String Currently used only in one case to indicate that task was \"Cancelled by expiry\". If the image recognition fails, or network error, or infrastructure error, or any other error happens, an exception is thrown from the `` plugin. ocr image To pass an image to OCR, you need to pass a byte array as a body of `. Make sure you do not pass an image as a string, otherwise you may get a runtime error or data corrupted. The plugin must be nested in the corresponding plugin. You can use multiple per one plugin. In this case, the images will be joined and recognized together into a single resulting document. In other words, the number of resulting images in the getResults() property depends on the number of export formats specified in the export format attribute, but not on the number of images passed with `. The plugin does not accept any attributes. Note that OCR for RPA does not support multiple images per single as well as images in the PDF format. An attempt to use those will fail. ocr pattern To pass a pattern to OCR, you need to pass a byte array as a body of `. Make sure you do not pass the pattern file as a string. The plugin must be nested in the corresponding ` plugin. You can use only one per one plugin. Which pattern will be applied if multiple are passed is unspecified. The plugin do not accept any attributes. Also, you cannot use with the use default pattern attribute of the plugin. To enable recognition of special symbols in the text, you might need to use a pattern. To retrieve a pattern, you can use the POST api v1 cloud trainPattern action. For more information, see POST trainPattern. For the advanced information on pattern training, see here. Examples The typical plugin usage is as follows. The variable ocrResult of type OcrResult contains the result. Due to WebHarvest design peculiarities, the access to the result looks like this. You can pass the following entities: multiple input images multiple input images in a loop patterns multiple export formats a sample image and a pattern `` with caching documentLink = \"http: yourdomain.com yourpath\"; cacheTable = \"yourocrcachetable_name\"; if (http.statusCode.toString().matches(' 45 d{2}')) { throw new RuntimeException(\"failed downloading the link: \" documentLink); } documentHash = org.apache.commons.codec.digest.DigestUtils.md5Hex(document.toBinary()); select * from @this where \"key\"=' {documentHash}'"},{"version":"10.0","date":"Aug-06-2019","title":"s3-plugins","name":"S3 Plugins","fullPath":"iac/core/control-tower/bot-tasks/s3-plugins","content":" s3 s3 get s3 list s3 delete s3 put s3 copy s3 The plugin is used to access and manage data on Amazon S3 server. It wraps all S3 calls and keeps the connection. S3 instance URL and default bucket name are defined in workfusion.properties using the following properties respectively: s3.endpoint.url s3.bucketName When running examples, mind to create a bucket on S3 and use its name in scripts, e.g., s3 bucket=\"my bucket name\". As S3 credentials contain sensitive information, they are stored in Secrets Vault and defined via the following secure properties: s3.access key s3.secret key When using the plugin in WorkFusion Studio, refer to the S3 Access section of WorkFusion Studio User Guide for information how to configure access to S3. Attributes Name Required Default Description bucket Yes S3 bucket name access key No S3 access key value secret key No S3 secret key value s3 keys type No S3 access secret keys type endpoint url No used to override the defined endpoint by the value from the parent S3 plugin or s3.endpoint.url in workfusion.properties. The plugin returns the list containing instance(s) of the com.freedomoss.crowdcontrol.webharvest.plugin.s3.S3ResultItem class (one per uploaded file). You can override S3 credentials in plugin, but it should be avoided on Production instances. s3 keys type for less then 9.1.x version A secret key to S3 can be mapped to a certain context where the key is used. The mapping is stored in Secrets Vault as shown below (for type = \"sfs\" and \"vds\"). s3.context.key.map : {\"sfs\":{\"secret key\":\"123Abc\",\"access key\":\"123Abc\"},\"vds\":{\"secret key\":\"123Abc\",\"access key\":\"123Abc\"}} To override it in the plugin, you can use the s3 keys type attribute Example s3 keys type for 9.1.x version or higher Acess key, secret key and url to S3 can be mapped to a certain context where the key is used. The mapping is stored in Secrets Vault as shown below (for type = \"sfs\" and \"vds\"): s3.context.key.map : {\"sfs\":{\"endpoint url\":\"https: s3.amazonaws.com\", \"secret key\":\"123Abc\",\"access key\":\"123Abc\"},\"vds\":{\"secret key\":\"123Abc\",\"access key\":\"123Abc\"}} To use these properties in the plugin, use the s3 keys type attribute. Example This allows to store specific data on a specifically designed S3 instance and provides the ability to switch across multiple VDS instances in a business process. s3 get Reads a single object (i.e., a file) from S3. Attributes Name Required Default Description name yes object name at S3 string yes object name at S3 binary yes object name at S3 auto no auto object name at S3 Example s3 list Gets a list of objects (files or folders) from Amazon S3 location. Supports filtering by RegExp file name pattern and dates in the dd MM yyyy hh:mm:ss aaa z format Attributes Name Required Default Description pattern no .* RegExp pattern to match against remote S3 location min date no min date for the last modified date in the dd mm yyyy hh:mm:ss aaa z format. max date no max date for the last modified date in the dd mm yyyy hh:mm:ss aaa z format. prefix no optional prefix parameter that restricts response to keys beginning with the specified prefix Example s3 delete Deletes a single object (i.e. file or folder) from S3. Attributes Name Required Default Description name yes object name at S3 Example s3 put Charset It is strongly recommended to provide charset in the content type, e.g., content type=\"text html; charset=utf 8\". Otherwise, the default html 4.0 ISO 8859 1 will be used. Async s3 put is asynchronous, and the operation is guaranteed to be done at exit from the ` plugin but not before. Do not mix the logic relying on the synchronous put inside of the single ` block. Upload file to Amazon S3. After uploading, it is accessible via the similar link on Amazon S3. Retry policy: up to three retries of IOException, 5xx error code and throttling error response. Attributes Name Required Default Description path yes full file name (including path) in Amazon S3 bucket acl no AwsExecRead access control list; possible values: AwsExecRead, Private, PublicRead, PublicReadWrite, AuthenticatedRead, LogDeliveryWrite, BucketOwnerRead, BucketOwnerFullControl For more information, refer API. content type no content type of uploaded file (pass to Amazon S3) content disposition no content disposition of uploaded file (pass to Amazon S3) expires in seconds no no signed URL generated period of time denoting how many seconds the signed url for this file will be live Signed URL will be stored in the signedUrl property of the returned object. You can access it via the getSignedUrl() method or via property getter – see the first example below. Examples Example 1 Expand source Example 2 Expand source s3 copy Copies a single object or folder to another folder (i.e. file or folder) from S3. Attributes Name Required Default Description from yes file path or folder (must end with \" \") to yes file path or folder (must end with \" \") acl no access control policy for any new buckets or objects Values: Private, PublicRead, PublicReadWrite, AuthenticatedRead, LogDeliveryWrite, BucketOwnerRead, BucketOwnerFullControl. For more information, refer to Amazon S3 docs. Do not try to copy a folder into a file as the plugin will throw an exception. Example "},{"version":"10.0","date":"Aug-19-2019","title":"required-plugin","name":"required","fullPath":"iac/core/control-tower/bot-tasks/required-plugin","content":" The plugin declares which column names (variables) must be passed in the input data file. If these columns are missing: in WorkFusion, a warning is displayed when saving a process: Warning! The use case you have selected does not correspond to some columns in the upload file. In this case, you are prompted to map input columns. in WorkFusion Studio, an error occurs: The following fields are required but were not defined: list of fields. `"},{"version":"10.0","date":"Oct-16-2019","title":"standard-web-harvest-processors","name":"Standard Web-Harvest processors","fullPath":"iac/core/control-tower/bot-tasks/standard-web-harvest-processors","content":" config This is the root element of every configuration file. Syntax configuration body Attributes Name Required Default Description charset no UTF 8 Defines default charset used throughout configuration. Every processor that needs charset information uses this value if other not explicitly set. scriptlang no beanshell Defines default scripting engine used throughout configuration. Allowed values are groovy or beanshell. Default engine is used wherever not other is specified. script and template processors have ability to specify scripting engine within the same Web Harvest configuration, that way giving possibility to even mix different scripting languages. Example Here, file processor uses explicitly defined encoding ISO 8859 1 and script processor uses explicitly defined language groovy. text Converts embedded value to the string representation. Syntax wrapped body Attributes Name Required Default Description charset no default configuration's charset Charset used if body is converted from binary to text value. delimiter no new line character Delimiter string used to separate items when concatenating them into single string. Example {i} (.)(2.3)(.*) {1}here were 2 and 3 {3} Variable named digits is defined using the while processor, producing sequence of 9 values. Next, the regexpr processor is invoked in order to do search replace on variable value. text processor is used to concatenate all digit values into single one. Without text processor, regular expression search would be applied to each item in the list (every digit) and that way no replace would occur, because there is no sequence \"2 *3\". var def Defines new or overrides existing variable with specified name and value. Syntax body as value of the variable Attributes Name Required Default Description name yes The name of variable. Should be valid like in most programming languages. overwrite no TRUE Boolean value (true or false) telling if existing variable with the same name will be overwriten or not. Example {i} This example defines the variable digitList which is the sequence of 9 values (digits from 1 to 9), and 10 simple variables digit1, digit2, ..., digit9 with values ranging from 1 to 9. var Returns value of defined variable. Throws an exception if variable is not defined. Syntax Attributes Name Required Default Description name yes Variable name Example google After execution, file named \"google _content.html\" contains page content of www.google.com. file Reads and writes content of the file or search directory for specified files. Syntax body defining content of the file if action=\"write\" or action=\"append\" ' Attributes Name Required Default Description action no read Defines file action. Valid values are read, append, write and list. path yes File path, relative to the working directory. type no text Type of file: text or binary. charset no Default charset for config Charset for text files. Has no effect if type is binary. listdirs no yes Tells whether to list directories (action = list). listfiles no yes Tells whether to list files (action = list). listrecursive no no Tells whether to recursively search directories (action = list). listfilter no Filename pattern to search for (* is replacement for any sequence, for any character). Works only for action = list. Example 1 Here, new file is created containing appended contents of three existing files, separated with lines. Example 2 ZIP file consisting of all JPEG images taken from specified directory is created. html to xml Cleans up the content of the body and transforms it to the valid XML. The body is usually HTML obtained as a result of http processor execution. Actual parsing and cleaning job is delegated to HtmlCleaner tool. Altough no special tuning is needed in most cases, cleaner may be configured with the several parameters defined with the processor's attributes. Syntax body as html to be cleaned Attributes Name Required Default Description outputtype no simple Defines how the resulting XML will be serialized. Allowed values are simple, compact, browser compact and pretty. advancedxmlescape no true If this parameter is set to true, ampersand sign (&amp;) that proceeds valid XML character sequences (&amp;XXX;) will not be escaped with &amp;amp;XXX; usecdata no true If true, HtmlCleaner will treat SCRIPT and STYLE tag contents as CDATA sections, or otherwise it will be regarded as ordinary text (special characters will be escaped). specialentities no true If true, special HTML entities (i.e. &amp;ocirc;, &amp;permil;, &amp;times;) are replaced with unicode characters they represent (ô, ‰, ×). This doesn't include &amp;, &lt;, &gt;, \", '. unicodechars no true If true, HTML characters represented by their codes in form &amp; XXXX; are replaced with real unicode characters (i.e. &amp; 1078; is replaces with ж). omitunknowntags no false Tells whether to skip (ignore) unknown tags during cleanup. treatunknowntagsascontent no false Tells whether to treat unknown tags as ordinary content, i.e.&lt;something...&gt; will be transformed to&amp;lt;something...&amp;gt;. This attribute is applicable only ifomitUnknownTags is set to false. omitdeprtags no false Tells whether to skip (ignore) deprecated HTML tags during cleanup. treatdeprtagsascontent no false Tells whether to treat deprecated tags as ordinary content, i.e. &lt;font...&gt; will be transformed to &amp;lt;font...&amp;gt;. This attribute is applicable only if omitDeprecatedTags is set to false. omitcomments no false Tells whether to skip HTML comments. omithtmlenvelope no false Tells whether to remove HTML and BODY tags from the resulting XML, and use first tag in the BODY section instead. If BODY section doesn't contain any tags, then this attribute has no effect. allowmultiwordattributes no true Tells parser wether to allow attribute values consisting of multiple words or not. If true, attribute att=\"a b c\" will stay like it is, and if false parser will split this into att=\"a\" b=\"b\" c=\"c\" (this is default browsers' behaviour). allowhtmlinsideattributes no false Tells parser wether to allow html tags inside attribute values. For example, when this flag is set att=\"here is &lt;a href='xxxx'&gt;link&lt; a&gt;\" will stay like it is, and if not, parser will end attribute value after \"here is \". This flag makes sense only if allowMultiWordAttributes is set as well. namespacesaware no true If true, namespace prefixes found during parsing will be preserved and all neccessery xml namespace declarations will be added in the root element. If false, all namespace prefixes and all xmlns namespace declarations will be stripped. prunetags no empty string Comma separated list of tags that will be complitely removed (with all nested elements) from XML tree after parsing. For exampe if pruneTags is \"script,style\", resulting XML will not contain scripts and styles. trimattributevalues no true By default white spaces from the start and end of attribute values are trimmed. This can be disabled to get the untouched, original attribute values in the resulting XML. omitdoctype no false Tells whether to remove HTML DOCTYPE section from the resulting XML. omitxmldecl no false Tells whether to put the XML declaration line at the beginning of the resulting XML or skip it. useemptyelementtags no true Specifies how to serialize tags with empty body if true, compact notation is used( &lt;xxx &gt;), otherwise &lt;xxx&gt;&lt; xxx&gt; hyphenreplacement no = XML doesn't allow double hyphen sequence ( ) inside comments. This parameter tells which replacement to use for it when double hyphen is encountered during parsing. booleanatts no self Tells cleaner what value to give to boolean attributes, like checked , selected and similar. Allowed values are self value of attribute is the same as attribute name (checked = \"checked\"), empty attribute value is empty string (checked = \"\") and true value of attribute is \"true\" (checked = \"true\"). Example Downloads the * www.motors.ebay.com * page and cleans it up producing pretty prented XML content. regexp Searches the body for the given regular expression and optionally replaces found occurrences with specified pattern. If body is a list of values then the regexp processor is applied to every item and final execution result is the list. Syntax body as pattern value body as the text source body as the result For each group inside the search pattern and for each found occurrence variables with names_ are created. See some Regular Expression tutorial for better explanation of groups. Attributes Name Required Default Description replace no false Logical value telling if found occurrences of regular expression will be replaced. Valid values are: true false or yes no. If this value is true (yes), then the regexp result needs to be specified with replacement value. max no Limits the number of found pattern occurrences. There is no limit if it is not specified. flag canoneq no no Enables canonical equivalence. flag caseinsensitive no no Enables case insensitive matching. flag dotall no yes Enables dotall mode. flag multiline no no Enables multiline mode. flag unicodecase no yes Enables Unicode aware case folding. Example 1 ( _ w d ) s = s ( w d s ) , . ; * var1= test1, var2 = bla bla; index=16; city = Delhi,town=Kingston; Value of variable \" {1}\" is \" {2}\"! Here, regular expression is looking for specified pattern in two strings, producing as a result list of five values: Value of variable \"var1\" is \"test1\"!, Value of variable \"var2\" is \"bla bla\"! ... Example 2 s , . ; var1= test1, var2 = bla bla; index=16; city = Delhi,town=Kingston; s , . ; var1= test1, var2 = bla bla; index=16; city = Delhi,town=Kingston; Here, the regular expression replacement produces single value as the result: var1= test1 var2 = bla bla index=16 city = Delhi town=Kingston . xpath Uses an XPath language expression to search an XML document. Syntax body as xml Attributes Name Required Default Description expression yes XPath language expression. Example The result is sequence of links from the page retrieved from www.nba.com. xquery Uses an XQuery language expression to query an XML document. Syntax body as xquery parameter value * body as xquery language construct Attributes Name Required Default Description name yes Name of XQuery parameter type no node() Type of XQuery parameter one of the values: node(), integer, long, float, double, boolean, string, node(), integer, long, float, double, boolean, string*. It is allowed to optionally specify multiple external parameters for the query. In most cases at least one, containing XML document is needed. For every specified xquery parameter the declaration inside the xq expression in the form: declare variable as external; is required in order to match the name and type of proceeded parameter. Valid parameter types supported by Web Harvest are: node(), integer, long, float, double, boolean, string and analog sequence types: node() , integer , long , float , double , boolean , string *. If not specified, default XQuery parameter is node(). Example { title} { author} { text} > The xquery is applied to the downloaded page resulting XML containing information about newspaper's articles. xslt Applies XSLT transformation to the XML document. Syntax body as xml body as xsl Example XSLT transformation, taken from the file is applied to the downloaded content. script Executes code written in specified scripting language – Groovy or BeanShell. WorkFusion Studio supports debugging for Groovy scripts only. BeanShell is not recommended for new script creation and is deprecated (backward compatibility only). Body of script processors is executed in specified language and optionally evaluated expression specified in return attribute is returned. All variables defined during configuration execution are also available in the script processor. However, it must be noted that variables used throughtout Web Harvest are not simple types they all are org.webharvest.runtime.variables.Variable objects (internal Web Harvest class) that expose convenient methods: String toString() byte toBinary() boolean toBoolean() int toInt() long toLong() double toDouble() double toDouble() Object toArray() java.util.List toList() Object getWrappedObject() The way to push value back to the Web Harvest after script finishes is command sys.defineVariable(varName, varValue, overwrite ) which creates appropriate wrapper around specified value : list variables for java.util.List and arrays and simple variables for other objects. The best way to illustrate this is simple example bellow. Each script engine used in the single Web Harvest configuration, once created, preserves its variable context throughout the configuration, meaning that all variables and objects are available in further script processors that use the same language. Syntax body as script Attributes Name Required Default Description language no Default scripting language if defined in config element, or beanshell if nothing is defined. Defines which scripting engine is used in the processor. Valid values are groovy or beanshell. return no Empty value Specifies what this processor should evaluate at the end and return as processing value. Example 11 4 1958 The day when you were born was {namedDay}. This example also shows that script internal variables once defined, are available in all the following script and template processors (namedDay). template For the given text content, parts surrounded with { and } are evaluated using the specified scripting engine. If no scripting language is specified, default one is used (see config element). Syntax body as text for templating Attributes Name Required Default Description language no Default config language Specifies script language that will be used for evaluation of parts surrounded with { and }. Valid values are groovy or beanshell . Example {sys.datetime(\"yyyy MM dd, HH:mm:ss\")} {sys.lf} {sys.lf} {my.process(content.toString())} Template uses some built in constants, functions and some user defined objects from variable context in order to produce desired content. case (if else) Executes conditional statement. Sequentially checks if some of the specified conditions in inner if elements is satisfied and if found one returns its body as the result. If no true statement is found result of execution is body of else statement if specified, or empty value otherwise.Syntax if body else body Attributes Example Contact is not defined! Here, conditional processor is used to check if previous xpath search has found contact information on the page. loop Iterate through the specified list and executes specified body logic for each item. Result is the list of processed bodies. Syntax body as list value body for each list item Attributes Name Required Default Description item no Name of the variable that takes the value of current list item. index no Name of the index variable, initial value for the first loop is 1. maxloops no Limits number of iterations. There is no limit if it is not specified. filter no Expression for filtering iteration list. It consists of arbitrary number of restrictions separated by comma. There are the following types of restrictions: n m , for specifying index range, for example: 3 6, 5. n m , for specifying sublist starting at index n and including items at indexes n m, n 2m, n 3m, ..., for example 1:2 for all odd, 2:2 for all even. unique, that removes duplicates from list comparing string values of list items. Valid filter which is combination of allowed restrictions is for example: 1 20,1:2,unique. empty no no If the value is true, equals to surrounding body by empty element, producing empty result of iteration. Example Loop iterates over the all unique image URLs from the yahoo website and for each URL downloads the image and stores it to the file system. Iterating over an input data array The loop_result output value will have results of your step executed for each element of the array from the input: {\"documenttitle\":\"Proof of document\",\"contentgroupf\":2,\"documentlinkf\":\"http: unknow.name 1474360372873testid.pdf\"}, {\"documenttitle\":\"Proof of identity\",\"contentgroupf\":1,\"documentlinkf\":\"http: unknow.name 1474360372888testid.pdf\"}, {\"documenttitle\":\"Proof 2\",\"contentgroupf\":10,\"documentlinkf\":\"http: unknow.name 1474360372880testid.pdf\"}, {\"documenttitle\":\"Proof of income\",\"contentgroupf\":10,\"documentlinkf\":\"http: unknow.name 1474360372884testid.pdf\"} while Loops while specified condition is satisfied. The result is list made of processed bodies in each iteration. Syntax: Attributes Name Required Default Description condition yes Expression that is evaluated for every loop and if its value is true, the body is executed. index no Name of the index variable, initial value for the first loop is 1. maxloops no Limits number of iterations. There is no limit if it is not specified. empty no no If set to true, equals to surrounding body by empty element, producing empty result of iteration. function Declares the user defined function. Syntax function body Attributes Name Required Default Description name yes The name of user defined function Example {sys.fullUrl(pageUrl, nextLinkUrl)} http: images.google.com images q=harvest&hl=en&btnG=Search Images&nojs=1 a @shape='rect' and .='Next' @href img contains(@src, 'images q=tbn') @src 5 Here the function named download multipage list is defined in order to serve multiple extractions. It collects link URLs from series of pages where XPath expression parameter is used to determine URL of next page with links if it exists. This situation is typical for list of products, or list of search results spanning multiple web pages. After that, the function is called with specified parameters in order to collect image links from Google images search limiting number of resulting pages to 5. return Returns value from the user defined function. Syntax body as return value Example See example of the function processor. call Calls the user defined function.Syntax body as actual parameter value Attributes Name Required Default Description name yes The name of user defined function Example See example of the function processor. try catch Wraps execution and for any recoverable exception returns default value without crashing the whole process. Syntax try body catch body Example No report file! File read exception is caught if occurred and default value is stored in the variable.To get log an error error description in the `` block, you can use the exception, exception message, and exception _stacktrace objects: exit Conditionally breaks the configuration execution. Syntax Attributes Name Required Default Description condition no true Condition that determines if execution will stop. Must be boolean value (true, yes, false, no). message no Optional message to the user if configuration is exiting. Will be part of logging information, or dialog warning will popup if Web Harvest is used in GUI mode. Examples Configuration is stopping execution if variable username is not defined. empty Wraps execution sequence and returns empty value. This element is used in situations when execution result is needless. Syntax Example > {amazonContent} New variable is created but its value doesn't participate in the result, because it is inside empty element. Instead, its value is used in the following template processor. mail Sends an email.Syntax mail content with optional attachments (mail attach elements) Attributes Name Required Default Description smtp host yes SMTP server host. smtp port no 25 SMTP server port. type no text Content type of the mail body: text or html. from yes The senders email address. reply to no The email address where replies should be sent to. to yes Comma separated list of recipient email addresses. cc no Comma separated list of cc email addresses. subject no Subject of the email. charset no default configuration's charset Charset of the email. username no SMTP server username. password no SMTP server password. security no none SMTP server security type: none, ssl or tls Example SPA Using HTML content in your email: WorkFusion Inc. is providing SPA* > > * Intelligent Automation Cloud > mail attach Adds an email attachment. Can be used only as part of mail processor of html type.Syntax body of the attachment Attributes Name Required Default Description name no Attachment N Name of the attachment. mimetype no image jpeg for inline attachments, application octet streamotherwise Mime type of the attachment. inline no no Tells whether attachment is embeded in the mail body. Example Here is me with ... > And this is ... > zip Creates a ZIP archive by compressing inner content defined by zip entry elements. To unzip, use the unzip plugin. Syntax ... entry content * ... Attributes Name Required Default Description name yes Name of the file inside ZIP archive. charset no default configuration's charset Charset of text file inside zip archive. Example This example creates an archive that includes list of specified files. This ZIP archive can further be sent via email, stored to database or file system, so that zip element can be inside mail, database, file or any other valid processor. ftp Creates FTP connection and executes some of valid ftp based operations against the server: ftp list,ftp get, ftp put, ftp del, ftp mkdir, ftp rmdir. Syntax * content to save Attributes Name Required Default Description server yes FTP server address. port no 21 FTP server port. username yes FTP server username. password yes FTP server password. account no FTP server account name. remotedir no Working remote directory on FTP server. path yes Path of the file directory to be accessed added removed. listfiles no yes Tells whether to include files in the list. listdirs no yes Tells whether to include directories in the list. listlinks no yes Tells whether to include links in the list. listfilter no Filter used for listing files. May include * and , i.e. my*.ex xml to json Converts given XML content to JSON. Syntax XML content Example select * from @this; json to xml Converts given JSON content to XML. See also JSON Manipulation Plugins. Syntax JSON content database Execute query against database. JDBC driver library file(s) should be provided on the classpath if used programatically, or on the same path with Web Harvest executable if used standalone. In case of SELECT sql statement, it returns list of row objects. They can be accessed with special accessor methods: .getColumnCount() returns number of columns returned. .getColumnName(index) returns name for column number. .get(column_index) returns field value for column number. .get(column_name) returns field value for column name. The whole list of returned db rows can be accessed by index to get individual row: .get(rowindex) For example: mydb.get(0).get(\"image\") Syntax select, insert or delete SQL query Attributes Name Required Default Description connection yes Properly formatted JDBC string for the database. It depends on database driver vendor. jdbcclass yes Fully qualified class name of the JDBC driver. username no Username to access database. password no Password to access database. autocommit no true Whether commit is performed automatically after query execution. max no no limit Maximum number of returned rows from the SELECT statement. Example 1 select name, salary from employee Salary of {emp.get(\"name\")} is {emp.get(\"salary\")} Example 2 insert into news (id, url, text, source) values ( {myId}, ' {myUrl}', ' {myText}', ' {mySource}') db param Specifies database parameter inside database element. Can be used for storing BLOBs (Binary Large OBjects). Syntax parameter value Attributes Name Required Default Description type no Binary if it's value is recognized as binary, text otherwise. Type of the parameter. Valid values are: int, long, double, text and binary. Example insert into logos (id, img) values ( 1, ) tokenize Splits given text to elements (tokens). Syntax content to tokenize Attributes Name Required Default Description delimiters no new line character Tells which characters are used as token delimiters. trimtokens no yes Tells whether to trim resulting tokens. allowemptytokens no no Tells whether to include empty tokens in the resulting list (consisting only of whitespaces). "},{"version":"10.0","date":"Oct-16-2019","title":"stateless-execution-of-bot-tasks","name":"Stateless Execution of Bot Tasks","fullPath":"iac/core/control-tower/bot-tasks/stateless-execution-of-bot-tasks","content":" Stateless Execution – is a feature that allows execution of Business Process Bot steps in memory without saving intermediate results to the database. Available from SPA 8.4.5 Business Cases This feature is extremely useful for the business processes of chatbot execution. Bot steps can be logically joined into one long transaction (Bot steps chain). For this transaction (this particular Bot steps chain) only input and output are important, intermediate results and analytics are not visualized, as the intermediate data are not saved. Results data of the intermediate steps are confidential and should not be saved to the database. Value Performance: the transition records between steps is accelerated, while the database load is decreased. Confidentiality: results from the intermediate steps of stateless execution chain are kept in memory only without storing in the database. Multi step transaction execution: a single logical transaction (for example, placing an order) can be split into multiple steps (login, navigate to order page, submit order and logout). Should a step fail, the transaction processing will start from the very beginning without retrying to complete the failed step. How to use In Business Process Designer the step context menu has got a new item called Stateless Execution as shown on the image below. To enable Stateless Execution select the following option: If a Bot step is selected for Stateless Execution its border will be shown as the dashed line to distinguish it from the normal Bot task execution. Note The stateless execution is aimed to be used in a chain of a few bot steps, rather than a single task. Records transition between Step1→Step2 and Step2→Step3 occurs in memory. Constraints Only the first stateless Bot step will be actually created (and stored in database), the next bot steps of stateless execution chain will be created in memory only. No statistics on intermediate steps are available. Results tab: Second column step is not actually created, all processed results are stored in One column step The WebHarvest context objects such as AwsHitDto or RunDto are created once in the first step of the stateless execution chain and reused in the next stateless steps. More details in the following table DTOs type Created in the first step Created in an active step RunDto X AwsHitDto X SubmissionDto X AwsHitAssignmentDto X CampaignDto X ProxySettingsDto X HitSubmissionDataItemDto X HitSubmissionDataItemValueDto X AutomationInfoDto TODO: actualize in the next sprint * * End step of a sub process is always in Stateless Execution state. It prevents the chain from being broken when using sub processes For a Stateless Execution step Split record and Split data are not supported One record into several and one record goes after rule to several branches (like split rule Join rule) If the \"Split Rule\" composite rule has the SPLIT _DATA option, a user will see the following validation message when saving the process. Record processing fails, if it tries to go through more that one step after the \"Split Rule\" rule has been executed. The error message appears in \"Events\" Threshold for streaming between stateless steps is always 1 Task Release plugin is not supported in stateless bot tasks. In case the release plug in is used, results are not exported and minimum execution date is applied to the hit in the beginning of the chain. So it behaves similar to retry. All errors in the Event log are related to the first stateless step only. Each failed stateless step will be mentioned in the message. In case of failure in the middle of stateless execution chain, Repair Retry starts from the first stateless step. Start, End, Manual Tasks and Rules cannot be marked as stateless. Additional cases A step can take part in stateless execution chain and regular execution at the same time. Example After the first step 5 records go to the Yes outcome and 5 records go to No: Third column step does not have statistics from stateless execution, but only history about 5 records from the regular execution Single Stateless Execution step works like a regular step with statistics and stores the results in the database: Troubleshooting Performance Analysis To analyse performance gained from executing a step as \"Statless\" you can use a jstack to identify the required step. The jstack contains the following information: run {runid} hit {hitid} step {stepid} (if stepid is not presented then it's the first step). Once the step has been identified, you can analyse how fast the step is executed as \"Statless\" and as a regular. Additionally, you can check information in the logs. Business Process Example The Business Process contains ~ 30 steps with several conditional rules, all of the steps are marked as stateless. Execution time of each record is logged to Event Log and a Data Store (MonitorExecutionDataStore), so the execution time improvements can be measured by running this Business Process with or without the stateless option enabled for the steps. Business Process Package Stateless BP with Rules (CT 117).zip Input file input file for stateless bp 50 records.csv "},{"version":"10.0","date":"Oct-16-2019","title":"secrets-vault-plugins","name":"Secrets Vault Plugins","fullPath":"iac/core/control-tower/bot-tasks/secrets-vault-plugins","content":" Versioning Info Available since Intelligent Automation Cloud 9.0. For 8.5 and older versions, see Deprecated Secure Store Plugins. secrets vault get secrets vault put secrets vault delete secrets vault update secrets vault reset Secrets Vault plugins provide functionality to manage Secrets Vault. Secrets Vault plugins work using the \" secrets vault\" WorkFusion Rest API. To learn more, refer to Secure Store API. At present, only the `` plugin is available for WorkFusion Studio. secrets vault get The plugin provides functionality to retrieve secure values from Secrets Vault. Attributes Name Required Default Description alias yes secure entry alias provider no SecureStorage secure entry provider Result Stores a secure entry in the config context. Example ... ... secrets vault put The plugin provides functionality to save confidential values to Secrets Vault. Attributes Name Required Description alias yes key value pair alias key yes secure name (usually username) value yes secure value (usually password) Result Boolean value: true or false, depending on whether an entry has been saved to Secrets Vault or not. Example ... ... secrets vault delete The plugin provides functionality to delete secure entries from Secrets Vault. Attributes Name Required Default Description alias yes secure entry alias Result Boolean value: true or false depending on whether the entry was deleted from Secrets Vault or not. Example ... ... secrets vault update 8.4.2 The plugin provides functionality to update secure entries (key and value) by alias. Attributes Name Required Description alias yes key value pair alias key yes secure name (usually username) value yes secure value (usually password) Result Boolean value: true or false depending on whether the entry was updated in Secrets Vault or not. Example ... ... secrets vault reset 8.4.2 The plugin provides functionality to reset a secure entry value for a specific alias. Attributes Name Required Default Description alias yes secure entry alias Result Boolean value: true or false depending on whether the entry value has bene reset in Secrets Vault or not. A new value is randomly generated (20 symbols). Example ... ..."},{"version":"10.0","date":"Aug-06-2019","title":"working-with-bot-tasks-in-control-tower","name":"Working with Bot Tasks in Control Tower","fullPath":"iac/core/control-tower/bot-tasks/working-with-bot-tasks-in-control-tower","content":" This chapter contains information on how to open, tweak, and edit Bot tasks in Control Tower. It is a good practice to develop and test Bot configs locally using WorkFusion Studio prior to including them into a BP in WorkFusion Control Tower. Create a Bot Use Case Add Bot Task to BP ETL: Converting Bot Configs to Forms "},{"version":"10.0","date":"Oct-16-2019","title":"workfusion-repository","name":"WorkFusion Repository","fullPath":"iac/core/control-tower/bot-tasks/workfusion-repository","content":" Versioning Info This article refers to Intelligent Automation Cloud Version 8.4 and newer. For information on Version 8.3, see 8.3 WorkFusion Repository (https: kb.workfusion.com display WF %5B8.3%5D WorkFusion Repository). Introduction WorkFusion Repository is a feature that allows developers to package Bot tasks in WorkFusion Studio and publish them to a repository in the WorkFusion instance for inclusion in Business Processes. These artifacts contain everything that's needed to run a Bot task, including Java and Groovy classes, input data, Data Stores, and any required settings. Repository Usage Demo Deployment Options This article describes how to automatically build and deploy Bot Config Bundle (BCB) project from local WorkFusion Studio to remote server based Control Tower. Configure Maven and Kickstart Archetype Maven is used to build projects for publishing, so first you need to set some preferences for Maven archetypes and provide their URLs. Archetypes define a special WorkFusion project structure which can be compiled into a java archive, uploaded to a Nexus repository and automatically published into WorkFusion instance, ready for integration testing and production usage. Open WorkFusion Studio. Go to Window > Preferences → Maven → Archetypes: Add two Remote Catalogs by clicking the Add Remote Catalog button: {width=\"600\" height=\"536\"} Use following Remote Archetype Catalog settings for training: Catalog File: Description: Workfusion Archetypes On premises installation configuration If there is no Internet access, edit Maven settings.xml file. It is located in the %user home% .m2 folder (for example, if your username is User, the path will be: C: Users User .m2) and should look as follows: settings.xml local nexus Local Nexus http: {host}:{port} nexus service local repositories {repository} content central where {repository} stands for the repository name with Maven dependencies required for goals execution. To resolve Bot Config Bundle (BCB) dependencies during training, you can use Nexus from WATT training instance. To point your Maven to it, edit the settings.xml file. It is located in the %user home% .m2 folder (for example, if your username is User, the path will be: C: Users User .m2) and should look as follows: mcb repo bcbdeploy Workfusion!5 wf repo mcb repo https: watt db1.workfusion.com nexus content repositories local mcb repo wf repo Click the Verify button to test connection and the repository contents Finally, click Apply and OK See on premises detailed configuration... Maven Repository Configuration To proceed, you should add Maven dependencies. It can be done in 2 ways: Using the settings.xml file: settings.xml Expand source ... profile id repo id https: {host}:{port} nexus content repositories repo profile id ... or, alternatively, Using pom.xml: pom.xml Expand source ... repo id https: {host}:{port} nexus content repositories repo ... Usage Example Let's say, we configure for the following repository: https: watt db1.workfusion.com nexus service local repositories wf machine config bundle content So configuration for this repository will have the following outlook: pom.xml ... watt https: watt db1.workfusion.com nexus service local repositories wf machine config bundle content ... Maven Repository User In case your Nexus repository requires authentication, you have to configure your Nexus credentials for Maven. By default Maven will look for settings in the following file: C: Users YOURWINDOWSUSER .m2 settings.xml For example, if your username is User, the path will be: C: Users User .m2 settings.xml You can create it using following content: Expand source repo id repo user Password99 Server id should match repository id defined in settings.xml or pom.xml If you get the \"Authentication ERROR\" exception during deployment, the possible reason is: the updated settings.xml file was not linked to WF Studio. You can manually link it the following way: Select Update Settings. Select your settings.xml in Window > Preferences > Maven > User Settings Creating Bot Config Bundle Project Now you can create a new project with a special type WorkFusion Bot Config Bundle (BCB): Create a new project by pressing CTRL N. Select the Workfusion >WorkFusion Bot Task Bundle item and click Next. Select an Archetype: currently WATT runs Intelligent Automation Cloud 9.2.0 so archetype version should be 9.2.0 The kickstart archetype contains a set of demo files and working config examples. It is perfect for understanding and testing the entire development workflow. The simple archetype contains only bare BCB project structure. Enter the Archetype parameters: Group Id Artifact Id Version (optional step) Modify the following project properties if needed: bundle name – name of the project wf version – version of your WorkFusion instance mcb repo url – Nexus repository URL for BCB deployment Click Finish. As a result, a new project will be created and displayed in the project explorer. Here is an overview of the BCB project structure: BCB project structure The src java folder contains Java and Groovy classes. Included and main Bot configs are in the resources folder. In a Bot config bundle, you can create your custom Java and Groovy classes and import them into Bot configs. By default, all results are written to the target output folder. The wfs data folder contains datastores and input csv files. The pom.xml file stores all information about the project and the URL to Nexus repository. Running and Developing Bot Configs The config development process is the same as standard WorkFusion Studio projects process. You can edit configs code, use include config, run, and debug configs. Refer to the Using WorkFusion Studio documentation. BCB project allows the user to: Create custom Java and Groovy classes in the src main java folder. For example, ExampleEntity.java, ExampleUtils.java Import these classes in the Bot configs (src main resources configs) in their `` sections. Use properties and methods from custom Java and Groovy classes inside `` sections. Expand source import com.workfusion.robotics.ExampleEntity; import com.workfusion.robotics.ExampleUtils; ExampleEntity entity = new ExampleEntity(); entity.setExampleStringProperty(some_variable.toString()); calcResult = ExampleUtils.multiply(2, 5); > When Bot configuration has been tested, you need to* run the project as a Maven build* and set the \"clean deploy\" command as a target. Maven creates a Bot Config Bundle (BCB) – a jar file in the \"target\" folder that contains main and included Bot configs together with Java and Groovy classes – and uploads this bundle to the Nexus repository. Deploying BCB to Nexus Repository Deploying BCB means pushing a Maven Artifact into a Maven Nexus repository. URL can be provided by changing the mcb.repo.url property during the project creation or by modifying the pom.xml file. To deploy into WATT training Maven repository change mcb.repo.url as following: https: watt db1.workfusion.com nexus content repositories wf machine config bundle BCB repository should come from the archetype. Usage Example Let's say, we have the following repository: https: watt db1.workfusion.com nexus service local repositories wf machine config bundle content link to 'no internet' section local nexus Local Nexus http: {host}:{port} nexus service local repositories {repository} content central If your Nexus repo requires authentication, you can setup it using the following manual: To deploy the BCB to Nexus: Right click the project folder → Run As → Maven Build. In the Goals field, enter the \"clean deploy\" command. Click Run. Watch the build process logs. If the build was successful, you will see the following result: Importing BCB to Intelligent Automation Cloud There is an option in Control Tower to enable Bot Config Bundles update and plugins update from a Nexus repository. This option can be enabled in the WorkFusion Secure Properties. Secure Properties are as follows: nexus.url – URL to a server where Nexus is located; nexus.user – a user name to access Nexus (the user should have the read permission in the respective repository); nexus.password – user password. Example nexus.user=nexus api user nexus.password=nexus api password For more information on Secure properties management refer to Manage Secure Properties (https: kb.workfusion.com display ID %5BCurrent%5D Manage Secure Properties) topic. Note When the access to a Nexus repository is configured as described above, the Import from Repository button in Advanced → **Bot Configurations** is enabled: Additional parameters of the import function are defined in the Platform Configuaration properties (workfusion.properties file). Platform Configuration Properties are as follows: webharvest.plugins.dir – local directory to store the downloaded plugins. webharvest.plugins.repo – a Nexus repository to download the plugins from. webharvest.machine.config.bundle.dir – local directory to store the downloaded Bot config bundles. webharvest.machine.config.bundle.repo – a Nexus repository to download the Bot config bundles from. The cron Bot config bundle syncronization is not available for Intelligent Automation Cloud 9.2.2, as the scheduled BCB update is removed. BCB update is triggered by user action or REST API command. webharvest.machine.config.bundle.import.cron – schedule defined as a cron expression to check for new versions of plugins and Bot config bundles and download them. webharvest.plugins.dir – local directory to store the downloaded plugins. webharvest.plugins.repo – a Nexus repository to download the plugins from. webharvest.machine.config.bundle.dir – local directory to store the downloaded Bot config bundles. webharvest.machine.config.bundle.repo – a Nexus repository to download the Bot config bundles from. The default values of the properties are as follows: webharvest.plugins.repo=webharvest plugins webharvest.machine.config.bundle.dir= {webapp.root} .. crowdcontoldata machine config bundle webharvest.machine.config.bundle.repo=wf machine config bundle webharvest.machine.config.bundle.import.cron=0 0 2 * * * webharvest.plugins.repo=webharvest plugins webharvest.machine.config.bundle.dir= {webapp.root} .. crowdcontoldata machine config bundle webharvest.machine.config.bundle.repo=wf machine config bundle Automatic Import from Nexus Control Tower scans through the configured Nexus repository and imports new versions of Bot Config Bundles on a predefined schedule in Platform Configuration properties. Manual Import To import all Bot config bundles manually: Open Campaigns → Bot Configurations menu in Control Tower. Click the Import from Repository button that will obtain all the new config versions from Nexus. Wait for some time until the repository is indexed and a new config appears in the list: When an BCB has been imported from Nexus, the WorkFusion engine automatically creates a Bot campaign, and your **robotics tasks are available in the process designer**. Afterwards, developers can create new versions of the same project, make changes, and upload them to Nexus where they will be version controlled. Troubleshooting Dependencies Load from the Central Maven Repository instead of Nexus Problem: On premise installation: dependencies load from the central Maven repository instead of Nexus. Solution: Go to Preferences → Maven → Installations and add you local maven installation where you have configured settings.xml with mirror. No JDK Compiler in the Environment Problem: Error during maven goals execution: ERROR Failed to execute goal org.apache.maven.plugins:maven compiler plugin:31:compile (default compile) on project trial1: Compilation failure ERROR No compiler is provided in this environment. Perhaps you are running on a JRE rather than a JDK ERROR &gt; Help 1 Solution: It means that you need to check **Preferences → Java → Installed JREs**. You need to add JDK at this page and switch configuration to it. Artifact does not appear in WorkFusion Problem: Artifact does not appear in WorkFusion. Solution: Possible reasons: Your artifact is not deployed to the right repository in Nexus. WF scans the following folder by default : http: {host}:{port} nexus service local repositories wf machine config bundle content Your artifact has not loaded automatically yet (artifacts are synchronized by a background job to synchronize artifacts but it launched ones a day by default). You can import artifacts manually: Bot Configurations → \"Import from Repository\" button Your artifact was already exist in WF. If you redeploy artifact to nexus and do not change groupId, artifactId and version (gav) then WF will not update this artifact, because we check: does this gav exist in application and download only artifacts with nonexistent gav. Wrong URL Setup Problem: On premise installation yields the following error: ERROR Plugin org.apache.maven.plugins:maven clean plugin:2.5 or one of its dependencies could not be resolved: Failed to read artifact descriptor for org.apache.maven.plugins:maven clean plugin:jar:2.5: Could not transfer artifact org.apache.maven.plugins:maven clean plugin:pom:2.5 from to internal repository (http: {host}:{port} nexus service local repositories): Not authorized , ReasonPhrase: Unauthorized. &gt; Help 1 Solution: Check mirror setup in Maven settings.xml. The error points out to the wrong URL setup. On This Page: Introduction Repository Usage Demo Deployment Options Configure Maven and Kickstart Archetype Maven Repository Configuration Maven Repository User Creating Bot Config Bundle Project Running and Developing Bot Configs Deploying BCB to Nexus Repository Importing BCB to SPA Automatic Import from Nexus Manual Import Troubleshooting Dependencies Load from the Central Maven Repository instead of Nexus No JDK Compiler in the Environment Artifact does not appear in WorkFusion Wrong URL Setup "},{"version":"10.0","date":"Aug-06-2019","title":"utilities-and-services","name":"Utilities and Services","fullPath":"iac/core/control-tower/bot-tasks/utilities-and-services","content":" alexa cache date format excel to csv export http extended include config language extractor list to csv log mail check pool release script var send message similarity score split task start to text unzip url validator validate var global who is "},{"version":"10.0","date":"Aug-06-2019","title":"accessing-rest-api-and-parsing-json-response","name":"Accessing REST API and parsing JSON response","fullPath":"iac/core/control-tower/bot-tasks/tricks/accessing-rest-api-and-parsing-json-response","content":" REST API is the way how one application can get information from or make another application perform action by Internet communication. In this sample you will be shown how to make REST API request to service provided by Yahoo for getting forecast and parse it. The sample will request REST service and than parse response by JsonSlurper class form standard Groovy library. Actual response can be viewed here. Our goal is to get temperature now, count of cloudy days and date of next rain. Accessing REST API and parcing JSON responce select * from weather.forecast where woeid in (select woeid from geo.places(1) where text=\"minsk, by\") and u='c' json "},{"version":"10.0","date":"Aug-06-2019","title":"apache-poi-working-with-ms-office-files","name":"Apache POI - Working with MS Office Files","fullPath":"iac/core/control-tower/bot-tasks/tricks/apache-poi-working-with-ms-office-files","content":" Study Apache POI API Reading Data from Excel Algorithm Description Advanced Example of Work with Excel BP consists of 3 Bot Tasks Read files from s3 task Format base files task (java poi) tasktask) Write to excel task Save XLS File to Data Store Apache POI Integration Save Read text from MSWORD doc and docx files Study Apache POI API http: poi.apache.org spreadsheet quick guide.html Reading Data from Excel For example, you need a way to open Excel sheet and read data from it within your RPA script. For this purpose, use the Apache POI library, which allows you to read, create, and edit Microsoft Office documents using Java. The classes and methods we are going to use to read data from Excel sheet are located in the org.apache.poi.ss.usermodel package. Algorithm Description Let's review simple BeanShell code which creates XLSX file and saves it into local file system. Include Apache POI packages into Bot Task. WorkFusion already includes JAR files of POI, you should not care about this. Just use it. Create an Excel spreadsheet in memory and fill it with some generated data. Save a generated spreadsheet into file. Advanced Example of Work with Excel BP consists of 3 Bot Tasks Input Data: Apache POI Working with MS Office Files Read files from s3 task This code is merely an example. In order for the code sample to work, replace the s3 bucket name with your bucket name. Read from S3 Expand source {datafileurl} {xmlfileurl} {templatefile_url} Format base files task (java poi) task Format base files Expand source convert list of maps to json string jsonString=com.freedomoss.workfusion.utils.gson.GsonUtils.GSON.toJson(list3); methods void parseXml() throws ParserConfigurationException, SAXException, IOException { this.columnsDetails = new LinkedHashMap(); Map rowLengthsMap = new LinkedHashMap(); DocumentBuilderFactory factory = DocumentBuilderFactory.newInstance(); DocumentBuilder builder = factory.newDocumentBuilder(); Document document = builder.parse(new File(xmlfileurl.toString())); get the startingchar from xml this.blockStartingChar = document.getElementsByTagName(\"StartingChar\").item(0).getTextContent(); get endingchar from xml this.blockEndingChar = document.getElementsByTagName(\"EndingChar\").item(0).getTextContent(); get starting distance from xml this.startingDistance = Integer.parseInt(document.getElementsByTagName(\"StartingDistance\").item(0).getTextContent()); get the row lengths and add them to a map NodeList rowLenghts = document.getElementsByTagName(\"RowLengths\").item(0).getChildNodes(); for (int i = 1; i getAllRowsList() throws IOException { Text file row list List rowsList = new ArrayList(); BufferedReader br = new BufferedReader(new FileReader(datafileurl.toString())); String currentLine; while ((currentLine = br.readLine()) != null) { add Text file rows to a list if (!currentLine.trim().equals(\"\")) { rowsList.add(currentLine); log.error(\"** \" currentLine); } } br.close(); return rowsList; } List filterRows(List rows) { List returningList = new ArrayList(); boolean flag = false; int count = 0; StringBuffer joinedLine = new StringBuffer(); for (int i = 0; i > getColumnsList(List rowsList) { return data separated list List> separatedColumnsList = new ArrayList>(); for (int i = 0; i columnsMap = new HashMap(); create iterator Iterator> it = columnsDetails.entrySet().iterator(); get the column lenths of the each column while (it.hasNext()) { Map.Entry pair = (Map.Entry) it.next(); get the column Object column = row.substring(0, pair.getValue()).trim(); get the other column row after substring row = row.substring(pair.getValue()); add to columns to list columnsMap.put(pair.getKey(), column); } separatedColumnsList.add(columnsMap); } return separatedColumnsList; } > {jsonlistof_maps} Write to excel task This code is merely an example. In order for the code sample to work, replace the s3 bucket name with your bucket name. Write to excel Expand source {excelfile} {fileLink.toString()} Save XLS File to Data Store This Bot config opens XLS file, reads the first tab, and creates a json data to insert into Data Store. Saving XLS file to Data Store Expand source jsonMap = new com.google.gson.Gson().toJson(listOfMap.get(Integer.valueOf(index.toString()) 1)); Apache POI Integration POI usage Expand source maxColumnNum) { maxColumnNum = srcRow.getLastCellNum(); } } } } for (int i = 0; i j; j ) { Cell oldCell = srcRow.getCell(j); ancienne cell System.out.println(oldCell); Cell newCell = destRow.getCell(j); new cell Enter the correct column selection condition for the if clause. if (oldCell.getColumnIndex() = 6) { switch (criteria) { case \"IGNORELAST3DIGITS\": baseNumber = cellValue.substring(0, cellValue.length() 3); break; case \"TAKELAST6DIGITS\": baseNumber = cellValue.substring(cellValue.length() 6); break; case \"IGNORELAST3TAKE6PRECEDINGDIGITS\": baseNumber = cellValue.substring(cellValue.length() 9, cellValue.length() 3); break; default: break; } } } System.out.println(\"::::: \" baseNumber); return baseNumber; } public int getBaseNumberColumn(String sheetName) { int filterColumnIndex = 0; switch (sheetName) { case \"C4CH1000 LC\": filterColumnIndex = 3; break; case \"C1101000 Outstanding\": filterColumnIndex = 5; break; case \"CFCBA032 LND OUSTANDING\": filterColumnIndex = 4; break; case \"CN CFCC000 CUSTOMER ACCOUNTS LI\": filterColumnIndex = 2; break; case \"CFCFX123 C CB7185 FX OUTSTAND\": filterColumnIndex = 2; break; default: filterColumnIndex = 1; break; } return filterColumnIndex; } public CellRangeAddress getMergedRegion(Sheet sheet, int rowNum, short cellNum) { for (int i = 0; i Save Read text from MSWORD doc and docx files This Bot config opens DOCX or DOC file (all formats are supprted using Apache POI library), reads text from it and saves txt version to s3. Input columns are: bp run id any UUID to keep files on s3 original document url link to DOCX or DOC file Output is txt document link column with txt file link Expand source {bprunid} "},{"version":"10.0","date":"Aug-06-2019","title":"bot-task-examples","name":"Bot Task Examples","fullPath":"iac/core/control-tower/bot-tasks/tricks/bot-task-examples","content":" Store Files in S3 Storage Read Data From Data Stores Crawl Website And Extract Information Using XPath Interaction with Web services (SOAP) ) Store Files in S3 Storage Sample will create simple text file in RAM and upload it into S3 storage. Then sample reads same file from S3 storage. Machine Task export data contains: URL to stored file Content of file As a preparation step you required to have access to AWS S3 OR S3 Emulator (depends on type of instance installation) and create your folder in \"aharhots.bucket\" to make example work. For input data use void input data.csv Work with S3 storage Expand source Read Data From Data Stores Example shows how to run SQL SELECT against WorkFusion DataStore. Machine Task export data contains all SQL SELECT results. As preparation create DataStore with name website monitoring using website monitoring.csv For input data use input _data.csv Work with DataStores Expand source Crawl Website And Extract Information Using XPath Example shows data extraction technique from real world goverment website. Using website search script gets companies (by iterating over alphabet as search keywords). For each company found we going to REGDEX documents (Documents button). Then we look for 1st available link to file with .txt of .htm extension. Link is provided in result data. Use alphabet.csv as input data Create simple BP containing 2 Machine Tasks and 1 Rule: \"CIK Codes Lookup\" Machine Task Expand source \",\"\"); > \"REGDEX Links Extractor\" Machine Task Expand source \",\"\"); > \",\"\"); reportsFixDoctype = reportsFixDoctype.replace(\" >\",\"\"); reportsFixDoctype = reportsFixDoctype.replace(\"&nbsp;\",\"\"); > Interaction with Web services (SOAP) Workfusion can definitely interact with SOAP web services. There is no magic behind the SOAP interaction, as long as you pass correct XML to your WS via plugin in your machine config. The snippet for the reference is below Expand source application soap xml; charset=utf 8 http: tempuri.org ISearchWorksAPIService Login text xml;charset=UTF 8 http: tempuri.org ISearchWorksAPIService Login {uname} {pwd} > When integrating with SOAP services, make sure you understand differences between SOAP1.1 and SOAP 1.2 standards. Just to summarize the most common differences WS SOAP 1.1 protocol a. Use the namespace on SOAP envelope b. pass value text xml on the Content Type header of the HTTP Request c. pass soap action definition via SOAPAction header WS SOAP 1.2 protocol a. Use the namespace on SOAP envelope b. pass value application soap xml; charset=utf 8 for the Content Type header c. Specify SOAP action according to the namespace definition"},{"version":"10.0","date":"Oct-16-2019","title":"connecting-to-windows-shared-folder","name":"Connecting to Windows Shared Folder","fullPath":"iac/core/control-tower/bot-tasks/tricks/connecting-to-windows-shared-folder","content":" There are several ways to download files from a Windows shared folder (SMB). Mount the folder to the application server Read files using RPA Java CIFS library (SMB1)) Other third party solutions Mount the folder to the application server You can mount this shared folder to Linux server with WorkFusion Intelligent Automation Cloud installation. Read files using RPA You can read files and upload them to S3 using RPA: Run a Groovy script on RPA server, read file and save it to s3 folder Expand source {testnodeid} {file_name} It will open file from C: users user name profile Downloads folder. Put the file into this folder only. node _id property can help to specify a particular RPA node id to launch and view this sample. Java CIFS library (SMB1) You can use the Java CIFS library , bundled with WorkFusion Intelligent Automation Cloud installation. Below you can find code example for reading list of files in shared Windows folder: List Files import jcifs.smb.NtlmPasswordAuthentication; import jcifs.smb.SmbException; import jcifs.smb.SmbFile; import jcifs.smb.SmbFileOutputStream; we need domain, folder path, user and password auth = new NtlmPasswordAuthentication(domain, user, password); file = new SmbFile(path, auth); files = file.listFiles(); Copying Files String user = \"user:user\"; NtlmPasswordAuthentication auth = new NtlmPasswordAuthentication(user); String path = \"smb: 192.168.101.40 Public test 123.txt\"; SmbFile sFile = new SmbFile(path, auth); FileUtils.copyInputStreamToFile(sFile.getInputStream(), new File(\"D: Test 123 test.txt\")); > Another one sample using Java CIFS Library API , which could be used for automation: SAMBA API Sample calls: SAMBA API, List files select * from @this; Save from Samba to S3 sample ,using Samba API code Save file from shared folder, Samba to S3 {secureEntryMap.getWrappedObject().get(\"sharedfolderaccountaa\").getKey()} {secureEntryMap.getWrappedObject().get(\"sharedfolderaccountaa\").getValue()} For more details of jcifs API refer to: Other third party solutions Use one established at your organization, or find an existing one. "},{"version":"10.0","date":"Aug-06-2019","title":"dealing-with-non-ascii-characters","name":"Dealing With non-ASCII Characters","fullPath":"iac/core/control-tower/bot-tasks/tricks/dealing-with-non-ascii-characters","content":" Sometimes you may get language texts with characters beyond standard ASCII. In this case if no action taken and strings passed to some API for example instead of characters you'll get broken ones. This can be handled by encoding initial strings to UTF 8 charset. "},{"version":"10.0","date":"Aug-06-2019","title":"exception-strategy","name":"Exception Strategy - Go to Next Step if Exhausted Retrying","fullPath":"iac/core/control-tower/bot-tasks/tricks/exception-strategy","content":" In case you want your step not to stop in the flow with the red icon. But instead you want it to go to the next step where you properly route it. Exception strategy "},{"version":"10.0","date":"Aug-06-2019","title":"file-conversion-tiff-pdf-xml-html","name":"File conversion - TIFF, PDF, XML, HTML","fullPath":"iac/core/control-tower/bot-tasks/tricks/file-conversion-tiff-pdf-xml-html","content":" TIFF to PDF PDF to Image PDF to XML PDF Manipulations: cut, get number of pages How to see tagged text locally the same as in IE answer TIFF to PDF tiff2pdf needs to be installed on the linux environment, where WorkFusion is running. tiff2pdf is not a part of the WorkFusion offering. Major browsers have some limitations displaying TIFF files inside (most of them propose to download original tiff). But sometimes there is a need to show content of tiff files in Human Tasks (e.g. for data extraction). Widely used solution is to convert original TIFF file into a PDF and show the converted PDF inside your Human Task. We recommend to use a built in Linux library – tiff2pdf (it's not part of WorkFusion package). Below you can find example of Bot config, which converts a TIFF file into a PDF and uploads the converted PDF to S3 file storage. TIFF to PDF Expand source import com.google.common.io.Files; import org.apache.commons.io.FileUtils; import org.apache.commons.io.FilenameUtils; import com.freedomoss.workfusion.utils.gson.GsonUtils; static File convert(String documentLink) throws IOException, InterruptedException { File baseDir = Files.createTempDir(); File inputFolder = new File(baseDir, \"input\"); URL documentUrl = new URL(documentLink); String inputFileName = FilenameUtils.getName(documentUrl.getPath()); if (inputFileName == null inputFileName.isEmpty()) { inputFileName = \"input.tiff\"; } File inputFile = new File(inputFolder, inputFileName); FileUtils.copyURLToFile(documentUrl, inputFile); File outputFolder = new File(baseDir, \"output\"); FileUtils.forceMkdir(outputFolder); File result = new File(outputFolder, \"output.pdf\"); invoke(new ProcessBuilder(new String { \"tiff2pdf\", inputFile.getAbsolutePath(), \" o\", result.getAbsolutePath() })); FileUtils.forceDelete(inputFolder); return result; } static void invoke(ProcessBuilder builder) throws IOException, InterruptedException { builder.redirectErrorStream(true); System.out.println(builder.command()); Process process = builder.start(); BufferedReader in = new BufferedReader(new InputStreamReader(process.getInputStream())); String line; while ((line = in.readLine()) != null) { System.out.println(line); } in.close(); int code = process.waitFor(); if (code != 0) { throw new RuntimeException(\"Failed to invoke process: \" builder.command() \". Return code: \" code); } } > Another version of the TIFF to PDF conversion, where user actually has control over quality of the output PDF file, is to use ImageMagic with GhostScript. If both utilities are installed on the Linux where WF is running, you may try changing invoke call to something like: the compress and quality parameters for convert utility are worth experimenting with to produce PDF of the acceptable quality and size. PDF to Image You may convert PDF document to a set of images. For example, if you need to pass those images to OCR. Refer javadoc for details. Expand source > PDF to XML On many online resources one may find that the task of conversion of PDF file into XML representation is like converting hamburgers to cows. In any case here is a few options to do that. Here is two ways how to do that (using Apache PDFBox and using Apache Tika): PDF to XML Expand source PDF Manipulations: cut, get number of pages PDF manipulation functions Expand source File baseDir = null; public String getCombinedDocumentsAbsolutePath(String documentLink) throws IOException, InterruptedException { baseDir = Files.createTempDir(); System.out.println(baseDir); File inputFolder = new File(baseDir, \"input\"); String links = documentLink.split(\" \"); String inputFilesStr= new String links.length ; for(int i=0;i firstXPages lastXPages}\"> 0 ; i ){ pdfPages.add(totalPagesCount i 1); } document.selectPages(StringUtils.join(pdfPages, \",\")); ByteArrayOutputStream cutPdfOutput = new ByteArrayOutputStream(); PdfStamper pdfStamper = new PdfStamper(document, cutPdfOutput); pdfStamper.close(); document.close(); int lastIndexOfSlash=targetFileName.toString().lastIndexOf(\" \"); String newFileS3Name = targetFileName.toString().substring(lastIndexOfSlash).replaceAll(\"\", \"_\") \" cut.pdf\"; > {org.apache.commons.lang3.StringUtils.join(resultLinks, \" \")} These functions can be used with the following parameters: Function usage Expand source How to see tagged text locally the same as in IE answer Run the following example using WorkFusion Studio. Replace in Bot config path to local files: INSERT YOUR PATH TO FOLDER WITH FILES Converting HTML tagged document Expand source *>)\", \" 1\" java.util.regex.Matcher.quoteReplacement(viewer.toString())); > "},{"version":"10.0","date":"Aug-06-2019","title":"fetching-whole-table-with-single-xpath-and-parse-with-groovy-wh","name":"Fetching whole table with single XPath and parse with Groovy/WH","fullPath":"iac/core/control-tower/bot-tasks/tricks/fetching-whole-table-with-single-xpath-and-parse-with-groovy-wh","content":" There are several ways, how to parse table on a web page, for example, with different XPathes cell by cell, using cycles. Here we demonstrate other approach, using single XPath. Lets suppose we have the following table from web site . The code can be like this:. The result .csv file will look like: "},{"version":"10.0","date":"Aug-06-2019","title":"executing-bash-script-from-bot-config","name":"Executing a Bash script from Bot Config","fullPath":"iac/core/control-tower/bot-tasks/tricks/executing-bash-script-from-bot-config","content":" If the shell script needs to be executed on the same server where Workfusion instance is running, you can execute it via java.lang.ProcessBuilder The code below may help you understand how to call a bash script. return shellScriptOutput; } * Invokes the to return any result, your script must output something to the out stream, you then capture it and return as the resutl of invoke() * static String invoke(ProcessBuilder builder) throws IOException, InterruptedException { builder.redirectErrorStream(true); log.debug(\"Executing shell command: {}\", builder.command()); Process process = builder.start(); BufferedReader reader = null; String output = null; try { reader = new BufferedReader(new InputStreamReader(process.getInputStream())); output = IOUtils.toString(reader); } finally { if (reader != null) { reader.close(); } } int code = process.waitFor(); if (code != 0) { throw new RuntimeException(\"Failed to invoke process: \" builder.command() \". Return code: \" code \". Output: \" output); } return output; } > Note, the above script will try to run a command bin bash your_shell.sh param1 param2 param3 If you can establish a ssh connect from the linux server where Workfusion is running to the Linux server where you need to execute your script, you may use the above code as well, but this time your invoke call should be something as below: this will print value of HOME variable declared for user2 on host2 "},{"version":"10.0","date":"Aug-06-2019","title":"html-to-xml-transformation-and-xpath-selectors","name":"HTML to XML Transformation and XPath Selectors","fullPath":"iac/core/control-tower/bot-tasks/tricks/html-to-xml-transformation-and-xpath-selectors","content":" There are some issues using XPath for huge documents, sometime document is valid but XPath does not return any values due to document formatting issues, nested comments, scripts and TDT slow validation. This workaround can help make it working: &lt;root&gt; {contentValue.toString().replace(\"&lt; xml version= \"1.0 \" encoding= \"UTF 8 \" &gt;\",\"\")}&lt; root&gt; "},{"version":"10.0","date":"Aug-06-2019","title":"loop-inside-business-process-repetitive-bot-execution","name":"Loop inside Business Process. Repetitive bot execution","fullPath":"iac/core/control-tower/bot-tasks/tricks/loop-inside-business-process-repetitive-bot-execution","content":" Attached business process contains logic with repetitive bot execution. That solution is very helpful for some RPA execution, when it is required to execute any particular bot few times during the day or week or when some customer infrastructure is not available for some time. Described approach is able to process one record multiple times until some successful condition will met. Please take a look into picture bellow. For example, lets suppose it is required to cancel some insurance policy using RPA bot, but system is not working as expected due some system error or communication issues. Provided business process is allowing to run with error bot \"Cancel Policy in TV\" 5 times with one day delay. If cases of one successful run, business process will complete processing. Repetitive bot execution.zip "},{"version":"10.0","date":"Aug-06-2019","title":"loading-external-jars","name":"Loading external JARs","fullPath":"iac/core/control-tower/bot-tasks/tricks/loading-external-jars","content":" Below is an example of how a developer can load in runtime external JAR files and start using right away. Loading external JARs to RPA node side. If you need to add external JAR files for executeScript (Groovy) on node side you must copy your JAR to the folder like c: RPA rpa grid dependency on your RPA machine and after that you can use them in your script code. View example for check below: executeSript example To make a test with this code you can download Apache common net lib from the official site and place it to dependency folder on RPA machine: When you place a new JAR file to the node side you must restart your RPA environment. "},{"version":"10.0","date":"Aug-06-2019","title":"manipulating-data-stores","name":"Manipulating Data Stores","fullPath":"iac/core/control-tower/bot-tasks/tricks/manipulating-data-stores","content":" Iterate through Data Store Query Result Convert datastore to Map Expand source Save All Input Records to Data Store Saving Records to Data Store Save Archived Data Store to S3 Input: Data Store name S3 bucket name Amount of records in one archive Path to a temp folder Result: link to S3 for each created archive Code example Expand source temp_bucket tmp {ds_name} true 1 0 200000 {start.toInt() amount.toInt()} SELECT count(*) FROM @this WHERE @id > {start.toInt()} AND @id &lt;= {end.toInt()}; {count.get(0).get(0).toInt()} {basefilename} {counter} COPY ( SELECT * FROM @this WHERE @id > {start.toInt()} AND @id &lt;= {end.toInt()} ) TO ' {filename}' DELIMITER ',' CSV HEADER; {end.toInt()} {counter.toInt() 1} false Fill Data Store from a ZIP File Located on S3 Input: Data Store name S3 link to a Data Store archive Output: fills a Data Store on a target WF instance Code Example Expand source tmp {ds_name} COPY @this FROM ' {file_name}' DELIMITER ',' CSV HEADER; "},{"version":"10.0","date":"Aug-06-2019","title":"how-to-wait-for-condition-in-bot-config-before-executing-further","name":"How to wait for a condition in bot config before executing further","fullPath":"iac/core/control-tower/bot-tasks/tricks/how-to-wait-for-condition-in-bot-config-before-executing-further","content":" Summary Imagine you are trying to delay further processing until some event happens or want to wait for an external system to produce a result. There are quite a few possible approaches to achieving this, each having it's own pros and cons. Here below, we cover a few examples. While loop with Thread.sleep This approach is considered to be a bad practice. You are strongly encouraged to use other, more effective approaches. Example of ineffective approach {taskid} {serviceurl} Waiting for the task to be finished by the xxx service {taskid} {serviceurl} Task finished by the xxx service Highlight of the issues in the code above: Memory leak Missing empty=\"true\". So plugin accumulates output of every iteration The logic inside of while is as a rule more complex and may cause heavy object creation, and accumulating those objects in memory Execution of other tasks prevented Such \"eager\" wait holds a thread from bot source. No error handling In case the service returns error response the code above will keep waiting Code duplication Invocation of getStatus Many of the issues can be mitigated by code refactoring or by utilizing alternative approaches. Release plugin Detailed description: release {taskid} {serviceurl} Task finished by the xxx service Waiting for the task to be finished by the xxx service The code demonstrated addresses the points described in the while loop with Thread.sleep section. There are still a few considerations to keep in mind: Wait timeout. If the remote service returns \"In Progress\" status for unexpectedly long time, the code above will still be patiently waiting and pinging the service every 3 minutes. Launching of the service. The code expects that the task execution has been already started, and will not try to send a re launch command. If this is required it should be a part of the business process logic. Trigger Inversion Sometime it's possible to control the system or events at the other end (where such events occur). For example once the task is finished launch the post processing business process or embed the required steps right there. See also: 9.x WorkFusion REST API task start Schedule Simple schedule can be configured to start a checker process to validate a condition on defined dates times. More details: Schedule Tasks and BPs "},{"version":"10.0","date":"Aug-06-2019","title":"memory-usage-optimization","name":"Memory usage optimization","fullPath":"iac/core/control-tower/bot-tasks/tricks/memory-usage-optimization","content":" There are some common mistakes in Bot Task design which can lead to memory consumption issues: Using and plugins without empty=\"true\" attribute Example of inefficient code: Recommended approach: Almost in all cases, you should add the empty attribute with the \"true\" value. Advanced: You don't need it only in case you assign use output of this plugin execution. Example when this attribute should not be present or should have value \"false\": The \"empty\" attribute is not used here because the output of the plugin is used in the plugin. Select all the data from a datastore Example of inefficient code: This script selects all the data from a Datastore, but only the first record is needed. As a result, big amount of unused data is loaded into memory. Recommendation: select only required rows and columns. Export large documents between steps The Control Tower engine stores all export results in its data base and loads them for the next processing step(s). Therefore, it's recommended to save large documents JSONs to Data Stores or S3 file storage (if data are not changed on next steps) and read them afterwards only when needed. When using a split rule, the Control Tower engine duplicates all outcoming data for each new record. For examle, you have a 10 MB document and split its content into 100 records. In this case, the Control Tower application will need 1Gb of RAM to successfully process the step with split data. In version 8.4, the batch processing was introduced for such cases, but it doesn't eliminate the issue completely. Export not useful data between steps The Control Tower engine stores all export results in its data base and loads them for the next processing step(s). Therefore, it's recommended not to export data which will not be used on the subsequent steps and not needed in the final snapshot. Consider usage of the include original data=\"false\" attribute in the plugin. "},{"version":"10.0","date":"Aug-06-2019","title":"restart-business-process-from-itself","name":"Restart Business Process from itself","fullPath":"iac/core/control-tower/bot-tasks/tricks/restart-business-process-from-itself","content":" Useful when one needs constantly running Human Task (e.g. 'Upload batch file') which triggers execution of the whole Business Process. Restart this BP "},{"version":"10.0","date":"Aug-06-2019","title":"sql-injection-prevention","name":"SQL Injection Prevention","fullPath":"iac/core/control-tower/bot-tasks/tricks/sql-injection-prevention","content":" Groovy groovy.sql.Sql class can be used to perform sanitization of SQL parameters: Groovy SQL "},{"version":"10.0","date":"Oct-16-2019","title":"s3-advanced-usage","name":"S3 Advanced Usage","fullPath":"iac/core/control-tower/bot-tasks/tricks/s3-advanced-usage","content":" Save Content on S3 Download ZIP from S3 and extract content Save Data Store on S3 in CSV format How to Create S3 Bucket Programmatically Save Content on S3 The following config saves the specified fields content on S3 (useful for dealing with * _tagged fields) Overrides the fields content by S3 link Skips the fields which are not present in the input Skips the fields which are already an http link (safe to use the step several times in the flow) Adds proper UTF 8 headers Save Content on S3 Expand source Download ZIP from S3 and extract content This is a universal example – an archive can contain files and nested folders. Read archive from S3 Expand source returns ListVariable with single (or empty) NodeVariable inside which wrap FileEntity FileEntity root = sourceFile.getWrappedObject().get(0).getWrappedObject(); This root FileEntity is always directory >>> root.isDirectory() always return true List files = root.getChildren(); void processFilesInDirectory(FileEntity dir) { if (dir.isDirectory()) { for (FileEntity fileEntity : files) { if (fileEntity.isFile()) { processFile(fileEntity); } else { can be recursive call processFilesInDirectory(fileEntity); } } } } void processFile(FileEntity fileEntity) { do something with content ... For example, log content and all available data from FileEntity byte content = fileEntity.getContent(); String fileName = fileEntity.getName(); FileType fileType = fileEntity.getType(); always return FileType.FILE for files String mimeType = fileEntity.getMimeType(); List children = fileEntity.getChildren(); always return null for files String toString = fileEntity.toString(); log.warn(\"File name: {}\", fileName); log.warn(\"File type: {}\", fileType); log.warn(\"File mime type: {}\", mimeType); log.warn(\"File children: {}\", children == null \"null\" : children.toString()); log.warn(\"File content size: {}\", content.length); } processFilesInDirectory(root); > Save Data Store on S3 in CSV format Provide bucket name and datastore name for the snippet below: Saving Data Store on S3 Expand source How to Create S3 Bucket Programmatically Versioning infoThe config is valid for WorkFusion platform version 8.4.5 trough 9.1 see 9.1 Deprecations page for deprecation note This config checks for existence of a specific S3 bucket, and if it is missing creates it. BasicAWSCredentials Access Key, Secret Key. Uncomment line with the region definition, if necessary. Expand source Click here to view config for versions before 8.4.5 BasicAWSCredentials Access Key, Secret Key. Uncomment line with the region definition, if necessary. Expand source "},{"version":"10.0","date":"Aug-06-2019","title":"split-json-array-into-individual-records","name":"Split json array into individual records","fullPath":"iac/core/control-tower/bot-tasks/tricks/split-json-array-into-individual-records","content":" The following example demonstrates a way to parse and process JSON array, and then pass all the items to the step output. Every item in the original JSON array would result in a separate record exported to the task output. Export json array 0; > {\"application_users\": {\"id\": 1, \"name\": \"Lala Puga\"}, {\"id\": 2, \"name\": \"John Doe\"}, {\"id\": 15, \"name\": \"Elton John\"} } "},{"version":"10.0","date":"Aug-06-2019","title":"using-google-authenticator-in-rpa","name":"Using Google Authenticator in RPA","fullPath":"iac/core/control-tower/bot-tasks/tricks/using-google-authenticator-in-rpa","content":" In case two factor authentication (2FA) is enabled to authenticate users, it can be rather challenging for a bot to reproduce the same behavior. A common practice in such cases is to turn off the 2FA or MFA for bots, though, there might be some issues with getting all of the security approvals quickly. Generate authentication code programmatically Instead of relying on Google Authenticator tool or similar software, it's possible to generate the authentication code programmatically from a bot config. Requirements Robot has access to Secret Key. Use Secrets Vault for keeping the keys safe TOTP (Time based One time Password) algorithm is used. Other algorithms are also supported, though are less common. Server time is correct Add the latest version of googleauth library (BSD license) to the classpath Project development portal: At the time of writing the latest version of the library was 1.1.5: Generate authentication code in script block Get from Secure Storage instead: String secret = \"JBSWY3DPEHPK3PXP\"; GoogleAuthenticator gAuth = new GoogleAuthenticator(); int code = gAuth.getTotpPassword(secret); Enter to the system instead: System.out.println(code); > See also Hardware Security Token Robotics "},{"version":"10.0","date":"Oct-16-2019","title":"working-with-emails","name":"Working with Emails","fullPath":"iac/core/control-tower/bot-tasks/tricks/working-with-emails","content":" Sending Email with HTML Body EML File Handling Sending Email with HTML Body To send emails from Bot configs, use the standard Web Harvest plugin: Do not forget to enter your SMTP server settings (host, port, username, password, security) and email type (text or html). To insert a variable value into mail body, use the plugin with a respective name attribute. If you want to insert HTML tags into mail body, surround them with CDATA section(s). Example Intelligent Automation Cloud Using HTML content in your email: WorkFusion Inc. is providing SPA* > > * Intelligent Automation Cloud > The following email will be sent to the target address: EML File Handling Input: path to a EML file Output: email body and all attachments separately Getting email body and attachments from a EML file Expand source "},{"version":"10.0","date":"Aug-06-2019","title":"tips-and-tricks-with-examples","name":"Tips and Tricks with Examples","fullPath":"iac/core/control-tower/bot-tasks/tricks/tips-and-tricks-with-examples","content":" id: version 10.0 tips tricks original_id: tips tricks Check nested pages we are collecting examples of Bot Task usage to solve common issues across different use cases. Feel free to reuse change extend those samples. Any further questions you can ask on WorkFusion forum For general overview of Best Practices please check out webinar conducted by WorkFusion Business Processes Development Best Practices Apache POI Working with MS Office Files Manipulating Data Stores S3 Advanced Usage Working with Emails Loading external JARs File conversion TIFF, PDF, XML, HTML Connecting to Windows Shared Folder HTML to XML Transformation and XPath Selectors Executing a Bash script from Bot Config Restart Business Process from itself Exception Strategy Go to Next Step if Exhausted Retrying Dealing With non ASCII Characters How to wait for a condition in bot config before executing further Memory usage optimization Bot Task Examples Using Google Authenticator in RPA SQL Injection Prevention Split json array into individual records Accessing REST API and parsing JSON response Fetching whole table with single XPath and parse with Groovy WH Loop inside Business Process. Repetitive bot execution. "},{"version":"10.0","date":"Aug-06-2019","title":"date-format","name":"date-format","fullPath":"iac/core/control-tower/bot-tasks/utilities/date-format","content":" Date format plugin allows to format string representation of a date to a desired format. This plugin accepts dates in multiple formats and recognizes the most appropriate one. This plugin uses the SimpleDateFormat Java class. Date formats recognized by default: M d yyyy mm:hh:ss aa yy MM dd mm:hh:ss aa MMM dd yy mm:hh:ss aa M d yy mm:hh:ss aa yyyy MM dd mm:hh:ss aa MMM dd yyyy mm:hh:ss aa M d yy mm:hh:ss aa yyyy MM dd mm:hh:ss aa MMM dd yyyy mm:hh:ss aa M d yy mm:hh:ss aa dd MMM yy mm:hh:ss aa MMM dd, yy mm:hh:ss aa MM dd yy mm:hh:ss aa dd MMM yyyy mm:hh:ss aa MMMMM dd yyyy mm:hh:ss aa MM dd yyyy mm:hh:ss aa dd MM yyyy mm:hh:ss aa MMMMM dd, yy mm:hh:ss aa Attributes Name Required Default Description output format yes MM dd yyyy Specifies output data format. on error no ' ' (empty value) A value to return when the date can not be parsed. input formats no See default date formats above A collection of acceptable date formats, can be used to provide custom formats. Example "},{"version":"10.0","date":"Aug-06-2019","title":"cache","name":"cache","fullPath":"iac/core/control-tower/bot-tasks/utilities/cache","content":" The plugin is intended to obtain an object form the global server cache by a key or to store an object in cache for future requests. The plugin body is executed only when the key cannot be found in cache, otherwise the last executed result is returned. Current implementation (version 8.3 and below) uses single cache shared inside JVM (WorkFusion application). The cache is shared between all users, but its element lifetime is limited to 10 mins by default. Maximal cache size is 50 elements, new cache key will delete the oldest one. Attributes Name Required Description key yes Key in the cache. If this key exists, its value will be returned. If this key is not found, the script body will be executed. return no Variable name where result will be recorder. language no Language for ScriptEngine, for example Groovy Use case: you can store login token in the cache to prevent errors when multiple Bot configs try to log in a system under the same credentials through REST API. Example: cache plugin for auth token Alternative use case: you can store some temporary needed information using the plugin instead of saving it to the snapshot using plugin if a 6 minute lifetime fits your BP (e.g. several Bot steps). Caching an HTTP request result: no need to create multiple HTTP requests to obtain the same information. Caching HTTP request {now} When you run a BP with multiple records, all these records will have the same response value with the same time and json (taken from the 1st record execution). Sample use case. Imagine that you have 1000s records in BP but you need to send just 1 email. Rather than merge records into one you can use for this task: Note how unique key is generated. If you hardcode it next run of this BP will NOT send email because cache record will be found! "},{"version":"10.0","date":"Aug-06-2019","title":"excel-to-csv","name":"excel-to-csv","fullPath":"iac/core/control-tower/bot-tasks/utilities/excel-to-csv","content":" The plugin can be used to convert an excel file to a csv file. Attributes Name Required Default Description url yes (if file is missing) URL to excel file (xls or xlsx). file yes (if url is missing) Path to excel file on file system (local or server). separator no ; Separator for csv cells This plugin returns a text string that can be written to a file. to csv example "},{"version":"10.0","date":"Aug-06-2019","title":"export","name":"export","fullPath":"iac/core/control-tower/bot-tasks/utilities/export","content":" single column Output multiple variables from Bot multi column put to column put to column getter put to column method put to plugin method chain In order to store collected information to AwsHitQuestion and related entities (in the same way like we do it for Human Task), we need to use the export plugin. A Bot config without plugin will be executed permanently. See the release plugin to have an idea how to use it. Attributes Name Required Default Description include original data yes Boolean value defining whether to save input data in the result snapshot or not. export type no CSV Type of snapshot: XLS for Excel until 2007 XLSX for Excel 2007 other or empty for CSV column name case no keep Define the case of column names in the output file: upper lower export columns no List of all columns that will be added to the output file. Example: \"invoicedate, issuername, CUSIP\" single column When you need to save 1 answer for 1 submission, use the single column plugin. Examples: obtaining alexa rank for website, obtaining information about website from Who Is, and other. Attributes Name Required Default Description name yes Name of the column in result snapshot to store answer to. value no Value to store. Optional, because we can use the body of single column element to obtain this value. Code Example Output multiple variables from Bot High 100 Yahoo multi column When you need to save multiple answers for 1 submission, use the multi column plugin. Examples: searching sport club addresses by input zip. Attributes Name Required Default Description list yes List with some objects that need to be stored. split results no false Boolean value: true – system performs splitting of multiple data to different answers; false – system does not perform any splitting. The multi column plugin should contains at least 1 of the following child elements: put to column put to column getter put to column method put to plugin method chain These plugins determine the way to access each object and can be specified explicitly or using loop or case plugin (see examples below). If no child elements are specified, nothing will be displayed for the object. More information and examples of using these plugins are provided below. Basic Usage with Split results Split results with dynamic amount of output columns put to column This plugin is intended to access string representation of each object in the list. Attributes Name Required Default Description name yes Name of the column in result snapshot to store answer to. put to column getter This plugin is intended to access data by the stated getter of each object in the list. Attributes Name Required Default Description name yes Name of the column in result snapshot to store answer to. property yes Name of the accessed object property. For example, if each object in the list has a getName method, you need to state the name property. put to column method This plugin is intended to access data by the stated method of each object in the list. Attributes Name Required Default Description name yes Name of the column in result snapshot to store answer to. method yes Name of the accessed object method. For example, if each object in the list has an obtainBalance method, you need to state the obtainBalance method. put to plugin method chain This plugin is intended to access data like in EL syntax using method calls with parameters or method chains calls. Attributes Name Required Default Description name yes Name of the column in result snapshot to store answer to. method chain yes Methods or method chain to access the object. For example, getLanguage().getLang() Splitting JSON object to multiple records { \"grid\": { \"firstname\":\"alex\", \"lastname\":\"zi\" }, { \"firstname\":\"joe\", \"lastname\":\"doe\" } } "},{"version":"10.0","date":"Aug-06-2019","title":"language-extractor","name":"language-extractor","fullPath":"iac/core/control-tower/bot-tasks/utilities/language-extractor","content":" This plugin is used to detect the language of the given website. Attributes Name Required Default Description url yes The url of the website to detect the language. This plugin returns a list containing instance(s) of the com.freedomoss.crowdcontrol.webharvest.plugin.langextractor.LanguageResultItem class. You can use the following methods to get the Language and its Probability: getLanguage().getLang() getLanguage().getProb() Example "},{"version":"10.0","date":"Aug-06-2019","title":"http-extended","name":"http-extended","fullPath":"iac/core/control-tower/bot-tasks/utilities/http-extended","content":" This plugin is a backport of the plugin from Web Harvest trunk codebase. Base description could be found here. New features added with this plugin are as follows: multiple parameters with the same name Example retries with interval Example standard error handling with Standard http param and http header will not work with this plugin. Use http param extended and http header extended instead. Example of using a translation API {\"from\":\"eng\",\"to\":\"fra\",\"text\":\"We help customers do business better by leveraging our industry wide experience, deep technology expertise, comprehensive portfolio of services and vertically aligned business model\"} LC apiKey=Ur6H9PHyudrxZhqWLWjNOg%3D%3D By default, we include some headers with every query (they are listed below). All of them can be overridden with the matching http header extended header. It is also possible to disable all of them at once by using the include default header=false plugin property. Example Prefer English headers.put(\"Accept Language\", \"en us,en gb,en;q=0.7,*;q=0.3\"); Prefer UTF 8 headers.put(\"Accept Charset\", \"utf 8,ISO 8859 1;q=0.7,*;q=0.7\"); Prefer understandable formats headers.put(\"Accept\", \"text html,application xml;q=0.9,application xhtml xml,text xml;q=0.9,text plain;q=0.8,image png, ;q=0.5\"); Force validation of the resource in the intermediate proxies. Web browsers usually send the same header. Important for sites like http: skillnet.com transactions . headers.put(\"Cache Control\", \"no cache\"); http param extended The plugin adds the http parameter for the first enclosing HTTP processor for both post and get requests. If used outside the HTTP processor, an exception is thrown. Syntax Attributes Name Required Default Description name yes The name of HTTP parameter. isfile no no Tells if parameter is file for upload (applies only to multipart requests). contenttype no MIME type of the upload file (effective for multipart forms where parameter is file). filename no Name of uploaded file (effective for multipart forms where parameter is file). Example Th plugin sends needed parameters to www.nytimes.com auth login in order to log in. http header extended The plugin defines the HTTP header for the first enclosing HTTP processor. If used outside the HTTP processor, an exception is thrown. Syntax Attributes Name Required Default Description name yes The name of HTTP header. Example In the example, the plugin identifies itself to www.google.com as Firefox browser. "},{"version":"10.0","date":"Aug-06-2019","title":"include-config","name":"include-config","fullPath":"iac/core/control-tower/bot-tasks/utilities/include-config","content":" This plugin is used to execute a specified WorkFusion Bot configuration in scope of current execution. Recursive inclusions are supported. Attributes Name Required Default Description code yes Unique identifier of included config (persisted in MACHINE_CONFIG.inclusionCode) You can view the identifier of a Bot config on the Design Task tab, by default it has the following format: 6a65fc70 c23e 45ab ae1a 102ad4ee4269. It's a good idea to change it to something more meaningful. When developing code in WorkFusion Studio file name without extension plays the role of the Bot config's code Example "},{"version":"10.0","date":"Aug-06-2019","title":"list-to-csv","name":"list-to-csv","fullPath":"iac/core/control-tower/bot-tasks/utilities/list-to-csv","content":" The plugin can be used to export JSON formatted info to a given csv file. Attributes Name Required Default Description separator no , (comma) Separator for csv cells quote symbol no no symbol Quote symbol escape symbol no no symbol Symbol for escaping special characters. With default value it still escapes characters that can break csv structure, like commas in case of comma separator. Input Format Use list of simple json objects as input: Output This plugin returns a byte array. list to csv example "},{"version":"10.0","date":"Aug-06-2019","title":"log","name":"log","fullPath":"iac/core/control-tower/bot-tasks/utilities/log","content":" This plugin logs messages that can be seen on View Results > Summary > Run's events popup and in log file. By default WARN and higher messages will appear on the events view. Configuration can be changed using standard log4j. Attributes Name Required Default Description message no logging message, message can be set in body. Body's value has more priority than attribute value. The message limit is 1000 characters. If the limit is exceeded, only the first 1000 characters will be saved. level no logging level, possible values: ERROR, WARN, INFO, DEBUG, TRACE, default INFO. For events view DEBUG and TRACE logging level equal to INFO. Example of usage custom message details body message, level=default. Message details: {messageToLog} body message level=default log4j.xml appender configuration "},{"version":"10.0","date":"Aug-06-2019","title":"mail-check","name":"mail-check","fullPath":"iac/core/control-tower/bot-tasks/utilities/mail-check","content":" The plugin is intended for connecting to a mail server and checking for new unread emails with a specified subject pattern. If there are some new emails matching all defined criteria, this plugin returns a list of these emails with a brief description of each (from, subject, body, priority, size, attachments). output Email{from='John Smith ', subject='SSI docs', body='Here are docs...', priority=normal, size=53257, attachments= EmailAttachment{fileName='SSIDocument_AUD.PDF', contentType='application pdf', size=48094, data(size)=35144} } Email{from='Bjorn ', subject='New SSI', body='New documents..', priority=normal, size=6228, attachments= } Attributes Name Required Default Description connection props Yes A map containing protocol, host, port, and security. user Yes Your user name (email address) password Yes Password. You can use Deprecated Secure Store Plugins to store username and password. folder No INBOX A mail box folder to check the messages subject pattern No any string Only emails that contain this string in their subject will be included into config output. max messages No 10 Maximal number of emails in result. max message size No 20971520 Only emails that do not exceed this size will be included into config output. Example Data Store with input parameters. key value mail.store.protocol imaps mail.host your_host mail.imaps.port 993 mail.user.name email@example.com mail.user.password supersecurepassword subject pattern Invoice max messages 5 max message size 0 mail.smtp.host your.smtp.host mail.smtp.port 465 mail.smtp.security ssl mail.notification.reply email.reply@example.com select * from @this; {initParamsMap.put(initParam.get(\"key\").toString(),initParam.get(\"value\").toString())} "},{"version":"10.0","date":"Aug-06-2019","title":"release","name":"release","fullPath":"iac/core/control-tower/bot-tasks/utilities/release","content":" This plugin is used to postpone the next execution of current record. Works only if export plugin is not invoked. Currently, the plugin does not work with interval less than 30s. Attributes Name Required Default Description time in seconds yes Minimal number of seconds HIT execution will be postponed for. It's minimal, not exact value Usage It can be used for remote jobs with long execution time: submit task to such job from Bot config and provide HIT uuid for callback when job is finished it invokes REST API to wake up HIT by uuid or in some cases as a replacement for Thread.sleep which is not efficient in Bot configs (see OCR example) Examples: automation training, OCR of large document (it can be used instead of Thread.sleep during OCR processing status query which is not efficient the idea is to have one config for preprocessing and OCR task posting and another config to query OCR task status periodically), etc. Example false If you need to execute current record prior to its release timeout, you can call the following API operation (using hitUuid of current record): rest api hit resume {hitUuid} "},{"version":"10.0","date":"Aug-06-2019","title":"script-var","name":"script-var","fullPath":"iac/core/control-tower/bot-tasks/utilities/script-var","content":" Version Info The plugin is available since WorkFusion platform version 8.4.0 The ` plugin can be alternatively used in scripts instead of the ` plugin to facilitate coding to access the resulting value: script var theVar var def theVar.get(0).wrappedObject() or theVar.getWrappedObject().get(0) Syntax Attributes Name Required Description name Yes The name of variable. Should be valid like in most programming languages. return No Specifies what this processor should evaluate at the end and return as a processing value It works in the same way as &lt;script&gt; processor return attribute. Examples Using with plugin Using with plugin "},{"version":"10.0","date":"Aug-06-2019","title":"pool","name":"pool","fullPath":"iac/core/control-tower/bot-tasks/utilities/pool","content":" General Overview Attributes Example with conversion turned off Example with timeouts General Overview Initially, the plugin was created to* provide exclusive access to a record from a data store* with external system credentials. Example One user can use only one thread simultaneously. In the current implementation, it is generalized to any object list. The plugin provides the semaphore functionality for objects in the `` section. For each invocation, the plugin returns any not borrowed object from the evaluated list. Check whether the object is borrowed in a pairwise comparison based on the borrowed object equals method. For example, for converted to Map objects, it measures equality of all entries. By default, objects are converted to Map : DbRowVariable based on values NodeVariable based on wrapped object value Map is passed as it is POJOs are converted using BeanMap If the object list is empty, then an exception is thrown. If the `` plugin has not been invoked for more than 10 minutes, all objects in the pool become accessible again. The ` plugin does not support the external classes loaded from MCB, if the following attribute is set convert to map=false. It is fixed in 8.5. releases by setting the property webharvest.config.pool.plugin.distributed=false `in the workfusion.properties file. The `` plugin has two mandatory sections. These are as follows: ` must contain expression evaluated to ListVariable` (may be evaluated multiple times based on the timeout settings) `` contains logic using borrowed object Attributes Name Required Description key yes Name of the pool. Each request to the pool will retrieve object not currently borrowed. Check whether borrowed object is equal based. If the &lt;datastore&gt; plugin is used in the list section, it is recommended to have \"key\" equal to the data store name used. item no Variable name where result will be recorder. Default value: poolableObject convert to map no Defines whether listed objects need to be converted to Map. DbRowVariable is converted based on column name and column value. Map is left as it is. Other objects are converted using BeanMap with removed \"class\" attribute to work with dynamic classes. Default value: true polling interval no Value in milliseconds, negative means no limitation. It is used when there are more threads than available objects in the pool and the pool is modified. It sets maximum wait for object borrow before retry. E.g., when there are two objects in the pool and five threads. Three threads are waiting for the object with the &lt;list&gt; value already resolved. If the waiting period is too long, their list can become obsolete. After max wait milliseconds, the list will be re read and borrow repeated. Default: 1 wait timeout no Value in milliseconds. If it is exceeded in borrow object part, PluginException is thrown. Default value: 120000 (2 minutes). Plugin usage on data store content Plugin usage on simple POJO list Example with conversion turned off Example with timeouts The list is reevaluated each second until object is borrowed from the pool. If after five minutes, the object is still not borrowed, an exception is thrown. "},{"version":"10.0","date":"Aug-06-2019","title":"send-message","name":"send-message","fullPath":"iac/core/control-tower/bot-tasks/utilities/send-message","content":" The plugin allows to send messages to Workers using their ids or to all Workers in a Crowd using the Crowd name. Attributes Name Required Description subject yes Subject of the message to send message yes Body of the message to send workers no Worker ids divided by comma crowds no Crowd names divided by comma Usage example {result.get(0).getWrappedObject()} {result.get(1).getWrappedObject()} "},{"version":"10.0","date":"Aug-06-2019","title":"similarity-score","name":"similarity-score","fullPath":"iac/core/control-tower/bot-tasks/utilities/similarity-score","content":" This plugin is intended for comparing two text strings and providing their similarity score. As an output, the plugin returns a double number, for example: 0.27 or 40.0. To understand how the score is calculated, see the String metric article. Attributes Name Required Default Description content1 no First string to compare content2 no String to compare with content1 distance no ScaledLevenstein It operates between two input strings, returning a number equivalent to the number of substitutions and deletions needed in order to transform one input string into another. Scoring algorithm (string distance calculation) http: secondstring.sourceforge.net javadoc com wcohen ss AbstractStringDistance.html For example: com.wcohen.ss.SmithWaterman com.wcohen.ss.NeedlemanWunsch Similarity score usage "},{"version":"10.0","date":"Aug-06-2019","title":"split","name":"split","fullPath":"iac/core/control-tower/bot-tasks/utilities/split","content":" The plugin is intended for splitting text content into chunks (sentences, words). Attributes Name Required Default Description content no Text string to be split. splitter no com.freedomoss.crowdcontrol.webharvest.plugin.nlp.SentenceSplitter Custom string splitter. The following variant(s) available for this moment: com.freedomoss.crowdcontrol.webharvest.plugin.nlp.WordSplitter com.freedomoss.crowdcontrol.webharvest.plugin.nlp.SentenceSplitter As an output, this plugin provides a text string with line breaks after each sentence or word, depending on the splitter set. "},{"version":"10.0","date":"Aug-06-2019","title":"to-text","name":"to-text","fullPath":"iac/core/control-tower/bot-tasks/utilities/to-text","content":" This plugin utilizes the Apache Tika toolkit to parse the content and extract the text. See the list of Tika supported parsers at . Attributes Name Required Default Description parser no Fully qualified name of the class implementing org.apache.tika.parser.Parser that will be utilized to parse the content handler no Fully qualified name of the class implementing org.xml.sax.ContentHandler that will be utilized to parse the content extractor no Fully qualified name of the class implementing de.l3s.boilerpipe.extractors.ExtractorBase will be utilized to extract additional information. Useful when you need to extract an Article from a web page. Example (basic) Example (extracting news article) "},{"version":"10.0","date":"Aug-06-2019","title":"unzip","name":"unzip","fullPath":"iac/core/control-tower/bot-tasks/utilities/unzip","content":" Unzip plugin is generally used to extract content of .zip files. Unzip plugin accepts byte array as a source in the definition body. It can be used in conjunction with any plugins that produce byte (e.g. , ). Unzip plugin returns the FileEntity object as a recursive structure see JavaDoc Java Example Expand source public byte getContent() { return content; } public String getMimeType() { return mimeType; } public String getName() { return name; } public FileType getType() { return type; } public boolean isDirectory() { return FileType.DIRECTORY == type; } public boolean isFile() { return FileType.FILE == type; } } Basic Usage Getting file names and content from archive Expand source dir_content = \" {filecontent.toString()}\" }; > Example Expand source With s3 S3 put plugin can accept a byte or FileEntity structure in the body. In the second case, all files will be extracted and placed in S3 bucket. Empty extracted folders are not created on S3. Example Expand source "},{"version":"10.0","date":"Aug-06-2019","title":"url-validator","name":"url-validator","fullPath":"iac/core/control-tower/bot-tasks/utilities/url-validator","content":" This plugin is used to detect availability of the given url. Attributes Name Required Default Description url column yes The url of the website to detect availability. This plugin returns a list containing instance(s) of the com.freedomoss.crowdcontrol.webharvest.plugin.url.validator.UrlValidationResultDto class. You can use the following list properties: valid endUrl statusCode statusText attemptsCount Example "},{"version":"10.0","date":"Aug-06-2019","title":"task-start","name":"task-start","fullPath":"iac/core/control-tower/bot-tasks/utilities/task-start","content":" This plugin is used to start a Task Business Process from a specific Business Process with new input data. Using this plugin you can start a new Task BP depending on your current BP results. This functionality eliminates the need to upload CCF and CSV files to external server (e.g. Amazon S3) and then start a new scheduled Task BP. Atributes Name Required Default Description campaign uuid yes Unique identifier of campaign. See 9.x WorkFusion REST API UUIDs for how to obtain the Definition (prev. Campaign) UUID main data yes Content of a main data (file content) workforce uuid no UUID of workforce to use. If empty default will be used. qualification run no Boolean flag if this task is qualification. Data for qualification task should be provided via main data attribute. qualification training no Boolean flag if this task is in training mode. The flag is ignored if qualification run is not true. file separator no Appropriate separator used within the input data file. Valid separators include: “,” comma “;” semicolon “ ” pipe “ t“ tab example file separator=&quot; &quot; block size no 0 Defines the number of Records to display in one single Worker Task. skip bad lines no false Boolean, if true, skips lines that can not be parsed. task display priority no Run display priority. Should be priority name (e.g. low, normal, max) due date no ISO formatted date time. Tasks will expire after that date. use gold bucket no false If true, Task uses gold bucket storage to mix golds. gold run no false Indicates if the run is gold. gold data no Is used to provide the gold data for the Run. gold percentage no 0.0 Percentage of the gold records to be used for Tasks. gold repeated no false If true, WorkFusion can reuse the Gold records if there is not enough records provided within Gold file set. gold bucket repeated no false If true, the system can reuse the gold record used before when all unique records. gold bucket block size no 0 Number of gold records per single task. encoding no UTF 8 File encoding. tags no User defined tags for the Run. expiration data no Expiration data, json format. custom attributes no Custom attributes that should be applied on a triggered run. String in format: \"key1\": \"value1\", \"key2\": \"value2\" stream type no Type of streaming. Possible value 'PermanentOpenTask'. Any other or absent value will be considered as 'Immediately'. stream value no Possible value: positive Long. Used with 'PermanentOpenTask' type only. stream threshold no Minimum task threshold required to initialize streaming. Possible values: integer (absolute task number) or percent(relative), e.g. 1 or 10%. automation training no false If true, an automation BP starts in training mode. Example Result Example Error { \"valid\":false, \"errors\": \"Incorrect 'campaignUuid' parameter\" , \"output\":{} } 8.4.2 Result Example Error task start throw Exception 409 Conflictorg.springframework.web.client.HttpClientErrorException: 409 Conflict at org.springframework.web.client.DefaultResponseErrorHandler.handleError(DefaultResponseErrorHandler.java:91) at org.springframework.web.client.RestTemplate.handleResponse(RestTemplate.java:700) at org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:653) at org.springframework.web.client.RestTemplate.execute(RestTemplate.java:613) at org.springframework.web.client.RestTemplate.exchange(RestTemplate.java:559) at com.freedomoss.crowdcontrol.webharvest.connection.StatefulRestTemplate.getResponseEntity(StatefulRestTemplate.java:98) at com.freedomoss.crowdcontrol.webharvest.connection.StatefulRestTemplate.exchange(StatefulRestTemplate.java:82) at com.freedomoss.crowdcontrol.webharvest.connection.StatefulRestTemplate.exchange(StatefulRestTemplate.java:108) at com.freedomoss.crowdcontrol.webharvest.plugin.task.TaskStartPlugin.executePlugin(TaskStartPlugin.java:96) at org.webharvest.runtime.processors.WebHarvestPlugin.execute(WebHarvestPlugin.java:125) at org.webharvest.runtime.processors.BaseProcessor.run(BaseProcessor.java:127) at org.webharvest.runtime.processors.BodyProcessor.execute(BodyProcessor.java:27) at org.webharvest.runtime.processors.VarDefProcessor.execute(VarDefProcessor.java:59) at org.webharvest.runtime.processors.BaseProcessor.run(BaseProcessor.java:127) at org.webharvest.runtime.Scraper.execute(Scraper.java:169) at org.webharvest.runtime.Scraper.execute(Scraper.java:182) at com.freedomoss.crowdcontrol.webharvest.executor.LocalWebharvestTaskExecutor.executeWebHarvestTask(LocalWebharvestTaskExecutor.java:180) at com.freedomoss.crowdcontrol.webharvest.executor.LocalWebharvestTaskExecutor.executeWebHarvestTask(LocalWebharvestTaskExecutor.java:93) at com.workfusion.service.machine.SubmissionsPortionExecutionThread.processSubmission(SubmissionsPortionExecutionThread.java:343) at com.workfusion.service.machine.SubmissionsPortionExecutionThread.wrapIntoAllocationLogger(SubmissionsPortionExecutionThread.java:324) at com.workfusion.service.machine.SubmissionsPortionExecutionThread.lambda wrapIntoNamedThread 39(SubmissionsPortionExecutionThread.java:312) at com.workfusion.service.machine.SubmissionsPortionExecutionThread Lambda 115 1203416002.run(Unknown Source) at com.workfusion.utils.thread.NamedThreadTemplate.executeWithNamedThread(NamedThreadTemplate.java:10) at com.workfusion.service.machine.SubmissionsPortionExecutionThread.wrapIntoNamedThread(SubmissionsPortionExecutionThread.java:312) at com.workfusion.service.machine.SubmissionsPortionExecutionThread.processSubmissions(SubmissionsPortionExecutionThread.java:276) at com.workfusion.service.machine.SubmissionsPortionExecutionThread.run(SubmissionsPortionExecutionThread.java:180) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) "},{"version":"10.0","date":"Aug-06-2019","title":"validate","name":"validate","fullPath":"iac/core/control-tower/bot-tasks/utilities/validate","content":" This is a generic plugin to perform data validation. Attributes Name Required Default Description validator yes Fully qualified name of validator class. var def no Variable name to store validation results. Plugin could be used in two ways: Example (basic) In both cases, the results of validation will be stored in the provided variable. Validation result object has the following structure: Validation result object Validation result provides the following methods: isValid() returns a boolean, which tells you whether the string is valid according to the provided validator. getErrors() returns a list of strings, provides validation errors if any. To create a custom validator which can be used in validation plugin you need to implement interface: com.freedomoss.crowdcontrol.webharvest.plugin.validation.IWebHarvestValidator. Currently implemented validators: com.freedomoss.crowdcontrol.webharvest.plugin.validation.iban.IBANValidator validates IBAN (International Bank Account Number) according to ISO 13616. (For more info refer to wikipedia.org). "},{"version":"10.0","date":"Aug-06-2019","title":"var-global","name":"var-global","fullPath":"iac/core/control-tower/bot-tasks/utilities/var-global","content":" This plug in is used for inserting values of variables defined in the \"Global Variables\" Data Store. Global variables with values can be defined from BP Advanced Options. If value is not defined, then exception will be thrown on execution. Attributes Name Required Default Description name yes name of global variable. Because of storage limitations it is case insensitive and treats non word characters as _ , e.g. 'Test key' name will be treated as 'test_key' description no Description of the global variable. Usage example: export value of \"test _var\" global variable in \"exported value\" column. Usage Examples "}]