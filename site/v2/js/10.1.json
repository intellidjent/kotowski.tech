[{"version":"10.1","date":"Sep-18-2019","title":"iac","name":"Intelligent Automation Cloud Enterprise introduction","fullPath":"iac","content":" Check out an area of interest: Architecture; User Guide – basic WorkFusion concepts and how to guides; Automation Engineer Manual – complete guide for advanced users describing Bot tasks, automation, and robotics; Release Notes – the latest WorkFusion features with user friendly descriptions grouped by releases; WorkSpace – complete Worker and Requester guide for WorkFusion WorkSpace application that serves as a Worker portal; WorkFusion Infrastructure – WorkFusion installation, operational, and maintenance guides for IT OPS; or Search our Knowledge Base for the answer. "},{"version":"10.1","date":"Sep-06-2019","title":"iac-10-1-improvements","name":"Improvements","fullPath":"releases/iac/iac-10-1-improvements","content":" Security RPA .dll and .exe (binary) files have verified digital signatures, so that corporate security tools do not regard them as a threat. jSessionID cookie now has a 'secure' flag. The \"Change password\" fields are not available when SSO is enabled. WorkSpace web server messages and UI error messages are optimized to secure sensitive information. User input data is now hidden from javascript payload in WorkSpace. Updated javascript components to use guards that prevent javascript hijacking on returned javascript or json. Analytics You can now purge data from the Analytics database (by default, the data older than the last 30 days is cleaned) to improve dashboards and ETL performance. The encryption algorithm for the Analytics server is improved. WorkFusion Studio The working directory used by WebHarvest to save bot task plugins data is set per project workspace (not per instance as before). To find the data, go to the working directory within the current workspace (by default c: Users SomeUser workfusion workspace workingdirectory crowdcontoldata webharvest). RPA SAP: Improved performance for SAP xml generation and search elements Decreased time for SAP table interaction Improved SAP automation maintainability Bot Session and Credentials Manager: Automatically rotate passwords through Secrets Vault and NOT to manage credentials on the RPA machine (in Windows Credential Manager). If RDP session timeout occurs, unlock the bot session. Prevent my Bot screen from locking to keep UI session active, so that bot can click and type. Monitoring Proper navigation from CT Migrate to 7.x Better log viewing (structured java logs) Centralized log level management HA monitoring (metrics collection of HA components) Installation The installers of all components are amalgamated into a single package. RHEL 7.7 support added. The support of new shared file systems added, including NFS, GFS2, GPFS, AWS EFS, SAN NAS storage. SAN certificates support added. ELK updated to 7.3.0. ImageMagick is updated to 7.0.8 50. Nexus is updated to 2.14.13. Apache Mesos is updated to 1.6.1. The cron utility is replaced by supervisord. Back to 10.1 release notes "},{"version":"10.1","date":"Sep-20-2019","title":"iac-10-1-deprecations","name":"Deprecations","fullPath":"releases/iac/iac-10-1-deprecations","content":" Control Tower Removed support of PHP and separate TinyMCE web application, as now TinyMCE is a part of Control Tower services. Removed the following outdated and obsolete elements: Advanced and Active Tasks tabs on the System Preferences page 'Enable Manual Review' settings for Manual Task 'Receive notifications when Tasks returned from WorkSpace' tracking and settings ` and ` bot plugins cost and data only attributes for the export plugin Export XML action removed from the Manual Tasks list. It is no longer used since it is impossible to import XML. To explore alternative migration options, refer to Migrate Task, BP, Use Case to another CT instance. Import Use Case functionality removed in favor of Asset Import. Hit and EndPoint task activities are no longer registered in Control Tower Activity Log. OCR GET processDocument request deprecated in OCR API and will be removed in the next major release. Use POST processDocument request method instead. For usage tips, see OCR REST API. RPA Hub and Node (also known as Bot Relay and Bot) removed from the RPA Server deployment, as being redundant in the new BEP architecture. Now, RPA Worker sends commands directly to web or desktop drivers. Therefore each Bot Unit consists of a Bot Agent and an RPA Worker. Hub and Node processes removed from Bot Agent configuration package structure together with relevant hardcoded environment variables: bot agent nordp.yml, bot agent unit.yml, unit.yml, unit_nordp.yml. The following components inside RPA installer removed: start process.bat, start unit hub1.bat, start unit nod1.bat, unit hub.json, unit node.json Unused jar files from dependency directory removed in RPA installer. Unused analytics data on Bot Relay and Bot removed from botunitstatus and bot_unit tables in Bot Manager. Node searching mechanism and driver registry removed from ` and ` plugins. In compliance with the changes above, all unused maven dependencies and environment variables removed from rpa api and rpa desktop components. All constants, environment variables, and paths to drivers removed from .bat files and set up as Windows environment variables by the installer automatically during the installation process. Analytics As Bot Relay and Bot are removed from the RPA Server deployment, we remove Reporting API bot_unit view Bot Relay Port and Bot Port fields. This does not affect analytics statistics calculation. Bot Relay Host was renamed to Bot Unit all over the RPA dashboard. Back to 10.1 release notes "},{"version":"10.1","date":"Sep-06-2019","title":"iac-10-1-known-issues","name":"Known issues","fullPath":"releases/iac/iac-10-1-known-issues","content":" TBD Back to 10.1 release notes "},{"version":"10.1","date":"Sep-20-2019","title":"iac-10-1","name":"Intelligent Automation Cloud Enterprise 10.1","fullPath":"releases/iac/iac-10-1","content":" With 10.1 Smart Process Automation is finally reborn as The Intelligent Automation Cloud Enterprise — an entire automation ecosystem for large organizations digitizing their operations. This release crystallizes the groundbreaking capabilities of the 10 series into a solid and definitive Intelligent Automation experience. We worked hard to meet the demands of our clients and bring the best enterprise scale solution possible. Version 10.1 brings high availability and disaster recovery per the highest industry standards to keep your business up and running amidst the most catastrophic events. Asset bundle import allows to transfer any part of your project in development to a new environment with minimal effort and errors. BEP cluster demonstrates manifold performance and document throughput improvements on the same hardware. Localization delivers The Intelligent Automation Cloud Enterprise to the speakers of Japanese and Spanish. High availability and disaster recovery Here at WorkFusion, we believe that losing even the slightest piece of work is frustrating. Losing time and data in a real world enterprise environment where thousands of people rely on your system is simply unacceptable. With that in mind, we introduce a new type of system architecture which ensures Intelligent Automation Cloud Enterprise is available all the time whatever happens. High availability (HA) is the ability of a system to be continuously operational for a desirably long period of time. It is is aimed at zero data loss and a maximum of 10 minutes of service downtime, after which it must be fully restored. The Enterprise edition supports HA through all components. With the high availability feature you require several emergency servers for each system component of The Intelligent Automation Cloud. In case of a component's failure, this allows the system to automatically switch to a “healthy” machine and continue its operation with minimum or zero downtime and data loss. All your data is stored on a shared file system or a mounted NAS, thus it is always available. To provide the availability of services and distribute traffic among them, the load balancer (LB) component must be installed on the customer's side. It accepts all the traffic and forwards the application requests to the APP server. After that, the traffic is routed among other servers. Here’s a simplified diagram to make things clearer. IMAGE GOES HERE Bot Manager on the Application server is also highly available, with distributed architecture making Workers available by default. RPA hosts list is now saved in a secured database and highly available too. The Disaster Recovery (DR) architecture implies two sites: primary and emergency. The primary site hosts the full set of servers (either in the HA mode or without it) with running components. The DR site is the full and exact copy of the primary one. All the servers on the DR site are running, while the components on them are stopped. The replication of data between the sites is performed every 30 minutes. If the primary site fails, the system is switched to the emergency one with minimum loss of data. Here’s a simplified diagram to make things clearer. NEEDS TO BE re drawn by designers. IMAGE GOES HERE The high availability and disaster recovery features can be installed and configured with our common installer. The combination of the emergency modes is limited only by your needs and available servers: you can install HA and DR separately or combine them in one totally fault resilient system. Asset management Asset Bundle is a structure comprised of a Business Process and its supporting resources: AutoML models, bot configs, data, templates and more. Asset Bundle can represent either a complete BP or portable resources. It is intended for easy asset transport between environments. IMAGE GOES HERE Previously BP import required an in depth model knowledge, direct access to resources, multiple permissions and credentials. You had to follow a multiple page instruction for a long sequence of steps, including nuanced pre and post import steps. There was no way to check the import status or correct the fail cause. With the introduction of the Asset Bundle, everything required for a Business Process delivery is conveniently wrapped in a self sufficient bundle. The bundle be ported to a target environment via script execution or a REST request to the Control Tower API with minimal precautions. You only need correct permissions in Control Tower. Import mechanism supports large files and is less error prone than the traditional methods. Successful import is asynchronous—meaning fast—and verified by a checksum. This accelerates BP delivery, decreases total cost of development, and eventually contributes to continuous integration. To learn more about Asset Bundle, refer to the Import Asset Bundle documentation. Performance Remember we introduced a brand new Bot Execution Platform (BEP) in version 10.0 The team did a tremendous job rethinking the whole product architecture from the ground up. In 10.1 we focused on optimizing BEP communication with other system components, Control Tower in particular. BEP cluster performance now increases proportionally with the number of nodes eliminating the bottleneck of Control Tower execution tasks output. Okay, why is this important to me Based on a number of in house test Business Processes, maximum documents throughput per hour of The Intelligent Automation Cloud is now at least 30% higher than in 10.0 depending on the complexity of your Business Process. We see it as a significant boost given this improvement works on the same instances. IMAGE GOES HERE NEEDS MORE REAL LIFE FIGURES We also enhanced the performance of Control Tower decreasing the processing time of transition steps, as well as updated MS SQL queries to ensure our database contributes to the overall performance improvement. Last but not least, we updated a number of Kibana dashboards, like BEP Tasks, BEP Tasks trace by BP steps to help you track BEP cluster performance and make data driven decisions. For more information, refer to Monitoring documentation. OCR ABBYY FineReader Engine SDK upgrade to version 12 resulted in a number of improvements, enabling our users to select between FRE SDK 11 and FRE SDK 12. Thus, we added a second (alternative) OCR engine with advanced recognition of tables and texts in images, as well as significantly improved recognition of Japanese characters. With the help of enhanced layout reconstruction, you can easily detect complex tables and reconstruct them to match the layout in the original document. Text classifier easily extracts textual data from any image and ensures high precision and classification accuracy. Based on support of Artificial Intelligence algorithms, we provide higher accuracy and increased speed in recognition of the Japanese language, as well as dictionary enhancements and alphabet corrections. RPA Security Mutual TLS authentication is now in place in communication between RPA server and Application server. Certificates exchange means all traffic between RPA components (Control Tower, Bot Manager, Bot Agent, Worker) is secured. RPA server gets commands from Application server only — so that no one can access RPA data or connect to any of the components without authentication, even from the customer Intranet and having gained Control Tower credentials. Credentials rotation All RPA related credentials are now saved in Secrets Vault for an RPA bot not to request RPA server for credentials. When credentials rotation is configured in Control Tower, new credentials automatically apply for all bots. The credentials rotation mechanism makes RDP timeout no longer affect bot execution due to Worker's automatic reconnection. If a bot is inactive for some period of time, for example, 15 minutes, an RDP session becomes locked. On detecting the idle state, Worker closes the current RDP window, gets new credentials from Secrets Vault, and connects again. Performance To add an RPA machine to RPA cluster or remove it, you no longer need SSH access to the Application server — a Control Tower user with sufficient permissions is enough. Previously, getting SSH was tricky and time consuming in corporate environment, thus RPA cluster resizing happening on the UI only contributes to deployment acceleration. We also simplified the procedure of adding a new RPA machine by providing the available option in Bot Manager UI. For more details, refer to Bot Manager. IMAGE GOES HERE SAP Let us not forget that WorkFusion Intelligent Automation Cloud is the only SAP certified automation solution. In the 10.1 release, SAP automation for huge tables trees as well as ALV trees is now two times faster (as measured by SAP Order Management: transaction codes MIRO, VA02). WorkFusion Studio All Studio improvements are aimed at simplifying developer flow, saving time and avoiding unnecessary actions. The result is faster deployment and happier, calmer coders. WorkFusion Studio now provides full BEP Worker support. You can launch and debug bot tasks on a local BEP compatible Worker. BEP Worker starts, stops, and restarts fast and in real time. Studio console displays Worker's exceptions. With the new Bot Execution architecture, where Bot Relay and Bot (or Hub and Node) are redundant now, all interactions occur directly with BEP Worker, ensuring each \"Run a Bot\" action to happen instantly with no more ten second delays and Studio freezes that drive developers insane. WorkFusion Studio and RPA package are tightly coupled. You only need to point WorkFusion Studio to the RPA package location once to reuse it in all development activities. When recording bot actions, we create and inspect an application snapshot. This brings the following important advantages: RPA Recorder supports capturing keystrokes (for example, Alt F4 or Alt Tab) without applying them to operating system. When getting a selector from a desktop application, the action a click triggers does not happen. This means you can inspect buttons that previously triggered unwanted actions (like close page or pay money) and include these actions into the bot code without actually performing them. You can also easily inspect dynamic interface elements (dropdowns, tooltips, etc.). Installation RPA Installer RPA server installation on a Windows machine now takes place in the RPA setup wizard with next next navigation and requires no additional pre or post installation manual steps. There is no more need to make numerous network and RPA bots configuration changes in various places, with the installation procedure saving up to one hour of installation time per each RPA machine. From now on, the easy to follow wizard with tooltips and user friendly UI makes everything work like magic. IMAGE GOES HERE A new RPA installer offers two installation modes to select from according to your needs: development – used for developing and debugging tasks from WorkFusion Studio, configured for one bot and a local task queue, has lightweight (without network) configuration production – used for executing bot tasks from an external task queue, configured for multiple bots and needs installation of other components Due to the removal of Bot Relay and Bot, we managed to decrease TCO by 1GB RAM together with dependencies for each bot, enabling big scripts with multiple loops to work lightning fast. Creation of centralized configuration during installation allows to seamlessly upgrade by running the RPA installer in the repair mode. It retains variables from the previous installation and prompts you to use them. To learn more, refer to the installation documentation. Analytics As business users immersed into the productive insights of the 10.0 release analytics, they became more knowledgeable and demanded more in depth information. Based on their feedback, the 10.1 release adds a new dimension to analytics with a sharper visualization. The Overview dashboard now presents a cross section of the transaction volume, showing a proportion of the completed and processing tasks among all the tasks created. Speed is now a double chart, presenting transaction average wait and processing time, as well as total transaction processing time. On the Manual dashboard, we've added a Volume chart to visualize a number of assignments submitted on a particular day. Speed has also become a double chart, presenting the statistics for assignments: average cycle time, which is the time spent working on task, wait time, and total processing time. Improved analytics focuses on a process ownership, allowing to distinguish between a fluctuation and a bottleneck. With this knowledge, a process owner can prompt IT operations engineer to monitor and investigate components performance. Localization With the 10.1 release, WorkFusion is expanding the Intelligent Automation global reach. Business users and subject matter experts (SMEs) can now use the Intelligent Automation Cloud in Japanese and Spanish. The user interfaces for several major components display in those languages, to increase accessibility especially for their native speakers. The following components have the new multi lingual options: Control Tower: build and schedule business processes WorkFusion Studio: develop bot tasks WorkSpace: perform and submit manual tasks Analytics: view business intelligence For example translated screens, see Select User Interface Language. Additional resources Improvements Deprecations Known issues "},{"version":"10.1","date":"May-28-2019","title":"spa-10-0-bug-fixes","name":"Bug Fixes & Improvements","fullPath":"releases/terra/spa-10-0-bug-fixes","content":" Control Tower TBD WorkSpace TBD RPA TBD WFBI TBD Platform Monitor TBD Back to Terra v10.0 Release Notes "},{"version":"10.1","date":"May-31-2019","title":"spa-10-0-deprecations","name":"Deprecations","fullPath":"releases/terra/spa-10-0-deprecations","content":" SPA v10.0 is a fresh start release with major architectural changes, so a number of legacy items and components that are no longer in focus, or not supported altogether, were removed. These items include WorkSpace Sandbox, all Smartcrowd integrations, WFBI service, and a number of minor leftovers scattered around the system. Here's the list of the most noticeable changes: MySQL, OCR MongoDB, WS PostgreSQL are consolidated in a single MS SQL database. Bot Sources are removed. Platform Monitor is no longer available being replaced by ELK and Bot Manager. Sandbox environment is gone since most of enterprise customers use a 3 stage environment stack: Dev, UAT, and Production. Below is the full list of deprecations grouped by component. Control Tower Control Tower MySQL, OCR MongoDB, WS PostgreSQL consolidated in a single MS SQL database. Bot Sources are removed. WFBI Service on APP server is removed from Control Tower and its logic for statistics is moved to AutoML service. Bot Configurations: Spring beans and Control Tower service beans are removed. Bot Configurations: Classes Constants are removed from packages. Basic authentication option in REST API is removed due to security concerns. Now authentication is form based (ligin and password) only. Learn more: 10.0 WorkFusion REST API WorkFusion REST API). userInternalCredentials object in Bot Tasks context is deprecated and contains only the username of the task author and empty password string. Learn more: Bot Tasks Context. Sandbox feature is removed. Confirmation dialog upon Run of the Business Process Manual Task in sandbox production is removed. Support of custom jars in Tomcat lib is removed. Qualifications Test Tab is removed. 'Support IE' inside Correlated Fields is removed. 'Bot assisted extraction for values close to accuracy threshold' functionality is removed. 'Online Learning' functionality support is removed. The following legacy items are removed from Control Tower, as SmartCrowds are not supported by WorkFusion since v8.5: Communication center Balances figures Mturk integration Bonus notifications Amazon notifications Idle predefined filter and settings. WorkSpace Sandbox feature is removed. google places scanner and google api client are removed. Learn more: google places scanner and google api client deprecation. Captcha during registration is removed. WorkSpace index page is eliminated (as referring to crowd system). Login page is used instead. Log4j logging service is replaced with Logback. The following legacy items are removed from WorkSpace, as SmartCrowds are not supported by WorkFusion since v8.5: Geo API functionality Elance, Facebook, Twitter, Clickworker, Upwork integration points Paypal integration Support of 'Payments' and earning related funionality RPA MongoDB and MySQL DB are removed and replaced with MS SQL. `` plugin is removed in favor of RPA API. nodeId attribute is removed from RoboticsFlowPlugin. Now, workers take jobs from the queue and there is always one node in a bot unit. For surface based automation, the in house solution is used, thus Sikuli Robot Framework library cannot be imported into scripts. Custom capabilities cannot be used anymore. Use ` attribute in ` plugin instead. Selenium RemoteDriver is removed. OCR GET processDocument request is removed in OCR API due to its length limitations. Since 9.2, it is superseded with POST processDocument request method. OCR plugin attribute use get process document method is not available anymore. If previously used, it should be removed. Flushing documents is turned off by default. To enable flushing, add the DflushEnabled=true parameter to the config file. See the link for instructions. Platform Monitor Platform Monitor is removed and replaced with Elasticsearch and Kibana stack (ELK). Commands API: Running commands using ID only is deprecated. Replaced with a new version with namespace. Back to Terra v10.0 Release Notes "},{"version":"10.1","date":"May-28-2019","title":"spa-10-0-known-issues","name":"Known Issues","fullPath":"releases/terra/spa-10-0-known-issues","content":" Control Tower TBD WorkSpace TBD RPA TBD WFBI TBD Platform Monitor TBD Back to Terra v10.0 Release Notes "},{"version":"10.1","date":"Jul-09-2019","title":"spa-10-0","name":"SPA 10.0","fullPath":"releases/terra/spa-10-0","content":" WorkFusion's latest 10.0 release is the first in a series of groundbreaking releases with a number of fundamental technology and UX overhauls. As such, it serves as a bridge to WorkFusion's Intelligent Automation Cloud, the only platform powered by artificial intelligence (AI) and built for the enterprise for simpler automation at scale. One of the biggest architectural changes in Terra 10.0 is the new bot orchestration platform designed to address current scalability limitations, as well as to reduce complexity and improve fault tolerance. Another major improvement is replacing 3 databases (MySQL, PostgreSQL, MongoDB) with a single unified database — MS SQL. This fundamental change allowed us to decrease maintenance complexity and enable consistent data backup, as well as to address security and high availability issues. Bot Execution Platform Imagine your business operations grow and expand and you need to process twice as many tasks as you currently have. How do you scale WorkFusion SPA What if you need to scale but the task load is hardly predictable and depends on multiple factors v9.x State In SPA v9.x, the correct answer is add another Control Tower with its infrastructure, services, etc. And then maybe another one. Now that you have 3 Control Towers, how do you manage, track and distribute the task load between them The answer is custom setup and orchestration. Needless to say, each Control Tower deployment is quite a time and resource consuming operation which requires custom actions for every specific use case. We knew, this could not last too long as such monolithic architecture was obviously too complex for large enterprise clients. v10.0 State In SPA v10.0, the very core concept of architecture has undergone a major re thinking. Introducing the new state of the art bot execution platform for a horizontally scalable, predictable, and fault tolerant system. Here's three immediate benefits you get: First off, now the platform dynamically scales the number of workers depending on the task load. This results in lean utilization of resources, which can be available for other tasks. No need to add more Control Tower to handle task load spikes. Second, tasks are performed by completely isolated workers. This allows to avoid situations when one failed task could lead to unexpected behavior or Control Tower being stuck, thus improving the overall system fault tolerance. Finally, we drew an explicit line between the application, such as Control Tower, and the execution platform itself. Now, the platform exposes API that triggers cluster execution, orchestration, monitoring, and analytics. Consolidated Database 9.x architecture utilized PostgreSQL, MySQL and MongoDB databases. SPA 10.0 is fundamentally rethinked to use a consolidated Microsoft SQL database. Let's put it simply: to install SPA 9.x, you had to have at least three different databases. In SPA 10.0, you only need one, and that is MS SQL — a solid enterprise solution. Microsoft SQL is an enterprise grade solution — comprehensive, well supported, reliable and trusted by customers across all industries. Consolidated Database introduction brings the following benefits: A single set of tools and single expertise. Less maintenance and troubleshooting effort for more effective support. Less potential failure points — easier identification of security issues. Consistent data backup in adherence to High Availability and Disaster Recovery standards. Monitoring SPA v10.0 changes the logs collecting approach by introducing a single point for logs and metrics aggregation — a well known enterprise grade solution ELK stack that replaces o ur good old Platform Monitor. Earlier, when investigating an incident, to collect logs and figure out what's going on in the system, you had to go to several servers for chunks of information and glue together pieces into a single picture. ELK is shipped as an out of the box tool and provides centralized logging. You can collect logs via Logstash and beautifully vizualize them on Kibana dashboards. The environment health metrics will also help you to understand the current state of your infrastructure and the possible bottlenecks that may affect the performance. ELK includes but is not limited to the following dashboards: System Overview to assess the CPU Memory Disk space load on servers. Application Logs to browse applications' logs. Environment Status to display the health status of the environment and list events that provide information about recent changes in health status. Robotics In Terra v10.0, we improved the governance and performance of our bots by applying the Bot Execution Platform architecture to manage RPA Windows workers. This change opened ways to the following improvements: faster RPA execution – the speed has been increased by 36% simplified bot installation and configuration enhanced security and stable performance due to execution of all task instructions on the same machine one unified method of task routing – Bot Fleets Based on results of 175 Bot runs. Tagging over Document One of the main stages of an automation use case development is dataset tagging. Preparing and labeling a high quality training set is time consuming. It equals to human worker teaching bot to mimic human work, that is, to recognize the values such as invoice number and date. Workers are accustomed to the real world documents structure, and therefore can perform swiftly and effectively on what's familiar to them: hard and digital copies, and scans. In SPA v9.x, workers tagged the plain text OCR result. It could look nothing like the original documents, because all information about sizes, fonts, positions and layouts was lost. It took significant effort to precisely identify the required values. This led to user mistakes and increased the labeling time. SPA v9.x: SPA v10.0 restores the natural workflow by introducing the Tagging over Document (TOD) experience. Worker views the original document with text values layered underneath. The scan is rotated and scaled, the image is optimized where needed for convenience. Word boundaries are highlighted and single value can be selected with one click. SPA v10.0: 1,100 experiments with 6 fields per document show the average tagging time decrease of 52%: TOD improves the overall user experience and dataset quality. It allows to double the speed of document tagging and cuts the time needed to teach a bot, thus streamlining automation use case delivery. Learn more about building a TOD use case: OOTB TOD Use Case. Analytics Capacity Dashboard New Capacity dashboard provides a quick and comprehensive overview of SPA components utilization for Business operations in a way that helps to understand how infrastructure resources are utilized. There are 4 dashboards explaining resource utilization data: CPU, Memory and Disk utilization over time answer if there are enough resources to dynamically scale to process peaks in SPA workload. Bot Execution Platform Agent chart visualizes how workers with different profiles (Control Tower and AutoML) share CPU and Memory resources. Link to be added. RPA Dashboard New RPA dashboard provides an overview for bots accuracy, performance and utilization, thus allowing to find ways for improvements at process or bot levels. Here are the key metrics: Volume, RPA Accuracy Rate, Bot Availability, Fleets Utilization, Bot Units Workload. Link to be added. AutoML Python Classifiers Python is widely considered as the preferred language for teaching and learning ML because of its relative simplicity. No wonder so many ML algorithms have powerful implementations in Python making it a go to language for this purpose. We couldn't stand aside, so starting with SPA v10.0, AutoML allows using a set of Python classifiers based on SciKit learn and MXNet that identify to which category an object belongs to. For more details, refer to Python Classifiers. Single Source Installation With each release we make steps to improving the process of installation and making it more simple and user oriented. While previous versions might have required dozens of operations to set up all the components, SPA v10.0 can be installed within several hours. The basic installation of all components is now performed 50% faster, compared to previous versions, and requires about 1.5 hours of your time. Such improvement is the result of switching to single source approach, which means you can install all Linux and Windows components from a single server by running a single script without logging in to each server and uploading packages to them. Additional Resources SPA 10.0 Deprecations SPA 10.0 Bug Fixes & Improvements SPA 10.0 Known Issues "},{"version":"10.1","date":"Sep-27-2019","title":"rpa-deployment","name":"RPA deployment","fullPath":"iac/rpa/administrator-guide/rpa-deployment","content":" General overview Task Distribution Control Tower sends bot tasks to RabbitMQ where they are distributed to queues. In case of the `` plugin, the tasks are submitted to RPA Queues according to their fleet attribute. For example, you can route SAP RPA bot tasks to SAP RPA Queue by defining SAP as fleet attribute. If no fleet attribute is provided, a task is sent to Shared RPA Queue. Tasks with no plugin are assigned to Common Queue to be transferred to the related server for execution. Depending on their fleet attributes, the tasks are distributed further to corresponding Bot Units. For more details on the distribution mechanism, see Task Distribution. Mutual TLS authentication is used for communication between the RPA server and the Application server. Certificates exchange ensures highly secured traffic between RPA components (Control Tower, Bot Manager, Bot Agent, RPA Worker). RPA Server gets commands from Application server only — so that no one can access RPA data or connect to any of the components without authentication, even from the customer intranet and having gained Control Tower credentials. RPA infrastructure deployment There are two scenarios of RPA infrastructure deployment: multiple Bots Units per server, or RDP in RDP 1 Bot Unit per Windows machine, or VDI To trigger Bot Units, an active RDP session should be established. For RDP in RDP deployment, Bot Master User starts an RDP session allowing Master Bot Agent to initiate Bot Units. For that, Bot Manager retrieves Bot Master credentials from Secrets Vault. For VDI deployment, Bot Agent startup is scheduled in order to autostart an RDP session by Bot Manager. RDP in RDP The RDP in RDP approach allows you to run Bot Units in isolated sessions. Refer to RPA Server on the Deployment scheme above. When the RDP in RDP deployment type is used, the following conditions are created: All the processes cannot be accessed remotely as they are executed on local interfaces. Thus, other processes cannot influence your Bot task execution. You can have a remote access from your workstation only to the necessary RDP session and check what task is in progress at that particular time. The credentials rotation mechanism makes RDP timeout no longer affect task execution due to automatic reconnection. If a bot is inactive for some time, for example, 15 minutes, an RDP session becomes locked. A new logic requires RPA Worker to automatically refer to Secrets Vault for credentials before executing each bot task. Thus, RPA Worker closes the current RDP window, gets new credentials from Secrets Vault, and connects again. VDI In the VDI deployment approach, there are two possible scenarios. RPA VDI – refers to a Windows machine having all the RPA components in a single active console session. No additional RDP connections are created. All the services (RPA Worker and Bot Agent) are run by one user, e.g., Administrator. Refer to RPA VDI on the Deployment scheme above. Master Session VDI – is responsible for opening multiple RDP sessions and keeping them active on several Windows machines. There are as many RDP connections as there are Windows machines. GUI active session support Bot Manager maintains active RDP sessions. As soon as the RDP session is started, Bot Units are triggered to perform task execution. Each Bot Unit contains two specific Java applications: Bot Agent and RPA Worker. Bot Agent is used to start RPA Worker, and is also responsible for sending data to Zookeeper. RPA Worker connects to the assigned RPA Queue, retrieves tasks and, as soon as RPA bot tasks are successfully executed, sends results to Result Queue. Management of Bot Units is performed via the Bot Manager UI. Communication between Bot Units and Bot Manager is established via the secured Nginx connection. To learn more about Bot Manager, refer to Bot Manager. Data As soon as Bot Units are started, Filebeat and Metricbeat are initiated to send logs and metrics to Logstash, that is a service used to ingest them, transform, and transfer to Elasticsearch. Bot Manager gets bot statuses and performance data from Bot Agent and sends it to MS SQL Server to store. Zookeeper stores information on bot configuration and queue addresses transferred from Bot Agent, RPA Worker, and Bot Master. Secrets Vault is used as a storage to hold all the RPA related credentials and not to request for the credentials on the RPA machine (in Windows Credential Manager). When credentials rotation is configured in Control Tower, new credentials automatically apply for all bots. "},{"version":"10.1","date":"Sep-20-2019","title":"automl-sdk-api","name":"AutoML SDK API","fullPath":"iac/automl/automl-sdk/automl-sdk-api","content":" Your browser does not support iframes. Method Description getId() gets unique identifier of the document getText() returns plain text of the document getContent() gives original document text remove(Element e) removes element from document removeAll() removes all elements from document add(ElementDescriptor ed) creates element in document from its description findAll() finds all elements in the document findAll(Class&lt;T&gt; type) finds all elements of given type in the document findCovered(Class&lt;T&gt; type, Element element) gets list of elements of the given element type overlapped by a 'covering' element findCovered(Class&lt;T&gt; type, int begin, int end) gets list of elements of the given element type which are completely located within the given interval findCovering(Class&lt;T&gt; type, Element element) gets list of elements of the given element type that 'cover' a certain element findCovering(Class&lt;T&gt; type, int begin, int end) gets list of elements of the given element type that a completely overlap the given interval findNext(Class&lt;T&gt; type, Element anchor, int count) returns the n elements next to the given element findPrevious(Class&lt;T&gt; type, Element anchor, int count) returns the n elements previous to the given element findAllCovered(Class&lt;T&gt; type, Class&lt;S&gt; covered type) returns covering elements of type T and collection with covered elements of type S from the document. findAllCovering( Class&lt; T &gt; type, Class &gt; covering type ) returns map with covered elements of type T and covering elements of type S from the document. IeDocument specific class for IE use case, used in conjunction with IeProcessor. Provides convenient methods specific for this use case: Method Description findField(String name) finds field in Document by its name findFields(String name) finds fields in Document by name findFields() finds all fields in Document. getFieldsInfo() returns filedInfo DTOs which represents Human Task structure for this model. getFieldInfo(String code) return fieldInfo by answer code. ClassificationDocument document in Classification use case. Has methods to work with document labels: Method Description findLabels() find all labels in the document findLabel(String name) find label by its name Main Elements Element basic class for all document elements. Exact elements representations will be described below. Element provides basic methods like: Method Description getId() unique identifier of the element getDocument() gets document associated with the document getText() returns original text of the element getBegin() gets begin index of element text in document getEnd() gets end index of element text in document Elements: Cell Field Label Line Page Row Table Tag Token Content EntityBoundary LayotBlock NamedEntity Sentence Field class which represents extracted fields in Document for IE use case. Method Description getName() field name, corresponds to answer code in Control Tower getValue() corrected value from text. If value is not corrected original text is returned. setValue(String value) sets corrected value getScore() score of field setScore() sets score of field getAttributes() custom field attributes Label class represents Document label for Classification use case. Method Description getName() gets label name. getScore() returns Label score. Read Only Elements Following elements represent document layout, so they are read only and can not be added or removed from document. Page represents document page. Method Description getPageIndex() returns the page index Table represents table in document, do not provides any additional methods, uses in getCovered getCovering methods. Row row in the table, corresponds to &lt;tr&gt; html tag. Method Description getRowIndex() returns the number of rows directly preceding the current row in the same table Cell table cell Method Description getRowIndex() gets row index of the cell getColumnIndex() returns column index of the cell Line represents OCR &lt;line&gt; element in xml. Method Description getLineIndex() the number of lines directly preceding the current line in the same block Tag class representing xml tag. (eg. &lt;a&gt;, &lt;br&gt;, etc.) Method Description getName() tag name getAttributes() return tag attributes Token represents token element in the Document. Content represents a full content of the document, do not provides any additional methods. EntityBoundary represents some portions of XML HTML text, do not provides any additional methods. LayotBlock represents a block of text based on OCR pixel positions. Method Description getRightPosition() returns right position getLeftPosition() returns left position getTopPosition() returns top position getBottomPosition() returns bottom position NamedEntity represents some real object inside Document, like person, location, bank code, etc. Method Description getType() returns type of mentioned entity getScore() returns score value Sentence represents a sentence element in the document, do not provides any additional methods. 1 : https: workfusion docs.s3.amazonaws.com vds ml sdk latest com workfusion vds sdk api nlp processing Processor.html process T 2 : https: workfusion docs.s3.amazonaws.com vds ml sdk latest com workfusion vds sdk api nlp processing Processor.html 3 : https: workfusion docs.s3.amazonaws.com vds ml sdk latest com workfusion vds sdk api nlp annotator Annotator.html > "},{"version":"10.1","date":"Jul-01-2019","title":"workfusion-analytics-custom-dashboards-backup-and-restore","name":"Backup and restore custom Analytics dashboards","fullPath":"iac/admin/analytics/workfusion-analytics-custom-dashboards-backup-and-restore","content":" id: version 10.1 backup restore custom dashboards original_id: backup restore custom dashboards General Migration Flow Workbook migration flow includes the following steps: Download a workbook. Change data source connections if needed. Rename a workbook according to the naming convention. Publish a workbook to a new instance. Backup To backup dashboards means to download them from WorkFusion Analytics Server. WorkFusion Analytics Server 9 To backup WorkFusion Analytics Server 9: Open the WorkFusion Analytics Server page and select appropriate Site. Click Projects. Select the required project. Select project Find a workbook for downloading and expand the workbook menu by clicking the menu button (...). Click Download. The workbook file in the .twbx format will be downloaded. Download workbook WorkFusion Analytics Server 10 Open the WorkFusion Analytics Server page and select appropriate Site. Click Projects. Select the required project: Select project Find a workbook for downloading and expand the workbook menu by clicking the menu button (...). Click Download. The workbook file in the .twbx format will be downloaded. Select project Restore WorkFusion Analytics Desktop Step 1: Change connections (optional) :::important If all connections to database server remain the same, skip the step and proceed further. ::: Open the recently downloaded workbook file in WorkFusion Analytics Desktop. In the Dashboard section, click Go to sheet. Go to sheet On the Data tab, right click the data source and select Edit data source. Edit data source Edit data base connection settings and click OK. Db connection Move to dashboard sheet. See step 2. Step 2: Sign in to server On the menu bar, click Server > Sign in. server sign in Enter the URL of the BI server, and then click Connect. connect tableau server Enter the WorkFusion Analytics Server credentials, and then click Sign in. The user must have the Publisher, Site or Server Administrator role. sign in tableau Select the required Site for publishing. Selectt site Step 3: Publish workbook On the menu bar,click Server > Publish Workbook. publish workbook Check Project and Name fields. We strongly recommend using the ' _custom' postfix in the name to meet the WF Dashboard configuration requirements. The name must not start with a digit. Once the workbook is published, the dashboard appears on the Analytics > Custom page of CT. Select Show sheets as tabs option. Optionally, you can add the Refresh extract schedule. refresh extract schedule in the Sheets section, click Edit, and select sheets to publish. edit sheets in the Data Sources section, click Edit and change the publish type to Embedded in workbook and Authentication mode to Allow refresh access. sign in tableau Click Publish. Check the published workbook on the BI server. "},{"version":"10.1","date":"Jul-01-2019","title":"using-tableau","name":"Setup end-to-end SSI audit chart","fullPath":"iac/admin/analytics/using-tableau","content":" This guide describes the usage of the WorkFusion Analytics Desktop, WorkFusion Analytics Server, and WorkFusion application integration. You can explore the end to end usage process based on simple Area Charts setup for one PostgreSQL table. The guide shows how the quantity of processed human tasks versus automation tasks changes in the course of time. WorkFusion Analytics Desktop Prepare Install WorkFusion Analytics Desktop on your local machine. Obtain access to the database on the environment where you will deploy your chart. In this example, the credentials to the 'wf _datastore' PostgreSQL database are required. Alternatively you can work with your local file (text, Excel, etc.). Obtain access to the WorkFusion Analytics Server integrated with the environment where you will deploy your chart. Connect to the DB Launch the WorkFusion Analytics Desktop. In the top left corner, click Connect to Data > PostgreSQL, and then enter the DB credentials. If you cannot connect to your DB, try to install the appropriate DB drivers from https: www.tableau.com support drivers. Now you can see the tables of your database. Drag the ds SSI AUDIT _TRAIL table to the white area. Click Update Now. The actual data from your table is displayed. Update now In this example, it is important to change the type in the Ssi Upload Timestamp column from String to Date & time to be able to take advantage of WorkFusion Analytics Date features. After you setup the data source, proceed to the chart visual editor. For that, in the center of the window, click Go to Worksheet. Setup Worksheet The Worksheet contains the following working areas: a . Dimensions Measures; b . Pages Filters Marks; c . Chart Preview Area; d . Columns Rows; e . Chart Type (Show Me). tableau workbook.png To setup a worksheet: Drag the following elements: Dimensions > Ssi Upload Timestamp to Columns Measures > Number of Records to Rows. In the Columns (d) area, right click the Ssi Upload Timestamp element and select the Day option. Upload timestamp In the Marks (b) toolbox, select the Area type. The area under the chart will be filled with color. To add chart differentiation by the Processed By column, drag the Processed By column from Dimensions (a) to the bottom of the Marks (b) toolbox. Select Color from the left context menu left. tableau marks.png Setup Dashboard As the Chart is ready, you can polish it with some filters. To setup Dashboard: Create a Dashboard for the chart. For that, in the main menu, select Dashboard > New Dashboard. On the Dashboard view, drag your chart from the Dashboard section to the design area. The Chart opens. tableau dashboard From the Chart context menu, add filter by Ssi Upload Timestamp and change the position of filter and the legend. tableau filter It's important to meet markup guides for charts in WF application. Change the Size as shown below. tableau width Change the locale by clicking File > Workbook Locale > English (United States). Edit chart colors, fonts, labels, etc., if needed. Deploy Dashboard After the above operations, you are ready to deploy the Dashboard on the WorkFusion Analytics Server. To deploy Dashboard: Connect to the server. For that, in the main meny, select Server > Sign In, and then enter the server name. To start the deployment, select Server > Publish Workbook, and then select your site from the drop down menu. Give a name to your Workbook for the WorkFusion Analytics Server. It's important to use ' _custom' postfix in name to meet WF Dashboard configuration requirements. The name must not start with a digit. The name of project must be the same as the name of the instance. In the Views to Share panel, check only the SSI Dashboard. Click Authentication button, and then select the Embedded password to let users see the chart in WF without entering the WorkFusion Analytics Server password. tableau publish.png Click Publish. The confirmation dialog with the preview appears. WorkFusion Analytics Server To manage the deployed chart, sign in to the WorkFusion Analytics Server web Admin with the same credentials as for WorkFusion Analytics Desktop. Here you can delete and change permissions to the Workbook. WF Application Dashboard The published chart appears on the WorkFusion Analytics > Custom page. "},{"version":"10.1","date":"Jun-28-2019","title":"import-root-or-intermediate-ca-to-store","name":"Import Root or Intermediate CA to Store","fullPath":"iac/admin/info-sec/import-root-or-intermediate-ca-to-store","content":" Microsoft Windows uses a global certificate storage to keep certificates. The certificates must be imported to each workspace, to enable the browser to pass the certificate chain check successfully. To import certificates: Download the CA certificate file in the PEM CRT format and save it to your desktop. Right click the root certificate and click Install Certificate. Install certificates In the Store location group, select Local Machine and click Next. Install certificates Select Place all certificates in the following store and click Browse. Install certificates Select Trusted Root Certification Authorities and click OK. Install certificates Click Next, and then click Finish. Install certificates Double check that your connection is secured now. Install certificates "},{"version":"10.1","date":"Jun-28-2019","title":"ldap-and-ad-integration","name":"LDAP and AD integration","fullPath":"iac/admin/info-sec/ldap-and-ad-integration","content":" Overview Lightweight Directory Access Protocol (LDAP) is an open, vendor neutral, industry standard application protocol for accessing and maintaining distributed directory information services over an IP network. Directory services play an important role in developing intranet and Internet applications by allowing the sharing of information about users, systems, networks, services, and applications throughout the network. As examples, directory services may provide any organized set of records, often with a hierarchical structure, such as a corporate email directory. Similarly, a telephone directory is a list of subscribers with an address and a phone number. Active Directory (AD) is a directory service that Microsoft developed for Windows domain networks. It is included in most Windows Server operating systems as a set of processes and services. Initially, Active Directory was only in charge of centralized domain management. Starting with Windows Server 2008, however, Active Directory became an umbrella title for a broad range of directory based identity related services. WorkFusion Control Tower and WorkSpace can provide security (authentication and authorization) through LDAP AD integration. Authentication is based on Spring Security standard provider. Control Tower Configuration By default, Control Tower provides local user groups role management. LDAP AD integration can be enabled by setting the following parameters in: workfusion conf workfusion.properties: ldap properties ldap.enabled=true to activate LDAP with WF roles switch this property to true. No groups from LDAP will be used. Properties ldap.group.base and ldap.group.filter will be ignored ldap.internal.authorization.enabled=false container of corporate user database used for authentication support several base directories pipe separated ... ldap.user.base=ou=People,dc=somecompanydomain,dc=com container of corporate user groups used for authorization support several base directories pipe separated ... ldap.group.base=ou=Groups,dc=somecompanydomain,dc=com user filter for search, optional, alternative = (uid={0}) ldap.user.filter=(sAMAccountName={0}) group filter for search, optional, alternative = (memberUid={1}) ldap.group.filter=(member={0}) page size to load big amount of records to avoid size limit error ldap.search.page.size=500 group filter for list of all group names ldap.search.all.groups.filter=( (objectClass=group)(objectClass=groupOfNames)(objectClass=groupOfUniqueNames)(objectclass=posixGroup)) ldap object attribute name that contains group name ldap.search.groups.name.attribute=cn Default configuration: ldap properties ldap.enabled=false ldap.internal.authorization.enabled=false ldap.user.filter=(sAMAccountName={0}) ldap.group.filter=(member={0}) ldap.search.page.size=500 ldap.search.all.groups.filter=( (objectClass=group)(objectClass=groupOfNames)(objectClass=groupOfUniqueNames)(objectclass=posixGroup)) ldap.search.groups.name.attribute=cn Secure properties: ldap server url ldap.server.url=ldaps: {host}:{port} system user cridentials ldap.bind.dn= ldap.bind.password= External authorization Before enabling LDAP, at least one group must be mapped. User groups are mapped in Control Tower during create edit a User Group (default user roles you can find in Role Management): After successful login, a user will be created updated with data fetched from LDAP AD. The user properties cannot be changed in Control Tower, if LDAP configuration is enabled, only available filters can be added. The direct role selection is unavailable too. Edit LDAP user If LDAP is enabled, A user cannot be created, deleted, or disabled in Control Tower. The Create, Delete, and Disable buttons are unavailable. Sections System Preferences → User Settings: Account Details, and Change Password are disabled too. Internal authorization To enable internal authorization, set property ldap.internal.authorization.enabled to true. Common assumptions: If LDAP authentication is passed successfully, a user is checked for existence in Control Tower, and Control Tower Roles are applied to the user, if such user exists. Otherwise, unable to login. User Properties cannot be changed in Control Tower, if LDAP Internal configuration is enabled. You can only edit roles and enable disable the user. User can be created, deleted, and disabled in Control Tower. The Create, Delete, and Disable buttons on the User Management page are available, if LDAP with internal authorization is enabled. The System Preferences → User Settings: Account Details, and Change Password sections are disabled too. The following properties are not used, if LDAP with internal authorization is enabled: ldap.group.base ldap.group.filt Integrate LDAP AD This is an example to give an overview of LDAP AD integration. You must have distinqueshedName and password of the \"system user\" to initialize the connection properties. It can be any user with privileges to read the directory tree and the object attributes. ldap.bind.dn=CN=System User,OU=demo,DC=somecompany,DC=local ldap.bind.password=** System user Configure the base directories for users and groups. ldap.group.base=OU=Groups,OU=demo,DC=somecompany,DC=local ldap.user.base=OU=Minsk,OU=Users,OU=demo,DC=somecompany,DC=local It supports multi values with pipe separator for users and groups. ldap.user.base=OU=Minsk,OU=Users,OU=demo,DC=somecompany,DC=local OU=New York,OU=Users,OU=demo,DC=somecompany,DC=local When the base directories are configured properly, it can help reducing time for searching users and or groups. ldap.group.base is a base directory for loading user's groups during login and a list of groups for \"External Group Name\" mapping on the Edit User Group page. User groups User groups Configure user filter. User filter Usually, a username attribute for Microsoft Active Directory is sAMAccountName. In this case ldap.user.filter=(sAMAccountName={0}). For example, if we are going to use the email for login, so the filter looks as ldap.user.filter=(mail={0}). Keep in mind, that a user filter must return a unique result, otherwise it is configured incorrectly. Configure a user groups filter. User groups filter ldap.group.filter=(member={0}). In this case the {0} parameter is distinqueshedName of the logging in user. For example, an alternative filter ldap.group.filter=(memberUid={1}), In this case the {1} parameter is username of the logging in user. Configure the filter to fetch all group names. For that in Control Tower → Create Edit User Group page, fill External Group Name. ldap.search.all.groups.filter=(objectClass=groupOfNames) ldap.search.groups.name.attribute=cn Fetch group names WorkSpace configuration By default, WorkSpace provides local user role management. LDAP AD integration can be enabled by setting the following parameters: In workspace conf workspace.properties or workspace sandbox.properties: ldap properties ldap.enabled=true activate internal authorization if swithed to true with ldap.enabled ldap.internal.authorization.enabled=false Comma separated group names from LDAP for mapping to worker users ldap.worker.groups=CC Dev,Galiot,CC Ops Comma separated group of roles from LDAP which are mapped to the WorkSpace Requester role ldap.requester.groups=CC Admin Name of license that is set to requester if new requester is created ldap.requester.license=license_name container of corporate user database used for authentication support several base directories pipe separated ... ldap.user.base=ou=People,dc=crowdcomputingsystems,dc=com container of corporate user groups used for authorization support several base directories pipe separated ... ldap.group.base=ou=Groups,dc=crowdcomputingsystems,dc=com user filter for search, optional, alternative= (uid={0}) ldap.user.filter=(sAMAccountName={0}) group filter for search, optional, alternative = (memberUid={1}) ldap.group.filter=(member={0}) optional block ldap.user.attribute.email=mail ldap.user.attribute.firstName=givenname ldap.user.attribute.lastName=sn ldap.user.attribute.country=country User group mapping is configured through ldap.worker.groups Default configuration: ldap properties ldap.enabled=false ldap.internal.authorization.enabled=false ldap.user.filter=(sAMAccountName={0}) ldap.group.filter=(member={0}) ldap.user.attribute.email=mail ldap.user.attribute.firstName=givenname ldap.user.attribute.lastName=sn ldap.user.attribute.country=country Secure properties: ws.secure.ldap.server.url=ldaps: {host}:{port} system user cridentials ws.secure.ldap.bind.dn= ws.secure.ldap.bind.password= External authorization After successful login, a user will be created updated with data fetched from LDAP AD. Register and Forgot password functionality is unavailable, if LDAP AD is integrated. workspace login Internal authorization To enable internal authorization, set property ldap.internal.authorization.enabled to true. Common assumptions: If LDAP authentication is passed successfully, a user is checked for existence in WorkSpace, and WorkSpace Roles are applied to the user, if such user exists. Otherwise, unable to login. User Properties cannot be changed in WorkSpace, if LDAP Internal configuration is enabled. You can only enable and disable the user. A user can be created, deleted, or disabled in WorkSpace. The Create, Delete, and Disable buttons on the Workers and Requesters pages are available for requesters. The following properties are not used, if LDAP with internal authorization is enabled: ldap.group.base ldap.group.filter "},{"version":"10.1","date":"Jul-08-2019","title":"sso-integration","name":"SSO integration","fullPath":"iac/admin/info-sec/sso-integration","content":" :::important Before starting the SSO integration, request the IdP metadata (file or URL) from the security team. ::: SAML 2.0 Security Assertion Markup Language (SAML) is an XML based, open standard data format for exchanging authentication and authorization data between parties, in particular, between an identity provider (IdP) and a service provider (SP). The SAML specification defines three roles: the principal (typically a user) the identity provider (IdP) the service provider (SP) In the use case addressed by SAML, the principal requests a service from the service provider. The service provider requests and obtains an identity assertion from the identity provider. On the basis of this assertion, the service provider can make an access control decision, that means it can decide, whether or not to perform some service for the connected principal. Before delivering the identity assertion to the SP, the IdP can request some information from the principal – such as a user name and password – in order to authenticate the principal. SAML specifies the assertions between the three parties: in particular, the messages that assert identity that are passed from the IdP to the SP. In SAML, one identity provider may provide SAML assertions to many service providers. Similarly, one SP may rely on and trust assertions from many independent IdPs. SAML does not specify the method of authentication at the identity provider; it may use a username and password, or other form of authentication, including multi factor authentication. A directory service such as LDAP, RADIUS, or Active Directory that allows users to log in with a user name and password is a typical source of authentication tokens at an identity provider. 3 The popular Internet social networking services also provide identity services that in theory could be used to support SAML exchanges. Use case SAML Web Browser SSO (Service Provider initiated authentication): Request the target resource at the SP. The principal (via an HTTP user agent) requests a target resource at the service provider: https: sp.example.com myresource The service provider performs a security check on behalf of the target resource. If a valid security context at the service provider already exists, skip steps 2–7. Redirect to the SSO Service at the IdP. The service provider determines the user's preferred identity provider (by unspecified means) and redirects the user agent to the SSO Service at the identity provider: https: idp.example.org SAML2 SSO Redirect SAMLRequest=request The value of the SAMLRequest parameter is the Base64 encoding of a deflated `` element. Request the SSO Service at the IdP. The user agent issues a GET request to the SSO service at the identity provider where the value of the SAMLRequest parameter is taken from the URL query string at step 2. The SSO service processes the AuthnRequest and performs a security check. If the user does not have a valid security context, the identity provider identifies the user (details omitted). Respond with an XHTML form. The SSO service validates the request and responds with a document containing an XHTML form: ... The value of the SAMLResponse parameter is the base64 encoding of a `` element. Request the Assertion Consumer Service at the SP. The user agent issues a POST request to the assertion consumer service at the service provider. The value of the SAMLResponse parameter is taken from the XHTML form at step 4. Redirect to the target resource. The assertion consumer service processes the response, creates a security context at the service provider and redirects the user agent to the target resource. Request the target resource at the SP again The user agent requests the target resource at the service provider (again): https: sp.example.com myresource Respond with requested resource. Since a security context exists, the service provider returns the resource to the user agent. Control Tower SAML Integration Security profiles The following security profiles for SAML are supported: PKIX. The java.security.cert package is utilized for assertion and validation of the certificates. It implements RFC 5280. MetaIOP. Trusted certificates are not mandatory. Other mechanisms are used for validation (e.g. explicitly supplying the keys to trust). The desired profile is specified in the SAML metadata file. Related documentation: Package java.security.cert (https: docs.oracle.com javase 8 docs api java security cert package summary.html) Configuring SAML Extension. Security profiles (http: docs.spring.io spring security saml docs 1.0.x reference html security.html configuration security profiles) Application Properties conf workfusion.properties Property Name Value Description wf.sso.saml.enable true Enables or disables SSO (SAML 2.0 protocol) authentification for Control Tower. Accepted values: true false wf.sso.saml.entity.base.url https: HOSTNAME workfusion Base URL of Control Tower (Service Provider). Used in SP metadata. (see Service Provider metadata example) wf.sso.saml.response.skew 60 Maximum difference between local time and time of the assertion creation which still allows message to be processed.Basically determines maximum difference between clocks of the IDP and SP machines. Defaults to 60 (seconds) workfusion.properties sample option to enable disable SSO authentication wf.sso.saml.enable=true base url used during generation SP metadata wf.sso.saml.entity.base.url=https: HOSTNAME workfusion response skew in seconds wf.sso.saml.response.skew=60 Secure Properties These properties are placed to Secure Storage. Property Name Value Description wf.sso.saml.idp.file.metadata path to local meta Local path to a file with IdP metadata parameters. wf.sso.saml.idp.metadata https: SAMLSERVICEURL idp sso URL that points to a file with IdP metadata parameters. Must be provided by a customer.This parameter is an alternative to wf.sso.saml.idp.file.metadata, which is a recommended option. wf.sso.saml.sp.metadata workfusion metadata id A unique string. ID of Service Provider. wf.sso.saml.username.attribute uid Name of attribute that will be used for authentication.uid for LDAPsAMAcountName for Active Directory Must be provided by customer. Secure Storage sample properties url to IdP metadata, use url or file wf.sso.saml.idp.metadata=none path to IdP metadata file wf.sso.saml.idp.file.metadata= path to local metadata some unique name wf.sso.saml.sp.metadata=wf sp hostname the name of an attribute taken from IdP response that contains username wf.sso.saml.username.attribute=uid Service Provider metadata example For example Control Tower is installed and available by URL Metadata for this Service Provider can be generated and downloaded by URL SP metadata example Meta ` has attributes with Service Provider ID and entityID. These attributes are configured through property wf.sso.saml.sp.metadata`. Identity Provider metadata example Usually IdP metadata is provided by a security team. It can be a file or URL. The file should be placed on the application server (SP) and be accessible for Service Provider (Control Tower). It is configured via the property wf.sso.saml.idp.file.metadata. If URL provided, it can be configured via wf.sso.saml.idp.metadata. Both properties wf.sso.saml.idp.file.metadata and wf.sso.saml.idp.metadata cannot be configured at the same time. Show IdP metada example ``** must contain information about login logout endpoints: SingleSignOnService, SingleLogoutService, and attributes' description – saml:Attribute. `` is the username attribute name and must be mapped via wf.sso.saml.username.attribute ws.sso.saml.username.attribute in Secure Storage. (for example, wf.sso.saml.username.attribute=ACCOUNT). The value of this mapped attribute in IdP response will be used for authorization. Identity Provider Authentication Response After a user logs in on the IdP side, the server sends a response to SP (CT WS) and this response must include username attributes configured (if not encrypted) in wf.sso.saml.username.attribute or ws.sso.saml.username.attribute. How to decrypt SAML Response To catch SamlResponse from IdP, use a debuger (press F12 to open it) in your browser and then try to login . Then select the SSO name among all names of the Network tab and there will be present decoded SamlResponse. Copy this value and decode it using https: www.samltool.com decode.php tool. Example of the IdP response with not encrypted data: The Destination attribute in Response must contain an URL that leads to the `` tag into SP metadata. The ` tag from ` must contain a value of wf.sso.saml.sp.metadata ws.sso.saml.sp.metatdata from vault (depends on component you are configuring SSO for). ` is \"someusername\" and will be used for authentication on the SP side (if configuration is wf.sso.saml.username.attribute=uid`). Example wf.sso.saml.sp.metadata = test.workfsuin.com_workspace sandbox In this case, Audience will look like: test.workfusion.com_workspace sandbox Example of the IdP response with the encrypted data: IdP and SP metadata description Tag Source Description EntityDescriptor IdP SP metadata main metadata's tag with ID EntityID attributes IDPSSODescriptor IdP metadata IdP configuration description SPSSODescriptor SP metadata SP configuration description SingleSignOnService IdP metadata Endpoint to sign on SingleLogoutService IdP SP metadata Endpoint to sign out AssertionConsumerService SP metadata SP endpoint to assert response from IdP side saml:Atribute IdP metadata response Attribute description goes from IdP side saml:AtributeValue IdP response Attribute value saml:AudienceRestriction IdP response Container for saml:Audience tag saml:Audience IdP response SP EntityID for which the response is sent (used in response Assertion) samlp:StatusCode IdP response Response status, success example value (urn:oasis:names:tc:SAML:2.0:status:Success) Configure SSO Before enabling SSO, in Control Tower, create a user with the Administrator role and a name that is suitable for your SSO account. Create user Configure application and secure properties to enable SSO: workfusion.properties file wf.sso.saml.enable=true wf.sso.saml.entity.base.url=https: control tower host workfusion Secure Storage wf.sso.saml.idp.file.metadata= path to ipd metadata provided by security team wf.sso.saml.sp.metadata=unique service provider name wf.sso.saml.username.attribute=ACCOUNT the name of the attribute into SSO response with username, usually described in IdP metadata file. Restart the Application server. Go to https: control tower host workfusion saml metadata. The metadata file will be downloaded. Send it to security team to be added to Identity Provider (SSO server). Open any CT page. From now you'll be redirected to your Identity Provider Login Page. Enter your account. User authentication and authorization This implementation assumes that the user, who is going to log in via the identity provider (IdP)* already exists in Control Tower* The Admin user who is responsible for creation new users and assigning privileges must be primarily created directly via database. The Admin user must create new users with usernames that already exists in IdP. If a deleted, disabled, or non existing user tries to log in, they will be redirected to the WorkFusion login failed screen with a validation message. Login fail Users are authorized on the WorkFusion side. The admin user can manage user roles. IdP configuration Assertion Consumer Service, EntityID and Public Key can be found in the metadata XML file. Service Provider metadata can be downloaded by the url: https: workfusion saml metadata, for example, https: SOMESITE.WORKFUSION_HOSTNAME workfusion saml metadata. LDAP AD authorization Click here to expand LDAP authrization use case diagram sso ldap integration.png For information on enabling LDAP and AD auhotization, and mappping user groups, see LDAP and AD integration WorkSpace SAML Integration Application Properties Property Name Value Description ws.sso.saml.enable true Enables or disables SAML 2.0 protocol for Control Tower. Accepted values: true and false ws.sso.saml.entity.base.url https: HOSTNAME workspaceorhttps: HOSTNAME workspace sandbox Base url of Workfusion SAML provider ws.sso.saml.response.skew 60 Maximum difference between the local time and the time of the assertion creation, which still allows a message to be processed.Basically determines maximum difference between clocks of the IDP and SP machines. By default, 60 (seconds) Sample of workspace.properties (workspace sandbox.properties) option to enable disable SSO authentication ws.sso.saml.enable=true base url used during generation SP metadata ws.sso.saml.entity.base.url=https: WORKSPACE_HOSTNAME workfusion response skew in seconds ws.sso.saml.response.skew=60 Secure Properties Property Name Value Description ws.sso.saml.idp.file.metadata path to local meta Path to file with IdP metadata parameters. ws.sso.saml.idp.metadata none URL that points to a file with IdP metadata parameters. Must be provided by a customer.This parameter is an alternative to wf.sso.saml.idp.file.metadata, which is a recommended option. ws.sso.saml.sp.metadata ws sp hostname A unique string. ID of Service Provider, for example, https: workspace saml metadata ws.sso.saml.username.attribute mail A name of the attribute that will be used for authentication Secure Storage sample properties url to IdP metadata, use url or file ws.sso.saml.idp.metadata=none path to IdP metadata file ws.sso.saml.idp.file.metadata= path to local metadata some unique name ws.sso.saml.sp.metadata=ws sp hostname the name of an attribute taken from IdP response that contains user's email address ws.sso.saml.username.attribute=mail :::note At least one of the \"idp\" properties must be set. Otherwise, you'll get the \"No IdP was configured\" page. ::: User Authentication and Authorization This implementation assumes that the user that is going to log in via the identity provider (IdP)** already exists in WorkSpace. A requester must be primarily created directly via the database.The user's email in IdP must be the same as the username in WorkSpace. A Worker can get registered by himself. The email in IdP must be the same as the username in WorkSpace. If a deleted, disabled, or non existing user tries to log in to Workspace, they will be redirected to the the index page with a validation message. A disabled (non active) user can be activated by email or by a Requester. IdP Configuration Assertion Consumer Service, EntityID and Public Key can be found in metadata xml file Service Provider metadata can be downloaded by the url: https: workspace saml metadata. "},{"version":"10.1","date":"Jul-08-2019","title":"bi-windows-server-installation","name":"Install BI Windows Server","fullPath":"iac/admin/legacy/bi-windows-server-installation","content":" Prerequisites Installation packages are uploaded to one or several servers and unarchived. WorkFusionAnalyticsInstaller.zip contains the following directories with installers: WorkfusionAnalyticsServer WorkfusionAnalyticsDesktop License keys for WorkFusion Analytics Server and Desktop (optional) are provided by Workfusion Team. The predefined common variables are the following: . WorkFusion group. Default is wfuser. . WorkFusion user. Default is wfuser. . WorkFusion home directory C: workfusion. Defaults directories are the following: WorkfusionAnalyticsServer – C: workfusion WorkFusionServer WorkfusionAnalyticsDesktop – C: workfusion WorkFusionDesktop . WorkFusion Installer Directory. Default is C: workfusion WorkfusionAnalyticsServer. **. DNS of Application server (APP), for example, app.example.com. FileBeat installation directory. Default is C: workfusion filebeat. Analytics server hostname, for example, bi.example.com. Integration server hostname, for example,int.example.com. port of INT Logstash service. Default port is 4567. Install WorkFusion Analytics Server Always run installation of WorkFusion Analytics Server as a Windows Administrator user. The Installation process takes about 20 25 minutes. Before the installation, the Analytics databases must be migrated. To migrate the databases, run: . install.sh migrate bi The databases can also be migrated after the installation. In this case, run the earlier command and restart the Analytics server to apply the changes. To install the Analytics Server: On your prepared server, start the Command Prompt (not PowerShell) and, in the unpacked installation package, go to the WorkfusionAnalyticsServer directory. Set the following environment variables: SET win_user=ec2 user SET win_pass=1q2w3e! SET TABLEAUUSER=tableauuser SET TABLEAU_PASS=tab1q2w! Here, win _user is a user with Administrator rights under which installation is performed. win pass is a password of a win user. TABLEAU USER is a tableau dashboard _user of Workfsion Analitycs Server. TABLEAU PASS is a tableaudashboardpass of a TABLEAU USER. Note that special symbols are not allowed in the password. Copy the bi.crt and bi.key certificate files to the directory. Note: SSL configuration is mandatory starting from SPA version 10.0. Run the installation Batch file install _server.bat with the specified installation directory and APP host name. install_server.bat Example: install_server.bat C: workfusion WorkFusionServer app.exapmle.com Log in to the server and activate it with the license key. Optional. tsm login u tsm licenses activate k By default, the install _server.bat script activates the trial Tableau license for two weeks. Thus the step is optional. On the BI server, in a browser, open . CReate Tableau acc Install WorkFusion Analytics Desktop To install the Analytics Desktop: On your server, start the Command Prompt (not PowerShell) and go to the WorkfusionAnalyticsDesktop directory in the unpacked installation package. Run the installation Batch file install _desktop.bat with the specified installation directory: install_desktop.bat Example: install_desktop.bat C: workfusion WorkFusionDesktop Click the WorkFusion Analytics Desktop shortcut to start the application, and then enter the license key. Enter key For more information on Activation options, see Manage the WorkFusion Analytics license. Install WorkFusion Analytics Filebeat To install Filebeat: Start the Command Prompt (not PowerShell) and go to the Filebeat directory in the unpacked installation package. To enable log gathering, run the installation file install _filebeat.bat with the specified parameters. Specify as a directory on WorkfusionAnalyticsServer. install_filebeat.bat : Example: install_filebeat.bat C: workfusion filebeat C: workfusion WorkFusionServer bi.example.com int.example.com:4567 Copy the root certificate ca.crt file, which has been generated earlier, to the BI server's directory , and in the filebeat.yml config file, add the path to the certificate. Filebeat Start the Filebeat service: sc start filebeat Install WorkFusion Analytics Metricbeat To install Metricbeat: Start the Command Prompt (not PowerShell) and go to the metricbeat directory in the unpacked installation package. To enable metrics gathering, run the installation file install _metricbeat.bat with the specified parameters. Specify as a directory on WorkfusionAnalyticsServer. install_metricbeat.bat : Example: install_metricbeat.bat C: workfusion metricbeat C: workfusion WorkFusionServer bi.example.com INT.example.com:4569 Copy the root certificate ca.crt file, which was generated earlier, to BI server's directory and in the metricbeat.yml config file, add the path to the certificate: Metricbeat Start the etricbeat service: sc start metricbeat Deploy workbooks to WorkFusion Analytics Server This section describes deployment of Tableau workbooks to Tableau 2018.3 Server OEM on WorkFusion Analytics Server. The following workbooks must be uploaded to WorkFusion Analytics Server: Overview _WB Capacity _WB AutoML _WB RPA _WB Process _WB Speed _WB Manual _WB AA ETL Scheduler Prerequisites Pre installed software: Microsoft Server 2008 R2 or newer WorkFusion Analytics Server 2018.3.2 or newer Workbooks packages are uploaded to the servers and unarchived. The WorkFusionAnalitycsWorkbooks.zip archive contains the following directories: deploy _scripts dashboards Deploy workbooks To deploy workbooks, log in to the BI server with the installed WorkFusion Analytics Server and perform the following steps: Create the c: workfusion tdeploy home install directory and copy files from the deploy scripts directory to it. Pre installation directories after unpacking .zip c: workfusion tdeploy_home install (file: install.cmd) lib (installation file pack) Open the Command Prompt (not PowerShell), and run the following installation command: cd c: workfusion tdeploy_home install install.cmd c: workfusion tdeploy_home Created files and directories after install.cmd c: workfusion tdeploy_home deploy (files: createsitefolder.cmd, publish.cmd, manageserver.cmd) config (files: config.sample.cmd, deployconfig.cmd, importusers.csv) lib (files: deployworkbook.cmd, deploydatasource.cmd, getdatetime.vbs, getlogfile.cmd, logecho.cmd, replacestr.vbs) to_deploy (empty) deployed (empty) log (empty) The installation log file in the install YYYY MMDD HHMI.log format is created in c: workfusion tdeploy home install . Create directories for a New website: cd c: workfusion tdeploy_home deploy createsitefolder.cmd sitename Here, site_name is a part of the APP instance host. For example, if APP is hosted at https: wfapp.company.com , the command parameter is wfapp. A website and a project with the same name will be created on the WorkFusion Analytics Server. To create several site folders, specify parameters for the create sitefolder.cmd command. The command creates the new configuration file and new directories. The configuration file is copied from deploy config config. sample _.cmd. New files and directories after create _sitefloder.cmd c: workfusion tdeploy_home deploy config config.sitename.cmd (settings for the site \"sitename\") to_deploy site_name (dashboards for deploying) dashboard (generic dashboard) datasource (data sources for generic dashboard) deployed sitename (already deployed dashboards: are being moved from todeploy site_name during deploy process) dashboard datasource Final directory structure c: workfusion tdeploy_home install lib deploy config lib to_deploy site_name dashboard datasource deployed site_name dashboard datasource log Prepare configuration files. You can download the configuration files from the APP Server. For more information, see Prepare RPA, OCRWIN and BI configurations. If needed, change the files manually. To edit the configuration files manually, in c: workfusion tdeploy _home deploy config , open config.site _name.cmd, and specify the appropriate values. Common config parameters REM MSSQL DB options SET DATABASESERVERNAME=servername (MSSQL_HOSTNAME) SET DATABASE_NAME=workfusion SET DATABASE_PORT=1433 SET DATABASE_USERNAME=dbuser SET DATABASE_PASSWORD=dbpass REM WorkFusion Analytics Dashboard option SET REQUIRE_SSL=1 REM WorkFusion Analytics (Tableau) Server options SET TABLEAUSERVERURL=\"https: tableau.com\" SET TABLEAUSITE=usersite (sitename) SET TABLEAU_USERNAME=tabuser (user role must be Server Administrator) SET TABLEAU_PASSWORD=tabpass Config parameter Values for Control Tower config.site_name.cmd {{ x }} Ansible variable defined earlier Description DATABASESERVERNAME MSSQL_HOSTNAME {{ mssql_hostname }} A DNS name of the MS SQL database server DATABASE_NAME workfusion An MS SQL database name DATABASE_PORT 1433 {{ mssql_port }} The default value is 1433 for MS SQL DATABASE_USERNAME {{ mssqldmuser }} mssqldmuser from secrets.yml DATABASE_PASSWORD {{ mssqldmpass }} mssqldmpass from secrets.yml REQUIRE_SSL 1 The option for dashboard connection to require SSL. Possible values are 0 or 1. Default value is 1 TABLEAUSERVERURL &quot;https: {{ bi_hostname }}&quot; The URL of the Tableau Server TABLEAU_SITE site_name {{ wftableausite }} A part of the APP_HOSTNAME if APP is hosted at https: wfapp.company.com the value will be wfapp TABLEAU_USERNAME {{ bi_user }} A Tableau Server user name with \"Server Administrator\" rights TABLEAU_PASSWORD {{ bi_pass }} The \"Server Administrator\" user's password In c: workfusion tdeploy home deploy config , open import users.csv, and specify the appropriate values. A new user is created on WorkFusion Analytics Server and must be used for Control Tower – Analytics integration. The new user is added with the 'Viewer' role. Also, replace the existing values with already configured credentials. Name and Password must be equal to {{wf tableau dashboard username}} and {{wf tableau dashboard password}} properties generated in config.yml file during preparation. import _users.csv tableauuser,tableaupass,,,,,email@example.com Do not use '%', '&', ' ' or '=' characters and other special symbols in the password. The ' ' symbol is allowed.Do not add or delete any commas in import _users.csv. Create a WorkFusion Analytics website, a project and a new user by running the following command in the command line tool: cd c: workfusion tdeploy_home deploy manageserver.cmd sitename A website and a project are created on WorkFusion Analytics Server as sitename and WF Automation respectively. The new user is added to a newly created website. To create several websites on the Server, specify the required parameters for the manage server.cmd command. Publish workbooks To publish workbooks: Download the Workbook packages, which are provided by the WorkFusion team, and copy files from folder dashboards workbook to the c: workfusion tdeploy home deploy to deploy site _name dashboard directory. The datasource directory must remain unchanged for backward compatibility. Run publish.cmd to consequently publish (add or replace) each file on WorkFusion Analytics Server. cd c: workfusion tdeploy_home deploy publish.cmd site_name To publish workbooks on several websites on WorkFusion Analytics Server, specify the corresponding parameters for publish.cmd command. This command also moves the published file to the c: workfusion deploy deployed site _name dashboard directory. As a result, the updated workbooks appear at WorkFusion Analytics page.The log file is created in deploy log. Do not use TWB TWBX files located in the deploy to _deploy or deploy deployed directories repeatedly. Reload them from the delivery package each time. Schedule Configuration A user with the administrator privileges configures a Schedule on WorkFusion Analytics Server. The new Schedule must be added to the Server along with other already existing pre configured Schedules. To create a schedule: In Tableau, in the main menu, click Schedules, and then click New Schedule. Schedules Specify the following options: Name. Execution: serial. Select this option for correct data update. Frequency: minimum 15 minutes. New schedule Assign a Schedule to a Workbook: Go to Content → Workbooks, select a required Workbook, click More (...) to expand the options, and then click Refresh Extracts. The Schedule must be assigned to the AA ETL Scheduler and all * _WB Workbooks. Refresh Go to the Schedule a Refresh tab, select the required refresh time and click Schedule Refreshes. Schedule A Schedule is now created for the selected Workbook. Specify the Workbooks refresh order. The priority for ETL workbook mustbe changed to prevent data gap. For that: Go to Tasks → Extract Refreshes, select the AA ETL Scheduler workbook, and then click More (...) → Change Priority. Change priority Set priority to 10 and click Change Priority. Priority The ETL workbook will be refreshed first. Enable WorkFusion Analytics Dashboard in Control Tower To enable BI Analytics Server on WorkFusion platform: Install WorkFusion Analytics server, as described in Install WorkFusion Analytics Server. Deploy workbooks to WorkFusion Analytics server, as described in Deploying Workbooks to WorkFusion Analytics Server. Enable WorkFusion Analytics server on WorkFusion platform: On the INT Server, check and configure the Vault secure properties file. The file must already be configured before the installation. tableau.dashboard.host=https: bi.example.com tableau.automation.host=https: bi.example.com tableau.dashboard.username=tableau_user tableau.dashboard.password=tableau_pass tableau.automation.username=tableau_user tableau.automation.password=tableau_pass Property Name Example Value Description Required for tableau.dashboard.host https: bi.example.com WorkFusion Analytics Server website url for Dashboard WorkFusion Analytics Dashboard tableau.automation.host https: bi.example.com WorkFusion Analytics Server website url WorkFusion Analytics UI tableau.dashboard.username tableauuser WorkFusion Analytics Server website username for Dashboard (was used in importusers.csv) WorkFusion Analytics Dashboard tableau.dashboard.password tableaupass WorkFusion Analytics Server website password for Dashboard (was used in importusers.csv) WorkFusion Analytics Dashboard tableau.automation.username tableau_user WorkFusion Analytics Server website username for WF Automation. Automation is published on the same server so the same credentials as for tableau.dashboard should be used. WorkFusion Analytics UI tableau.automation.password tableau_pass WorkFusion Analytics Server website password for WF Automation. Automation is published on the same server so the same credentials as for tableau.dashboard should be used. WorkFusion Analytics UI On the APP Server, restart the Workfusion service: sudo su wfuser; wfmanager restart workfusion Wait for about one minute and check that there are no any errors in the Control Tower's application logs: sudo su wfuser; tailf n 50 opt workfusion supervisord log workfusion.out.log Check the installation To check the installation: In a browser, open URL https: > to go to the APP instance. The following menu appears: ct menu In the main menu, go to the Analytics group to check that the the dashboard is displayed. check dashboard "},{"version":"10.1","date":"Jul-08-2019","title":"install-ocr-windows-server","name":"Install OCR Windows server","fullPath":"iac/admin/legacy/install-ocr-windows-server","content":" Prepare for installation Before manual OCR Windows Server installation, you need to Initialize secure storage (Secrets Vault) Run DB migrations Generate configuration files for copying to OCR Windows Server. All the commands are run on Integration Server. To prepare for installation: Run DB migrations for OCR Windows Server. To populate MSSQL database with required tables in OCR schema, run the following command: . install.sh migrate ocrwin Initialize Secrets Vault for OCR Windows Server. . install.sh configure ocrwin Generate OCR Windows Server config files and OCR certificates to be copied to OCR Windows Server while installation. . install.sh generate_conf ocrwin As soon as generation is complete, go to distr win configs ocrwin configs to view the generated files. See the output example below. ls la distr winconfs ocrwinconfigs total 24 drwxr xr x. 2 wfuser wfuser 100 Jun 13 07:44 . drwxr xr x. 3 wfuser wfuser 28 Jun 13 07:44 .. rw r r . 1 wfuser wfuser 2984 Jun 13 07:44 ca.crt rw r r . 1 wfuser wfuser 3085 Jun 13 07:44 ocr_config.ini rw r r . 1 wfuser wfuser 4835 Jun 13 07:44 ocrwin.crt rw r r . 1 wfuser wfuser 1674 Jun 13 07:44 ocrwin.key rw r r . 1 wfuser wfuser 2517 Jun 13 07:44 workfusion.p12 Install OCR Windows Server To install OCR Windows Server: Extract all files from the OCRInstaller {version}.zip to any folder on your Windows PC. Go to distr win configs ocrwin configs and copy all the generated config files ( prepare for installation) to the folder with the extracted OCR installer on the Windows machine. Extracted If you use the self signed certificates, ca.cert must be imported into Local Machine's Certificate TrustStore > the Trusted Root Certificates Authorities section. For more information, see Import Root or Intermediate CA to Store. Launch OCR Installer. Click Options to specify the installation location. Install location Wait until the installation process is finished. Restart all services. Activate OCR License. Configure Nginx on Windows machines The section describes using a secure connection between OCR and Workfusion Services. Skip the step, if the setup does not use TLS encryption. Configure Windows Firewall In Windows Firewall, create a new rule. Allow inbound SSL secure connection to applications on the OCR server. New rule ocr allow connection Define the local port 443 to allow the connection. port 443 "},{"version":"10.1","date":"Jul-08-2019","title":"install-rpa-windows-server-using-generated-configs","name":"Install RPA Windows servers with generated configs","fullPath":"iac/admin/legacy/install-rpa-windows-server-using-generated-configs","content":" Install RPA package Install RPA as the Administrator user on a Windows machine. To install the RPA package: Download the installer file, and then extract all files from the RPAInstaller {version}.zip to any folder on your PC. Launch RPAInstaller.exe and follow the RPA Setup Wizard steps. Allow the installer to make changes to your computer. Accept the License Agreement and proceed. Set the Destination Folder for the RPA installation and wait until the installation process is finished. By default, the destination directory for RPA installation is C: RPA that will be further referred to as . Destination Configure Users Before RPA Windows Server installation, users must already be created and have local profiles on the RPA machine. In case you need to create users, see the instructions. Run the following commands on a Windows machine, in PowerShell, as Bot Master. Bot Master is the user that triggers Bot Units. in Control Tower, register Bot Master in Secrets Vault. To do that, add two secret entries with Alias for Bot Master user and Alias for Bot Master password. The Key box must be the same as Alias, while Value stands for either username or password. Use the following format to proceed: Bot Master User secret entry secretsentry user Bot Master Password secret entry secretsentry user Grant RPA with full folder permissions to all created users – Bot Master, Bot Unit X. The RPA folder is the folder where your RPA is installed. Make sure that Bot Master is a member of the Administrator group. For more information on configuring groups and permissions in Windows, see Configuring permissions and groups. Enable RDS to use multiple bots on one server Multiple RDP Sessions are configured on RPA Windows Server. To enable RDS to use multiple bots: Go to Server Manager > Manage > Add Roles and Features. Dashboard The Add Roles and Features Wizard opens. Dashboard Go to Server Selection > Server Roles > Remote Desktop Services > Next. Dashboard Select Remote Desktop Session Host > Next > Add Features. Dashboard Restart your machine. Dashboard Verify Internet Explorer settings (optional) Verification is required in case Internet Explorer is used. The settings are verified for each added user. For more details, see Configuring Internet Explorer for RPA. Configure firewall for Nginx Nginx configuration is required to provide secure communication among RPA machines. To provide access to Nginx, configure Windows Firewall to define a port for connection: Go to Control Panel > Windows Defender Firewall. Select Advanced settings > Inbound Rules, and then click New rule to allow inbound secure communication for applications to use the SSL connection. Select TCP and define the local port 443 to allow the connection. Specify the connection name and click Finish. Generate configuration files for RPA Windows Server RPA Windows Server configuration files are generated on the Integration Server. Log in to the Integration Server via SSH. Change to default user wfuser. su wfuser cd opt workfusion wf_installer Mind that your default username may differ. When installing SPA, you can set up the username in the config.yml file. The default value is wfuser. In the hosts.yml file, specify the actual hostnames for rpa _hostnames. If there is more than one hostname, use a comma to separate them. Make sure that certificates are generated and located in certificates. In case of multiple bots, specify the rpa bots per _server parameter and define the number of bots. . install.sh edit_config int Run a configuration generation command. . install.sh generate_conf rpa Configuration files are placed to distr win confs rpa configs. For more information, see prepare RPA, OCRWIN and BI configurations. Apply generated configuration files All the generated configuration files are formed in the corresponding RPA folder structure for each RPA server separately. To apply configuration files: Archive all the configuration files for each RPA server: zip r rpa testserver rpa2.workfusion.com.zip . rpa testserver rpa2.workfusion.com Copy all the generated files from the Integration server to a required RPA server. Use the following path to the files: distr win confs rpa configs. Unarchive and put all the generated configuration files to the RPA package home directory(or ), while preserving the directory structure. Schedule Bot Agent startup on logon Scheduling of tasks can be performed using the task scheduler or advanced group policies. Bot Agent startup is scheduled to autostart an RDP session (Bot Master, Bot Unit X) by Bot Manager. To schedule Bot Agent's startup: For one Bot Unit per server: Open the Windows console and enter the following commands: SchTasks Create SC onlogon TN \"bot agent BotMaster\" TR \" bot agent bin bot agent nordp.bat\" RU BotMaster IT F For several Bot Units per server: Configure tasks – one for Bot Master user and one for each Bot Unit. To do that, enter the following commands: SchTasks Create SC onlogon TN \"bot agent BotMaster\" TR \" bot agent bin bot agent master.bat\" RU BotMaster IT F SchTasks Create SC onlogon TN \"bot agent BotUnit1\" TR \" bot agent bin bot agent unit1.bat\" RU BotUnit1 IT F SchTasks Create SC onlogon TN \"bot agent BotUnitX\" TR \" bot agent bin bot agent unitX.bat\" RU BotUnitX IT F Configure Bot Master autostart Bot Master autostart is set up to run RDP sessions. To allow autostart, provide correct RPA Windows Server hosts. To configure Bot Master autostart: On the Application Server, go to the bot manager config directory. In machines.yml, provide correct RPA Windows Server hosts. machines.hosts: test1.workfusion.com test2.workfusion.com test3.workfusion.com Restart Bot Manager. wfmanager restart bot manager Start Bot Units Before starting Bot Units, make sure you made necessary changes. There are several ways to run Bot Units – either with Bot Manager (automatic start from Application linux server call), or by using Bot Agent. We recommend avoiding manual start. Use automatic start from the Applicationlinux server call instead. To start Bot Units manually... Before starting the services, select Deployment Architecture. For one Bot Unit per server (VDI setup): Run bot agent bin bot agent nordp.bat to start Nginx, Bot Master, and Bot Unit. Your user session must be active to guarantee correct work. For multiple Bot Units per server (RDP in RDP): Run bot agent bin bot agent master.bat to start Nginx, Unit RDP sessions in a separate RDP session. Make sure to change The Master Agent configuration that depends on your sessions Unit count and create additional configs for Units. For correct work, Unit sessions must be active internally in the master session, but the master session can have the disconnected status. Run Unit agents in each Unit RDP session: bot agent bin bot agent unit{Unit_ID}.bat. Check RPA Installation There are several ways to check RPA Windows Server installation. To check installation: Make sure that Bot Units are available, and WebDrivers exist on the connected Bots. To do that, open the following link as any user on the RPA machine. http: localhost:1500{unit_id} grid console On evoking the command, view the list of available Bot Units in your browser. To check RPA Worker, open the following link in your browser under as any user on the RPA machine: http: localhost:1520{unit_id} health. You receive the following response: { \"status\": \"UP\", \"details\": { \"rabbit\": { \"status\": \"UP\", \"details\": { \"version\": \"3.7.14\" } }, \"diskSpace\": { \"status\": \"UP\", \"details\": { \"total\": 53317988352, \"free\": 25653489664, \"threshold\": 10485760 } }, \"refreshScope\": { \"status\": \"UP\" }, \"zookeeper\": { \"status\": \"UP\", \"details\": { \"connectionString\": \"hostname.workfusion.com:port\", \"state\": \"STARTED\" } } } } Open Bot Manager and make sure that your RPA Windows Server hostname is available, and that you are able to send commands to Bot Unit components. Run any RPA task and view logs in Kibana. "},{"version":"10.1","date":"Jul-08-2019","title":"install-rpa-windows-server","name":"Install RPA Windows server","fullPath":"iac/admin/legacy/install-rpa-windows-server","content":" Install RPA Windows Server 1. Install RPA Package Run the following steps as the Administrator user on a Windows machine. Download the installer file, and then extract all files from the RPAInstaller {version}.zip to any directory on your PC. Launch RPAInstaller.exe, follow the RPA Setup Wizard steps, and allow the program to make changes to your computer. Allow changes Accept the License Agreement and proceed. Specify the Destination Folder for RPA installation and wait until the installation process is finished. By default, the destination directory for RPA installation is C: RPA that will be referred to as below. 2. Import and configure certificates The list of your certificates needed at this step is as follows. Certificate Usage Description ca.crt Filebeat Metricbeat Nginx Java Root CA certificate in the .pem format (the file may have .crt, .cer, .pem, or .cert extension). All the other certificates in the table must be signed by this Root CA that is imported into Java Truststore on all servers. The certificate file must include only Root cert, without any intermediate certs. rpa.crt &gt; Rename to server.pem while configuration. rpa.key &gt; Rename to server.key while configuration. Nginx Certificate and its private key files in the .pem format. Certificate's CommonName or SANs must match all servers in the list of rpa_hostnames. The certificate file must include end certificate plus intermediate cert(s) if exist(s). Private key should not be password protected. mtls client.crt mtls client.key Nginx Certificates used to implement Mutual TLS authentication between services. vault_workfusion.p12 &gt; Rename to workfusion.p12 while configuration. Secrets Vault Specific Secret Storage certificate used for authentication with Secrets Vault. For more information on certificates, see Prepare TLS certificates for installation. To import certificates: To locate the trusted certificates, follow the path opt workfusion wf _installer certificates . The certificates are placed in the certificates directory on the Integration Server. Import your root certificate file ca.crt to Local Machine's Certificate TrustStore > Trusted Root Certificates Authorities. Configure certificates for: Filebeat Metricbeat Secrets Vault Nginx Java Filebeat The Filebeat service sends content of specific log files. To get logs, Filebeat must be imported. For that, in your filebeat directory, create the ssl directory, and copy ca.crt to filebeat ssl. Metricbeat The Metricbeat service sends metrics data to the Logstash service. To import the Metricbeat certificates, in the metricbeat directory, create the ssl directory, and copy ca.crt to metricbeat ssl. Secrets Vault To import the Secrets Vault certificates: In the bot agent directory, create the ssl directory, and upload the vault _workfusion.p12 file to bot agent ssl. Rename the file to workfusion.p12. In the worker directory, create the ssl directory, and upload vault _workfusion.p12 to the worker ssl. Rename the file to workfusion.p12. Nginx To import the Nginx certificates: In your nginx directory, create the ssl directory, and copy ca.crt to nginx ssl. Upload SSL Certificate and SSL key files to the nginx ssl directory. Rename rpa.crt to server.pem, and rpa.key to server.key. Upload the TLS certificate mtls client.crt and key file mtls client.key to the same directory. Update Bot Agent keystore RPA Bot Agent does not support SSL while Application Server does. For Server, RPA Bot Agent keystore needs to be updated. To update the keystore: Go to the Java bin directory java bin. Import the downloaded ca.crt to the RPA Java keystore. Run the following command with your arguments from the java bin directory. keytool import alias localCA file \"path to ca.crt\" keystore \"path to cacerts\" storepass changeit where Arguments are as follows: Argument Description Example path to ca.crt the absolute path to downloaded ca.crt nginx ssl ca.crt path to cacerts absolute path to default RPA Java keystore java jre lib security cacerts changeit default password for RPA Java keystore 3. Configure users Before RPA Windows Server installation, users must be created and have local profiles on RPA machine. For more information on user creation, see Create RPA users. Run the following commands on a Windows machine, in PowerShell as the Bot Master user. Bot Master is the user that triggers Bot Units. To configure users: in Control Tower, register Bot Master in Secrets Vault. To do that, add two secret entries with Alias for Bot Master user and Alias for Bot Master password. The Key field must be the same as Alias, while Value stands for either username or password. Bot Master User secret entry: Bot Master Password secret entry: Grant RPA full directory permissions to all created users: Bot Master Bot Unit X The RPA full directory is the directory where your RPA is installed. Make sure that Bot Master is a member of the Administrator group. Follow the link to learn more on configuring groups and permissions in Windows. 4. Enable RDS to use multiple bots on one server The number of RDS sessions must be the same as the number of bots. To configure multiple RDP sessions on the RPA Windows server: Navigate to Server Manager > Manage > Add Roles and Features. The Add Roles and Features Wizard opens. Go to Server Selection > Server Roles > Remote Desktop Services > Next. Select Remote Desktop Session Host > Next > Add Features. Restart your machine. 5. Verify Internet Explorer settings (optional) Verification is required in case Internet Explorer is used. The settings are verified for each added user. For more details, see Configuring Internet Explorer for RPA. 6. Configure Nginx Nginx configuration is required to provide secure communication among RPA machines. To provide access to Nginx, configure Windows Firewall to define a port for connection: Go to Control Panel > Windows Defender Firewall. Select Advanced settings > Inbound Rules, and then click New rule to allow inbound secure communication for applications to use the SSL connection. Select TCP and define the local port 443 to allow the connection. Specify the connection name and click Finish. To configure Nginx: Go to nginx conf nginx.conf. Replace all {RPA _HOSTNAME} with a valid DNS name of your RPA Windows Server. Replace {APP HOST IP} with the IP address of Application Server. Replace {RPA HOST IP} with the IP address of your RPA Windows Server. To set up communication with Control Tower, add the following settings to nginx.conf. Replace APP DNS NAME} with Application Server's FQDN. mTLS client config for communication with Control Tower. Used by worker. server { listen 127.0.0.1:7080; location workfusion internal api { proxysslcertificate .. ssl mtls client.crt; proxysslcertificate_key .. ssl mtls client.key; proxysslciphers HIGH:!aNULL:!MD5; proxysslprotocols TLSv1.1 TLSv1.2; proxysslsession_reuse on; proxysslverify on; proxyssltrusted_certificate .. ssl ca.crt; proxysslverify_depth 2; proxypass https: {APPDNS_NAME}:7083 workfusion internal api; } location workfusion { deny all; return 403; } } To set up communication with BEP, add the following settings to nginx.conf. Replace BEP MASTER DNS _NAME} with BEP Master Server's FQDN. mTLS client config for communication with automl gateway service server { listen 127.0.0.1:9070; location automl gateway service { proxysslcertificate .. ssl mtls client.crt; proxysslcertificate_key .. ssl mtls client.key; proxysslciphers HIGH:!aNULL:!MD5; proxysslprotocols TLSv1.1 TLSv1.2; proxysslsession_reuse on; proxysslverify on; proxyssltrusted_certificate .. ssl ca.crt; proxysslverify_depth 2; proxypass https: {BEPMASTERDNSNAME}:9073 automl gateway service; } } 7. Configure Bot Agent For correct performance of Bot Units, you should configure Bot Agent environment parameters as well as to set up Bot Master autostart and Bot Agent startup. Configure Bot Agent environment parameters For correct RPA environment work, set the required environment parameters. In bot agent conf environment.yml, replace parameters with {} brackets. Do not replace parameters in {} with the symbol, for example, {rpa.rabbitmq.worker.queue.template} . Specify the following parameters: {ZOOKEEPER _HOSTNAME} – hostname of the Integration Server where the Zookeeper service in installed. {ZOOKEEPER _PORT} – by default, 2181. {BOT MANAGER HOSTNAME} – hostname of Application Server where Bot Manager is installed. {RPA _HOSTNAME} – RPA Windows Server hostname. {LOGSTASH _HOSTNAME} – hostname of Integration Server where the Logstash service is installed. Default environment parameters file: spring: cloud: zookeeper: connect string: {ZOOKEEPERHOSTNAME}:{ZOOKEEPERPORT} environment: rpanginxport: 443 botmanagerhostname: {BOTMANAGERHOSTNAME} botmanagerport: 443 botmanagerscheme: https botmanagerpath_context: \"bot manager\" botmanageruser: ' {bot.manager.security.user.name}' botmanagerpassword: ' {bot.manager.security.user.password}' agent.hostname: {RPA_HOSTNAME} agent.port: 443 agent.scheme: https logstashhost: {LOGSTASHHOSTNAME} logstash_port: 4567 metricbeat_port: 4569 rabbitmqqueuefull_name: ' {rpa.rabbitmq.worker.queue.template}. {fleet}' worker_gavp: ' {rpa.default.worker.gavp}' workerclientid: ' {rpa.default.worker.client.id}' agentzookeeperconnectstring: {ZOOKEEPERHOSTNAME}:{ZOOKEEPER_PORT} server_address: 127.0.0.1 :::important Bot Manager credentials are not visible in bot agent conf environment.yml. The credentials are set up and stored in Secrets Vault. ::: Schedule Bot Agent startup on logon Scheduling of tasks can be performed using the task scheduler or advanced group policies. Bot Agent startup is scheduled to autostart an RDP session (Bot Master, Bot Unit X) by Bot Manager. To schedule Bot Agent's startup: For one Bot Unit per server: Open the Windows console and enter the following commands: SchTasks Create SC onlogon TN \"bot agent BotMaster\" TR \" bot agent bin bot agent nordp.bat\" RU BotMaster IT F For several Bot Units per server: Configure tasks – one for Bot Master user and one for each Bot Unit. To do that, enter the following commands: SchTasks Create SC onlogon TN \"bot agent BotMaster\" TR \" bot agent bin bot agent master.bat\" RU BotMaster IT F SchTasks Create SC onlogon TN \"bot agent BotUnit1\" TR \" bot agent bin bot agent unit1.bat\" RU BotUnit1 IT F SchTasks Create SC onlogon TN \"bot agent BotUnitX\" TR \" bot agent bin bot agent unitX.bat\" RU BotUnitX IT F Configure Bot Master autostart Bot Master autostart is set up to run RDP sessions. To allow autostart, provide correct RPA Windows Server hosts. To configure Bot Master autostart: On the Application Server, go to the bot manager config directory. In machines.yml, provide correct RPA Windows Server hosts. machines.hosts: test1.workfusion.com test2.workfusion.com test3.workfusion.com Restart Bot Manager. wfmanager restart bot manager 8. Configure Bot Units The configuration step includes configuring multiple Bot Units as well as Master Bot Agent. Configure multiple Bot Units To configure Nginx for multiple Bot Units: In the nginx conf bot units directory, create as many templates as the number of Bot Units you have to set up, for example, five templates for five Bot Units. Copy unit.template to unit{unit id}.conf where unit id is a digit between 1...99 (if your unit id more than **9, remove last \"0\" in the proxy pass port). Open the template and replace all the unit id placeholders with the same unit id number that you used to name the file. Modify or copy file bot agent conf units unit{Unit _id}.yml. (Unit1.yml base example). Change unit_id and fleet. Here, shared is a default fleet. If your unit_id is more than 9, remove the last \"0\" in each port – maximum 99 units for one machine. unit_id: 2 fleet: shared server.port: 1000 {2} logging.directory: logs unit {2} node_port: 1510 {2} hub_port: 1500 {2} worker_port: 1520 {2} bot.agent.context.path: unit {2} environment.agent.ns: \" {environment.agent.hostname}: {server.port}\" If you need to have custom fleets to submit tasks to, specify fleet for each Bot Unit. Fleets are used to distribute tasks between RPA Workers. To learn more, see Task Distribution. unit_id: 3 fleet: SAP Create the start .bat file for each Unit: bot agent bin bot agent unit{Unit _ID}.bat, and then change unitid. @ECHO OFF set unitid=2 cd %~dp0 set config=conf bootstrap.yml,conf environment.yml,conf units unit%unitid%.yml,conf bot agent unit.yml call bot agent.cmd run Unit%unitid% \"%config%\" Configure Master Bot Agent To configure Master Bot Agent: Edit or copy bot agent conf bot agent master.yml. Add additional Unit processes for start. Default config part: processes: id: rdp1 expression: \"cmd c start wait mstsc unit.rdp v:127.0.0.1\" directory: \"rdp\" id: rpa.nginx expression: \"cmd c start wait start_nginx.bat\" directory: \".. nginx\" tag: \"nginx\" id: rpa.filebeat expression: \"cmd c start wait filebeat pipeline.bat {environment.agent.hostname} {environment.logstashhostname} {environment.logstashport}\" directory: \".. filebeat\" tag: \"filebeat\" The code example for two Bot Units is the following. processes: id: rdp1 expression: \"cmd c start wait mstsc unit.rdp v:127.0.0.1\" directory: \"rdp\" id: rdp2 expression: \"cmd c start wait mstsc unit.rdp v:127.0.0.2\" directory: \"rdp\" id: rpa.nginx expression: \"cmd c start wait start_nginx.bat\" directory: \".. nginx\" tag: \"nginx\" id: rpa.filebeat expression: \"cmd c start wait filebeat pipeline.bat {environment.agent.hostname} {environment.logstashhostname} {environment.logstashport}\" directory: \".. filebeat\" tag: \"filebeat\" 9. Start Bot Units Before starting Bot Units, make sure you made necessary changes. There are several ways to run Bot Units – either with Bot Manager (automatic start from Application linux server call), or by using Bot Agent. We recommend avoiding manual start. Use automatic start from the Applicationlinux server call instead. To start Bot Units manually... Before starting the services, select Deployment Architecture. For one Bot Unit per server (VDI setup): Run bot agent bin bot agent nordp.bat to start Nginx, Bot Master, and Bot Unit. Your user session must be active to guarantee correct work. For multiple Bot Units per server (RDP in RDP): Run bot agent bin bot agent master.bat to start Nginx, Unit RDP sessions in a separate RDP session. Make sure to change The Master Agent configuration that depends on your sessions Unit count and create additional configs for Units. For correct work, Unit sessions must be active internally in the master session, but the master session can have the disconnected status. Run Unit agents in each Unit RDP session: bot agent bin bot agent unit{Unit_ID}.bat. 10. Check RPA Windows Server installation There are several ways to check RPA Windows Server installation. To check installation: Make sure that Bot Units are available, and WebDrivers exist on the connected Bots. To do that, open the following link as any user on the RPA machine. http: localhost:1500{unit_id} grid console On evoking the command, view the list of available Bot Units in your browser. To check RPA Worker, open the following link in your browser under as any user on the RPA machine: http: localhost:1520{unit_id} health. You receive the following response: { \"status\": \"UP\", \"details\": { \"rabbit\": { \"status\": \"UP\", \"details\": { \"version\": \"3.7.14\" } }, \"diskSpace\": { \"status\": \"UP\", \"details\": { \"total\": 53317988352, \"free\": 25653489664, \"threshold\": 10485760 } }, \"refreshScope\": { \"status\": \"UP\" }, \"zookeeper\": { \"status\": \"UP\", \"details\": { \"connectionString\": \"hostname.workfusion.com:port\", \"state\": \"STARTED\" } } } } Open Bot Manager and make sure that your RPA Windows Server hostname is available, and that you are able to send commands to Bot Unit components. Run any RPA task and view logs in Kibana. "},{"version":"10.1","date":"Aug-02-2019","title":"prepare-for-installation","name":"Prepare for installation","fullPath":"iac/admin/legacy/prepare-for-installation","content":" Create RPA users Before installation, the Bot Master and Bot Unit users must be created on each server that you plan to deploy and specified later in hosts.yml, in the rpa_hostnames list (see the Configure installation step). The users must meet the following requirements: A Bot Master user must have the same username and password on all RPA servers. A Bot Master user must be a member of the Administrators and Remote Desktop Users groups on all RPA servers. All Bot Unit users must have the same username and password on all RPA servers. A Bot Unit user must be a member of the Remote Desktop Users group on all RPA servers. The number of created Bot Unit users must be specified in config.yml in the rpabotsper_server parameter, on the Configure installation) step. All Bot Unit users must have common basename and different index numbers in the end of the basename. For more information, see config.yml options, section RPA. For example, if, in config.yml*, you set rpa bots per server: 2, and bot unit base name: BotUnit, then the following users must be present on each RPA server: BotUnit1, BotUnit2,... To create an RPA user on the Windows server: Connect to the remote Windows server via Remote Desktop, or any other software, as a user with the Administrator's privileges. Run PowerShell as Administrator. In PowerShell, run the following commands to create a Bot Master user and add it to groups: net user BotMaster add net localgroup \"Administrators\" BotMaster add net localgroup \"Remote Desktop Users\" BotMaster add Here, is the password for the Bot Master user. In PowerShell, run the following commands to create the Bot Unit users and add them to groups: net user BotUnit1 add net localgroup \"Remote Desktop Users\" BotUnit1 add Here, is the password for the Windows Bot Unit user. Prepare the MS SQL Server For Workfusion SPA installation you need to have an existing activated instance of the MS SQL server. The server must be up and running during the Workfusion SPA installation. MS SQL Server must run in SQL Server and Windows Authentication mode: SQL Server and Windows Authentication mode To prepare a MS SQL server: Create the database, and specify the database name. For other options you may use the default values. Create DB Create the owner with the full access permissions for the database. This user must be later specified in config.yml. Note: for all logins, the following password policies are applied: Allowed special symbols for db password for monitoring tools: @ *():,.} Allowed special symbols for db password for the Analytics component: @ *():,.;} To create : On a MS SQL server, create the mssql login, specify the login name, and then make sure that the Enforce password policy check box is not selected. In the Default database box, specify the database, and then choose English as Default language. Create DB owner In the Select a page group, click User mapping, and then select the check box to map the user to the database. Leave all other options unchanged. Create DB user In the database Users, select the db owner check box to assign to the db owner role. Assign to group Create the following MS SQL login names and passwords for the Workfusion components: mssql dba user mssql ct user mssql ws user mssql sqc user mssql ds user mssql rpa user mssql pm user mssql dm user mssql rapi user mssql ocr user See the password policy earlier. For the description of these parameters, see config.yml options. For each login, specify the following parameters: If you use the SQL Server authentication, Specify the login name. Make sure that the Enforce password policy check box is not selected. In the Default database box, specify the database. Choose English as Default language. SQL auth If you use Windows authentication, Select a corresponding login name. In the Default database box, specify the database. Choose English as Default language. For other options leave the default values. Win auth When configuring the Integration server, on the INT server, in the config.yml file, in the MSSQL section, remember to specify the corresponding login names and passwords, that you have created on the previous step. Prepare the INT Server All further steps must be performed on the Integration server. Extract installation packages To extract installation packages: Log in to the INT server as a user with the root privileges, by using Linux Terminal, Windows PuTTY, or any other SSH client: Example: Linux Terminal ssh i @ sudo su root In the directory, create a directory for the WorkFusion installation package, for example, opt workfusion wf _installer. Grant the read access for this directory to all users: mkdir p opt workfusion wf_installer chmod 0755 opt workfusion wf_installer Extract the downloaded package to the directory: tar xzvf opt workfusion workfusion full package .tar.gz strip 1 C opt workfusion wf_installer Configure installation To configure installation: On your INT Server, in the directory, edit the hosts.yml configuration file. The vi text editor is used in the example below: vi hosts.yml Refer to the hosts.yml options section for more details on each option. On your INT Server, in the directory, open config.yml, and in the ansiblevaultpassword parameter, change the password. vi config.yml The password is used for encrypting the config.yml at the following step and will be asked several times during the installation of the components. We strongly recommend to secure and memorize it. When creating passwords, make sure they are strong enough and meet the following requirements: Ansible pass If needed, change the other secrets and parameters as well. After that, save the changes and close the file. Refer to the config.yml options section for more details on each configuration option. Optionally, on the INT server, run the following command to automatically generate passwords for the WorkFusion internal services according to product policies. We strongly recommend to change them manually for security reasons. Note that the command doesn't generate end user password. . install.sh passwords generate As a result, in config.yml, the passwords for most services, except for the following end user ones, are created: \"wf_password\" \"ldapbindpassword\" \"mail_pass\" \"tableaudashboardpass\" \"tableauautomationpass\" \"windowsinstallationpass\" \"elkadminpass\" \"ansiblevaultpassword” \"botrelayuser_pass\" \"botuserpass\" If you prefer to set the passwords manually, remember to use only Latin letters and allowed symbols. If needed, change other secrets and parameters, as well. After that, save and close the file. Remember to specify the corresponding MS SQL login names and passwords, that you have created earlier, in the MSSQL section. Encrypt config.yml. . install.sh encrypt config Now config.yml is encrypted via ansible vault with the password that you have provided earlier. If needed, you can edit the encrypted file with the following command: . install.sh edit_config int Enter ansiblevaultpassword, when asked. Prepare the TLS certificates, as described in Prepare TLS certificates for installation. Distribute the certificates folder, along with config.yml files across all other Linux servers: Create the wf _configs.tar.gz archive with these files: tar czvf wf_configs.tar.gz certificates config.yml hosts.yml Copy wf _configs.tar.gz to all other servers. For that a python SimpleHTTPServer can be used. Start the HTTP server on any INT Server's port, which is accessible by other servers: Example: SimpleHTTPServer is on port 9999 python m SimpleHTTPServer 9999 After running this command in a specific directory, the content of the directory can be downloaded via HTTP. Prepare other Linux servers The following steps must be performed on each other Linux server. To prepare another Linux server: Log in to a targer server, as a root user, and download wf _configs.tar.gz from the INT server: cd opt workfusion wf_installer install wget if not installed yum y install wget wget http: inthostname:9999 wfconfigs.tar.gz O wf_configs.tar.gz Unpack configuration files and certificates: tar xzvf wfconfigs.tar.gz C opt workfusion wfinstaller Install the WorkFusion license The following step must be performed on APP Server. To install the license, copy the provided license.properties file to the directory, for example, opt workfusion wf_installer license.properties. "},{"version":"10.1","date":"Jul-08-2019","title":"rpa-server-overview","name":"RPA server overview","fullPath":"iac/admin/legacy/rpa-server-overview","content":" RPA Deployment Diagram rpa bep components To learn more about deployment, see RPA Deployment. For general SPA deployment architecture, go here. Installation options There are two ways to install RPA Windows Server: RPA Windows Server installation based on generated config files. Recommended. Fully manual installation and configuration of RPA Windows Server and related components. RPA Folder Structure As soon as you have downloaded the installer file, extract all the files from the RPAInstaller {version}.zip to any directory on your PC. The packages inside the directory are as follows... Package name Package usage autoit executes autoit scripts bot agent contains Bot Agent together with its configuration and logs filebeat contains Filebeat configuration and logs java used to execute Java processes (for 64 bit Windows) java _x86 used to execute Java processes (for 32 bit Windows) libs contains dll files used for native work of the system logs contains logs of Bot Relays, Bots, and Workers metricbeat contains Metricbeat together with its configuration and logs nginx contains Nginx configuration and logs registry contains registry files used while installation installer runs machine settings and user settings rpa grid performs configuration and runs Bot Relay and Bot files scripts collects logs from the package, contains scripts for both logs and configuration worker contains executable code Prerequisites Internet Explorer 11. The CA certificate must be imported into Trusted Root Certificates Authorities of Local Machine's Certificate TrustStore. For more information, see Import Root or Intermediate CA to Store. Before configuring users, make sure your Internet Explorer browser is turned off and no Java processes are running. The MS SQL, Integration, and Application Servers must be installed. "},{"version":"10.1","date":"Jul-09-2019","title":"activate-the-ocr-license","name":"Activate OCR license","fullPath":"iac/admin/licenses/activate-the-ocr-license","content":" WorkFusion provides the following license categories: Base category: includes supported languages, except for Arabic, Farsi, Chinese, Japanese, Korean, Thai, Hebrew, Yiddish, and Vietnamese. CJK category: includes the Base category languages plus Chinese Traditional, Chinese Simplified, Japanese, Korean, and Korean Hangul. All category: all supported languages are included (see the language list). Activate license FRE SDK is located in the following directory: Linux: ABBYY _FRE11 Windows: FineReaderEngine After installation of the OCR components, on the OCR server, the license _request.txt file appears. To activate the license: Send an email to your account manager in WorkFusion, or create a Service Desk ticket according to the following template. Attach license _request.txt to the email. > Dear Support Team, > > Please provide OCR license according to the following requirements: > > Company name: _ Number of pages to include: _ *) Platform type: _(Linux Windows) > >Status of the server (Production, POC, UAT, DEV) Category of languages: Base, or CJK (supports Chinese, Japanese, Korean), or All Token returned by prepareLicense API: _ (content of the file license _request.txt on the OCR server) *) The number of OCR pages must be estimated during POC on basis of the customer's workload. For example, if 1000 pages is processed per one day, then per month the amount will be 30.000 pages. As stated further, the non prod licenses have some predefined values 10K, 50K, 100K pages depending on complexity of the customer workload. Wherever practical, request the minimum possible license. The account manager is always aware of the number of pages included into the contract – it’s a part of negotiation with the customer. Note that the license cost depends on the number of pages as well as on the category. Make sure your contract with WorkFusion covers it. If you have multiple OCR servers to balance the workload, request multiple OCR licenses: a separate license for each server. In response to your email, WorkFusion sends you file license response.txt. Copy the file to license response.txt on the OCR server. On your OCR server, go to the directory, and then run the following script according to the OS of the OCR server: activate _wf.sh on Linux activate _ocr.bat on Windows in SPA Example: cd . activate_wf.sh If everything is OK, the command reports a success result: {\"message\":{\"content\":\"License Activation Succeeded\"}} Note that, if the category of the newly activated license differs from your previous one, the change takes effect immediately after activation, even if you have a lot of pages left from your previous license. If you migrate from SPA versions 8.5 and earlier, follow the steps below to reuse the available number of pages in your new environment. Get the number of unused pages from pre 8.5 versions... Perform the following operations on all OCR servers for all activated licenses. To re use the available number of pages: Run License Manager as any user. export LDLIBRARYPATH= LDLIBRARYPATH: apps ABBYY FREngine11 Bin apps ABBYY FREngine11 Bin LicenseManager.Console Select the activated license. Activated license The license serial number and other license parameters can differ from the examples. Open license parameters. Activated license Make a screenshot of the first page. Make sure that line Volume → Regular Text → Remains is visible. Activated license Request and activate the license by sending an email to your account manager in WorkFusion, or create a Service Desk ticket according to the template (see earlier). Remember to attach all license screenshots, if Remains is greater than 0. Verify license After activation, you can check the state of your active license. For that, connect via SSH to the OCR server, and then use the activeLicense API. If authentication is disabled (the auth disabled profile is configured), no authentication header is needed and the command looks as follows: curl s http: localhost:9002 api v1 cloud activeLicense python m json.tool If authentication is enabled, the command is as follows: curl s H \"Authorization: \" http: localhost:9002 api v1 cloud activeLicense python m json.tool Replace `` with your credentials according to the OCR Service configuration. The header may look like: \"Authorization: Basic your username pwd _base64\" for basic authentication \"Authorization: Bearer your jwt token\" for JWT authentication "},{"version":"10.1","date":"Jul-08-2019","title":"update-the-elk-license","name":"Update ELK license","fullPath":"iac/admin/licenses/update-the-elk-license","content":" Initial installation of WorkFusion SPA version 10.0 enables the 30 day trial license for ELK. For production purposes, it is required to update the license in Kibana. :::important The WorkFusion ELK License file must be provided by the installation team or your account manager. ::: To update the ELK license: Go to Kibana, for example, at . Log in Authenticate to Kubana, using the ELK admin credentials that you specify in the config.yml file during the installation. Pass On the left menu, click Management. Pass Go to the license management section. Pass Click Update license. Pass Select or drag your license file to the window. The license is uploaded to Kibana and updated automatically. "},{"version":"10.1","date":"Jul-09-2019","title":"manage-workfusion-analytics-license","name":"Manage WorkFusion Analytics license","fullPath":"iac/admin/licenses/manage-workfusion-analytics-license","content":" License type Current Tableau version is 2018.3.2. The license for Tableau Server is requested by an Account manager and provided by WorkFusion. For SPA 10.0 you can also use the licenses that have been requested for earlier SPA versions. The license includes three activations, for example, for DEV, UAT, and PROD. Each license is issued for one year. If you have to change the default dashboards, your Account manager may also request the Tableau Desktop license from Workfusion. BI Scheme View product keys and license details The current number of licensed seats and product key information for WorkFusion Analytics Server administrators can be viewed on the server web interface. Single Site License To view product keys: on the Analytics server, on the main menu, click Settings, and then go to the Licenses tab. Licenses Multi Site License To view product keys: On the main menu, click All Sites > Manage All Sites. Manage all sites The Manage All Sites item is displayed only when you are signed in as a server administrator. On the main menu, click Settings, and then go to the Licenses tab. Licenses tab To view license details, on the main menu, click Users. License details The License Details page provides summarized information for all product keys installed on the server, including: Seat licenses total: the total number of seats – unique WorkFusion Analytics Server users – covered by all product keys Seat licenses in use: the total number of seats in use Seat licenses available: the total number of seats available Unlicensed users: the number of users on WorkFusion Analytics Server without a license assigned For each Product key you can see the following information: Product Key: a key to activate the license. Seats: a number of seats covered by the license. Maintenance Ends: the date when the Maintenance contract expires. Expires: the date when the license expires. Valid: the license status. Activate and deactivate products After WorkFusion Analytics Server or WorkFusion Analytics Desktop are installed, you must activate these products. To complete the activation process, receive the WorkFusion Analytics Server and Desktop product keys from the WorkFusion team. If WorkFusion Analytics Server and Desktop are to be upgraded, deactivated keys will be used to activate the new versions. There are two ways to activate or deactivate product keys depending whether the computer has internet access or not. Follow the official Tableau guide that best suits your scenario. :::note The account that you use to sign in to TSM must have administrative access to the local computer where Tableau Server is installed. ::: Activate: Tableau Desktop Tableau Server Deactivate: Tableau Desktop Tableau Server "},{"version":"10.1","date":"Jul-08-2019","title":"aggregate-and-view-logs","name":"Aggregate and view logs","fullPath":"iac/admin/maintenance/aggregate-and-view-logs","content":" This article describes basic concepts of log aggregation. Overview The following log level options are available: prod — logs contain only error messages and warnings dev — logs contain error messages, warnings, info and debug messages The default log level is prod, unless user specifies dev option before installation. User can also change log level for specific server or component, except for RPA and BI servers. Learn more in Change a component's log level. The solution for logs aggregation consists of the following components: Kibana with logs filtering dashboards Elasticsearch for storing logs data Logstash for aggregation and forwarding log records The log aggregation process is the following: An application writes logs to the filesystem. The Filebeat component reads the application log and forwards it to the log aggregation server. On the log aggregation server, Logstash receives log events from Filebeat and stores lines on the filesystem in a predefined structure. By default, WorkFusion SPA stores all component logs on the Integration server in the logs directory. All application logs remain both on source and Integration servers but with different retention policies. Logs are stored with the following convention: logs filename.log. Example: opt workfusion int int.workfusion.com nginx 2018 10 25 nginx.err.log. Element Description Example install dir A directory to place user files, logs, and configs opt workfusion server group An alias for group of components app, int, ocr, bi, rpa, bepmaster, bepagent hostname A real hostname of the server that sends logs int.workfusion.com component A component's name wfagent, workfusion, nginx date A date when a log event was created on the source server. Pattern: YYYY MM DD The date is created based on UTC time and thus may differ from the timezone configured on the server. 2018 11 30 filename A log file name metrics.log, status agent error.log View logs on UI To view logs on UI: Go to Kibana, for example, at https: app.example.com kibana. Alternatively, in the Control Tower dropdown menu, select Platform monitor. Platform Monitor Authenticate to Kibana using the ELK admin credentials, which you specify during the installation in the config.yml file. ELK elkadminpass: 'sc@R@ S 9V' password for elasticsearch user 'admin' used as login for Kibana UI In Kibana, go to the Dashboard tab to open the preconfigured dashboards, including the designated one for logs. Dashboards With this dashboard you can search and browse logs from all the servers and components. To find logs from a particular component, in the **Log source selector** section, select one or several following parameters: Hostname – hostname of a server where the component is running. Service name – the name of a service. Log name – the name of a log to find. Click Apply changes. Dashboards You can also refine the results by using the advanced filter with Kibana queries syntax (https: www.elastic.co guide en beats packetbeat current kibana queries filters.html) in the Search field. :::tip For the services' description, see the General services Information section. "},{"version":"10.1","date":"Jun-26-2019","title":"backup-and-restore-data","name":"Backup and restore data","fullPath":"iac/admin/maintenance/backup-and-restore-data","content":" Currently, if DNS names are changed, the following backup and restoration process cannot be used as a part of migration process, due to the fact that the VAULT DB component will contain records with old DNS names from the previous installation. The described backup script can backup and restore only SPA data components. General Information The Ansible installation features the script that implements the following cold backup solution: Backup Backup script Syntax wf backup.sh { OPERATION } { COMPONENT } { ROLE } Usage: . wf backup.sh {prepare,backup,restore} {all,minio,vault,nexus,distr} {int} EXTRA_VARS... Examples: . wf backup.sh prepare all int . wf backup.sh backup all int . wf backup.sh restore all int Available procedures Backup script supports the following operations: Prepare (optional) Hot sync data. It is part of the Backup operation, but can be launched separately before the main backup process to achieve minimal downtime. Backup All SPA services are turned off, components are archived. Once completed, the services are restarted. Restore The script searches for backup archives in the backup directory and performs the restoration process. Components for backup The script supports backup and restoration of the following components: minio vault nexus distr Script variables cat config.yml install_dir: opt workfusion backupdirectory: '{{ installdir }} backup' Backup data To backup data: On the INT Server, run the script with the prepare parameter (hot sync step): . wf backup.sh prepare all int The script copies (hot sync) all INT components to {{ backup_directory }}. Result ls 1 opt workfusion backup minio nexus vault distr On the APP Server, manually terminate all APP services: wfmanager stop all Run the main backup operation. . wf backup.sh backup all int The script checks if there are no external connections to WF databases, shuts WF services down, then performs cold sync of database components files. After that, the files are archived. As result in your {{ backup directory }}, you find the archives with backups of components' content: ls 1 opt workfusion backup ... minio 08 47 21 02 05 2019.tar.gz nexus 08 47 21 02 05 2019.tar.gz vault 08 47 21 02 05 2019.tar.gz distr 08 47 21 02 05 2019.tar.gz Move the archives to a backup storage. Restore data from archives On the INT Server, run installation as a root. . install.sh preinstall int su wfuser . install.sh install int && . install.sh check int Place the backup archives with the INT components to a backup directory. Before proceeding, make sure that all services on the APP Server are stopped. Run the restore command: . wf backup.sh restore all int Verify that the services have started successfully. wfmanager status After restoration, the old data is located in {{ backup directory }} state before _restore . "},{"version":"10.1","date":"Jul-08-2019","title":"change-a-component-s-log-level","name":"Change a component's log level","fullPath":"iac/admin/maintenance/change-a-component-s-log-level","content":" This article describes the steps to change a log level type between default prod (WARN ERROR) and dev (DEBUG INFO) for a specific server or a component. The log level can be updated for the following components: ocr sqc workspace workfusion Make sure to run the following steps as the runtime user, which is defined in config.yml, in the wf_user variable. To change a component's log level: Login via SSH to the server, where the required component is installed. For the list of components and their locations, see Product Architecture. Supported servers include: APP, INT, OCR. The log level type cannot be modified for the RPA and BI servers. Change a user to default wfuser. su wfuser Change the directory to . cd opt workfusion wf_installer If this directory is missing, the installer has been either deleted or unpacked to a different location. In this case unpack the installer to the right location once again. In , in config.yml, update the logleveltype variable with the prod or dev value. . install.sh edit_config int logleveltype: prod possible values 'prod' or 'dev' With the The logleveltype variable you can switch a log level for specific SPA components, prod is less verbose than dev. For more information, in group vars all main.yml, see the logconfig variable. There are two valid scenarios to follow: On the current server, update the log level for all components. In this case, specify a server role, for example, app, as the server role in the example below. For more information, see Server roles description. . install.sh updateloglevel app Update log level for a specific component. In this case, specify a component's name (workspace in the example below): . install.sh updateloglevel workspace "},{"version":"10.1","date":"Jun-26-2019","title":"search-a-specific-message-in-logs","name":"Search a specific message in logs","fullPath":"iac/admin/maintenance/search-a-specific-message-in-logs","content":" This article describes necessary steps to search and filter for specific messages in selected logs. To search a mesage in logs: Open the Kibana Dashboard page. Open Workfusion SPA Application Logs dashboard. app logs dashboard Select Hostname, Service name and Log name, if needed, and then click Apply changes. app dash settings In the Search field, enter a message to search, for example, message:\"automl gateway service\", and then click Refresh. search kibana For more details on search queries, see Kibana queries and filters (https: www.elastic.co guide en beats packetbeat current kibana queries filters.html). To change filtering time range, open Time range menu and choose the time interval for logs filter. time range "},{"version":"10.1","date":"Jun-21-2019","title":"investigate-performance-hot-spot","name":"Investigate performance hot spot in environment","fullPath":"iac/admin/monitoring/investigate-performance-hot-spot","content":" To find performance hot spots: In Kibana, click Dashboard > System Overview. System overview In the Top hosts by CPU group, click a host with the highest CPU load. Top hosts Find a process with the highest CPU load. Highest load To return to the first dashboard, click System Overview. Return "},{"version":"10.1","date":"Jul-08-2019","title":"monitoring-overview","name":"Monitoring overview","fullPath":"iac/admin/monitoring/monitoring-overview","content":" The monitoring solution consists of the following components: Kibana with system performance dashboards Elasticsearch for storing metrics data Logstash for aggregation and forwarding metrics data Open monitoring dashboards To browse monitoring dashboards on Kibana UI: Go to Kibana, for example, at https: apm.example.com kibana. Log in to Kibana by using the predefined user admin, and the password, specified in config.yml during installation. Kibana On the main menu, click Dashboard to open the list of preconfigured dashboards. Kibana Browse dashboards System overview This dashboard contains summary of all configured hosts: Kibana Click a host to open page Host overview dashboard with detailed information on the respective instance. Host overview This dashboard contains detailed time series information of the host: Kibana Environment status This Dashboard displays services' status of SPA environment: Kibana The TCP monitors status and HTTP monitors status diagrams display the overall status: Kibana See diagrams Heartbeat Services, HTTP up status, and HTTP ping times for detailed information: Kibana Kibana "},{"version":"10.1","date":"Jun-21-2019","title":"сhange-elk-admin-password","name":"Сhange ELK admin password in Kibana","fullPath":"iac/admin/monitoring/сhange-elk-admin-password","content":" To change a password: Go to Kibana, for example, , and log into it with your current password. On the main menu, click Management. Management In the Security section, click Users. The Users window appears. Security In the Users window, select user admin. Admin Click Change password. Change password In the corresponding fields, enter your Current password, and New password with confirmation. Edit user Click Update user. "},{"version":"10.1","date":"Sep-17-2019","title":"overview","name":"Overview","fullPath":"iac/admin/install-spa/overview","content":" install install You can install SPA in two different ways: the simple one that is we have introduced in SPA version 10.0 and, which we indeed recommend to follow, and the former complex one. The difference between them is clearly seen on the diagrams. Single point Legacy way of installation Legacy Regardless of how you are going to install Intelligent Automation Cloud Enterprise, you have to prepare the environment. Preparation includes: Deployment of the required amount of servers, including MS SQL. Creating users on them. Creating certificates. Before proceeding, you have to decide whether you want to have Intelligent Automation Cloud Enterprise installed with the high availability support or in a common mode. The high availability features makes your installed system completely fault resilient and protected from the data loss in case of minor outages. Note that in this case, you will need several servers for each installed component and an installed load balancer. For more information, see Requirements for HA Installation. "},{"version":"10.1","date":"Sep-17-2019","title":"overview","name":"Prerequisites","fullPath":"iac/admin/install-spa/overview","content":" Linux servers Linux Servers: INT, APP, BEP Master, BEP Agent(s), OCR (if OCR is decided to be installed on Linux OS). Servers must be up and running and all system requirements are met as described in System Requirements. The SSH access is provided to the servers. Linux Installation user must exist on each server and is provided with the SSH access via the private key authentication and temporary sudo privileges ONLY for installation and ONLY for tools that are being used during installation: usr bin sh, usr bin su, usr bin mkdir, usr bin chmod, usr bin chown. After the successful installation the sudo privileges can be revoked. If a user doesn't exist, see Create installation users. DNS names for all servers are configured according to DNS Names Configuration. Application Ports are open between servers. The default Workfusion installation directory (for example, opt workfusion) is created on the Integration server. TLS certificates are placed to the certificates directory on the INT server (for example, opt workfusion wf_installer certificates). The time synchronization service must be set up on all servers. All servers must have the same time zone. Windows servers Windows Servers: OCR (Windows), RPA, BI. Servers are up and running and all system requirements are met as described in System Requirements. An installation user is created on each server and is provided with the administrator privileges. If a user doesn't exist, see Create installation users. DNS names for all servers are configured according to DNS Names Configuration. Application Ports are open between servers. The default Workfusion installation directory C: workfusion () is created on the server. The time synchronization service must be set up on all servers. All servers must have the same time zone. "},{"version":"10.1","date":"Jul-18-2019","title":"prepare-servers","name":"Prepare servers","fullPath":"iac/admin/install-spa/prepare-servers","content":" Prepare MS SQL server For Workfusion SPA installation you need to have an existing activated instance of the MS SQL server. The server must be up and running during the Workfusion SPA installation. To prepare a MS SQL server: Create the database, and specify the database name. For other options you may use the default values. Create DB Create the owner with the full access permissions for the database. This user must be later specified in config.yml. Note: for all logins, the following password policies are applied: Allowed special symbols for db password for monitoring tools: @ *():,.} Allowed special symbols for db password for the Analytics component: @ *():,.;} To create the owner: On a MS SQL server, create the mssql login, specify the login name, and then make sure that the Enforce password policy check box is not selected. In the Default database box, specify the database, and then choose English as Default language. In the Select a page group, click User mapping, and then select the check box to map the user to the database. Leave all other options unchanged. Create DB User In the database Users, select the db owner check box to assign to the db owner role. Assign to group Create the following MS SQL login names and passwords for the Workfusion components: mssql dba user mssql ct user mssql ws user mssql sqc user mssql ds user mssql rpa user mssql pm user mssql dm user mssql rapi user mssql ocr user See the password policy earlier. For the description of these parameters, see config.yml options. For each login, specify the following parameters: If you use the SQL Server authentication, Specify the login name. Make sure that the Enforce password policy check box is not selected. In the Default database box, specify the database. Choose English as Default language. SQL auth If you use Windows authentication, Select a corresponding login name. In the Default database box, specify the database. Choose English as Default language. For other options leave the default values. Win auth When configuring the Integration server, on the INT server, in the config.yml file, in the MSSQL section, remember to specify the corresponding login names and passwords, that you have created on the previous step. Prepare Integration server Extract installation packages To extract installation packages: Log in to the Integration server as a Linux installation user, by using Linux Terminal, Windows PuTTY, or any other SSH client: ssh i @ In , create a directory for the WorkFusion installation package, for example, opt workfusion wf _installer. Grant the read access for this directory to all users: sudo mkdir p opt workfusion wf_installer sudo chmod 0755 opt workfusion wf_installer Make sure that the Linux installation user is an owner of the installation directory: sudo chown R : opt workfusion Download the provided installation package with any file retrieving tool, for example, wget or curl: If the Internet access is available on the server: curl 'https: linktoinstaller' output opt workfusion workfusion full package .tar.gz If the Internet access is not available on the Integration server, download the SPA installer and copy it as opt workfusion workfusion full package .tar.gz on the Integration server. Extract the downloaded package to the directory: tar xzvf opt workfusion workfusion full package .tar.gz strip 1 C opt workfusion wf_installer Configure installation To configure installation: In your directory, edit the hosts.yml configuration file. The vi text editor is used in the example below: vi hosts.yml Refer to the hosts.yml options section for more details on each option. In the same directory, change the permissions for the config.yml file: sudo chmod 644 config.yml Open the config.yml file, and in the parameter ansiblevaultpassword, change the ansible vault password: vi config.yml The password is used for encrypting the config.yml at the following step and will be asked several times during the installation of the components. We strongly recommend to secure and memorize it. When creating passwords, make sure they are strong enough and meet the following requirements: Ansible pass Optionally, on the INT server, run the following command to automatically generate passwords for the WorkFusion internal services according to product policies. We strongly recommend to change them manually for security reasons. Note that the command doesn't generate end user password. . install.sh passwords generate As a result, in config.yml, the passwords for most services, except for the following end user ones, are created: \"wf_password\" \"ldapbindpassword\" \"mail_pass\" \"tableaudashboardpass\" \"tableauautomationpass\" \"windowsinstallationpass\" \"elkadminpass\" \"ansiblevaultpassword” \"botrelayuser_pass\" \"botuserpass\" If needed, change other secrets and parameters, as well. After that, save and close the file. Remember, in the MSSQL section, specify the corresponding MS SQL login names and passwords, that you have created earlier. In your , encrypt the config.yml file: . install.sh encrypt config Note that WorkFusion SPA 10 installer will not let you start the installation until config.yml is encrypted. If needed, you can later edit the encrypted file with the following commands: cd opt workfusion wf_installer . install.sh edit_config int Refer to the config.yml options section for more details on each configuration option. Prepare the TLS certificates, as described in Prepare TLS certificates for installation. Copy the provided license.properties file to the installation directory to install the WorkFusion license, for example, opt workfusion wf _installer license.properties. If you perform installation on your own, request the license file from your WorkFusion Account Manager in the same way, as you receive the Tableau license. Prepare Windows servers The following operations must be performed on all Windows servers. To configure access for SPA installation: Connect to the remote Windows server via Remote Desktop or any other client as a Windows installation user (see Create installation users). Run PowerShell as Administrator. In PowerShell, run the following commands: url = \"https: s3.amazonaws.com workfusion installer blobs artifacts winrm config ConfigureRemotingForAnsible.ps1\" file = \" env:c ConfigureRemotingForAnsible.ps1\" (New Object TypeName System.Net.WebClient).DownloadFile( url, file) powershell.exe ExecutionPolicy ByPass File file Verbose The script checks the current WinRM (PS Remoting), enables CredSSP authentication, creates SSL listener and configures firewall for WinRM HTTPS connections to allow Ansible to connect, authenticate, and execute PowerShell commands. If the Internet access is not available on the target server: Download the script from https: s3.amazonaws.com workfusion installer blobs artifacts winrm config ConfigureRemotingForAnsible.ps1 and save it to the target windows server manually, for example, to C: tmp ConfigureRemotingForAnsible.ps1. Run PowerShell as Administrator. In PowerShell, run the following command: powershell.exe ExecutionPolicy ByPass C: tmp ConfigureRemotingForAnsible.ps1 "},{"version":"10.1","date":"Aug-23-2019","title":"control-tower","name":"Control Tower","fullPath":"iac/core/control-tower","content":" Navigation Analytics Business processes Manual tasks Bot tasks Advanced System settings Introduction Control Tower is one of the main components of Smart Process Automation. Use Control Tower to: design, run and schedule Business Processes create Manual Tasks create and manage Users Starting Control Tower Before you start working with Control Tower, make sure there are no Windows updates pending, as some of them may interfere with Control Tower. Control Tower in SPA is started by infrastructure administrator. To launch Control Tower: Open the link provided by your administrator in browser. Log in using your credentials (username and password provided by your administrator): Control Tower Overview After launching Control Tower, you will see a dashboard with the following links: Account Information Business Processes Manual Tasks Datastores Secrets Vault (Secure Storage) Schedules Account Information Clicking your user name in the top right corner of your screen will bring up your account menu. Here you can: View and update your User Settings. Backup all your data Logout User Settings In the User settings tab, you can customize your user settings, i.e. your name, email, and timezone. Business Processes Overview To view your business processes, go to Business Processes → View All. It will open the list of your business processes: You can use a filter with multiple parameters to sort your business processes. Info RPA Express comes with a set of sample business processes in Control Tower: Account Payable – collects invoices as images from a web application into an S3 bucket, processes them using OCR and extracts the required information in a Manual task. License Verification – finds the licenses online by their numbers and retrieves required information about them. Check criminal records – finds the criminal records using personal information and saves the results in HTML. Business Processes Details A business process can have the following statuses: Draft (hasn’t been run yet) Running (in the process of execution) Completed (executed successfully) Errors (started but wasn’t completed due to errors) When expanding the business process list, you can see the details about each instance: when it was executed how it was launched (manually or via scheduler) the number of processed records in the business process executed tasks in the business process (bot and manual), their status and the number of processed records in each task. Clinking on the BP instance will open the workflow (for drafts) or the results tab (completed processes). Main Operations with business process You can perform the following operation with a Business Process: Link to Workflow – view the workflow defined for the Business Process as a diagram, including all tasks (Manual and Bot) and rules for transitions between tasks. View events log – opens the events log of the Business Process with an option to export it to Excel. Open process definition in a new tab – opens the Business Process with all its instances in a separate tab. Actions – opens the list of actions you can perform with the Business Process. Note The items displayed in the menu depend on the status of the Business Process. 1. Copy – create a copy of the Business Process as an instance linked to the source Business Process, i.e. if the source Business Process structure is modified, all referenced Business Processes will inherit the changes respectively, but each copy can be run separately without affecting the others. Note You can copy a Business Process, if you need to re run a completed Business Process. The Business Process copy function has the following options: Include the input data – the input data from the source Business Process will be included to the new one, alternatively you can provide new input data for the copied Business Process. Create an independent process definition – a new Business Process is created without a link to the source Business Process. View data – view the input data provided in the Business Process; Pause – pause execution of the Business Process with the possibility to resume it later (for running Business Processes only); Starting from Version 10.0 with BEP orchestration: In local mode Pause operation works in the same way as Stop: when BP is paused, next steps are not executed. It can then be resumed. In cluster mode Pause continues to execute stateless steps until a non stateful step. Learn more about Stateless Execution of Bot Tasks. Resume – resume execution, if the Business Process has been paused (for paused Business Processes only); Stop – complete execution of the Business Process at the current stage (for running Business Processes only); Download Original Data – download the input data as a .csv file. Delete – remove the Business Process completely from Control Tower. "},{"version":"10.1","date":"Aug-21-2019","title":"business-process-modelling","name":"Business Process Modelling","fullPath":"iac/core/delivery-manager/business-process-modelling","content":" Introduction One of the most powerful weapons is the process map. As professionals, we are tasked with creating efficient and useful tools for our organizations, often with little understanding of the underlying business process. Often we are collaborating with customers who don’t understand their own business processes, much less the upstream and downstream processes. A process map can go a long way to building that understanding while highlighting problems, miscommunications, gaps, redundancies, workarounds, rework loops and waste. In short, process maps show us “what” we do, “how” we do and “where” we do it. And above every process map should hang a huge sign that reads: “Why ” Understanding how to create and analyze business processes is a skill that can benefit anyone, in any role, in any industry. Business Process Defined “Processes are how people within an organization collaborate in order to accomplish a goal. Essentially everything we do in an organization involves or contributes to some type of process.“ A business process has typical characteristics: It exists to meet a specific business need Collaboration between multiple people or groups Takes place over a period of time Often has more than one iteration ¾ is repeatable. So what is a process map Process mapping consists of a collection of tools and methods used to understand an organization and its processes. Those tools allow us to document, analyze, improve, streamline and redesign business processes to realize organizational efficiencies. A process map is a visual aid for picturing work processes and shows how inputs and tasks are linked and highlights the steps required to consistently produce a desired output. A process map encourages new thinking about how work is done, where it is done, who performs it, what problems frequently occur and how best to solve them. A common analogy relates to road maps: you can’t plot a route to get to where you want to go…unless you know where you are. Process models and maps can also be used to identify appropriate Process Modelling & Mapping: The Basics quality improvement team members, identify who provides inputs or resources to whom, establish important areas for monitoring (critical control points) or data collection and to identify areas for improvement. Flowcharts can help us understand the flow of a wide variety of things: information, documentation, forms, patients, products, supplies, customers or employees. Benefits of using Process Maps Benefits of process maps: Enables everyone to visualize (see) the process in the same way Acts as a training and educational tool for new and existing staff and help reduce procedural errors Focuses stakeholders on the process itself Builds understanding between cross functional work areas Provides a “current state” upon which to base future improvements Identifies objective measurements and metrics for ongoing evaluation and future improvement activities Identifies existing workarounds, rework loops and information gaps Illustrates opportunities for improvement Improves compliance with, or provide documentation for, quality and regulatory standards (SOX, C SOX, CCHSA, JCAHO, OH&S, etc) Business Process Modelling Tools Process Modelling levels Level one: this very high level map outlines the operational levels of an organization and are rarely, if ever, actually drawn. Examples include: customer processes, administrative processes. Level two: shows end to end processes across the above operational areas. For example a level two process for purchasing capital equipment would cross several operational areas: the requesting department, purchasing, accounts payable, asset management, receiving and biomedical maintenance. Also called top down or high level process maps. They are quick and easy to draw, but may not provide the detail necessary to build understanding or realize improvements. Level three: shows the roles, inputs, outputs and steps required to complete a specific process within an operational area. For example, the purchasing process (request, sourcing, cut PO) might be depicted as a process map. Also called cross functional or deployment diagrams. Usually contain enough information for improvement efforts, but often miss inefficient details and don’t function well for training or as operational documentation. Level four: is the documentation of systems, instructions and procedures required to complete steps in the level three processes and shows inputs, outputs, associated steps and decision points. For example, specific steps necessary to cut a PO in the enterprise application would require a level four process map. The procedures and system instructions can be represented as text, an algorithm or detailed process map. Because of the level of detail, they can be resource intensive to create, but offer the greatest improvement potential. Since they illustrate decisions and subsequent actions, they are excellent training and reference materials. But if your organization is new to process mapping, use these sparingly. The time and effort may turn stakeholders off before they’ve had a chance to experience the benefits of the work. Process Maps Choose the type of process map appropriate for your specific goal. What follows is a description of different types of process maps, when to use each, and a brief outline of how to create them. What is common to all of them is the “process oriented approach”. This approach: Defines the process first Specifies the customers of the process Defines and refines the requirements for all process customers Specifies the steps involved in getting something accomplished Indicates the sequence of steps Top Down Process Maps Top Down or High level process maps (Parking Lot diagrams to some) are good for illustrating the major clusters of activity in a process; those key steps essential to the process while limiting the amount of detail documented. The top down flowchart starts with the major steps drawn horizontally. The detail is provided in numbered sub tasks under each major task. The top down flowchart does not show decision points or rework loops and does not show the same detail as a cross functional chart or detailed flowchart. How to Draw Top Down Diagrams (Level 2) List the most basic steps in the process. Limit yourself to no more than 5 or 6 basic steps and place them in order horizontally across a whiteboard, flip chart or large piece of paper Under each step, list the sub steps that make up that element, list them in the order they occur and again, limit yourself to 5 or 6 sub steps Cross Functional or Deployment Diagrams Use cross functional flowcharts to show the relationship between a business process and the functional units (such as departments) responsible for that process. Bands represent the functional units. Shapes representing steps in the process are placed in bands that correspond to the functional units responsible for those steps. You can represent a process either vertically or horizontally. A vertical layout, where bands representing the functional units run vertically from the top to the bottom of the page, places slightly more emphasis on the functional units. In a horizontal layout bands representing functional units run horizontally across the drawing page, highlighting the process. These charts emphasize where the people or groups fit into the process sequence, and how they relate to one another throughout the process. Cross functional or deployment charts are excellent tools for illustrating how a process flows across organizational boundaries. They can be very effective at identifying delays, redundancy, excessive inspection, rework and potential points of process failure. How to Draw a Cross Functional Diagram (Level 3) Gather together a team representing the different functional areas Place a large piece of paper on a wall or flat surface Identify your process stakeholders players (people or functional areas) Down the left hand side of the paper starting with the process customer at the top, list the process players in order based on the closeness of their relationship to the process customer. Draw horizontal lines between each process stakeholders, using a double line if they are external to your organization (customer, supplier, regulatory body). These are referred to as swim lanes and may also be used to represent different roles in a process or a key piece of software. The bottom axis is time and moves from left to right Write out the process steps in order and on the line of the person functional area that performs that task, moving from left to right as time elapses As you draw boxes, connect them with lines and arrowheads showing the direction of input. Concurrent activities should be aligned vertically and shared activities (between people or functional areas) should be drawn on the swim lane (when possible). Alternatively you can also write the process steps on 'sticky” notes and place them on the map. You can then move them around until the team members are satisfied that the steps are properly identified and in the correct order. Finish by adding labels, decisions, and arrows (showing direction of input) to the map. Detailed Flowcharts Flowcharts are maps or graphical representations of a process. Steps in a process are typically described within a specific set of symbols, which communicate ‘what’ or ‘where’ something is happening. There are literally hundreds of different symbols to choose from, but most experienced process analysts will recommend that you choose a handful of basic symbols and stick with them. Accurate flowcharts can be created using very few symbols (e.g. oval, rectangle, diamond, delay) and the International Standards Organizations standard 9004.4 recommends just four. A chart of commonly used symbols is shown on the right. In quality improvement work, flowcharts are particularly useful for establishing a common understanding of a process, or allowing people to “see” the process in the same way. Displaying the “current state” of a process helps identify illogical flows, potential miscommunications, redundancy, rework, delays, dead ends, and missing critical control points that might otherwise go unnoticed. Another methodology, from the Ben Graham Group (www.worksimp.com) is a very effective method of collecting data and drawing detailed process charts. Ensure that you are working with the people actually doing the work. Managers will often want to be involved in process work, and their support is critical. But unless they do the tasks on a day to day basis, they can often be more of a hindrance than help. Ensure team members feel safe in sharing and understand that this isn’t a fault finding mission. Their description of the process as it is, is critical, good, bad or otherwise Decision symbols are appropriate when those working in the process make a decision that will affect how the process will proceed. For example, when the outcome of the decision or question is YES, the person would follow one set of steps, and if the outcome is NO, the person would do another set of steps. Be sure the text in the decision symbol would generate a YES or NO response, so that the flow of the diagram is logical In deciding how much detail to put in the flowchart (i.e., how much to break down each general step), remember the purpose of the flowchart. For example, a flowchart to better understand the problem of long waiting times would need to break down in detail only those steps that could have an effect on waiting times. Steps that do not affect waiting times can be left without much detail. Detailed Flowcharts How to draw a detailed process flow chart (Level 4) Describe the process to be charted and define the process boundaries. (scope) Always start with the process trigger (business need that drives the process) You may want to start by determining major and minor inputs into the process with a cause & effect (Ishikawa) diagram or using the six sigma SIPOC tool Complete the big picture before filling in the details, often by sketching a cross functional diagram first Keep the descriptions concise. If necessary, cross reference other maps or documentation If you are the facilitator, make generous use of the Socratic dialogue. Ask lots of questions. Ask lots of ‘why’ questions. It is best not to facilitate mapping your own process. If the participants are able to describe the process in a way that makes sense to someone who knows nothing about it...then you are probably capturing enough detail Note down each successive action taken. Actions should be clearly described in as few words as possible Pay attention to the questions. These are often critical control points: places in the process where multiple alternative flows appear, based on questions, inspections etc. When you are charting a branch, always follow the most important alternative to flow chart first. Then go back and complete the remaining flows Validate the process chart with others involved in the process. 9. Identify responsibility for each step Process Analysis Processes are rarely designed intentionally and they all evolve over time. Throughout this evolution, inefficiency, redundancy and waste creep in. It is critical to avoid finding fault with a specific process: recognize that people and departments make changes in an effort to get the job done, often without knowledge of how those changes might impact upstream and downstream processes. The performance of individuals is only as good as the process will allow it to be. Processes, especially cross functional business practices, are usually not documented, not standardized, not measured, not systematically and continually improved and not managed by the micro process doer or owner. Analyzing Process Maps Processes evolve (or devolve) over time as staff or business conditions change and in response to other changes in the organization. This evolution often results in growing process complexity. The trouble spots in a process usually begin to appear as a team constructs a detailed flowchart. Begin by challenging the necessity of each process step. Is this step redundant Does it add value to the product or service Is it problematic Could errors be prevented in this activity How Does this process step physically change the product Is it done right the first time Is it another inspection, control point or approval Is it required to meet a regulation or legislation If you stopped doing this step, would your customer even notice Asking these questions will help determine if the process step adds value for your customer or your organization. Combine, simplify or eliminate activities that do not contribute value. Watch for over inspection or multiple approvals in your process as they may suggest a lack of confidence in the process. Eliminate control steps that are not critical for quality outcomes Identify time lags or delays in the process. Look for ways to eliminate those delays Watch for multiple hand offs and specialized work areas. Who is involved What could go wrong Is the output of one area meeting the input needs of the next Could 30 seconds of effort at point A save 3 minutes of effort at point B or service meeting the needs of the next person in the process Each time a process crosses organizational boundaries, there is increased potential for delay or miscommunication. Consider opportunities to increase the scope of functional areas to allow for more work to be done in a specific area. Some efficiency proponents argue that reducing specialization of tasks can lead to decreased turnaround times, fewer errors and more efficient processes of work tasks. Cellular work areas are taking the place of assembly line processes, both in operational and administrative work environments. Communication is improved, silos are taken down and the workforce is better trained and more flexible to upcoming changes Examine decision points. Does it evaluate an activity to see if everything is going well Is it effective Is it redundant Is it designed to catch an error that could be prevented (or caught) earlier in the process Look carefully at rework loops and quality control points. Does this rework loop prevent the problem from recurring How is feedback from this loop returned to the area of origin Could the problem be prevented or caught earlier in the process Are repairs being made long after the step where the errors originally occurred Are their methods for preventing the problem that could be employed Look hard at each document symbol and ask if this document or data entry is necessary Where does the form or data go afterward Is it up to date Is it redundant Is there another way of capturing this data Is there a single source for the information Could this information be used for monitoring and improving the process Evaluate each Delay symbol. Is the delay necessary For what Some delays are valid, i.e. “delay while resin cures,” or “wait for Lab results” but many are not. Ask, “How long is the wait” Can it be eliminated or reduced Examine the overall process: Is the flow logical Are the steps in the right order Could some work be done in parallel to reduce overall processing time > Are there places where the process leads off to nowhere Is there repetition Other Questions for Analyzing Process Maps Challenge your map and process by asking the following questions: Is this map comprehensive Are there other key outputs What activities lead to this output What are the major inputs into these activities (Think of inputs as both physical entities and information.) How is information tracked Written specs or paper invoices What percentage of this information is automated What technology or application is used to convert a particular input into an output What equipment or job aids are used What are the major decisions made with the process Where and when are these decisions made By whom Are approval signatures required When How long does this step take Why does it take this long Is there a range Why is there a range What is the cost of performing these activities Can you give me an estimate What are the problems you encounter in performing this step What causes If everyone is thinking alike, then somebody isn't thinking. George S. Patton Page 11 Process Modelling & Mapping: The Basics these problems What are the roadblocks in this process What are the strengths of this process How do we know when this process is successful Is this process successful as it currently exists How do we know Conclusion There are many benefits of using process maps, and many different ways to draw them. Because people have limited understanding of an organizations processes and usually visualize them differently, a process map can build understanding and often create the ‘aha’ moment necessary to drive action, improvement or change. From a hastily sketched top down diagram on a flip chart to a complex detailed computer drawn deployment chart, visual representations of “how” we do things helps to crate understanding of “where we are” and sometimes “how we got here”. But you can’t set a new course or plan a new route if you don’t know where you are to begin with. Process Modelling & Mapping: The Basics by Kelly Halseth Regional Coordinator, Forms Management & Production and* David Thompson Health Region* "},{"version":"10.1","date":"Aug-06-2019","title":"project-tracking","name":"Project Tracking","fullPath":"iac/core/delivery-manager/project-tracking","content":" Do not read this section simply as hints for some helpful tools; project tracking is a must regardless of in which format you end up doing it. Why is it necessary You will not be able to structure and track all the events, plans and dependencies solely in your mind unless you possess super powers; thinking that you possess them will not help, either For each decision, issue, delay there must be a written proof that was shared with all the parties (dev team, customer, partner) involved at the time when event occurred Each project should produce a set of materials that would allow to make future use of its experience and outcomes The below paragraphs describe project tracking components based on Project Tracking Template. Project Plan is a visual representation of each project task timeline with their dependencies (like Gantt chart). It should be prepared before the project start and briefly presented on the kickoff. The plan should be prepared by PM or DM with the help of the development team; all the timeline estimates should come from or be agreed by the developers who are going to implement the project. Apart from day to day tasks, it is useful to integrate holiday team vacations schedule here as it is going to affect timelines and should be clarified in advance. Milestones Tracker (UC1 UC2 or \"Delivery Plan\" in the template) lists milestones and dates in an easy to read format. Day to day plan is useful for the implementation team but contains too many details for executive level stakeholders. List of milestones is useful for high level tracking and for presenting to wider circle of stakeholders. Weekly Monitor lists tasks for each week based on project plan and additional activities that arise. Weekly monitor is the most concise and useful way for DM to stay on track with the progress. With the help of a simple list on a weekly (for some projects daily) call you can check status of each item, plan activities for next week. Each week's activities include: (1) items based on project plan; (2) additional actions that came up in progress (e.g. \"organize call with IT team to resolve access problem\"; \"contact X in another department to check on potential use case\"); (3) items not completed last week and thus moving to next week. While high level milestones can be set far in the future so it's hard to understand if you are on track to reach them until it becomes too late, such short term aims allow to check the health of implementation and adjust the course regularly e.g. if at each status call you find some items not done as planned. If any of the planned items is not done, you need to clearly understand the reason and the impact. Risks & Issues Log lists all risks with their impact, mitigation, dates, owners. This list must be reviewed regularly (especially it's top priority items) and updated whenever something changes regarding its items. Every risk and issue should be tracked, none can be left without owner and mitigation. Additional Templates Summary another way to represent progress in concise and visual way, can be useful for high level stakeholders Releases can use to plan and track process releases, along with their major features Features may be used to list and present features that are planned developed. It is not necessary to use each and every of the trackers proposed here. All in all, as a DM you need to have a detailed plan and track risks and issues. In other respects just choose what formats fit best your purpose and your audience, balancing their visual appeal with their ease to use; after all you want not only to accurately reflect all that is happening on the project but also not turn it into a tedious task that occupies hours of work. RAG Status RAG is a popular method of rating issues or status based on Red, Amber (yellow) or Green colours. In the above examples of documentation you can find it used both for issues rating and for reporting status in the tracker. GREEN status is applicable when: the project is on track and on budget quality is at expected levels and success criteria are likely to be met stakeholders and teams are generally satisfied no issues minor issues AMBER status is applicable when: overspend and or delays are taking place problems with quality dissatisfaction of customer stakeholders other risks and issues where mitigation an plan to resolve are identified RED status is applicable when one or more of the following is taking place: significant time slippage customer unhappiness, escalation acceptance success criteria not being met legal risk or inpact financial impact blocker (technical, organization, any other) Oftentimes putting a red status means the situation is going beyond your control and you are asking for help from senior management. RAG status is provided along with other key information, including reasons for this status and, most importantly, mitigation plan (aka Path to Green) in case of red or amber. Also status report includes risks and issues, progress from the previous status and activities planned for the next time period. Here is a sample template of status report: Status report Status Update: RED AMBER GREEN If Red or Amber, why are we in Red or Amber status And what is the Path to Green Risks Issues: Are there any risks or issues that have a probability of arising that will impact the budget, scope, quality, resources or timelines of the project If so, summarize the risk issues, impact to the project customer and mitigation strategy and action plan. Progress this week sprint month: Summarize progress made since the last update. Next week's plan: Summarize the key action items for the next period (week sprint month) Cc: Executive Sponsor, Account Manager, Project Team Project Plan Milestones Tracker Weekly Monitor Risks & Issues Log Additional Templates RAG Status "},{"version":"10.1","date":"Aug-06-2019","title":"improve-ocr-results","name":"Improve OCR Results","fullPath":"iac/core/ocr/improve-ocr-results","content":" What are quality requirements of an input document Scanned images may require some pre processing prior to recognition, for example, if scanned documents contain background noise, skewed text, inverted colors, black margins, wrong orientation or resolution. Resolution r ecommendation for source image: 300 dpi for typical texts (10pt or larger) and 400 600 dpi for texts in smaller fonts (9pt or smaller). Brightness: This image is suitable for recognition. Lower the brightness to make the image darker. Increase the brightness to make the image brighter. Why PDF searchable document is preferable for recognition than high dpi image created from it OCR analyses internal information within the source PDF files such as: annotations, metadata, text objects, font dictionaries, content stream. OCR enchanges PDF conversion performance and speed by efficient and accurate text selection. If text is embedded into the PDF ifle, the OCR engine examines the integrity of the text layer, and makes a decision as to whether or not to extract the text or apply OCR on a block by block bases Why we should cache OCR results Abbyy OCR is part of WorkFusion Product. The way WF partners with Abbyy requires licensing for every commercial user whom we re sell OCR capability. License cost is based on pages images processed. This is why it is important to avoid re processing of exactly same files. WorkFusion core feature is exposure business Processes as use Cases allows us to create and distribute OCR sub process, which contains solution for caching. Approach of caching is: For the file which we need to OCR calculate unique hash based on binary of this file Query DataStore is we already have this hash If NO execute OCR Write to DataStore new hash with results of OCR If YES just get OCR results from DataStore How to improve OCR results Preprocessing use ImageMagic or GhostScript convert colorspace rgb density 300 input.pdf monochrome output.tif Use PDF searchable document instead of high dpi image created from it. Use custom dictionary. Use removeGarbageSize parameter for scanned images to remove garbage (excess dots that are smaller than a certain size) from the image Use allowedRegionTypes parameter to exclude incorrect region types. Use xml:writeRecognitionVariants to understand the recognition problems Example 1 Step 1 (default properties) file exportFormat = html, xml Output results: HTML XML You can see that by default OCR recognize this text with garbage like an image (see html output img tag). Xml format more informative. We can see recognized block types: Picture and Separators Step 2.A (default removeGarbageSize properties) We can try to remove garbage and will see if result changed removeGarbageSize=1 Output results: Federal Credit Union, July 25, 2005 HTML XML Like we can see block type was changed from Picture to Text and image recognized successfully. Step 2.B (default allowedRegionTypes properties) Usage of allowedRegionTypes Please, be careful using allowedRegionTypes properties as it may break original document layout. We can choose second way to fix previous problem We will exclude Picture block type from allowed regions allowedRegionTypes=BT Table,BT Text,BT Barcode,BT Separator,BT SeparatorGroup,BT Checkmark,BT _CheckmarkGroup HTML XML Example 2 Step 1 (default properties) file = Output results: Facility Namer Nortfawoods Home Health and Hospice Cit> : Lancaster State r NH HTML Step 2 (default properties image preprocessing) We will improve vertical and horizontal density of the image (dpi) ImageMagic convertion tool command: convert colorspace rgb density 250 example2.png example2 250.tif file = example2 250.tif Output results: Facility Name: Northwoods Home Health and Hospice City: Lancaster State :NH Example 3 Step 1 (default properties) file = Output results: Step 2 (default properties customRegions) Single table was devided on 2 tables. To fix this issue we can try to use customRegions = { \"type\": \"BT _Table \" , \"left\":0, \"top\":0, \"right\":10000, \"bottom\":10000} select full page like a table On this page: What are quality requirements of an input document Why PDF searchable document is preferable for recognition than high dpi image created from it Why we should cache OCR results How to improve OCR results Example 1 Step 1 (default properties)) Step 2.A (default removeGarbageSize properties)) Step 2.B (default allowedRegionTypes properties)) Example 2 Step 1 (default properties).1) Step 2 (default properties image preprocessing)) Example 3 Step 1 (default properties).2) Step 2 (default properties customRegions)) See Also "},{"version":"10.1","date":"Aug-06-2019","title":"role-management","name":"Role Management","fullPath":"iac/core/security/role-management","content":" Add Edit a Role Permissions Description and Default Role Setup Permission Restrictions Main menu items are not displayed Action buttons links are not displayed 'Access Restricted' message when trying to open a direct URL Known Issues Versioning Info Available in SPA 10.0. To learn about previous version, see 9.2 Role Management. The Role Management lists all the current roles in the system and provides access to all Role Management Functions. It is accessed via the Configuration Menu: System Settings > Role Management. On this page, you can perform the following actions: create a new role edit an existing role delete an existing role Add Edit a Role Click Create Role (or click the appropriate link in the \"Name\" column to edit). The following screen is displayed: Enter a unique Name for the role. Provide role Description. Tick checkboxes with appropriate Permissions. Click the Save button. As a result, the created role appears in the Role Management table and can be granted to WorkFusion users on Configuration > User Management page. Default role setup Permissions Roles Administrator Developer Operator View Analytics ✓ ✓ ✓ View Tasks ✓ ✓ Task Designer ✓ ✓ View Results ✓ ✓ ✓ Task Business Process Actions ✓ ✓ ✓ View Business Processes ✓ ✓ ✓ Schedules ✓ ✓ ✓ Manage Templates ✓ ✓ Manage Bot Configurations ✓ ✓ Manage Rules ✓ ✓ Manage Data Stores ✓ ✓ View Data Stores ✓ ✓ ✓ Manage Workers ✓ ✓ ✓ Manage Qualifications ✓ ✓ ✓ System Preferences ✓ ✓ Manage Data Purge ✓ ✓ Manage Data Sources ✓ ✓ ✓ Manage Answer Types ✓ ✓ Manage Use Cases ✓ ✓ Manage Users and Groups ✓ Activity Log ✓ View Secrets Vault Aliases ✓ ✓ ✓ Manage Secrets Vault Aliases ✓ View Secrets Vault Entries Manage E mails ✓ Import Export ✓ ✓ ✓ Advanced Package Import ✓ ✓ Process Deep Copy ✓ ✓ Bot Task Monitoring Access ✓ ✓ Manage Platform Monitor ✓ ✓ View Platform Monitor ✓ ✓ Manage AutoML Settings ✓ ✓ Permissions Description To be added Permission Restrictions When a user has a Role without appropriate Permissions, the following restrictions can be applied in the user interface: Main menu items are not displayed Restricted Permissions All Permissions Action buttons links are not displayed Restricted Permissions All Permissions 'Access Restricted' message when trying to open a direct URL Known Issues Impossible to delete a role assigned only to group. No messages appear after Delete Confirmation operation. "},{"version":"10.1","date":"Aug-06-2019","title":"secrets-vault","name":"Secrets Vault","fullPath":"iac/core/security/secrets-vault","content":" WorkFusion platform provides Secrets Vault (formerly Secure Storage) functionality intended for storing sensitive data (i.e., login, password). To start managing Secrets Vault, go to System Settings > Secrets Vault. Managing Secure Storage from Bot Configs You can read, write, and delete records from Secrets Vault by using Secrets Vault Plugins (Secure Store Plugins). Documentation on Secrets Vault Plugins is available in the Help section in RPA Express. To access it, in RPA Express menu, go to Help > Help Contents > Bot Task Plugins > Secrets Vault Plugins (Secure Store Plugins). Filtering Starting with SPA 9.2, standard filter feature was introduced for Secrets Vault which allows the following: filtering the required aliases as per your rules limiting visibility of the aliases to other users See Filters for Collaborative Work for more information. Assigning Permissions From SPA 9.2, it is possible to assign granular permissions to users in Secrets Vault. Refer to the 9.2 Role Management page. WorkFusion platform provides Secrets Vault (formerly Secure Storage) functionality intended for storing sensitive data in (login, password, alias) format. Managing Secure Storage from Bot Configs Filtering Assigning Permissions Adding Secure Entry Adding Bulk Entry Viewing Secure Keys and Values Updating Existing Secure Entry Deleting Secure Entries Known Issues Adding Secure Entry To add a new Secret Entry, do as follows. Click the Add button above the grid. The following popup appears. Enter Alias, Key and Value for this Secret Entry. Alias serves to distinguish secure entries and must be unique. Click Save. As a result, a new entry appears in the grid. Adding Bulk Entry You can also create Secure Entries from file (for example, if you want to create multiple entries). Create a CSV file with 3 columns: alias, key, value. See the example below. Click the Upload Data button. The following popup appears: Click Add and browse the CSV file on your local machine. (Optional) Click the Options link and set File Separator and or File Encoding, if needed. Tip We recommend using Comma separator and UTF 8 encoding. Viewing Secure Keys and Values All existing Secure Entries are displayed in the grid with hidden Key and Value column items ( *). To view Key or Value, do as follows. Hover over appropriate grid cell. The View Update button appears. Click the View Update button. Enter Password popup appears. Enter the master password and click the Ok button. As a result, the appropriate key or value is displayed in the grid without asterisks. You can hide key or value by hovering over a cell and clicking the* Hide key* or Hide value button. All Keys and Values are automatically hidden when user leaves the page or reloads it. Updating Existing Secure Entry To edit a Secure Entry, proceed as follows. Hover over appropriate grid cell in the Alias column. The Update button appears. Manage Secrets Vault Aliases permission provided View Secrets Vault Entries permission provided Click the Update button. The following popup appears. Manage Secrets Vault Aliases permission provided View Secrets Vault Entries permission provided Enter New Key and New Value. Alias cannot be modified. Click the Update button. Deleting Secure Entries Tick check boxes opposite appropriate grid entries. Click the Delete button. Known Issues Known Issue Workaround The following combination is not saved in Secrets Vault: ! ! Do not use the combination When importing list of Secret Entries ( ) symbol is not recognized Do not use the combination "},{"version":"10.1","date":"Aug-06-2019","title":"workfusion-repository","name":"WorkFusion Repository","fullPath":"iac/core/control-tower/bot-tasks/workfusion-repository","content":" Versioning Info This article refers to SPA Version 8.4 and newer. For information on Version 8.3, see 8.3 WorkFusion Repository (https: kb.workfusion.com display WF %5B8.3%5D WorkFusion Repository). Introduction WorkFusion Repository is a feature that allows developers to package Bot tasks in WorkFusion Studio and publish them to a repository in the WorkFusion instance for inclusion in Business Processes. These artifacts contain everything that's needed to run a Bot task, including Java and Groovy classes, input data, Data Stores, and any required settings. Repository Usage Demo Deployment Options This article describes how to automatically build and deploy Bot Config Bundle (BCB) project from local WorkFusion Studio to remote server based Control Tower. Configure Maven and Kickstart Archetype Maven is used to build projects for publishing, so first you need to set some preferences for Maven archetypes and provide their URLs. Archetypes define a special WorkFusion project structure which can be compiled into a java archive, uploaded to a Nexus repository and automatically published into WorkFusion instance, ready for integration testing and production usage. Open WorkFusion Studio. Go to Window > Preferences → Maven → Archetypes: Add two Remote Catalogs by clicking the Add Remote Catalog button: {width=\"600\" height=\"536\"} Use following Remote Archetype Catalog settings for training: Catalog File: Description: Workfusion Archetypes On premises installation configuration If there is no Internet access, edit Maven settings.xml file. It is located in the %user home% .m2 folder (for example, if your username is User, the path will be: C: Users User .m2) and should look as follows: settings.xml local nexus Local Nexus http: {host}:{port} nexus service local repositories {repository} content central where {repository} stands for the repository name with Maven dependencies required for goals execution. To resolve Bot Config Bundle (BCB) dependencies during training, you can use Nexus from WATT training instance. To point your Maven to it, edit the settings.xml file. It is located in the %user home% .m2 folder (for example, if your username is User, the path will be: C: Users User .m2) and should look as follows: mcb repo bcbdeploy Workfusion!5 wf repo mcb repo https: watt db1.workfusion.com nexus content repositories local mcb repo wf repo Click the Verify button to test connection and the repository contents Finally, click Apply and OK See on premises detailed configuration... Maven Repository Configuration To proceed, you should add Maven dependencies. It can be done in 2 ways: Using the settings.xml file: settings.xml Expand source ... profile id repo id https: {host}:{port} nexus content repositories repo profile id ... or, alternatively, Using pom.xml: pom.xml Expand source ... repo id https: {host}:{port} nexus content repositories repo ... Usage Example Let's say, we configure for the following repository: https: watt db1.workfusion.com nexus service local repositories wf machine config bundle content So configuration for this repository will have the following outlook: pom.xml ... watt https: watt db1.workfusion.com nexus service local repositories wf machine config bundle content ... Maven Repository User In case your Nexus repository requires authentication, you have to configure your Nexus credentials for Maven. By default Maven will look for settings in the following file: C: Users YOURWINDOWSUSER .m2 settings.xml For example, if your username is User, the path will be: C: Users User .m2 settings.xml You can create it using following content: Expand source repo id repo user Password99 Server id should match repository id defined in settings.xml or pom.xml If you get the \"Authentication ERROR\" exception during deployment, the possible reason is: the updated settings.xml file was not linked to WF Studio. You can manually link it the following way: Select Update Settings. Select your settings.xml in Window > Preferences > Maven > User Settings Creating Bot Config Bundle Project Now you can create a new project with a special type WorkFusion Bot Config Bundle (BCB): Create a new project by pressing CTRL N. Select the Workfusion >WorkFusion Bot Task Bundle item and click Next. Select an Archetype: currently WATT runs SPA 9.2.0 so archetype version should be 9.2.0 The kickstart archetype contains a set of demo files and working config examples. It is perfect for understanding and testing the entire development workflow. The simple archetype contains only bare BCB project structure. Enter the Archetype parameters: Group Id Artifact Id Version (optional step) Modify the following project properties if needed: bundle name – name of the project wf version – version of your WorkFusion instance mcb repo url – Nexus repository URL for BCB deployment Click Finish. As a result, a new project will be created and displayed in the project explorer. Here is an overview of the BCB project structure: BCB project structure The src java folder contains Java and Groovy classes. Included and main Bot configs are in the resources folder. In a Bot config bundle, you can create your custom Java and Groovy classes and import them into Bot configs. By default, all results are written to the target output folder. The wfs data folder contains datastores and input csv files. The pom.xml file stores all information about the project and the URL to Nexus repository. Running and Developing Bot Configs The config development process is the same as standard WorkFusion Studio projects process. You can edit configs code, use include config, run, and debug configs. Refer to the Using WorkFusion Studio documentation. BCB project allows the user to: Create custom Java and Groovy classes in the src main java folder. For example, ExampleEntity.java, ExampleUtils.java Import these classes in the Bot configs (src main resources configs) in their `` sections. Use properties and methods from custom Java and Groovy classes inside `` sections. Expand source import com.workfusion.robotics.ExampleEntity; import com.workfusion.robotics.ExampleUtils; ExampleEntity entity = new ExampleEntity(); entity.setExampleStringProperty(some_variable.toString()); calcResult = ExampleUtils.multiply(2, 5); > When Bot configuration has been tested, you need to* run the project as a Maven build* and set the \"clean deploy\" command as a target. Maven creates a Bot Config Bundle (BCB) – a jar file in the \"target\" folder that contains main and included Bot configs together with Java and Groovy classes – and uploads this bundle to the Nexus repository. Deploying BCB to Nexus Repository Deploying BCB means pushing a Maven Artifact into a Maven Nexus repository. URL can be provided by changing the mcb.repo.url property during the project creation or by modifying the pom.xml file. To deploy into WATT training Maven repository change mcb.repo.url as following: https: watt db1.workfusion.com nexus content repositories wf machine config bundle BCB repository should come from the archetype. Usage Example Let's say, we have the following repository: https: watt db1.workfusion.com nexus service local repositories wf machine config bundle content link to 'no internet' section local nexus Local Nexus http: {host}:{port} nexus service local repositories {repository} content central If your Nexus repo requires authentication, you can setup it using the following manual: To deploy the BCB to Nexus: Right click the project folder → Run As → Maven Build. In the Goals field, enter the \"clean deploy\" command. Click Run. Watch the build process logs. If the build was successful, you will see the following result: Importing BCB to SPA There is an option in Control Tower to enable Bot Config Bundles update and plugins update from a Nexus repository. This option can be enabled in the WorkFusion Secure Properties. Secure Properties are as follows: nexus.url – URL to a server where Nexus is located; nexus.user – a user name to access Nexus (the user should have the read permission in the respective repository); nexus.password – user password. Example nexus.user=nexus api user nexus.password=nexus api password For more information on Secure properties management refer to Manage Secure Properties (https: kb.workfusion.com display ID %5BCurrent%5D Manage Secure Properties) topic. Note When the access to a Nexus repository is configured as described above, the Import from Repository button in Advanced → **Bot Configurations** is enabled: Additional parameters of the import function are defined in the Platform Configuaration properties (workfusion.properties file). Platform Configuration Properties are as follows: webharvest.plugins.dir – local directory to store the downloaded plugins. webharvest.plugins.repo – a Nexus repository to download the plugins from. webharvest.machine.config.bundle.dir – local directory to store the downloaded Bot config bundles. webharvest.machine.config.bundle.repo – a Nexus repository to download the Bot config bundles from. The cron Bot config bundle syncronization is not available for SPA 9.2.2, as the scheduled BCB update is removed. BCB update is triggered by user action or REST API command. webharvest.machine.config.bundle.import.cron – schedule defined as a cron expression to check for new versions of plugins and Bot config bundles and download them. webharvest.plugins.dir – local directory to store the downloaded plugins. webharvest.plugins.repo – a Nexus repository to download the plugins from. webharvest.machine.config.bundle.dir – local directory to store the downloaded Bot config bundles. webharvest.machine.config.bundle.repo – a Nexus repository to download the Bot config bundles from. The default values of the properties are as follows: webharvest.plugins.repo=webharvest plugins webharvest.machine.config.bundle.dir= {webapp.root} .. crowdcontoldata machine config bundle webharvest.machine.config.bundle.repo=wf machine config bundle webharvest.machine.config.bundle.import.cron=0 0 2 * * * webharvest.plugins.repo=webharvest plugins webharvest.machine.config.bundle.dir= {webapp.root} .. crowdcontoldata machine config bundle webharvest.machine.config.bundle.repo=wf machine config bundle Automatic Import from Nexus Control Tower scans through the configured Nexus repository and imports new versions of Bot Config Bundles on a predefined schedule in Platform Configuration properties. Manual Import To import all Bot config bundles manually: Open Campaigns → Bot Configurations menu in Control Tower. Click the Import from Repository button that will obtain all the new config versions from Nexus. Wait for some time until the repository is indexed and a new config appears in the list: When an BCB has been imported from Nexus, the WorkFusion engine automatically creates a Bot campaign, and your **robotics tasks are available in the process designer**. Afterwards, developers can create new versions of the same project, make changes, and upload them to Nexus where they will be version controlled. Troubleshooting Dependencies Load from the Central Maven Repository instead of Nexus Problem: On premise installation: dependencies load from the central Maven repository instead of Nexus. Solution: Go to Preferences → Maven → Installations and add you local maven installation where you have configured settings.xml with mirror. No JDK Compiler in the Environment Problem: Error during maven goals execution: ERROR Failed to execute goal org.apache.maven.plugins:maven compiler plugin:31:compile (default compile) on project trial1: Compilation failure ERROR No compiler is provided in this environment. Perhaps you are running on a JRE rather than a JDK ERROR &gt; Help 1 Solution: It means that you need to check **Preferences → Java → Installed JREs**. You need to add JDK at this page and switch configuration to it. Artifact does not appear in WorkFusion Problem: Artifact does not appear in WorkFusion. Solution: Possible reasons: Your artifact is not deployed to the right repository in Nexus. WF scans the following folder by default : http: {host}:{port} nexus service local repositories wf machine config bundle content Your artifact has not loaded automatically yet (artifacts are synchronized by a background job to synchronize artifacts but it launched ones a day by default). You can import artifacts manually: Bot Configurations → \"Import from Repository\" button Your artifact was already exist in WF. If you redeploy artifact to nexus and do not change groupId, artifactId and version (gav) then WF will not update this artifact, because we check: does this gav exist in application and download only artifacts with nonexistent gav. Wrong URL Setup Problem: On premise installation yields the following error: ERROR Plugin org.apache.maven.plugins:maven clean plugin:2.5 or one of its dependencies could not be resolved: Failed to read artifact descriptor for org.apache.maven.plugins:maven clean plugin:jar:2.5: Could not transfer artifact org.apache.maven.plugins:maven clean plugin:pom:2.5 from to internal repository (http: {host}:{port} nexus service local repositories): Not authorized , ReasonPhrase: Unauthorized. &gt; Help 1 Solution: Check mirror setup in Maven settings.xml. The error points out to the wrong URL setup. On This Page: Introduction Repository Usage Demo Deployment Options Configure Maven and Kickstart Archetype Maven Repository Configuration Maven Repository User Creating Bot Config Bundle Project Running and Developing Bot Configs Deploying BCB to Nexus Repository Importing BCB to SPA Automatic Import from Nexus Manual Import Troubleshooting Dependencies Load from the Central Maven Repository instead of Nexus No JDK Compiler in the Environment Artifact does not appear in WorkFusion Wrong URL Setup "},{"version":"10.1","date":"Aug-06-2019","title":"business-processes-list","name":"Business Processes List","fullPath":"iac/core/user-guide/work-with-bp/business-processes-list","content":" Overview List of Business Processes BP Instance Steps Automation Available Actions Filters Filter types Predefined filter Advanced filter Save filter Clear or Refine Filter The Business Processes List (Business Processes > View All) is aimed to fulfill the following functions: give a quick overview of statuses, details, progress, possible issues for all Business Process instances; enable access to all the instances of a Business Process with a possibility to manage them, including changing the state (start, pause, resume or stop), creation of another instance (copy) and deletion of an instance. provide possibility to perform Business Process Actions including the creation of New Business Process or importing a Business Process created in another Control Tower. Overview The Business Processes List consists of the following areas: List of Business Processes – in the list all BP instances are grouped under respective BP definition to enable a compact hierarchical view and quick recognition of their statuses. Actions – main operations with a Business Process, such as Create New and Import. Filters – using the filters you can narrow the overview to your needs with the help of predefined and custom criteria. List of Business Processes The list of Business Processes displays all processes available in Control Tower at the moment. A BP definition is represented as a box, where the name and date are displayed along with the states of all instances of Business Process. It provides a compact hierarchical view, especially for processes with a lot of instances. The BP date denotes when the latest instance has been created. The icons display the state of all instances of the Business Process: Draft Processing Paused Completed With Errors With Messages The icon is hidden if there are no instances with the respective status. Click the Open in new tab icon to open the specific Business Process definition in a new tab. This function can be useful when you are working with one BP and want to view only it instances. If a Business Process contains a Manual Task, which can be automated with a Machine Learning Model, its name will be indicated with the \"A\" sign: BP Instances To expand a Business Process and view all its instances click the arrow on the left side of the box. Definition UUID: The Definition ID, where the Business Process is executed. This field can be useful when using WorkFusion REST API. Status Tabs: Click on a tab to view the instances with the respective status. Bulk Actions: Tick the box to select specific instances to perform bulk actions, for example, Start, Stop, Delete or Repair. You can remove the selection from an instance or instances to exclude it them from the bulk action. Sorting: Arrange Competed Started instances in ascending descending order. BP Instance Steps Click the arrow to expand an instance to view its steps: Instance details: The following details are displayed: who and when started the execution of the BP instance status of the execution, Worker Platform (for instances containing Manual steps) and environment. Draft Processing Paused Completed With Errors With Messages WorkSpace Started by Schedule. Click the icon to view the Schedule Click the BP instance name to start editing this instance: changing process steps, scheme, input data, options, etc. Instance Progress Bar: Progress bar displays the percentage of completion of the instance execution. Click the Event log icon 🛈 near the progress bar to view the events logged during the execution of the instance. If some errors occur, this icon will have a red background. Generate Snapshot: You can generate a Snapshot for the BP instance (for non draft BPs). Actions Dropdown: A list of available Actions that can be performed on the instance: Copy Edit and Run Start Stop, Pause Resume View data Repair Download original data Delete Note that available actions depend on the BP instance status (draft, processing, completed, with errors) BP Steps (Tasks) and Sub Processes: Under each BP Instance, you can see a hierarchical list of its Steps (Tasks) and Sub Processes. This list contains only steps which have at least one active record, i.e. process execution reached these steps. Task Progress Bar: Progress bar displays the percentage of the task execution completion and ratio of input output records (documents). Generate Snapshot: You can generate a Task Snapshot – CSV or XLSX file containing step execution results. Actions Dropdown: View output data resulting from the task execution. Click the arrow near the Task's progress bar to view Task details. Task details contain: Launched on – date of the first execution of the current step. Completed – date when the current step was completed. Input records – number of records that reached the current step. Spent – amount spent on a Manual Task. (displayed for SmartCrowd product only) Workers – number of unique Workers who submitted answers to the particular Manual Task. Bot Configuration – link to the Bot configuration. Automation Available If Automation with a ML model can be applied to a Manual Task in the Business Process, the instance is labelled with the Automation Available sign and the respective step is indicated with the \"A\" sign: For more information about Applying Automation refer to VDS Workflow Part 2 > > Step 4 Apply Automation. Actions With the Action button you can create a new Business Process or import a Business Process Package. Filters Using the filters you can select the Business Processes to be displayed in the list. The filters are aimed to facilitate your work with the Business Processes List, as you have a rich set of criteria to define what instances are included to and excluded from the list. With the filters you can focus on the Business Processes, whose instances are in your scope at the moment. Filter state – shows how many records are filtered. With this option you always know the exact number of items matching criteria of the filter applied. Filter types There are two types of the filters: predefined – the filter uses criteria for selection from a built in set; advanced – in advanced filter you can set a number of criteria picked up from a list of options, such as state, time span and others. Predefined filter The predefined filter uses built in set of criteria to select what Business Processes will be shown. In the predefined filter you can select Business Processes containing instances created by you, with a certain state, executed in one or another environment and created within the defined period of time. Advanced filter In advanced filter you can use the following options: Text filter for Tags, Author, Title and Process UUID. In the text filter you can define the match of results such as contains does not contain or equals to not equals to. Filter for date when the instance Created, Started, Ended or Due. In the date filter you can define the match of results such as equals to not equals to, grater than less than, grater than or equals to* less than or equals to* and* period*, Filter for status – you can select the following states of instances in Business Processes: Completed, Active, Running, Answered, Paused, Errors, Draft, Has New Messages. The filter criterion is true. Filter for environment – Production or Sandbox. Click Add Parameter to include and define an option for the filter. You can choose as many filter parameters as needed to create the required filter. With the Match all parameters option you can define, if the filter should match to all the criteria specified or any of the parameters. Save filter You can save the filter for future use. Click the arrows and select Save Filter in the drop down. Key in the name for your filter and choose, if the filter is your private or it's available for the other users. Clear or Refine Filter If a filter is applied the yellow box is displayed in the filter area You can remove or edit the filter by clicking Clear filter or Refine filter there respectively. "},{"version":"10.1","date":"Sep-27-2019","title":"import-asset-bundle","name":"Import Asset Bundle","fullPath":"iac/core/user-guide/migrate-to-another-ct/import-asset-bundle","content":" Asset Bundle (formerly BP bundle package) is a structure comprised of a Business Process and its supporting resources: AutoML models, bot configs, data, templates and more. Asset Bundle can represent either a complete BP or portable resources. It is intended for easy asset transfer between environments. Previously it took a long time to transfer a business process with all dependencies. BP import required multiple operations and manual steps, in depth model knowledge, Nexus, and s3 credentials. Now everything required for a Business Process delivery is conveniently wrapped in a self sufficient bundle and can be ported to a target environment via few simple steps. Import mechanism supports large files and is less error prone than the traditional methods. Successful import is asynchronous—meaning fast—and verified by a checksum. This accelerates BP delivery, decreases total cost of development, and eventually contributes to continuous integration. Asset Bundle structure Bundle is a .zip archive. The following schema represents the folder structure of a complete Asset Bundle. ├── artifactory dependency ├── automl │ ├── model │ └── artifact ├── bot config ├── business process ├── datastore ├── rule ├── s3 ├── template ├── use case │ ├── bot task │ ├── business process │ └── manual task └── meta info.json artifactory dependency folder is required if a BCB was used for a Business Process, optional otherwise. Content of the folder maps to zero, one or multiple BCB. BCB must be organized in the following structure: artifactory dependency > bcb > folder1 > folder2 > folder3 > bcb name > bcb version > bcb name.jar. The folder structure in bold represents BCB's groupId, so it should have at least three folders. Example: a BCB with Group:Artifact:Version being com.workfusion.examples:demo bcb:0.5 should be represented as follows: artifactory dependency > bcb > com > workfusion > examples > demo bcb > 0.5 > demo bcb.jar automl folder to zero, one or multiple AutoML models (trained or not). Every trained model is a sub folder of the model folder, every model artifact (not trained yet) is a sub folder of the artifact folder. Absence means that all manual tasks will have the \"No Automation\" AutoML setting. business process folder contains BP packages stored as .zip archives. datastore folder contains zero, one or multiple .csv files, each representing a Data Store. File name maps to a Data Store name, file format is correspondent to the resulting format of a single data store export. rule folder contains zero, one or multiple .xml files, each representing a Rule. File name maps to a Rule name. s3 folder contains s3 resources needed for BP execution. Files without folders are not allowed in the s3 folder, as the top level folders in the s3 folder represent buckets on a target environment. S3 resources are uploaded to the corresponding locations with the top level folders inside the s3 folder representing buckets. Example: if an s3 folder in the bundle has the following structure: s3 > bucket1 > folder1 > folder2 > file.txt The file.txt will be placed in the folder1 folder2 structure in the target environment bucket1 s3 bucket. If the bucket is absent on a target environment, it will be created. template folder contains zero, one or multiple .xml files, each representing a Template. File name maps to a Template name. use case folder meta info.json file is required and must be a valid JSON file. The NAME and WF_VERSION parameter pairs are required. The recommended structure is as follows: { \"AUTHOR\": \"user user\", \"DESCRIPTION\": \"Some specific description for bundle\", \"INSTANCE\": \"example.workfusion.com\", \"NAME\": \"pack name\", \"PACKAGE_DATE\": \"2019 06 28 06:24:43\", \"TARGET_VERSION\": \"10.2.0.4\", \"WF_BRANCH\": \"bcb intake 9.2.0.3\", \"WF_REVISION\": \"bf19d7ff\", \"WF_VERSION\": \"10.2.0.4\" } If a bundle is correctly structured, it contains all the components needed for Business Process to run on a target (production) environment. However, any component from the structure above except meta info.json is optional. You can create an Asset Bundle containing one or several resources (for example, AutoML model, or template) and import it to target environment to add this component only. Before you start Prepare an Asset Bundle. For the asset bundling reference, see New Project Using ODF. AutoML supports 10.x and 10.x compatible models. 9.2.x and 9.3.x AutoML models transfer requires additional actions, see TBA. AutoML models earlier than 9.2.x are not supported. Be careful when including Global Variables into the Asset bundle datastore folder. GV typically contain env specific data. In doing so, you will re write Global Variables for the entirety of BPs on a target instance. The maximum Asset Bundle size allowed is 3 GB. If your bundle is larger, split it into several bundles and import them one by one. Data Stores, AutoML models, BCB, standalone rules, Templates, Use Cases, and S3 files with the names already present on a target instance will be re written upon import. In case a Business Process with the name already present on a target environment is imported, Control Tower behavior depends on whether the source and target BPs are identical. If the BP is the same, a new instance will be created within the BP definition of the same name. If the source BP changed significantly (step rule added removed, bot config changed, manual task inside the step changed), the new BP definition will be created instead. Schedules will run the new BP definition instead of the old one. Importing Asset Bundle with SSO enabled Whether you are using SSO or not determines which username password the import script will use. When using Asset Package with SSO enabled, you can not use your SSO id password. The recommended approach is as follows: Disable SSO via console command wf.sso.saml.enable=false. Restart Control Tower with wfmanager restart workfusion. Log in to WorkFusion with pre SSO id password. Create a special REST API user and grant them the necessary permissions. In this case, Advanced Package Import and Import Export permissions. Enable SSO with wf.sso.saml.enable=true. Restart Control Tower with wfmanager restart workfusion. Now SSO is enabled, but you can use that REST API user id password for REST API authentication. Deploy an Asset Bundle to a new environment To deploy an Asset Bundle to a new environment, follow these steps: Prepare the Asset Bundle. Asset Bundle can be generated by a Control Tower of versions 9.2.2 and 10.1. Make sure you have the necessary permissions in Control Tower: Advanced Package Import and Import Export. If a Business Process with the same name exists on the target instance, stop this BP to update it. Download the script for your operating system. asset bundle import windows.ps1 asset bundle import linux.sh asset bundle import macos.sh Follow the instructions for your operating system: Run PowerShell as Administrator. Execute the following command, providing a path to script directory: cd To run the script without restrictions, execute the following command: set executionpolicy unrestricted Agree to policy changes. Execute the above mentioned script providing relevant variables. When using SSO, you will need to additionally configure credentials as the standard CT credentials won't work. Control Tower Host URL must not end with ' '. . Asset Bundle Import windows.ps1 ctHost '' ctUser '' ctPassword '' See the example below: . Asset Bundle Import windows.ps1 ctHost 'https: demoserver.workfusion.com workfusion' ctUser 'demouser' ctPassword 'demopassword1!@@ ' Confirm running the script. After the script run successfully, a file selection dialog window opens. Select your Asset Bundle. Run Terminal. Execute the following command: cd Execute the above mentioned script providing relevant variables: When using SSO, you will need to additionally configure credentials as the standard CT credentials won't work. Control Tower Host URL must not end with ' '. . Asset Bundle Import Linux.sh '' '' '' See the example below: . Asset Bundle Import Linux.sh 'C: demo folder demo package.zip' https: demoserver.workfusion.com 'demouser' 'demopassword1!@@ ' The console will show the progress of operations. In the end, you will see the status of the import procedure. If the status is Successful, proceed to Control Tower and check the imported assets. If the status is Failed, open the import summary file, located in the script directory, read the error messages, correct the bundle and redo the import. Run Terminal Execute the following command: cd Execute the above mentioned script providing relevant variables: When using SSO, you will need to additionally configure credentials as the standard CT credentials won't work. Control Tower Host URL must not end with ' '. sh Asset Package Import Mac.sh See the example below: sh Asset Package Import Mac.sh Users Demouser Demo package.zip https: demoserver.workfusion.com demouser demopassword1!@@ The console will show the progress of operations. In the end, you will see the status of the import procedure. If the status is Successful, proceed to Control Tower and check the imported assets. If the status is Failed, open the import summary file, located in the script directory, read the error messages, correct the bundle and redo the import. After import checklist Check the BP and its structure, workflow, components, Manual Tasks' automation settings. If you imported an Asset Bundle that contains a BP with automated manual tasks, you need to select Accuracy threshold values for all models and Activate Cognitive Bots on the AutoML tab of the BP designer page. If your BP uses Global Variables and these Global Variables are not present in the Asset Bundle, configure them manually on the target environment. REST API reference If you wish to perform Asset Bundle import using REST API endpoints, refer to the Asset Bundle Migration API page. Known issues If you import a modified AutoML model artifact, it should have a version different from the one present on a target environment. Otherwise BP execution will fail until BEP is restarted. Repeatable import of the Rules of the Composite type results in duplicates created across the instance. "},{"version":"10.1","date":"Aug-06-2019","title":"moderation-flow","name":"Moderation Flow","fullPath":"iac/core/user-guide/sample-task-bp/moderation-flow","content":" Diagrams for Moderation Flow Creation Step and Adjudication Rules in Tasks Composite Rules in Moderation Flow Known Issues and Limitations Prior to reading this material, see the Work with Business Process BP topic. Moderation workflow is a BP that usually consists of two main steps: Creation step step where* creator* worker creates some info (writes review, translates text, transcribes audio fragment, etc.) Moderation step step where moderator worker checks the creator's work, and can accept or reject it (with different options). See How to create a Moderation Task for details. BP Examples: Translation Proofreading, Find Headquarters Address Moderate Address, etc. You can also use the Moderation Flow together with Qualification Tasks to educate your Workers and improve their proficiency by giving them feedback and correcting their answers. Diagrams for Moderation Flow All Tasks are moderated, without Reworking option. Moderation Flow All Tasks, without Reworking All Tasks are moderated, with Reworking option. Moderation Flow All Tasks, with Reworking Creation Step and Adjudication Rules in Tasks In the Creation step you should pick Adjudication Rule: Moderation Adjudication (Open task→ Task Properties → Advanced Options → Adjudication Tab) NOTE! No need to create new Rule, just use existing rule named Moderation Adjudication (see below) Moderation Adjudication Expand source Composite Rules in Moderation Flow The 1st Composite Rule (To moderation) defines whether the Moderation step can be optional (based on Worker's Qualification, Statistics, Confidence, etc.) or not. Enable the MODERATION _RULE in the Advanced Options. NOTE! If the Rework is needed (See cases: 2, 4), also enable the SPLIT _DATA option. The 2nd Composite Rule (Decision) defines possible Task flows depending on the Moderator decision. You can add the following Outcomes: Accept – an Outcome defining a case when Moderator accepts the Creator Task. Add a Condition with the Approve initial worker's task Advanced Option. Reject – an Outcome defining a case when Moderator rejects the Creator Task. Add a Condition with the Reject initial worker's task Advanced Option. Rework – an Outcome defining a case when Moderator wants the Creator to redo the Task. Enable the SPLIT_DATA in the Advanced Options and tick one of the following options: Rework task by the same worker. The Worker will not get Reward for reworking. Assign task to the same worker. The Worker will get Reward for each reworking iteration. Prohibit current worker to work on this task. The Creator will get Reward, but the Task will be assigned to another Worker. Known Issues and Limitations Currently \"Prohibit current worker to work on this task\" outcome option is not supported N A option is not restored correctly for Free Text, Number and other text fields "},{"version":"10.1","date":"Aug-06-2019","title":"create-moderation-task","name":"Create a Moderation Task","fullPath":"iac/core/user-guide/work-with-tasks/create-moderation-task","content":" A Moderation Task is intended for reviewing results of completed Tasks or Processes by accepting rejecting and optionally commenting each Record. Moderation Task Example All these actions can be done by downloading a Snapshot and manually editing each Record, but creating a Moderation Task has the following benefits: You can choose which Records need a review: how many Records and with what Confidence. You can assign a Moderation Task to a special group of reviewers – Moderation Workforce. All actions are done in WorkFusion platform – there is no need to download, edit, and upload Snapshots. Moderation Tasks can be a part of Moderation Flow. Start Creating a Moderation Task You can create a Moderation Task: from View All Tasks Actions Create a Moderation Task or from View All Business Processes or from BP > Design BP > right click any BP step. You need to switch to the View Mode. BP Diagram Create a Moderation Task Applying Moderation Settings After you have selected to create a Moderation Task, a Moderate Result popup appears: Moderation for Manual Task Moderation for Bot Task You need to set the following parameters: Sample Size (All, Number of Records, Percentage of Records) – how many Records are taken as an input from the original Task results. Confidence (All, High, Low) – only records with the Confidence specified will be included into the Moderation Task input Editable – if selected, all original Task output data will be available for editing to Moderators. Example A Worker provided a Company Website answer: \"https: workfusion.com\" If the Editable option was selected, a Moderator can edit the Worker's answer to \"https: workfusion.com\" and choose the Edit & Accept Moderation Decision. When all Moderation Settings are done, click the Apply button. Editing a Moderation Task After applying Moderation Settings, a new Task is created with the \"Moderate:\" prefix in the title. This task design and settings are copied from the original Task, with the following modifications: Input Data is taken from original Task output Data filtered according to Sample Size and Confidence set on the previous step. Task Design: 2 new answers are added after Worker answers: Task Moderation Decision and Task Moderation Comments. Confidence is displayed for each Worker answer. Moderation Task Preview Workforce. If workforce with \"Moderate Workforce\" name exists on your WorkFusion instance, this Workforce will be selected for Moderation Task. Otherwise, Workforce will be copied from the original Task. Adjudication: Rule = \"Moderation 1 0\"; Min and Max № Assignments = 1 Running the Moderation Task When all Task options are set and the Task Preview looks fine, run the Moderation Task as a regular Task and check the results when users from Moderation Workforce start submitting. Using Moderation for Information Extraction Tasks After creating a Moderation task out of Information Extraction (IE) one, you need to perform the following steps to display Worker answers correctly: Find out the column name with text tagged by Worker – it has the following name pattern: In the example below, the IE unique code is \"invoice\", therefore the column name is \"invoice _tagged\": In the Moderation task, open the IE answer: In Advanced Options > Default Value, add the column name obtained on the STEP 1 using the following pattern: For current example, you need to add {question.data.invoice _tagged} "},{"version":"10.1","date":"Aug-06-2019","title":"view-all-tasks","name":"View All Tasks","fullPath":"iac/core/user-guide/work-with-tasks/view-all-tasks","content":" The View All Manual Tasks page provides the following functionality: contains all the Tasks created and their links gives a quick overview of Task statuses, details, progress, possible issues provides possibility to perform Task Actions including the Creation of New Task. Before taking some actions, set the appropriate sorting (Sort by dropdown) and filtering (Filter dropdown or Show button) at the top of the grid. The displaying of Task parameters and functions varies depending on the Task Status. Task Name. Click this link to: continue creating a Task (for Draft Tasks) view Task results (for other Tasks) If several Tasks have the same name, you can distinguish them by defining unique Tags. You can edit Tags by hovering over the Tags label and clicking the pencil icon on the right side. Progress Bar. Progress Bar displays the number of completed Worker Tasks against the total number of Worker Tasks. The bar is clickable for all non Draft Tasks, the link redirects you to Task results. Indication Icons Business Process Task (if any). This icon informs you that current task is a step of a Business Process. Workforce: WorkSpace Task Status: Draft Processing Paused Resuming Completed with Expired Tasks indication (if any) Processing Issues (if any) Use Case New Messages with the indication of their quantity Actions Dropdown. A list of available Task Actions. Expanded Task Details. Additional Task information. Processing Task Name. Tasks with a Processing Status have names highlighted with blue. Business Process Name. If a Task is a part of a Business Process, the name of respective BP is displayed under the Task name. Generate Snapshot. You can generate a Task Snapshot (for non Draft Tasks). For Draft Tasks, this button is replaced by the Edit & Run button."}]